{"results": [{"createdAt": null, "postedAt": "2012-12-10T07:46:24.438Z", "modifiedAt": null, "url": null, "title": "[LINK] Two Modes of Discourse: Taking everything personally v. debate as sport", "slug": "link-two-modes-of-discourse-taking-everything-personally-v", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2W7DybNE34FnFeNxv/link-two-modes-of-discourse-taking-everything-personally-v", "pageUrlRelative": "/posts/2W7DybNE34FnFeNxv/link-two-modes-of-discourse-taking-everything-personally-v", "linkUrl": "https://www.lesswrong.com/posts/2W7DybNE34FnFeNxv/link-two-modes-of-discourse-taking-everything-personally-v", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Two%20Modes%20of%20Discourse%3A%20Taking%20everything%20personally%20v.%20debate%20as%20sport&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Two%20Modes%20of%20Discourse%3A%20Taking%20everything%20personally%20v.%20debate%20as%20sport%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2W7DybNE34FnFeNxv%2Flink-two-modes-of-discourse-taking-everything-personally-v%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Two%20Modes%20of%20Discourse%3A%20Taking%20everything%20personally%20v.%20debate%20as%20sport%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2W7DybNE34FnFeNxv%2Flink-two-modes-of-discourse-taking-everything-personally-v", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2W7DybNE34FnFeNxv%2Flink-two-modes-of-discourse-taking-everything-personally-v", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>A blog post by Alistair Roberts, <a href=\"http://isteve.blogspot.com/2012/12/intellectual-discourse-taking.html\">as curated by Steve Sailer</a>. (Steve's version is shorter and more targeted; the original blog post is the fourth in a series on triggering and suffers for its reliance on the particular issue.)<br /><br />It seems like a very useful dichotomy, and strongly reminds me of <a href=\"/lw/375/ask_and_guess/\">Ask and Guess</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2W7DybNE34FnFeNxv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 6, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "20571", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vs3kzjLhbdKsndnBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T12:43:24.333Z", "modifiedAt": null, "url": null, "title": "The challenges of bringing up AIs", "slug": "the-challenges-of-bringing-up-ais", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cECEkMm9EcPZPCtu2/the-challenges-of-bringing-up-ais", "pageUrlRelative": "/posts/cECEkMm9EcPZPCtu2/the-challenges-of-bringing-up-ais", "linkUrl": "https://www.lesswrong.com/posts/cECEkMm9EcPZPCtu2/the-challenges-of-bringing-up-ais", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20challenges%20of%20bringing%20up%20AIs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20challenges%20of%20bringing%20up%20AIs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcECEkMm9EcPZPCtu2%2Fthe-challenges-of-bringing-up-ais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20challenges%20of%20bringing%20up%20AIs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcECEkMm9EcPZPCtu2%2Fthe-challenges-of-bringing-up-ais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcECEkMm9EcPZPCtu2%2Fthe-challenges-of-bringing-up-ais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>At the current <a href=\"http://www.winterintelligence.org/oxford2012/agi-12/\">AGI-12</a> conference, some designers have been proponents of keeping AGI's safe by bringing them up in human environments, providing them with interactions and feedback in a similar way to how we <a href=\"http://www.agiri.org/AGIethical.pdf\">bring up human children</a>. Obviously that approach would fail for a fully smart AGI with its own values - it would pretend to follow our values for as long as it needed, and then defect. However, some people have confidence if we started with a limited, dumb AGI, then we could successfully inculcate our values in this way (a more sophisticated position would be that though this method would likely fail, it's more likely to succeed than a top-down&nbsp;friendliness&nbsp;project!).</p>\n<p>The major criticism of this approach is that it&nbsp;anthropomorphises the AGI - we have a theory of children's minds, constructed by evolution, culture, and our own child-rearing experience. And then we project this on the alien mind of the AGI, assuming that if the AGI presents behaviours similar to a well-behaved child, then it will become a moral AGI. The problem is that we don't know how alien the AGI's mind will be, and if our reinforcement is actually reinforcing the right thing. Specifically, we need to be able to find some way of distinguishing between:</p>\n<p><ol>\n<li>An AGI being trained to be friendly.</li>\n<li>An AGI being trained to lie and conceal.</li>\n<li>An AGI that will behave completely differently once out of the training/testing/trust-building environment.</li>\n<li>An AGI that forms the wrong categories and generalisations (what counts as \"human\" or \"suffering\", for instance), because it lacks human-shared implicit knowledge that was \"too obvious\" for us to even think of training it on.</li>\n</ol></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cECEkMm9EcPZPCtu2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 1.0546827089725198e-06, "legacy": true, "legacyId": "20576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T23:27:08.618Z", "modifiedAt": null, "url": null, "title": "Call for a Friendly AI channel on freenode", "slug": "call-for-a-friendly-ai-channel-on-freenode", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:27.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bsP43XBrSDMCqXPuG/call-for-a-friendly-ai-channel-on-freenode", "pageUrlRelative": "/posts/bsP43XBrSDMCqXPuG/call-for-a-friendly-ai-channel-on-freenode", "linkUrl": "https://www.lesswrong.com/posts/bsP43XBrSDMCqXPuG/call-for-a-friendly-ai-channel-on-freenode", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20a%20Friendly%20AI%20channel%20on%20freenode&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20a%20Friendly%20AI%20channel%20on%20freenode%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsP43XBrSDMCqXPuG%2Fcall-for-a-friendly-ai-channel-on-freenode%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20a%20Friendly%20AI%20channel%20on%20freenode%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsP43XBrSDMCqXPuG%2Fcall-for-a-friendly-ai-channel-on-freenode", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsP43XBrSDMCqXPuG%2Fcall-for-a-friendly-ai-channel-on-freenode", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 245, "htmlBody": "<p>I visited #lesswrong on freenode yesterday and was able to get in some discussion of FAI-related matters. But that channel also exists to allow discussion of rationalist fanfiction, political opinions, and whatever else people want to talk about. <br /><br />I would like for there to be a place on that network where the topic actually is Friendly AI - where you can go to brainstorm, and maybe you'll have to wait because they're already talking about cognitive neuroscience or automated theorem provers, but not because they're talking about ponies or politics. <br /><br />Surely there are enough people with a serious, technical interest in FAI and related topics (and I don't just mean among LW regulars) to make such a channel sustainable. I'll bet that there are other people holding back from participation precisely because existing forums are so full of uninformed noise and conversational tangents. It's inevitable that entropy would set in after a while, but if the default baseline was still that even the chatter was technically informed and focused on what's coming - that would be mission accomplished. <br /><br />I explored the freenode namespace a little. #FAI redirects to #unavailable, so it may be an abandoned project. #AGI exists but is invite-only. #AI exists but I'm told it's dull, and besides, the agenda here is meant to be, not just AI, but singularity-relevant AI. So there seems to be an opening. Or am I reinventing the wheel?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bsP43XBrSDMCqXPuG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 1.0550543809710866e-06, "legacy": true, "legacyId": "20578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T02:21:42.006Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Evolved Desires", "slug": "seq-rerun-evolved-desires", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sCWdpMRfWzL4ykRSQ/seq-rerun-evolved-desires", "pageUrlRelative": "/posts/sCWdpMRfWzL4ykRSQ/seq-rerun-evolved-desires", "linkUrl": "https://www.lesswrong.com/posts/sCWdpMRfWzL4ykRSQ/seq-rerun-evolved-desires", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Evolved%20Desires&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Evolved%20Desires%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCWdpMRfWzL4ykRSQ%2Fseq-rerun-evolved-desires%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Evolved%20Desires%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCWdpMRfWzL4ykRSQ%2Fseq-rerun-evolved-desires", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCWdpMRfWzL4ykRSQ%2Fseq-rerun-evolved-desires", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/evolved-desires.html\">Evolved Desires</a> was originally published on December 5, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>How much can evolutionary biology calculations tell us about how humans will consider competitive scenarios, and therefore help us predict whether or not a singleton will occur.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/fv1/seq_rerun_beware_hockey_stick_plans/\">Beware Hockey Stick Plans</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sCWdpMRfWzL4ykRSQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0551552037133117e-06, "legacy": true, "legacyId": "20581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZZizufEwJJRZj7o6Z", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T09:14:00.685Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Applied Rationality", "slug": "meetup-moscow-applied-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZsFTvva6RPoJATBmb/meetup-moscow-applied-rationality", "pageUrlRelative": "/posts/ZsFTvva6RPoJATBmb/meetup-moscow-applied-rationality", "linkUrl": "https://www.lesswrong.com/posts/ZsFTvva6RPoJATBmb/meetup-moscow-applied-rationality", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Applied%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Applied%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsFTvva6RPoJATBmb%2Fmeetup-moscow-applied-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Applied%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsFTvva6RPoJATBmb%2Fmeetup-moscow-applied-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsFTvva6RPoJATBmb%2Fmeetup-moscow-applied-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gv'>Moscow: Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 December 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Solving cases. You can propose any problem and together we will propose a solution using our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason. Please use the same from to propose the date for the next meetup, if you can't come this time.</p>\n\n<p>N. B. Google may show incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gv'>Moscow: Applied Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZsFTvva6RPoJATBmb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0553934197848512e-06, "legacy": true, "legacyId": "20592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality\">Discussion article for the meetup : <a href=\"/meetups/gv\">Moscow: Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 December 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Solving cases. You can propose any problem and together we will propose a solution using our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason. Please use the same from to propose the date for the next meetup, if you can't come this time.</p>\n\n<p>N. B. Google may show incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/gv\">Moscow: Applied Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T11:50:26.990Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 12/10/12", "slug": "group-rationality-diary-12-10-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:02.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p4khJwdApKwkmDe28/group-rationality-diary-12-10-12", "pageUrlRelative": "/posts/p4khJwdApKwkmDe28/group-rationality-diary-12-10-12", "linkUrl": "https://www.lesswrong.com/posts/p4khJwdApKwkmDe28/group-rationality-diary-12-10-12", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2012%2F10%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2012%2F10%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4khJwdApKwkmDe28%2Fgroup-rationality-diary-12-10-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2012%2F10%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4khJwdApKwkmDe28%2Fgroup-rationality-diary-12-10-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4khJwdApKwkmDe28%2Fgroup-rationality-diary-12-10-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of December 10th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/for/group_rationality_diary_112812/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p4khJwdApKwkmDe28", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0554838282343652e-06, "legacy": true, "legacyId": "20596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vy5MztkxZgwToCbqE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T14:40:17.527Z", "modifiedAt": null, "url": null, "title": "Rational subjects and rational practitioners", "slug": "rational-subjects-and-rational-practitioners", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LKZDoADoEzhSRevJT/rational-subjects-and-rational-practitioners", "pageUrlRelative": "/posts/LKZDoADoEzhSRevJT/rational-subjects-and-rational-practitioners", "linkUrl": "https://www.lesswrong.com/posts/LKZDoADoEzhSRevJT/rational-subjects-and-rational-practitioners", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20subjects%20and%20rational%20practitioners&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20subjects%20and%20rational%20practitioners%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKZDoADoEzhSRevJT%2Frational-subjects-and-rational-practitioners%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20subjects%20and%20rational%20practitioners%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKZDoADoEzhSRevJT%2Frational-subjects-and-rational-practitioners", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKZDoADoEzhSRevJT%2Frational-subjects-and-rational-practitioners", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 272, "htmlBody": "<p>Half-closing my eyes and looking at the <a href=\"/lw/fv3/by_which_it_may_be_judged/\">recent topic of morality</a> from a distance, I am struck by the following trend.</p>\n<p>In mathematics, there are no substantial controversies. (I am speaking of the present era in mathematics, since around the early 20th century. There were some before then, before it had been clearly worked out what was a proof and what was not.) There are few in physics, chemistry, molecular biology, astronomy. There are some but they are not the bulk of any of these subjects. Look at biology more generally, history, psychology, sociology, and controversy is a larger and larger part of the practice, in proportion to the distance of the subject from the possibility of reasonably conclusive experiments. Finally, politics and morality consist of nothing but controversy and always have done.</p>\n<p>Curiously, participants in discussions of all of these subjects seem equally confident, regardless of the field's distance from experimental acquisition of reliable knowledge. What correlates with <a href=\"http://xkcd.com/435/\">distance from objective knowledge</a> is not uncertainty, but controversy. Across these fields (not necessarily within them), opinions are firmly held, independently of how well they can be supported. They are firmly defended and attacked in inverse proportion to that support. The less information there is about actual facts, the more scope there is for <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">continuing the fight</a> instead of <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">changing one's mind</a>. (So much for the Aumann agreement of Bayesian rationalists.)</p>\n<p>Perhaps mathematicians and hard scientists are not more rational than others, but work in fields where it is easier to be rational. When they <a href=\"http://xkcd.com/793/\">turn into crackpots</a> outside their discipline, they were actually that irrational already, but have wandered into an area without safety rails.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"daguMTessgwBYvN4b": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LKZDoADoEzhSRevJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 50, "extendedScore": null, "score": 1.055581998537118e-06, "legacy": true, "legacyId": "20598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zqwWicCLNBSA5Ssmn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T16:59:19.755Z", "modifiedAt": null, "url": null, "title": "[LINK] Should we live to 1,000?", "slug": "link-should-we-live-to-1-000", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:28.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nqens4mjrGb4RTDHe/link-should-we-live-to-1-000", "pageUrlRelative": "/posts/Nqens4mjrGb4RTDHe/link-should-we-live-to-1-000", "linkUrl": "https://www.lesswrong.com/posts/Nqens4mjrGb4RTDHe/link-should-we-live-to-1-000", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Should%20we%20live%20to%201%2C000%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Should%20we%20live%20to%201%2C000%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqens4mjrGb4RTDHe%2Flink-should-we-live-to-1-000%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Should%20we%20live%20to%201%2C000%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqens4mjrGb4RTDHe%2Flink-should-we-live-to-1-000", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqens4mjrGb4RTDHe%2Flink-should-we-live-to-1-000", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>Peter Singer, makes a (refreshingly simple) ethical case&nbsp;for anti-aging research,&nbsp;and&nbsp;endorses&nbsp;increased funding.</p>\n<p><a href=\"http://www.project-syndicate.org/commentary/the-ethics-of-anti-aging-by-peter-singer\">http://www.project-syndicate.org/commentary/the-ethics-of-anti-aging-by-peter-singer</a></p>\n<blockquote>\n<p>On which problems should we focus research in medicine and the biological sciences? There is a strong argument for tackling the diseases that kill the most people &ndash;diseases like malaria, measles, and diarrhea, which kill millions in developing countries, but very few in the developed world.</p>\n<p>\n<p>Developed countries, however, devote most of their research funds to the diseases from which their citizens suffer, and that seems likely to continue for the foreseeable future. Given that constraint, which medical breakthrough would do the most to improve our lives?</p>\n<p>If your first thought is &ldquo;a cure for cancer&rdquo; or &ldquo;a cure for heart disease,&rdquo; think again. Aubrey de Grey, Chief Science Officer of SENS Foundation and the world&rsquo;s most prominent advocate of anti-aging research, argues that it makes no sense to spend the vast majority of our medical resources on trying to combat the diseases of aging without tackling aging itself. If we cure one of these diseases, those who would have died from it can expect to succumb to another in a few years. The benefit is therefore modest.</p>\n<p>[...]</p>\n</p>\n<p>De Grey has set up SENS Foundation to promote research into anti-aging. By most standards, his fundraising efforts have been successful, for the foundation now has an annual budget of around $4 million. But that is still pitifully small by the standards of medical research foundations. De Grey might be mistaken, but if there is only a small chance that he is right, the huge pay-offs make anti-aging research a better bet than areas of medical research that are currently far better funded.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nqens4mjrGb4RTDHe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 1.0556623752263273e-06, "legacy": true, "legacyId": "20599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-11T23:35:48.192Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ysTG6YoBwv2uFXsft/meetup-melbourne-social-meetup-2", "pageUrlRelative": "/posts/ysTG6YoBwv2uFXsft/meetup-melbourne-social-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/ysTG6YoBwv2uFXsft/meetup-melbourne-social-meetup-2", "postedAtFormatted": "Tuesday, December 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FysTG6YoBwv2uFXsft%2Fmeetup-melbourne-social-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FysTG6YoBwv2uFXsft%2Fmeetup-melbourne-social-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FysTG6YoBwv2uFXsft%2Fmeetup-melbourne-social-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/gw\">Melbourne social meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2012 07:00:00PM (+1100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">See mailing list, Carlton VIC 3053</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Melbourne's next social meetup is on Friday 21 December, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can message me or call me on 0412 996 288.</p>\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n<p>We always look forward to meeting new people!</p>\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/gw\">Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ysTG6YoBwv2uFXsft", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0558916344210923e-06, "legacy": true, "legacyId": "20601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/gw\">Melbourne social meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2012 07:00:00PM (+1100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">See mailing list, Carlton VIC 3053</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Melbourne's next social meetup is on Friday 21 December, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can message me or call me on 0412 996 288.</p>\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n<p>We always look forward to meeting new people!</p>\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/gw\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T00:30:57.342Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Sustained Strong Recursion", "slug": "seq-rerun-sustained-strong-recursion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cs6WdZCre3nmmyuYJ/seq-rerun-sustained-strong-recursion", "pageUrlRelative": "/posts/cs6WdZCre3nmmyuYJ/seq-rerun-sustained-strong-recursion", "linkUrl": "https://www.lesswrong.com/posts/cs6WdZCre3nmmyuYJ/seq-rerun-sustained-strong-recursion", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Sustained%20Strong%20Recursion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Sustained%20Strong%20Recursion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs6WdZCre3nmmyuYJ%2Fseq-rerun-sustained-strong-recursion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Sustained%20Strong%20Recursion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs6WdZCre3nmmyuYJ%2Fseq-rerun-sustained-strong-recursion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs6WdZCre3nmmyuYJ%2Fseq-rerun-sustained-strong-recursion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/wi/sustained_strong_recursion/\">Sustained Strong Recursion</a> was originally published on 05 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Sustained_Strong_Recursion\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Sustained strong recursion has a much larger effect on growth than other possible mechanisms for growth.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fvp/seq_rerun_evolved_desires/\">Evolved Desires</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cs6WdZCre3nmmyuYJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0559235329413957e-06, "legacy": true, "legacyId": "20603", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9wZnasT3uXzmFCcaB", "sCWdpMRfWzL4ykRSQ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T02:31:44.996Z", "modifiedAt": null, "url": null, "title": "[LINK] 23andme is 99$ now", "slug": "link-23andme-is-99usd-now", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jabberslythe", "createdAt": "2011-05-16T22:21:09.850Z", "isAdmin": false, "displayName": "Jabberslythe"}, "userId": "cKRinn5dDewj4wFc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AGid42yKAmzLozKjM/link-23andme-is-99usd-now", "pageUrlRelative": "/posts/AGid42yKAmzLozKjM/link-23andme-is-99usd-now", "linkUrl": "https://www.lesswrong.com/posts/AGid42yKAmzLozKjM/link-23andme-is-99usd-now", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%2023andme%20is%2099%24%20now&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%2023andme%20is%2099%24%20now%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAGid42yKAmzLozKjM%2Flink-23andme-is-99usd-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%2023andme%20is%2099%24%20now%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAGid42yKAmzLozKjM%2Flink-23andme-is-99usd-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAGid42yKAmzLozKjM%2Flink-23andme-is-99usd-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>It's been reduced to 99$ and it seems like it is a&nbsp;permanent&nbsp;reduction. I was thinking of buying it at 299$&nbsp;because&nbsp;it had not been on sale for a while, so I'm very pleased this happened.</p>\n<p>Their press release on it:</p>\n<p>http://blog.23andme.com/news/one-million-strong-a-note-from-23andmes-anne-wojcicki/</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AGid42yKAmzLozKjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 8, "extendedScore": null, "score": 1.055993402702979e-06, "legacy": true, "legacyId": "20600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T03:19:24.588Z", "modifiedAt": null, "url": null, "title": "Buffalo Meetup: Survey of Interest", "slug": "buffalo-meetup-survey-of-interest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:28.097Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8BBRYjdy6sSMfbKm7/buffalo-meetup-survey-of-interest", "pageUrlRelative": "/posts/8BBRYjdy6sSMfbKm7/buffalo-meetup-survey-of-interest", "linkUrl": "https://www.lesswrong.com/posts/8BBRYjdy6sSMfbKm7/buffalo-meetup-survey-of-interest", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Buffalo%20Meetup%3A%20Survey%20of%20Interest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuffalo%20Meetup%3A%20Survey%20of%20Interest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BBRYjdy6sSMfbKm7%2Fbuffalo-meetup-survey-of-interest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Buffalo%20Meetup%3A%20Survey%20of%20Interest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BBRYjdy6sSMfbKm7%2Fbuffalo-meetup-survey-of-interest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BBRYjdy6sSMfbKm7%2Fbuffalo-meetup-survey-of-interest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>I'd like to start a LW meetup group in Buffalo, NY and would like to get an idea of how many people may be interested in attending. I'm hoping to get meetups started sometime in January.&nbsp;If you're interested, email me at BuffaloLW@gmail.com (and comment below). Anyone who sends me an email will&nbsp;receive&nbsp;a link to the event on Doodle.com to try and work out a time and day of the week that works for most people.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Also,&nbsp;where would you like the <strong><em>first </em></strong>meeting to be held?&nbsp;</p>\n<p>&nbsp; &nbsp;1. Private Residence (my house, or you can offer yours if you like)</p>\n<p>&nbsp; &nbsp;2. Public Space (like Spot Coffee?)</p>\n<p>&nbsp; &nbsp;3. Don't Care</p>\n<p>&nbsp;</p>\n<p>Edit: I realized based on Alicorn's interest that there may be decent amount of people traveling to the area for the holidays who are interested in meeting during the holiday break. If you are one of these people, comment below&nbsp;because&nbsp;I would love to host you.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8BBRYjdy6sSMfbKm7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.056020972420708e-06, "legacy": true, "legacyId": "20611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T07:53:48.868Z", "modifiedAt": null, "url": null, "title": "Replaceability as a virtue", "slug": "replaceability-as-a-virtue", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.968Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8b5WhpiRBr8QSFAif/replaceability-as-a-virtue", "pageUrlRelative": "/posts/8b5WhpiRBr8QSFAif/replaceability-as-a-virtue", "linkUrl": "https://www.lesswrong.com/posts/8b5WhpiRBr8QSFAif/replaceability-as-a-virtue", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Replaceability%20as%20a%20virtue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReplaceability%20as%20a%20virtue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b5WhpiRBr8QSFAif%2Freplaceability-as-a-virtue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Replaceability%20as%20a%20virtue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b5WhpiRBr8QSFAif%2Freplaceability-as-a-virtue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b5WhpiRBr8QSFAif%2Freplaceability-as-a-virtue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1560, "htmlBody": "<p>I propose it is altruistic to be replaceable and therefore, those who strive to be altruistic should strive to be replaceable.</p>\n<p>As far as I can Google, this does not seem to have been proposed before. LW should be a good place to discuss it. A community interested in rational and ethical behavior, and in how superintelligent machines may decide to replace mankind, should at least bother to refute the following argument.</p>\n<h2>Replaceability<br /></h2>\n<p>Replaceability is \"the state of being <a title=\"replaceable\" href=\"http://en.wiktionary.org/wiki/replaceable\">replaceable</a>\". It isn't binary. The price of the replacement matters: so a cookie is more replaceable than a big wedding cake. Adequacy of the replacement also makes a difference: a piston for an ancient Rolls Royce is less replaceable than one in a modern car, because it has to be hand-crafted and will be distinguishable. So something is more or less replaceable depending on the price and quality of its replacement.</p>\n<p>Replaceability could be thought of as the inverse of the cost of having to replace something. Something that's very replaceable has a low cost of replacement, while something that lacks replaceability has a high (up to unfeasible) cost of replacement. The cost of replacement plays into Total Cost of Ownership, and everything economists know about that applies. It seems pretty obvious that replaceability of possessions is good, much like cheap availability is good.</p>\n<p>Some things (historical artifacts, art pieces) are valued highly precisely because of their irreplacability. Although a few things could be said about the resale value of such objects, I'll simplify and contend these valuations are not rational.</p>\n<h2>The practical example<br /></h2>\n<p>Anne manages the central database of Beth's company. She's the only one who has access to that database, the skillset required for managing it, and an understanding of how it all works; she has a monopoly to that combination.</p>\n<p>This monopoly gives Anne control over her own replacement cost. If she works according to the state of the art, writes extensive and up-to-date documentation, makes proper backups etc she can be very replaceable, because her monopoly will be easily broken. If she refuses to explain what she's doing, creates weird and fragile workarounds and documents the database badly she can reduce her replaceability and defend her monopoly. (A well-obfuscated database can take months for a replacement database manager to handle confidently.)</p>\n<p>So Beth may still choose to replace Anne, but Anne can influence how expensive that'll be for Beth. She can at least make sure her replacement needs to be shown the ropes, so she can't be fired on a whim. But she might go further and practically hold the database hostage, which would certainly help her in salary negotiations if she does it right.</p>\n<p>This makes it pretty clear how Anne can act altruistically in this situation, and how she can act selfishly. Doesn't it?</p>\n<h2>The moral argument<br /></h2>\n<p>To Anne, her replacement cost is an externality and an influence on the length and terms of her employment. To maximize the length of her employment and her salary, her replacement cost would have to be high.</p>\n<p>To Beth, Anne's replacement cost is part of the cost of employing her and of course she wants it to be low. This is true for any pair of employer and employee: Anne is unusual only in that she has a great degree of influence on her replacement cost.</p>\n<p>Therefore, if Anne documents her database properly etc, this increases her replaceability and constitutes altruistic behavior. Unless she values the positive feeling of doing her employer a favor more highly than she values the money she might make by avoiding replacement, this might even be <a href=\"http://wiki.lesswrong.com/wiki/Altruism\">true altruism</a>.</p>\n<p>Unless I suck at Google, replaceability doesn't seem to have been discussed as an aspect of altruism. The two reasons for that I can see are:</p>\n<ul>\n<li>replacing people is painful to think about</li>\n<li>and it seems futile as long as people aren't replaceable in more than very specific functions anyway.</li>\n</ul>\n<p>But we don't want or get the <a href=\"http://www.sciencedaily.com/releases/2011/12/111201105443.htm\">choice to kill one person to save the life of five</a>, either, and such practical improbabilities shouldn't stop us from considering our moral decisions. This is especially true in a world where copies, and hence replacements, of people are starting to look possible at least in principle.</p>\n<h2>Singularity-related hypotheticals<br /></h2>\n<ol>\n<li>In some reasonably-near future, software is getting better at modeling people. We still don't know what makes a process intelligent, but we can feed a couple of videos and a bunch of psychological data points into a people modeler, extrapolate everything else using a standard population and the resulting model can have a conversation that could fool a four-year-old. The technology is already good enough for models of pets. While convincing models of complex personalities are at least another decade away, the tech is starting to become good enough for senile grandmothers.<br /><br />Obviously no-one wants granny to die. But the kids would like to keep a model of granny, and they'd like to make the model before the Alzheimer's gets any worse, while granny is terrified she'll get no more visits to her retirement home.<br /><br />What's the ethical thing to do here? Surely the relatives should keep visiting granny. Could granny maybe have a model made, but keep it to herself, for release only through her Last Will and Testament? And wouldn't it be truly awful of her to refuse to do that?</li>\n<li>Only slightly further into the future, we're still mortal, but cryonics does appear to be working. Unfrozen people need regular medical aid, but the technology is only getting better and anyway, the point is: something we can believe to be them can indeed come back.<br /><br />Some refuse to wait out these Dark Ages; they get themselves frozen for nonmedical reasons, to fastforward across decades or centuries into a time when the <em>really</em> awesome stuff will be happening, and to get the immortality technologies they hope will be developed by then.<br /><br />In this scenario, wouldn't fastforwarders be considered selfish, because they impose on their friends the pain of their absence? And wouldn't their friends mind it less if the fastforwarders went to the trouble of having a good model (see above) made first?</li>\n<li>On some distant future Earth, minds can be uploaded completely. Brains can be modeled and recreated so effectively that people can make living, breathing copies of themselves and experience the inability to tell which instance is the copy and which is the original.<br /><br />Of course many adherents of soul theories reject this as blasphemous. A couple more sophisticated thinkers worry if this doesn't devalue individuals to the point where superhuman AIs might conclude that as long as copies of everyone are stored on some hard drive orbiting Pluto, nothing of value is lost if every meatbody gets devoured into more hardware. Bottom line is: Effective immortality is available, but some refuse it out of principle.<br /><br />In this world, wouldn't those who make themselves fully and infinitely replaceable want the same for everyone they love? Wouldn't they consider it a dreadful imposition if a friend or relative refused immortality? After all, wasn't not having to say goodbye anymore kind of the point?</li>\n</ol>\n<p>These questions haven't come up in the real world because people have never been replaceable in more than very specific functions. But I hope you'll agree that if and when people become more replaceable, that will be regarded as a good thing, and it will be regarded as virtuous to use these technologies as they become available, because it spares one's friends and family some or all of the cost of replacing oneself.</p>\n<h2>Replaceability as an altruist virtue<br /></h2>\n<p>And if replaceability is altruistic in this hypothetical future, as well as in the limited sense of Anne and Beth, that implies replaceability is altruistic now. And even now, there are things we can do to increase our replaceability, i.e. to reduce the cost our bereaved will incur when they have to replace us. We can teach all our (valuable) skills, so others can replace us as providers of these skills. We can not have (relevant) secrets, so others can learn what we know and replace us as sources of that knowledge. We can endeavour to live as long as possible, to postpone the cost. We can sign up for cryonics. There are surely other things each of us could do to increase our replaceability, but I can't think of any an altruist wouldn't consider virtuous.</p>\n<p>As an altruist, I conclude that replaceability is a prosocial, unselfish trait, something we'd want our friends to have, in other words: a virtue. I'd go as far as to say that even bothering to set up a good Last Will and Testament is virtuous <em>precisely because</em> it reduces the cost my bereaved will incur when they have to replace me. And although none of us can be truly easily replaceable as of yet, I suggest we honor those who make themselves replaceable, and are proud of whatever replaceability we ourselves attain.</p>\n<p>So, how replaceable are you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8b5WhpiRBr8QSFAif", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 6, "extendedScore": null, "score": 1.0561797315518106e-06, "legacy": true, "legacyId": "20289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I propose it is altruistic to be replaceable and therefore, those who strive to be altruistic should strive to be replaceable.</p>\n<p>As far as I can Google, this does not seem to have been proposed before. LW should be a good place to discuss it. A community interested in rational and ethical behavior, and in how superintelligent machines may decide to replace mankind, should at least bother to refute the following argument.</p>\n<h2 id=\"Replaceability\">Replaceability<br></h2>\n<p>Replaceability is \"the state of being <a title=\"replaceable\" href=\"http://en.wiktionary.org/wiki/replaceable\">replaceable</a>\". It isn't binary. The price of the replacement matters: so a cookie is more replaceable than a big wedding cake. Adequacy of the replacement also makes a difference: a piston for an ancient Rolls Royce is less replaceable than one in a modern car, because it has to be hand-crafted and will be distinguishable. So something is more or less replaceable depending on the price and quality of its replacement.</p>\n<p>Replaceability could be thought of as the inverse of the cost of having to replace something. Something that's very replaceable has a low cost of replacement, while something that lacks replaceability has a high (up to unfeasible) cost of replacement. The cost of replacement plays into Total Cost of Ownership, and everything economists know about that applies. It seems pretty obvious that replaceability of possessions is good, much like cheap availability is good.</p>\n<p>Some things (historical artifacts, art pieces) are valued highly precisely because of their irreplacability. Although a few things could be said about the resale value of such objects, I'll simplify and contend these valuations are not rational.</p>\n<h2 id=\"The_practical_example\">The practical example<br></h2>\n<p>Anne manages the central database of Beth's company. She's the only one who has access to that database, the skillset required for managing it, and an understanding of how it all works; she has a monopoly to that combination.</p>\n<p>This monopoly gives Anne control over her own replacement cost. If she works according to the state of the art, writes extensive and up-to-date documentation, makes proper backups etc she can be very replaceable, because her monopoly will be easily broken. If she refuses to explain what she's doing, creates weird and fragile workarounds and documents the database badly she can reduce her replaceability and defend her monopoly. (A well-obfuscated database can take months for a replacement database manager to handle confidently.)</p>\n<p>So Beth may still choose to replace Anne, but Anne can influence how expensive that'll be for Beth. She can at least make sure her replacement needs to be shown the ropes, so she can't be fired on a whim. But she might go further and practically hold the database hostage, which would certainly help her in salary negotiations if she does it right.</p>\n<p>This makes it pretty clear how Anne can act altruistically in this situation, and how she can act selfishly. Doesn't it?</p>\n<h2 id=\"The_moral_argument\">The moral argument<br></h2>\n<p>To Anne, her replacement cost is an externality and an influence on the length and terms of her employment. To maximize the length of her employment and her salary, her replacement cost would have to be high.</p>\n<p>To Beth, Anne's replacement cost is part of the cost of employing her and of course she wants it to be low. This is true for any pair of employer and employee: Anne is unusual only in that she has a great degree of influence on her replacement cost.</p>\n<p>Therefore, if Anne documents her database properly etc, this increases her replaceability and constitutes altruistic behavior. Unless she values the positive feeling of doing her employer a favor more highly than she values the money she might make by avoiding replacement, this might even be <a href=\"http://wiki.lesswrong.com/wiki/Altruism\">true altruism</a>.</p>\n<p>Unless I suck at Google, replaceability doesn't seem to have been discussed as an aspect of altruism. The two reasons for that I can see are:</p>\n<ul>\n<li>replacing people is painful to think about</li>\n<li>and it seems futile as long as people aren't replaceable in more than very specific functions anyway.</li>\n</ul>\n<p>But we don't want or get the <a href=\"http://www.sciencedaily.com/releases/2011/12/111201105443.htm\">choice to kill one person to save the life of five</a>, either, and such practical improbabilities shouldn't stop us from considering our moral decisions. This is especially true in a world where copies, and hence replacements, of people are starting to look possible at least in principle.</p>\n<h2 id=\"Singularity_related_hypotheticals\">Singularity-related hypotheticals<br></h2>\n<ol>\n<li>In some reasonably-near future, software is getting better at modeling people. We still don't know what makes a process intelligent, but we can feed a couple of videos and a bunch of psychological data points into a people modeler, extrapolate everything else using a standard population and the resulting model can have a conversation that could fool a four-year-old. The technology is already good enough for models of pets. While convincing models of complex personalities are at least another decade away, the tech is starting to become good enough for senile grandmothers.<br><br>Obviously no-one wants granny to die. But the kids would like to keep a model of granny, and they'd like to make the model before the Alzheimer's gets any worse, while granny is terrified she'll get no more visits to her retirement home.<br><br>What's the ethical thing to do here? Surely the relatives should keep visiting granny. Could granny maybe have a model made, but keep it to herself, for release only through her Last Will and Testament? And wouldn't it be truly awful of her to refuse to do that?</li>\n<li>Only slightly further into the future, we're still mortal, but cryonics does appear to be working. Unfrozen people need regular medical aid, but the technology is only getting better and anyway, the point is: something we can believe to be them can indeed come back.<br><br>Some refuse to wait out these Dark Ages; they get themselves frozen for nonmedical reasons, to fastforward across decades or centuries into a time when the <em>really</em> awesome stuff will be happening, and to get the immortality technologies they hope will be developed by then.<br><br>In this scenario, wouldn't fastforwarders be considered selfish, because they impose on their friends the pain of their absence? And wouldn't their friends mind it less if the fastforwarders went to the trouble of having a good model (see above) made first?</li>\n<li>On some distant future Earth, minds can be uploaded completely. Brains can be modeled and recreated so effectively that people can make living, breathing copies of themselves and experience the inability to tell which instance is the copy and which is the original.<br><br>Of course many adherents of soul theories reject this as blasphemous. A couple more sophisticated thinkers worry if this doesn't devalue individuals to the point where superhuman AIs might conclude that as long as copies of everyone are stored on some hard drive orbiting Pluto, nothing of value is lost if every meatbody gets devoured into more hardware. Bottom line is: Effective immortality is available, but some refuse it out of principle.<br><br>In this world, wouldn't those who make themselves fully and infinitely replaceable want the same for everyone they love? Wouldn't they consider it a dreadful imposition if a friend or relative refused immortality? After all, wasn't not having to say goodbye anymore kind of the point?</li>\n</ol>\n<p>These questions haven't come up in the real world because people have never been replaceable in more than very specific functions. But I hope you'll agree that if and when people become more replaceable, that will be regarded as a good thing, and it will be regarded as virtuous to use these technologies as they become available, because it spares one's friends and family some or all of the cost of replacing oneself.</p>\n<h2 id=\"Replaceability_as_an_altruist_virtue\">Replaceability as an altruist virtue<br></h2>\n<p>And if replaceability is altruistic in this hypothetical future, as well as in the limited sense of Anne and Beth, that implies replaceability is altruistic now. And even now, there are things we can do to increase our replaceability, i.e. to reduce the cost our bereaved will incur when they have to replace us. We can teach all our (valuable) skills, so others can replace us as providers of these skills. We can not have (relevant) secrets, so others can learn what we know and replace us as sources of that knowledge. We can endeavour to live as long as possible, to postpone the cost. We can sign up for cryonics. There are surely other things each of us could do to increase our replaceability, but I can't think of any an altruist wouldn't consider virtuous.</p>\n<p>As an altruist, I conclude that replaceability is a prosocial, unselfish trait, something we'd want our friends to have, in other words: a virtue. I'd go as far as to say that even bothering to set up a good Last Will and Testament is virtuous <em>precisely because</em> it reduces the cost my bereaved will incur when they have to replace me. And although none of us can be truly easily replaceable as of yet, I suggest we honor those who make themselves replaceable, and are proud of whatever replaceability we ourselves attain.</p>\n<p>So, how replaceable are you?</p>", "sections": [{"title": "Replaceability", "anchor": "Replaceability", "level": 1}, {"title": "The practical example", "anchor": "The_practical_example", "level": 1}, {"title": "The moral argument", "anchor": "The_moral_argument", "level": 1}, {"title": "Singularity-related hypotheticals", "anchor": "Singularity_related_hypotheticals", "level": 1}, {"title": "Replaceability as an altruist virtue", "anchor": "Replaceability_as_an_altruist_virtue", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T13:54:55.114Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Last call for 2012", "slug": "meetup-vancouver-last-call-for-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BttZf5MR4MyvggZyJ/meetup-vancouver-last-call-for-2012", "pageUrlRelative": "/posts/BttZf5MR4MyvggZyJ/meetup-vancouver-last-call-for-2012", "linkUrl": "https://www.lesswrong.com/posts/BttZf5MR4MyvggZyJ/meetup-vancouver-last-call-for-2012", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Last%20call%20for%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Last%20call%20for%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBttZf5MR4MyvggZyJ%2Fmeetup-vancouver-last-call-for-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Last%20call%20for%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBttZf5MR4MyvggZyJ%2Fmeetup-vancouver-last-call-for-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBttZf5MR4MyvggZyJ%2Fmeetup-vancouver-last-call-for-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gx'>Vancouver Last call for 2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2150 macdonald st </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>With crazy tree-killing, litany-chanting, fat-red-man worshiping solstice/Christmas coming up, this may be the last meetup in Vancouver for 2012.</p>\n\n<p>As usual, we'll meet up and mingle with other of our kind and talk about interesting things and have all sorts of fun.</p>\n\n<p>Location is 2150 Macdonald street. Big brown house on corner.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gx'>Vancouver Last call for 2012</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BttZf5MR4MyvggZyJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0563887156941371e-06, "legacy": true, "legacyId": "20620", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Last_call_for_2012\">Discussion article for the meetup : <a href=\"/meetups/gx\">Vancouver Last call for 2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2150 macdonald st </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>With crazy tree-killing, litany-chanting, fat-red-man worshiping solstice/Christmas coming up, this may be the last meetup in Vancouver for 2012.</p>\n\n<p>As usual, we'll meet up and mingle with other of our kind and talk about interesting things and have all sorts of fun.</p>\n\n<p>Location is 2150 Macdonald street. Big brown house on corner.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Last_call_for_20121\">Discussion article for the meetup : <a href=\"/meetups/gx\">Vancouver Last call for 2012</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Last call for 2012", "anchor": "Discussion_article_for_the_meetup___Vancouver_Last_call_for_2012", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Last call for 2012", "anchor": "Discussion_article_for_the_meetup___Vancouver_Last_call_for_20121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-12T19:07:53.470Z", "modifiedAt": null, "url": null, "title": "Singularity the hard way", "slug": "singularity-the-hard-way", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:28.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CCC", "createdAt": "2012-08-29T10:48:05.832Z", "isAdmin": false, "displayName": "CCC"}, "userId": "M78KBtEDyGEXYBJHu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9RJw44XrQGRBMDZK/singularity-the-hard-way", "pageUrlRelative": "/posts/t9RJw44XrQGRBMDZK/singularity-the-hard-way", "linkUrl": "https://www.lesswrong.com/posts/t9RJw44XrQGRBMDZK/singularity-the-hard-way", "postedAtFormatted": "Wednesday, December 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20the%20hard%20way&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20the%20hard%20way%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9RJw44XrQGRBMDZK%2Fsingularity-the-hard-way%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20the%20hard%20way%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9RJw44XrQGRBMDZK%2Fsingularity-the-hard-way", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9RJw44XrQGRBMDZK%2Fsingularity-the-hard-way", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 630, "htmlBody": "<p>So far, we only have one known example of the development of intelligent life; and that example is us. Humanity. That means that we have only one machanism that is <em>known</em> to be able to produce intelligent life; and that is evolution. But by far the majority of life that is produced by evolution is <em>not</em></p>\n<p>intelligent. (In fact, by far the majority of life produced by evolution appears to be bacteria, as far as I can tell. There's also a lot of beetles).</p>\n<p>Why did evolution produce such a steep climb in human intelligence, while not so much in the case of other creatures? That, I suspect, is at least partially because as humans we are not competing against other creatures anymore. We are competing against each other.</p>\n<p>Also, once we managed to start writing things down and sharing knowledge, we shifted off the slow, evolutionary timescale and onto the faster, technological timescale. As technology improves, we find ourselves being more right, less wrong; our ability to affect the environment continually increases. Our intellectual development, as a species, speeds up dramatically.</p>\n<p>And I believe that there is a hack that can be applied to this process; a mechanism by which the total intelligence of humanity as a whole can be rather dramatically increased. (It will take time). The process is simple enough in concept.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>These thoughts were triggered by an article on <a href=\"http://www.technologyreview.com/news/506466/given-tablets-but-no-teachers-ethiopian-children-teach-themselves\">some Ethiopian children</a> who were given tablets by OLPC. They were chosen specifically on the basis of illiteracy (through the whole village) and were given no teaching (aside from the teaching apps on the tablets; some instruction on how to use the solar chargers was also given to the adults) and in fairly short order, they taught themselves basic literacy. (And had modified the operating system to customise it, and re-enable the camera).</p>\n<p>My first thought was that this gives an upper limit to the minimum cost of world literacy; the minimum cost of world literacy is limited to the cost of one tablet per child (plus a bit for transportation).</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>In short, we need world literacy. World literacy will allow anyone and everyone to read up on that which interests them. It will allow a vastly larger number of people to start thinking about certain hard problems (such as any hard problem you care to name). It will allow more eyes to look at science; more experiments to be done and published; more armour-piercing questions which no-one has yet thought to ask because there simply are not enough scientists to ask them.</p>\n<p>World literacy would improve the technological progress of humanity; and probably, after enough generations, result in a humanity who we would, by todays standards, consider superhumanly intelligent. (This may or may not necessitate direct brain-computer interfaces)</p>\n<p>The aim, therefore, is to allow <em>humanity</em>, and not some human-made AI, to go *foom*. It will take some significant amount of time - following this plan means that our generation will do no more than continue a process that began some millions of years ago - but it does have this advantage; if it is humanity that goes *foom*, then the resulting superintelligences are practically guaranteed to be human-Friendly since they will be human. (For the moment, I discard the possibility of a suicidal superintelligence).</p>\n<p>It also has this advantage; the process is likely to be slow enough that a significant fraction of humanity will be enhanced at the same time, or close enough to the same time that none will be able to stop any of the others' enhancements. This drastically reduces the probability of being trapped by a single Unfriendly enhanced human.</p>\n<p>The main disadvantage is the time taken; this will take centuries at the least, perhaps millenia. It is likely that, along the way, a more traditional AI will be created.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9RJw44XrQGRBMDZK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -15, "extendedScore": null, "score": 1.056569903637177e-06, "legacy": true, "legacyId": "20621", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T01:11:13.339Z", "modifiedAt": null, "url": null, "title": "Why you must maximize expected utility", "slug": "why-you-must-maximize-expected-utility", "viewCount": null, "lastCommentedAt": "2021-10-21T22:48:40.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F46jPraqp258q67nE/why-you-must-maximize-expected-utility", "pageUrlRelative": "/posts/F46jPraqp258q67nE/why-you-must-maximize-expected-utility", "linkUrl": "https://www.lesswrong.com/posts/F46jPraqp258q67nE/why-you-must-maximize-expected-utility", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20you%20must%20maximize%20expected%20utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20you%20must%20maximize%20expected%20utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF46jPraqp258q67nE%2Fwhy-you-must-maximize-expected-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20you%20must%20maximize%20expected%20utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF46jPraqp258q67nE%2Fwhy-you-must-maximize-expected-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF46jPraqp258q67nE%2Fwhy-you-must-maximize-expected-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6279, "htmlBody": "<p><em>This post explains von Neumann-Morgenstern (VNM) axioms for decision theory</em>,<em> and what follows from them: that if you have a consistent direction in which you are trying to steer the future, you must be an expected utility maximizer. I'm writing this post in preparation for a sequence on <a href=\"/lw/4g6/updateless_anthropics/\">updateless</a> <a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">anthropics</a>, but I'm hoping that it will also be independently useful.</em></p>\n<p>The theorems of decision theory say that if you follow certain axioms, then your behavior is described by a utility function. (If you don't know what that means, I'll explain below.) So you should have a utility function! Except, why should you want to follow these axioms in the first place?</p>\n<p>A couple of years ago, Eliezer explained how violating one of them can <a href=\"/lw/my/the_allais_paradox/\">turn you into a money pump</a> &mdash; how, at time 11:59, you will <em>want</em> to pay a penny to get option B instead of option A, and then at 12:01, you will <em>want</em> to pay a penny to switch back. Either that, or the game will have ended and the option won't have made a difference.</p>\n<p>When I read that post, I was suitably impressed, but not completely convinced: I would certainly not want to behave one way if behaving differently <em>always</em> gave better results. But couldn't you avoid the problem by violating the axiom only in situations where it doesn't give anyone an opportunity to money-pump you? I'm not saying that would be <em>elegant</em>, but is there a reason it would be <em>irrational</em>?</p>\n<p>It took me a while, but I have since come around to the view that you really must have a utility function, and really must behave in a way that maximizes the expectation of this function, <a href=\"/lw/oj/probability_is_in_the_mind/\">on pain of stupidity</a> (or at least that there are strong arguments in this direction). But I don't know any source that comes close to explaining the reason, the way I see it; hence, this post.</p>\n<p>I'll use the <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">von Neumann-Morgenstern axioms</a>, which assume probability theory as a foundation (unlike the <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/\">Savage axioms</a>, which actually <em>imply</em> that anyone following them has not only a utility function but also a probability distribution). I will assume that you already accept Bayesianism.</p>\n<p align=\"center\">*</p>\n<p><em>Epistemic</em> rationality is about figuring out what's true; <em>instrumental</em> rationality is about <a href=\"/lw/crd/only_say_rational_when_you_cant_eliminate_the_word/\">steering the future where you want it to go</a>. The way I see it, the axioms of decision theory tell you how to have a consistent <em>direction</em> in which you are trying to steer the future. If my choice at 12:01 depends on whether at 11:59 I had a chance to decide differently, then perhaps I won't ever be money-pumped; but if I want to save as many human lives as possible, and I must decide between different plans that have different probabilities of saving different numbers of people, then it starts to at least seem <em>doubtful</em> that which plan is better at 12:01 could <em>genuinely</em> depend on my opportunity to choose at 11:59.</p>\n<p>So how do we formalize the notion of a coherent direction in which you can steer the future?</p>\n<p><a id=\"more\"></a></p>\n<p align=\"center\">*</p>\n<h2>Setting the stage<br /></h2>\n<p>Decision theory asks what you would do if faced with choices between different sets of options, and then places restrictions on how you can act in one situation, depending on how you would act in others. This is another thing that has always bothered me: If we are talking about choices between different lotteries with small prizes, it makes some sense that we could invite you to the lab and run ten sessions with different choices, and you should probably act consistently across them. But if we're interested in the big questions, like how to save the world, then you're not going to face a series of independent, analogous scenarios. So what is the <em>content</em> of asking what you would do if you faced a set of choices different from the one you actually face?</p>\n<p>The real point is that you have bounded computational resources, and you can't <em>actually</em> visualize the exact set of choices you might face in the future. A perfect Bayesian rationalist could just figure out what they <em>would</em> do in any conceivable situation and write it down in a giant lookup table, which means that they only face a single one-time choice between different possible tables. But <em>you</em> can't do that, and so you need to figure out general principles to follow. <a href=\"/lw/mt/beautiful_probability/\">A perfect Bayesian is like a Carnot engine</a> &mdash; it's what a theoretically perfect engine <em>would</em> look like, so even though you can at best approximate it, it still has something to teach you about how to build a real engine.</p>\n<p>But decision theory is <em>about</em> what a perfect Bayesian would do, and it's annoying to have our practical concerns intrude into our ideal picture like that. So let's give our story some local color and say that <em>you</em> aren't a perfect Bayesian, but you have a genie &mdash; that is, a powerful optimization process &mdash; that is, an AI, which <em>is</em>. (That, too, is physically impossible: AIs, like humans, can only approximate perfect Bayesianism. But we <em>are</em> still idealizing.) Your <em>genie</em> is able to comprehend the set of possible giant lookup tables it must choose between; <em>you</em> must write down a formula, to be evaluated by the genie, that chooses the best table from this set, given the available information. (An unmodified human won't <em>actually</em> be able to write down an exact formula describing their preferences, but we might be able to write down one for a paperclip maximizer.)</p>\n<p>The first constraint decision theory places on your formula is that it must order all options your genie <em>might</em> have to choose between from best to worst (though you might be indifferent between some of them), and then given any particular set of feasible options, it must choose the one that is least bad. In particular, if you prefer option A when options A and B are available, then you can't prefer option B when options A, B and C are available.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Meditation\"><strong>Meditation:</strong></a> <em>Alice is trying to decide how large a bonus each member of her team should get this year. She has just decided on giving Bob the same, already large, bonus as last year when she receives an e-mail from the head of a different division, asking her if she can recommend anyone for a new project he is setting up. Alice immediately realizes that Bob would love to be on that project, and would fit the bill exactly. But she </em>needs<em> Bob on the contract he's currently working on; losing him would be a pretty bad blow for her team.</em></p>\n<p><em>Alice decides there is no way that she can recommend Bob for the new project. But she still feels bad about it, and she decides to make up for it by giving Bob a larger bonus. On reflection, she finds that she genuinely feels that this is the </em>right<em> thing to do, simply because she </em>could<em> have recommended him but didn't. Does that mean that Alice's preferences are irrational? Or that something is wrong with decision theory?<br /></em></p>\n<p><strong>Meditation:</strong><em> One kind of answer to the above and to many other criticisms of decision theory goes like this: Alice's decision isn't between giving Bob a larger bonus or not, it's between (give Bob a larger bonus unconditionally), (give Bob the same bonus unconditionally), (only give Bob a larger bonus if I could have recommended him), and so on. But if </em>that<em> sort of thing is allowed, is there </em>any<em> way left in which decision theory constrains Alice's behavior? If not, what good is it to Alice in figuring out what she should do?</em></p>\n<p>...<br />...<br />...</p>\n<p align=\"center\">*</p>\n<h2>Outcomes<br /></h2>\n<p>My short answer is that Alice can care about anything she damn well likes. But there are a lot of things that she <em>doesn't</em> care about, and decision theory has something to say about <em>those</em>.</p>\n<p>In fact, deciding that some kinds of preferences should be outlawed as irrational can be dangerous: you might think that nobody in their right mind should ever care about the detailed planning algorithms their AI uses, as long as they work. But how certain are you that it's wrong to care about whether the AI has planned out your whole life in advance, in detail? (Worse: Depending on how strictly you interpret it, this injunction might even rule out <a href=\"/lw/x4/nonperson_predicates\">not wanting the AI to run conscious simulations of people</a>.)</p>\n<p>But nevertheless, I believe the \"anything she damn well likes\" needs to be qualified. Imagine that Alice and Carol both have an AI, and fortuitously, both AIs have been programmed with the same preferences and the same Bayesian prior (and they talk, so they also have the same posterior, because Bayesians cannot agree to disagree). But Alice's AI has taken over the stock markets, while Carol's AI has seized the world's nuclear arsenals (and is protecting them well). So Alice's AI not only doesn't want to blow up Earth, it couldn't do so <em>even if it wanted to</em>; it couldn't even bribe Carol's AI, because Carol's AI really doesn't want the Earth blown up either. And so, if it makes a difference to the AIs' preference function whether they <em>could</em> blow up Earth if they wanted to, they have a conflict of interest.</p>\n<p>The moral of this story is not simply that it would be <em>sad</em> if two AIs came into conflict even though they have the same preferences. The point is that we're asking what it means to have a consistent direction in which you are trying to steer the future, and it doesn't look like our AIs are on the same bearing. Surely, a direction for steering the world should only depend on features of the <em>world</em>, not on additional information about which agent is at the rudder.</p>\n<p>You <em>can</em> want to not have your life planned out by an AI. But I think you should have to state your wish as a property of the world: you want <em>all</em> AIs to refrain from doing so, not just \"whatever AI happens to be executing this\". And Alice can want Bob to get a larger bonus if the company could have assigned him to the new project and decided not to, but she must figure out whether <em>this</em> is the correct way to translate her moral intuitions into preferences over properties of the world.</p>\n<p align=\"center\">*</p>\n<p>You may care about any feature of the world, but you don't in fact care about most of them. For example, there are many ways the atoms in the sun could be arranged that all add up to the same thing as far as you are concerned, and you don't have <a href=\"/lw/l4/terminal_values_and_instrumental_values/\"><em>terminal</em> preferences</a> about which of these will be the actual one tomorrow. And though you might care about <em>some</em> properties of the algorithms your AI is running, mostly they <em>really</em> do not matter.</p>\n<p>Let's define a function that takes a complete description of the world &mdash; past, present and future &mdash; and returns a data structure containing all information about the world that matters to your terminal values, and <em>only</em> that information. (Our imaginary perfect Bayesian doesn't know exactly which way the world will turn out, but it can work with \"possible worlds\", complete descriptions of ways the world <em>may</em> turn out.) We'll call this data structure an \"outcome\", and we require you to be indifferent between any two courses of action that will always produce the same outcome. Of course, any course of action is something that your AI would be executing in the actual world, and you are certainly allowed to care about the difference &mdash; but then the two courses of action do not lead to the same \"outcome\"!<sup>1</sup></p>\n<p>With this definition, I think it is pretty reasonable to say that in order to have a consistent direction in which you want to steer the world, you must be able to order these outcomes from best to worst, and always want to pick the least bad you can get.</p>\n<p align=\"center\">*</p>\n<h2>Preference relations<br /></h2>\n<p>That won't be <em>sufficient</em>, though. Our genie doesn't <em>know</em> what outcome each action will produce, it only has probabilistic information about that, and that's a complication we very much do <em>not</em> want to idealize away (because we're trying to figure out the right way to <em>deal</em> with it). And so our decision theory amends the earlier requirement: You must not only be indifferent between actions that always produce the same outcome, but also between all actions that only yield <em>the same probability distribution</em> over outcomes.</p>\n<p>This is not at all a mild assumption, though it's usually built so deeply into the definitions that it's not even called an \"axiom\". But we've assumed that all features of the world you care about are already encoded in the outcomes, so it does seem to me that the only reason left why you might prefer one action over another is that it gives you a better trade-off in terms of what outcomes it makes more or less likely; and I've assumed that you're already a Bayesian, so you agree that <em>how</em> likely it makes an outcome is correctly represented by the probability of that outcome, given the action. So it certainly <em>seems</em> that the probability distribution over outcomes should give you all the information about an action that you could <em>possibly</em> care about. And that you should be able to order these probability distributions from best to worst, and all that.</p>\n<p>Formally, we represent a direction for steering the world as a set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" /> of possible outcomes and a binary relation <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" /> on the probability distributions over <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" /> (with <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\" /> is interpreted as \"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is at least as good as <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />\") which is a <a href=\"http://en.wikipedia.org/wiki/Total_preorder\">total preorder</a>; that is, for all <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" />:</p>\n<ul>\n<li>If <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq z\" alt=\"\" />, then <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq z\" alt=\"\" />&nbsp; (that is, <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" /> is <em>transitive</em>); and</li>\n<li>We have either <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\" /> or <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\" /> or both&nbsp; (that is, <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" /> is <em>total</em>).</li>\n</ul>\n<p>In this post, I'll assume that <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" /> is finite. We write <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" /> (for \"I'm indifferent between <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />\") when both <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\" />, and we write <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\" /> (\"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is strictly better than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />\") when <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\" /> but <em>not</em> <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\" />. Our genie will compute the set of all actions it could possibly take, and the probability distribution over possible outcomes that (according to the genie's Bayesian posterior) each of these actions leads to, and then it will choose to act in a way that maximizes <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" />. I'll also assume that the set of possible actions will always be finite, so there is always at least one optimal action.</p>\n<p><strong>Meditation:</strong> <em><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> is in the neighbourhood and invites you to participate in one of its little games. Next Saturday, it plans to flip a fair coin; would you please indicate on the attached form whether you would like to bet that this coin will fall heads, or tails? If you correctly bet heads, you will win $10,000; if you correctly bet tails, you'll win $100. If you bet wrongly, you will still receive $1 for your participation.</em></p>\n<p><em> </em></p>\n<p><em>We'll assume that you prefer a 50% chance of $10,000 and a 50% chance of $1 to a 50% chance of $100 and a 50% chance of $1. Thus, our theory would say that you should bet heads. But there is a twist: Given recent galactopolitical events, you estimate a 3% chance that after posting its letter, Omega has been called away on urgent business. In this case, the game will be cancelled and you won't get any money, though as a consolation, Omega will probably send you some book from its rare SF collection when it returns (market value: approximately $55&ndash;$70). Our theory so far tells you nothing about how you should bet in this case, but does Rationality have anything to say about it?<br /></em></p>\n<p>...<br />...<br />...</p>\n<p align=\"center\">*</p>\n<h2>The Axiom of Independence<br /></h2>\n<p>So here's how I think about that problem: If you already <em>knew</em> that Omega is still in the neighbourhood (but not which way the coin is going to fall), you would prefer to bet heads, and if you <em>knew</em> it has been called away, you wouldn't care. (And what you bet has no influence on whether Omega has been called away.) So heads is either better or exactly the same; clearly, you should bet heads.</p>\n<p>This type of reasoning is the content of the von Neumann-Morgenstern <em>Axiom of Independence</em>. Apparently, that's <a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/\">the most controversial of the theory's axioms</a>.</p>\n<p>You're already a Bayesian, so you already accept that if you perform an experiment to determine whether someone is a witch, and the experiment can come out two ways, then if one of these outcomes is evidence that the person is a witch, <a href=\"/lw/ii/conservation_of_expected_evidence/\">the other outcome must be evidence that they are <em>not</em></a>. New information is allowed to make a hypothesis more likely, but not <em>predictably</em> so; if <em>all</em> ways the experiment could come out make the hypothesis more likely, then you should <em>already</em> be finding it more likely than you do. The same thing is true even if only one result would make the hypothesis more likely, but the other would leave your probability estimate exactly unchanged.</p>\n<p>The Axiom of Independence is equivalent to saying that if you're evaluating a possible course of action, and one experimental result would make it seem more attractive than it currently seems to you, while the other experimental result would at least make it seem no <em>less</em> attractive, then you should <em>already</em> be finding it more attractive than you do. This does <em>seem</em> rather solid to me.</p>\n<p align=\"center\">*</p>\n<p>So what does this axiom say formally? <em>(Feel free to skip this section if you don't care.)</em></p>\n<p>Suppose that your genie is considering two possible actions <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?a_2\" alt=\"\" /> (bet heads or tails), and an event <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\" /> (Omega is called away). Each action gives rise to a probability distribution over possible outcomes: E.g., <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid a_1]\" alt=\"\" /> is the probability of outcome <img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\" /> if your genie chooses <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\" />. But your genie can also compute a probability distribution <em>conditional on <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\" /></em>, <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid E,a_1]\" alt=\"\" />. Suppose that conditional on <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\" />, it doesn't matter which action you pick: <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid E,a_1] \\,=\\, \\mathbb{P}[i\\mid E,a_2]\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\" />. And finally, suppose that the probability of <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\" /> doesn't depend on which action you pick: <img src=\"http://www.codecogs.com/png.latex?%5Cmathbb{P}[E%5Cmid%20a_1]%5C,=%5C,%5Cmathbb{P}[E%5Cmid%20a_2]%5C,=:%5C,%20p\" alt=\"\" />, with <img src=\"http://www.codecogs.com/png.latex?0%20%3C%20p%20%3C%201\" alt=\"\" />. The Axiom of Independence says that in this situation, you should prefer the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_1]\" alt=\"\" /> to the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_2]\" alt=\"\" />, and therefore prefer <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\" /> to <img src=\"http://www.codecogs.com/png.latex?a_2\" alt=\"\" />, if and only if you prefer the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_1]\" alt=\"\" /> to the distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_2]\" alt=\"\" />.</p>\n<p>Let's write <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_1]\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_2]\" alt=\"\" />, and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" /> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid E,a_1] \\,=\\, \\mathbb{P}[\\;\\cdot\\mid E,a_2]\" alt=\"\" />. (Formally, we think of these as vectors in <img src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^{|\\mathcal{O}|}\" alt=\"\" />: e.g., <img src=\"http://www.codecogs.com/png.latex?z_i \\,=\\, \\mathbb{P}[i\\mid E,a_1]\" alt=\"\" />.) For all&nbsp;<img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\" />, we have</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid a_1] \\;=\\; \\mathbb{P}[\\neg E\\mid a_1]\\cdot\\mathbb{P}[i\\mid\\neg E,a_1] \\;+\\; \\mathbb{P}[E\\mid a_1]\\cdot\\mathbb{P}[i\\mid E,a_1],\" alt=\"\" /></p>\n<p>so <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_1] = (1-p)x + pz\" alt=\"\" />, and similarly <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_2] \\,=\\, (1-p)y + pz\" alt=\"\" />. Thus, we can state the Axiom of Independence as follows:</p>\n<ul>\n<li><img src=\"http://www.codecogs.com/png.latex?(1-p)x + pz \\,\\succcurlyeq\\, (1-p)y + pz \\;\\iff\\; x\\succcurlyeq y\" alt=\"\" />.</li>\n</ul>\n<p>We'll assume that you can't ever rule out the possibility that your AI might face this type of situation for any given <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" />, and <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" />, so we require that this condition hold for all probability distributions <em><img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /></em>,<em> <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> </em>and <em><img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" /></em>, and for all <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" /> with <img src=\"http://www.codecogs.com/png.latex?0&lt;p&lt;1\" alt=\"\" />.</p>\n<p align=\"center\">*</p>\n<p>Here's a <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">common criticism</a> of Independence. Suppose a parent has two children, and one old car that they can give to one of these children. Can't they be indifferent between giving the car to their older child or their younger child, but strictly prefer throwing a coin? But let <img src=\"http://www.codecogs.com/png.latex?x = z\" alt=\"\" /> mean that the younger child gets the gift, and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> that the older child gets it, and <img src=\"http://www.codecogs.com/png.latex?p = 1/2\" alt=\"\" />; then by Independence, if <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" />, then <img src=\"http://www.codecogs.com/png.latex?\\textstyle x \\;=\\; \\frac12x + \\frac12 z \\;\\sim\\; \\frac12y + \\frac12 z\" alt=\"\" />, so it would seem that the parent can <em>not</em> strictly prefer the coin throw.</p>\n<p>In fairness, the people who find this criticism persuasive may not be Bayesians. But if <em>you</em> think this is a good criticism: Do you think that the parent must be indifferent between throwing a coin and asking the children's crazy old kindergarten teacher which of them was better-behaved, as long as they assign 50% probability to either answer? Because if not, shouldn't you already have protested when we decided that decisions must only depend on the probabilities of different outcomes?</p>\n<p>My own resolution is that this is another case of terminal values intruding where they don't belong. <em>All</em> that is relevant to the parent's terminal values must <em>already</em> be described in the outcome; the parent is allowed to prefer \"I threw a coin and my younger child got the car\" to \"I decided that my younger child would get the car\" or \"I asked the kindergarten teacher and they thought my younger child was better-behaved\", but if so, then these must already be different <em>outcomes</em>. The thing to remember is that it isn't a property of the <em>world</em> that either child had a 50% probability of getting the car, and you can't steer the future in the direction of having this mythical property. It <em>is</em> a property of the world that <em>the parent assigned a 50% probability</em> to each child getting the car, and that <em>is</em> a direction you can steer in &mdash; though the example with the kindergarten teacher shows that this is probably not quite the direction you actually wanted.</p>\n<p>The preference relation is <em>only</em> supposed to be about <em>trade-offs</em> between probability distributions; if you're tempted to say that you want to steer the world towards one probability distribution or another, rather than one outcome or other, something has gone terribly wrong.</p>\n<p align=\"center\">*</p>\n<h2>The Axiom of Continuity<br /></h2>\n<p>And&hellip; that's it. These are all the axioms that I'll ask you to accept in this post.</p>\n<p>There is, however, one more axiom in the von Neumann-Morgenstern theory, the Axiom of Continuity. I do <em>not</em> think this axiom is a necessary requirement on any coherent plan for steering the world; I think the best argument for it is that it doesn't make a practical difference whether you adopt it, so you might as well. But there is also a good argument to be made that if we're talking about anything <em>short</em> of steering the entire future of humanity, your preferences <em>do</em> in fact obey this axiom, and it makes things easier technically if we adopt it, so I'll do that at least for now.</p>\n<p>Let's look at an example: If you prefer $50 in your pocket to $40, the axiom says that there must be <em>some</em> small <img src=\"http://www.codecogs.com/png.latex?\\epsilon &gt; 0\" alt=\"\" /> such that you prefer a probability of <img src=\"http://www.codecogs.com/png.latex?1-\\epsilon\" alt=\"\" /> of $50 and a probability of <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\" /> of dying today to a certainty of $40. Some critics seem to see this as the ultimate <em>reductio ad absurdum</em> for the VNM theory; they seem to think that no sane human would accept that deal.</p>\n<p>Eliezer was surely not the first to <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">observe</a> that this preference is exhibited each time someone drives an extra mile to save $10.</p>\n<p>Continuity says that if you strictly prefer <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> to <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, then there is <em>no&nbsp;<img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" /></em> so terrible that you wouldn't be willing to incur a small probability of it in order to (probably) get <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> rather than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, and <em>no</em> <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" /> so wonderful that you'd be willing to (probably) get <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> instead of <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> if this gives you some arbitrarily small probability of getting <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" />. Formally, for all <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\" />,</p>\n<ul>\n<li>If <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\" />, then there is an <img src=\"http://www.codecogs.com/png.latex?\\epsilon &gt; 0\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?(1-\\epsilon)x + \\epsilon z \\;\\succ\\; y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?x \\;\\succ\\; (1-\\epsilon)y + \\epsilon z\" alt=\"\" />.</li>\n</ul>\n<p>I think if we're talking about everyday life, we can pretty much rule out that there are things so terrible that for <em>arbitrarily</em> small <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\" />, you'd be willing to die with probability&nbsp;<img src=\"http://www.codecogs.com/png.latex?1-\\epsilon\" alt=\"\" /> to avoid a probability of <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\" /> of the terrible thing. And if you feel that <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">it's not worth the expense</a> to call a doctor every time you sneeze, you're willing to incur a <em>slightly</em> higher probability of death in order to save some mere money. And it seems unlikely that there is <em>no</em> <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\" /> at which you'd prefer a certainty of $1 to a chance <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\" /> of $100. And if you have some preference that is so slight that you wouldn't be willing to accept <em>any</em> chance of losing $1 in order to indulge it, it can't be a very strong preference. So I think for most practical purposes, we might as well accept Continuity.</p>\n<p align=\"center\">*</p>\n<h2>The VNM theorem<br /></h2>\n<p>If your preferences are described by a transitive and complete relation <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" /> on the probability distributions over some set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" /> of \"outcomes\", and this relation satisfies Independence and Continuity, then you have a utility function, and your genie will be maximizing expected utility.</p>\n<p>Here's what that means. A utility function is a function <img src=\"http://www.codecogs.com/png.latex?u : \\mathcal{O}\\to\\mathbb{R}\" alt=\"\" /> which assigns a numerical \"utility\" to every outcome. Given a probability distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> over&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" />, we can compute the expected value of&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> under&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" />,&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\textstyle\\sum_{i\\in\\mathcal{O}} u(i)\\,x_i\" alt=\"\" />; this is called the <em>expected utility</em>. We can prove that there is some utility function such that for all&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, we have <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\" /> if and only if the expected utility under <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is greater than the expected utility under <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />.</p>\n<p>In other words: <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" /> is <em>completely</em> described by <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" />; if you know&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" />, you know <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" />. Instead of programming your genie with a function that takes two outcomes and says which one is better, you might as well program it with a function that takes one outcome and returns its utility. Any coherent direction for steering the world which happens to satisfy Continuity can be reduced to a function that takes outcomes and assigns them numerical ratings.</p>\n<p>In fact, it turns out that the <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> for a given&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" /> is \"almost\" unique: Given two utility functions <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?v\" alt=\"\" /> that describe the same <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" />, there are numbers <img src=\"http://www.codecogs.com/png.latex?a&gt;0\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?b\\in\\mathbb{R}\" alt=\"\" /> such that for all&nbsp;<img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?v(i) = au(i) + b\" alt=\"\" />; this is called an \"affine transformation\". On the other hand, it's not hard to see that for any such <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\" />,</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\sum_{i\\in\\mathcal{O}} u(i)\\,x_i &gt; \\sum_{i\\in\\mathcal{O}} u(i)\\,y_i \\;\\iff\\; \\sum_{i\\in\\mathcal{O}} \\big(au(i) + b\\big)\\,x_i &gt; \\sum_{i\\in\\mathcal{O}} \\big(au(i) + b\\big)\\,y_i,\" alt=\"\" /></p>\n<p>so two utility functions represent the same preference relation if and only if they are related in this way.</p>\n<p align=\"center\">*</p>\n<p>You shouldn't read <em>too</em> much into this conception of utility. For example, it doesn't make sense to see a fundamental distinction between outcomes with \"positive\" and with \"negative\" von Neumann-Morgenstern utility &mdash; because adding the right <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\" /> can make any negative utility positive and any positive utility negative, without changing the underlying preference relation. The numbers that have real meaning are ratios between differences between utilities, <img src=\"http://www.codecogs.com/png.latex?\\textstyle\\frac{u(i) - u(j)}{u(k) - u(\\ell)}\" alt=\"\" />, because these don't change under affine transformations (the <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\" />'s cancel when you take the difference, and the <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"\" />'s cancel when you take the ratio). <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/#pre\">Academian's post</a> has more about misunderstandings of VNM utility.</p>\n<p>In my view, what VNM utilities represent is not <em>necessarily</em> how <em>good</em> each outcome is; what they represent is what trade-offs between probability distributions you are willing to accept. Now, if you strongly felt that the difference between <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" /> was about the same as the difference between <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?\\ell\" alt=\"\" />, then you should have <em>a very good reason</em> before you make your <img src=\"http://www.codecogs.com/png.latex?%5Ctextstyle%5Cfrac%7Bu%28i%29%20-%20u%28j%29%7D%7Bu%28k%29%20-%20u%28%5Cell%29%7D\" alt=\"\" /> a huge number. But on the other hand, I think it's ultimately your responsibility to decide what trade-offs you are willing to make; I don't think you can get away with \"<a href=\"/lw/cod/a_novel_approach_to_axiomatic_decision_theory/6onu\">stating how much you value different outcomes</a>\" and outsourcing the rest of the job to decision theory, without ever <em>considering</em> what these valuations should mean in terms of probabilistic trade-offs.</p>\n<p align=\"center\">*</p>\n<h2>Doing without Continuity<br /></h2>\n<p>What happens if your preferences do <em>not</em> satisfy Continuity? Say, you want to save human lives, but you're not willing to incur <em>any</em> probability, no matter how small, of <em>infinitely</em> many people getting tortured infinitely long for this?</p>\n<p>I do not see a good argument that this couldn't add up to a coherent direction for steering the world. I do, however, see an argument that in this case you care so little about finite numbers of human lives that in practice, you can probably neglect this concern entirely. (As a result, I doubt that your reflective equilibrium would want to adopt such preferences. But I don't think they're <em>in</em><em>coherent</em>.)</p>\n<p>I'll assume that your morality can still distinguish only a finite number of outcomes, and you can choose only between a finite number of decisions. It's not obvious that these assumptions are justified if we want to take into account the <em>possibility</em> that the true laws of physics might turn out to allow for infinite computations, but even in this case <em>you</em> and any AI <em>you</em> build will probably still be finite (though <em>it</em> might build a successor that isn't), so I do in fact think there is a good chance that results derived under this assumption have relevance in the real world.</p>\n<p>In this case, it turns out that you <em>still</em> have a utility function, in a certain sense. (Proofs for non-standard results can be found in the <a href=\"/r/discussion/lw/fq0/math_appendix_for_why_you_must_maximize_expected/\">math appendix</a> to this post. I did the work myself, but I don't expect these results to be new.) This utility function describes only the concern most important to you: in our example, only the probability of infinite torture makes a difference to expected utility; any change in the probability of saving a finite number of lives leaves expected utility unchanged.</p>\n<p>Let's define a relation <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" />, read \"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is <em>much better</em> than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />\", which says that there is nothing you wouldn't give up a little probability of in order to get <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> instead of <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> &mdash; in our example: <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> doesn't merely save lives compared to <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, it makes infinite torture less likely. Formally, we define <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" /> to mean that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\" /> \"close enough\" to <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> respectively; more precisely: <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" /> if there is an <img src=\"http://www.codecogs.com/png.latex?\\epsilon&gt;0\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\" /> with</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\sum_{i\\in\\mathcal{O}}|x'_i - x_i| \\;&lt;\\; \\epsilon \\qquad\\text{and}\\qquad \\sum_{i\\in\\mathcal{O}} |y'_i - y_i| \\;&lt;\\; \\epsilon.\" alt=\"\" /></p>\n<p>(Or equivalently: if there are open sets&nbsp;<img src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?V\" alt=\"\" /> around <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, respectively, such that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x'\\in U\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\\in V\" alt=\"\" />.)</p>\n<p>It turns out that if <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" /> is a preference relation satisfying Independence, then <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\" /> is a preference relation satisfying Independence and Continuity, and there is a utility function <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" /> iff the expected utility under <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is larger than the expected utility under <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />. Obviously, <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" /> implies <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\" />, so whenever two options have different expected utilities, you prefer the one with the larger expected utility. Your genie is <em>still</em> an expected utility maximizer.</p>\n<p>Furthermore, unless <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" /> for <em>all</em> <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> isn't constant &mdash; that is, there are <em>some</em> <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> with <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" />. (If this weren't the case, the result above obviously wouldn't tell us very much about <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" />!) Being indifferent between all possible actions doesn't make for a particularly interesting direction for steering the world, if it can be called one at all, so from now on let's assume that you are not.</p>\n<p align=\"center\">*</p>\n<p>It <em>can</em> happen that there are two distributions <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> with the same expected utility, but <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\" />. (<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> saves more lives, but the probability of eternal torture is the same.) Thus, if your genie happens to face a choice between two actions that lead to the <em>same</em> expected utility, it must do more work to figure out which of the actions it should take. But there is some reason to expect that such situations should be <em>rare</em>.</p>\n<p>If there are <img src=\"http://www.codecogs.com/png.latex?N\" alt=\"\" /> possible outcomes, then <a href=\"http://en.wikipedia.org/wiki/Simplex#The_standard_simplex\">the set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" /> of probability distributions over <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" /></a> is <img src=\"http://www.codecogs.com/png.latex?(N-1)\" alt=\"\" />-dimensional (because the probabilities must add up to 1, so if you know <img src=\"http://www.codecogs.com/png.latex?N-1\" alt=\"\" /> of them, you can figure out the last one). For example, if there are three outcomes, <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" /> is a triangle, and if there are four outcomes, it's a tetrahedron. On the other hand, it turns out that for any <img src=\"http://www.codecogs.com/png.latex?r\\in\\mathbb{R}\" alt=\"\" />, the set of all <img src=\"http://www.codecogs.com/png.latex?x\\in\\Delta_N\" alt=\"\" /> for which the expected utility equals <img src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" /> has dimension <img src=\"http://www.codecogs.com/png.latex?N-2\" alt=\"\" /> or smaller: if <img src=\"http://www.codecogs.com/png.latex?N=3\" alt=\"\" />, it's a line (or a point or the empty set); if <img src=\"http://www.codecogs.com/png.latex?N=4\" alt=\"\" />, it's a plane (or a line or a point or the empty set).</p>\n<p>Thus, in order to have the same expected utility, <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> must lie on the same hyperplane &mdash; not just on a plane <em>very close by</em>, but on <em>exactly</em> the same plane. That's not just a small target to hit, that's an infinitely small target. If you use, say, a <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff prior</a>, then it seems <em>very</em> unlikely that two of your finitely many options just <em>happen</em> to lead to probability distributions which yield the same expected utility.</p>\n<p>But we are bounded rationalists, not perfect Bayesians with uncomputable Solomonoff priors. We assign heads and tails exactly the same probability, not because there is no information that would make one or the other more likely (we could try to arrive at a best guess about which side is a little heavier than the other?), but because the problem is so complicated that we simply give up on it. What if it turns out that because of this, all the <em>difficult</em> decisions we need to make turn out to be between actions that happen to have the same expected utility?</p>\n<p>If you do your imperfect calculation and find that two of your options seem to yield exactly the same probability of eternal hell for infinitely many people, you <em>could</em> then try to figure out which of them is more likely to save a finite number of lives. But it seems to me that this is <em>not</em> the best approximation of an ideal Bayesian with your stated preferences. Shouldn't you spend those computational resources on doing a <em>better</em> calculation of which option is more likely to lead to eternal hell?</p>\n<p>For you <em>might</em> arrive at a new estimate under which the probabilities of hell are at least slightly different. Even if you <em>suspect</em> that the new calculation will again come out with the probabilities exactly equal, you don't <em>know</em> that. And therefore, can you truly in good conscience argue that doing the new calculation does not improve the odds of avoiding hell &mdash;</p>\n<p>&mdash; <em>at least a teeny tiny incredibly super-small for all ordinary intents and purposes completely irrelevant bit?</em></p>\n<p>Even if it <em>should</em> be the case that to a <em>perfect</em> Bayesian, the expected utilities under a Solomonoff prior were exactly the same, <em>you</em> don't know that, so how can you possibly justify stopping the calculation and saving a mere finite number of lives?</p>\n<p align=\"center\">*</p>\n<p>So there you have it. In order to have a coherent direction in which you want to steer the world, you must have a set of outcomes and a preference relation over the probability distributions over these outcomes, and this relation must satisfy Independence &mdash; or so it seems to me, anyway. And if you do, then you have a utility function, and a perfect Bayesian maximizing your preferences will always maximize expected utility.</p>\n<p>It <em>could</em> happen that two options have exactly the same expected utility, and in this case the utility function doesn't tell you which of these is better, under your preferences; but as a bounded rationalist, you can never <em>know</em> this, so if you have any computational resources left that you could spend on figuring out what your true preferences have to say, you should spend them on a better calculation of the expected utilities instead.</p>\n<p>Given this, we might as well just talk about <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\" />, which satisfies Continuity as well as Independence, instead of <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" />; and you might as well program your genie with your utility function, which only reflects <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\" />, instead of with your true preferences.</p>\n<p><em>(Note: I am not literally saying that you should not try to understand the whole topic better than this if you are </em>actually<em> going to program a Friendly AI. This is still meant as a metaphor. I </em>am,<em> however, saying that expected utility theory, even with boring old real numbers as utilities</em>,<em> is not to be discarded </em>lightly<em>.)</em></p>\n<p align=\"center\">*</p>\n<h2>Next post: Dealing with time</h2>\n<p>So far, we've always pretended that you only face <em>one</em> choice, at <em>one</em> point in time. But not only is there a way to apply our theory to repeated interactions with the environment &mdash; there are two!</p>\n<p>One way is to say that at each point in time, you should apply decision theory to set of actions you can perform <em>at that point</em>. Now, the actual outcome depends of course not only on what you do now, but also on what you do later; but you know that you'll still use decision theory later, so you can <em>foresee</em> what you will do in any possible future situation, and take it into account when computing what action you should choose now.</p>\n<p>The second way is to make a choice only once, not between the actions you can take at that point in time, but between complete <em>plans</em> &mdash; giant lookup tables &mdash; which specify how you <em>will</em> behave in any situation you might possibly face. Thus, you simply do your expected utility calculation <em>once</em>, and then stick with the plan you have decided on.</p>\n<p><a href=\"/lw/fu1/why_you_must_maximize_expected_utility/81h9\"><strong>Meditation:</strong></a> <em>Which of these is the <strong>right</strong> thing to do, if you have a perfect Bayesian genie and you want steer the future in some particular direction?</em> <em>(Does it even make a difference which one you use?)</em></p>\n<p>&nbsp;</p>\n<p><strong><a href=\"/r/discussion/lw/fq0/math_appendix_for_why_you_must_maximize_expected/\">&raquo; To the mathematical appendix</a></strong></p>\n<p>&nbsp;</p>\n<div style=\"font-size: smaller\">\n<p><strong>Notes</strong></p>\n<p><sup>1 </sup>The accounts of decision theory I've read use the term \"outcome\", or \"consequence\", but leave it mostly undefined; in a lottery, it's the prize you get at the end, but clearly nobody is saying decision theory should <em>only</em> apply to lotteries. I'm not changing its role in the mathematics, and I think my explanation of it is what the term always <em>wanted</em> to mean; I expect that other people have explained it in similar ways, though I'm not sure how similar precisely.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "dPPATLhRmhdJtJM2t": 1, "2YcmB6SLtHnHRe3uX": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F46jPraqp258q67nE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 48, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "20521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This post explains von Neumann-Morgenstern (VNM) axioms for decision theory</em>,<em> and what follows from them: that if you have a consistent direction in which you are trying to steer the future, you must be an expected utility maximizer. I'm writing this post in preparation for a sequence on <a href=\"/lw/4g6/updateless_anthropics/\">updateless</a> <a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">anthropics</a>, but I'm hoping that it will also be independently useful.</em></p>\n<p>The theorems of decision theory say that if you follow certain axioms, then your behavior is described by a utility function. (If you don't know what that means, I'll explain below.) So you should have a utility function! Except, why should you want to follow these axioms in the first place?</p>\n<p>A couple of years ago, Eliezer explained how violating one of them can <a href=\"/lw/my/the_allais_paradox/\">turn you into a money pump</a> \u2014 how, at time 11:59, you will <em>want</em> to pay a penny to get option B instead of option A, and then at 12:01, you will <em>want</em> to pay a penny to switch back. Either that, or the game will have ended and the option won't have made a difference.</p>\n<p>When I read that post, I was suitably impressed, but not completely convinced: I would certainly not want to behave one way if behaving differently <em>always</em> gave better results. But couldn't you avoid the problem by violating the axiom only in situations where it doesn't give anyone an opportunity to money-pump you? I'm not saying that would be <em>elegant</em>, but is there a reason it would be <em>irrational</em>?</p>\n<p>It took me a while, but I have since come around to the view that you really must have a utility function, and really must behave in a way that maximizes the expectation of this function, <a href=\"/lw/oj/probability_is_in_the_mind/\">on pain of stupidity</a> (or at least that there are strong arguments in this direction). But I don't know any source that comes close to explaining the reason, the way I see it; hence, this post.</p>\n<p>I'll use the <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">von Neumann-Morgenstern axioms</a>, which assume probability theory as a foundation (unlike the <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/\">Savage axioms</a>, which actually <em>imply</em> that anyone following them has not only a utility function but also a probability distribution). I will assume that you already accept Bayesianism.</p>\n<p align=\"center\">*</p>\n<p><em>Epistemic</em> rationality is about figuring out what's true; <em>instrumental</em> rationality is about <a href=\"/lw/crd/only_say_rational_when_you_cant_eliminate_the_word/\">steering the future where you want it to go</a>. The way I see it, the axioms of decision theory tell you how to have a consistent <em>direction</em> in which you are trying to steer the future. If my choice at 12:01 depends on whether at 11:59 I had a chance to decide differently, then perhaps I won't ever be money-pumped; but if I want to save as many human lives as possible, and I must decide between different plans that have different probabilities of saving different numbers of people, then it starts to at least seem <em>doubtful</em> that which plan is better at 12:01 could <em>genuinely</em> depend on my opportunity to choose at 11:59.</p>\n<p>So how do we formalize the notion of a coherent direction in which you can steer the future?</p>\n<p><a id=\"more\"></a></p>\n<p align=\"center\">*</p>\n<h2 id=\"Setting_the_stage\">Setting the stage<br></h2>\n<p>Decision theory asks what you would do if faced with choices between different sets of options, and then places restrictions on how you can act in one situation, depending on how you would act in others. This is another thing that has always bothered me: If we are talking about choices between different lotteries with small prizes, it makes some sense that we could invite you to the lab and run ten sessions with different choices, and you should probably act consistently across them. But if we're interested in the big questions, like how to save the world, then you're not going to face a series of independent, analogous scenarios. So what is the <em>content</em> of asking what you would do if you faced a set of choices different from the one you actually face?</p>\n<p>The real point is that you have bounded computational resources, and you can't <em>actually</em> visualize the exact set of choices you might face in the future. A perfect Bayesian rationalist could just figure out what they <em>would</em> do in any conceivable situation and write it down in a giant lookup table, which means that they only face a single one-time choice between different possible tables. But <em>you</em> can't do that, and so you need to figure out general principles to follow. <a href=\"/lw/mt/beautiful_probability/\">A perfect Bayesian is like a Carnot engine</a> \u2014 it's what a theoretically perfect engine <em>would</em> look like, so even though you can at best approximate it, it still has something to teach you about how to build a real engine.</p>\n<p>But decision theory is <em>about</em> what a perfect Bayesian would do, and it's annoying to have our practical concerns intrude into our ideal picture like that. So let's give our story some local color and say that <em>you</em> aren't a perfect Bayesian, but you have a genie \u2014 that is, a powerful optimization process \u2014 that is, an AI, which <em>is</em>. (That, too, is physically impossible: AIs, like humans, can only approximate perfect Bayesianism. But we <em>are</em> still idealizing.) Your <em>genie</em> is able to comprehend the set of possible giant lookup tables it must choose between; <em>you</em> must write down a formula, to be evaluated by the genie, that chooses the best table from this set, given the available information. (An unmodified human won't <em>actually</em> be able to write down an exact formula describing their preferences, but we might be able to write down one for a paperclip maximizer.)</p>\n<p>The first constraint decision theory places on your formula is that it must order all options your genie <em>might</em> have to choose between from best to worst (though you might be indifferent between some of them), and then given any particular set of feasible options, it must choose the one that is least bad. In particular, if you prefer option A when options A and B are available, then you can't prefer option B when options A, B and C are available.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Meditation\"><strong>Meditation:</strong></a> <em>Alice is trying to decide how large a bonus each member of her team should get this year. She has just decided on giving Bob the same, already large, bonus as last year when she receives an e-mail from the head of a different division, asking her if she can recommend anyone for a new project he is setting up. Alice immediately realizes that Bob would love to be on that project, and would fit the bill exactly. But she </em>needs<em> Bob on the contract he's currently working on; losing him would be a pretty bad blow for her team.</em></p>\n<p><em>Alice decides there is no way that she can recommend Bob for the new project. But she still feels bad about it, and she decides to make up for it by giving Bob a larger bonus. On reflection, she finds that she genuinely feels that this is the </em>right<em> thing to do, simply because she </em>could<em> have recommended him but didn't. Does that mean that Alice's preferences are irrational? Or that something is wrong with decision theory?<br></em></p>\n<p><strong>Meditation:</strong><em> One kind of answer to the above and to many other criticisms of decision theory goes like this: Alice's decision isn't between giving Bob a larger bonus or not, it's between (give Bob a larger bonus unconditionally), (give Bob the same bonus unconditionally), (only give Bob a larger bonus if I could have recommended him), and so on. But if </em>that<em> sort of thing is allowed, is there </em>any<em> way left in which decision theory constrains Alice's behavior? If not, what good is it to Alice in figuring out what she should do?</em></p>\n<p>...<br>...<br>...</p>\n<p align=\"center\">*</p>\n<h2 id=\"Outcomes\">Outcomes<br></h2>\n<p>My short answer is that Alice can care about anything she damn well likes. But there are a lot of things that she <em>doesn't</em> care about, and decision theory has something to say about <em>those</em>.</p>\n<p>In fact, deciding that some kinds of preferences should be outlawed as irrational can be dangerous: you might think that nobody in their right mind should ever care about the detailed planning algorithms their AI uses, as long as they work. But how certain are you that it's wrong to care about whether the AI has planned out your whole life in advance, in detail? (Worse: Depending on how strictly you interpret it, this injunction might even rule out <a href=\"/lw/x4/nonperson_predicates\">not wanting the AI to run conscious simulations of people</a>.)</p>\n<p>But nevertheless, I believe the \"anything she damn well likes\" needs to be qualified. Imagine that Alice and Carol both have an AI, and fortuitously, both AIs have been programmed with the same preferences and the same Bayesian prior (and they talk, so they also have the same posterior, because Bayesians cannot agree to disagree). But Alice's AI has taken over the stock markets, while Carol's AI has seized the world's nuclear arsenals (and is protecting them well). So Alice's AI not only doesn't want to blow up Earth, it couldn't do so <em>even if it wanted to</em>; it couldn't even bribe Carol's AI, because Carol's AI really doesn't want the Earth blown up either. And so, if it makes a difference to the AIs' preference function whether they <em>could</em> blow up Earth if they wanted to, they have a conflict of interest.</p>\n<p>The moral of this story is not simply that it would be <em>sad</em> if two AIs came into conflict even though they have the same preferences. The point is that we're asking what it means to have a consistent direction in which you are trying to steer the future, and it doesn't look like our AIs are on the same bearing. Surely, a direction for steering the world should only depend on features of the <em>world</em>, not on additional information about which agent is at the rudder.</p>\n<p>You <em>can</em> want to not have your life planned out by an AI. But I think you should have to state your wish as a property of the world: you want <em>all</em> AIs to refrain from doing so, not just \"whatever AI happens to be executing this\". And Alice can want Bob to get a larger bonus if the company could have assigned him to the new project and decided not to, but she must figure out whether <em>this</em> is the correct way to translate her moral intuitions into preferences over properties of the world.</p>\n<p align=\"center\">*</p>\n<p>You may care about any feature of the world, but you don't in fact care about most of them. For example, there are many ways the atoms in the sun could be arranged that all add up to the same thing as far as you are concerned, and you don't have <a href=\"/lw/l4/terminal_values_and_instrumental_values/\"><em>terminal</em> preferences</a> about which of these will be the actual one tomorrow. And though you might care about <em>some</em> properties of the algorithms your AI is running, mostly they <em>really</em> do not matter.</p>\n<p>Let's define a function that takes a complete description of the world \u2014 past, present and future \u2014 and returns a data structure containing all information about the world that matters to your terminal values, and <em>only</em> that information. (Our imaginary perfect Bayesian doesn't know exactly which way the world will turn out, but it can work with \"possible worlds\", complete descriptions of ways the world <em>may</em> turn out.) We'll call this data structure an \"outcome\", and we require you to be indifferent between any two courses of action that will always produce the same outcome. Of course, any course of action is something that your AI would be executing in the actual world, and you are certainly allowed to care about the difference \u2014 but then the two courses of action do not lead to the same \"outcome\"!<sup>1</sup></p>\n<p>With this definition, I think it is pretty reasonable to say that in order to have a consistent direction in which you want to steer the world, you must be able to order these outcomes from best to worst, and always want to pick the least bad you can get.</p>\n<p align=\"center\">*</p>\n<h2 id=\"Preference_relations\">Preference relations<br></h2>\n<p>That won't be <em>sufficient</em>, though. Our genie doesn't <em>know</em> what outcome each action will produce, it only has probabilistic information about that, and that's a complication we very much do <em>not</em> want to idealize away (because we're trying to figure out the right way to <em>deal</em> with it). And so our decision theory amends the earlier requirement: You must not only be indifferent between actions that always produce the same outcome, but also between all actions that only yield <em>the same probability distribution</em> over outcomes.</p>\n<p>This is not at all a mild assumption, though it's usually built so deeply into the definitions that it's not even called an \"axiom\". But we've assumed that all features of the world you care about are already encoded in the outcomes, so it does seem to me that the only reason left why you might prefer one action over another is that it gives you a better trade-off in terms of what outcomes it makes more or less likely; and I've assumed that you're already a Bayesian, so you agree that <em>how</em> likely it makes an outcome is correctly represented by the probability of that outcome, given the action. So it certainly <em>seems</em> that the probability distribution over outcomes should give you all the information about an action that you could <em>possibly</em> care about. And that you should be able to order these probability distributions from best to worst, and all that.</p>\n<p>Formally, we represent a direction for steering the world as a set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\"> of possible outcomes and a binary relation <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\"> on the probability distributions over <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\"> (with <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\"> is interpreted as \"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> is at least as good as <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">\") which is a <a href=\"http://en.wikipedia.org/wiki/Total_preorder\">total preorder</a>; that is, for all <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\">:</p>\n<ul>\n<li>If <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq z\" alt=\"\">, then <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq z\" alt=\"\">&nbsp; (that is, <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\"> is <em>transitive</em>); and</li>\n<li>We have either <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\"> or <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\"> or both&nbsp; (that is, <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\"> is <em>total</em>).</li>\n</ul>\n<p>In this post, I'll assume that <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\"> is finite. We write <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\"> (for \"I'm indifferent between <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">\") when both <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\">, and we write <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\"> (\"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> is strictly better than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">\") when <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq y\" alt=\"\"> but <em>not</em> <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq x\" alt=\"\">. Our genie will compute the set of all actions it could possibly take, and the probability distribution over possible outcomes that (according to the genie's Bayesian posterior) each of these actions leads to, and then it will choose to act in a way that maximizes <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\">. I'll also assume that the set of possible actions will always be finite, so there is always at least one optimal action.</p>\n<p><strong>Meditation:</strong> <em><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> is in the neighbourhood and invites you to participate in one of its little games. Next Saturday, it plans to flip a fair coin; would you please indicate on the attached form whether you would like to bet that this coin will fall heads, or tails? If you correctly bet heads, you will win $10,000; if you correctly bet tails, you'll win $100. If you bet wrongly, you will still receive $1 for your participation.</em></p>\n<p><em> </em></p>\n<p><em>We'll assume that you prefer a 50% chance of $10,000 and a 50% chance of $1 to a 50% chance of $100 and a 50% chance of $1. Thus, our theory would say that you should bet heads. But there is a twist: Given recent galactopolitical events, you estimate a 3% chance that after posting its letter, Omega has been called away on urgent business. In this case, the game will be cancelled and you won't get any money, though as a consolation, Omega will probably send you some book from its rare SF collection when it returns (market value: approximately $55\u2013$70). Our theory so far tells you nothing about how you should bet in this case, but does Rationality have anything to say about it?<br></em></p>\n<p>...<br>...<br>...</p>\n<p align=\"center\">*</p>\n<h2 id=\"The_Axiom_of_Independence\">The Axiom of Independence<br></h2>\n<p>So here's how I think about that problem: If you already <em>knew</em> that Omega is still in the neighbourhood (but not which way the coin is going to fall), you would prefer to bet heads, and if you <em>knew</em> it has been called away, you wouldn't care. (And what you bet has no influence on whether Omega has been called away.) So heads is either better or exactly the same; clearly, you should bet heads.</p>\n<p>This type of reasoning is the content of the von Neumann-Morgenstern <em>Axiom of Independence</em>. Apparently, that's <a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/\">the most controversial of the theory's axioms</a>.</p>\n<p>You're already a Bayesian, so you already accept that if you perform an experiment to determine whether someone is a witch, and the experiment can come out two ways, then if one of these outcomes is evidence that the person is a witch, <a href=\"/lw/ii/conservation_of_expected_evidence/\">the other outcome must be evidence that they are <em>not</em></a>. New information is allowed to make a hypothesis more likely, but not <em>predictably</em> so; if <em>all</em> ways the experiment could come out make the hypothesis more likely, then you should <em>already</em> be finding it more likely than you do. The same thing is true even if only one result would make the hypothesis more likely, but the other would leave your probability estimate exactly unchanged.</p>\n<p>The Axiom of Independence is equivalent to saying that if you're evaluating a possible course of action, and one experimental result would make it seem more attractive than it currently seems to you, while the other experimental result would at least make it seem no <em>less</em> attractive, then you should <em>already</em> be finding it more attractive than you do. This does <em>seem</em> rather solid to me.</p>\n<p align=\"center\">*</p>\n<p>So what does this axiom say formally? <em>(Feel free to skip this section if you don't care.)</em></p>\n<p>Suppose that your genie is considering two possible actions <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?a_2\" alt=\"\"> (bet heads or tails), and an event <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\"> (Omega is called away). Each action gives rise to a probability distribution over possible outcomes: E.g., <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid a_1]\" alt=\"\"> is the probability of outcome <img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\"> if your genie chooses <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\">. But your genie can also compute a probability distribution <em>conditional on <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\"></em>, <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid E,a_1]\" alt=\"\">. Suppose that conditional on <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\">, it doesn't matter which action you pick: <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid E,a_1] \\,=\\, \\mathbb{P}[i\\mid E,a_2]\" alt=\"\"> for all <img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\">. And finally, suppose that the probability of <img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\"> doesn't depend on which action you pick: <img src=\"http://www.codecogs.com/png.latex?%5Cmathbb{P}[E%5Cmid%20a_1]%5C,=%5C,%5Cmathbb{P}[E%5Cmid%20a_2]%5C,=:%5C,%20p\" alt=\"\">, with <img src=\"http://www.codecogs.com/png.latex?0%20%3C%20p%20%3C%201\" alt=\"\">. The Axiom of Independence says that in this situation, you should prefer the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_1]\" alt=\"\"> to the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_2]\" alt=\"\">, and therefore prefer <img src=\"http://www.codecogs.com/png.latex?a_1\" alt=\"\"> to <img src=\"http://www.codecogs.com/png.latex?a_2\" alt=\"\">, if and only if you prefer the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_1]\" alt=\"\"> to the distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_2]\" alt=\"\">.</p>\n<p>Let's write <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_1]\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid\\neg E,a_2]\" alt=\"\">, and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\"> for the distribution <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid E,a_1] \\,=\\, \\mathbb{P}[\\;\\cdot\\mid E,a_2]\" alt=\"\">. (Formally, we think of these as vectors in <img src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^{|\\mathcal{O}|}\" alt=\"\">: e.g., <img src=\"http://www.codecogs.com/png.latex?z_i \\,=\\, \\mathbb{P}[i\\mid E,a_1]\" alt=\"\">.) For all&nbsp;<img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\">, we have</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[i\\mid a_1] \\;=\\; \\mathbb{P}[\\neg E\\mid a_1]\\cdot\\mathbb{P}[i\\mid\\neg E,a_1] \\;+\\; \\mathbb{P}[E\\mid a_1]\\cdot\\mathbb{P}[i\\mid E,a_1],\" alt=\"\"></p>\n<p>so <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_1] = (1-p)x + pz\" alt=\"\">, and similarly <img src=\"http://www.codecogs.com/png.latex?\\mathbb{P}[\\;\\cdot\\mid a_2] \\,=\\, (1-p)y + pz\" alt=\"\">. Thus, we can state the Axiom of Independence as follows:</p>\n<ul>\n<li><img src=\"http://www.codecogs.com/png.latex?(1-p)x + pz \\,\\succcurlyeq\\, (1-p)y + pz \\;\\iff\\; x\\succcurlyeq y\" alt=\"\">.</li>\n</ul>\n<p>We'll assume that you can't ever rule out the possibility that your AI might face this type of situation for any given <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\">, and <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\">, so we require that this condition hold for all probability distributions <em><img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"></em>,<em> <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> </em>and <em><img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\"></em>, and for all <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\"> with <img src=\"http://www.codecogs.com/png.latex?0<p<1\" alt=\"\">.</p>\n<p align=\"center\">*</p>\n<p>Here's a <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">common criticism</a> of Independence. Suppose a parent has two children, and one old car that they can give to one of these children. Can't they be indifferent between giving the car to their older child or their younger child, but strictly prefer throwing a coin? But let <img src=\"http://www.codecogs.com/png.latex?x = z\" alt=\"\"> mean that the younger child gets the gift, and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> that the older child gets it, and <img src=\"http://www.codecogs.com/png.latex?p = 1/2\" alt=\"\">; then by Independence, if <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\">, then <img src=\"http://www.codecogs.com/png.latex?\\textstyle x \\;=\\; \\frac12x + \\frac12 z \\;\\sim\\; \\frac12y + \\frac12 z\" alt=\"\">, so it would seem that the parent can <em>not</em> strictly prefer the coin throw.</p>\n<p>In fairness, the people who find this criticism persuasive may not be Bayesians. But if <em>you</em> think this is a good criticism: Do you think that the parent must be indifferent between throwing a coin and asking the children's crazy old kindergarten teacher which of them was better-behaved, as long as they assign 50% probability to either answer? Because if not, shouldn't you already have protested when we decided that decisions must only depend on the probabilities of different outcomes?</p>\n<p>My own resolution is that this is another case of terminal values intruding where they don't belong. <em>All</em> that is relevant to the parent's terminal values must <em>already</em> be described in the outcome; the parent is allowed to prefer \"I threw a coin and my younger child got the car\" to \"I decided that my younger child would get the car\" or \"I asked the kindergarten teacher and they thought my younger child was better-behaved\", but if so, then these must already be different <em>outcomes</em>. The thing to remember is that it isn't a property of the <em>world</em> that either child had a 50% probability of getting the car, and you can't steer the future in the direction of having this mythical property. It <em>is</em> a property of the world that <em>the parent assigned a 50% probability</em> to each child getting the car, and that <em>is</em> a direction you can steer in \u2014 though the example with the kindergarten teacher shows that this is probably not quite the direction you actually wanted.</p>\n<p>The preference relation is <em>only</em> supposed to be about <em>trade-offs</em> between probability distributions; if you're tempted to say that you want to steer the world towards one probability distribution or another, rather than one outcome or other, something has gone terribly wrong.</p>\n<p align=\"center\">*</p>\n<h2 id=\"The_Axiom_of_Continuity\">The Axiom of Continuity<br></h2>\n<p>And\u2026 that's it. These are all the axioms that I'll ask you to accept in this post.</p>\n<p>There is, however, one more axiom in the von Neumann-Morgenstern theory, the Axiom of Continuity. I do <em>not</em> think this axiom is a necessary requirement on any coherent plan for steering the world; I think the best argument for it is that it doesn't make a practical difference whether you adopt it, so you might as well. But there is also a good argument to be made that if we're talking about anything <em>short</em> of steering the entire future of humanity, your preferences <em>do</em> in fact obey this axiom, and it makes things easier technically if we adopt it, so I'll do that at least for now.</p>\n<p>Let's look at an example: If you prefer $50 in your pocket to $40, the axiom says that there must be <em>some</em> small <img src=\"http://www.codecogs.com/png.latex?\\epsilon > 0\" alt=\"\"> such that you prefer a probability of <img src=\"http://www.codecogs.com/png.latex?1-\\epsilon\" alt=\"\"> of $50 and a probability of <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\"> of dying today to a certainty of $40. Some critics seem to see this as the ultimate <em>reductio ad absurdum</em> for the VNM theory; they seem to think that no sane human would accept that deal.</p>\n<p>Eliezer was surely not the first to <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">observe</a> that this preference is exhibited each time someone drives an extra mile to save $10.</p>\n<p>Continuity says that if you strictly prefer <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> to <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, then there is <em>no&nbsp;<img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\"></em> so terrible that you wouldn't be willing to incur a small probability of it in order to (probably) get <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> rather than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, and <em>no</em> <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\"> so wonderful that you'd be willing to (probably) get <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> instead of <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> if this gives you some arbitrarily small probability of getting <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\">. Formally, for all <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?z\" alt=\"\">,</p>\n<ul>\n<li>If <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\">, then there is an <img src=\"http://www.codecogs.com/png.latex?\\epsilon > 0\" alt=\"\"> such that <img src=\"http://www.codecogs.com/png.latex?(1-\\epsilon)x + \\epsilon z \\;\\succ\\; y\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?x \\;\\succ\\; (1-\\epsilon)y + \\epsilon z\" alt=\"\">.</li>\n</ul>\n<p>I think if we're talking about everyday life, we can pretty much rule out that there are things so terrible that for <em>arbitrarily</em> small <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\">, you'd be willing to die with probability&nbsp;<img src=\"http://www.codecogs.com/png.latex?1-\\epsilon\" alt=\"\"> to avoid a probability of <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\"> of the terrible thing. And if you feel that <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">it's not worth the expense</a> to call a doctor every time you sneeze, you're willing to incur a <em>slightly</em> higher probability of death in order to save some mere money. And it seems unlikely that there is <em>no</em> <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\"> at which you'd prefer a certainty of $1 to a chance <img src=\"http://www.codecogs.com/png.latex?\\epsilon\" alt=\"\"> of $100. And if you have some preference that is so slight that you wouldn't be willing to accept <em>any</em> chance of losing $1 in order to indulge it, it can't be a very strong preference. So I think for most practical purposes, we might as well accept Continuity.</p>\n<p align=\"center\">*</p>\n<h2 id=\"The_VNM_theorem\">The VNM theorem<br></h2>\n<p>If your preferences are described by a transitive and complete relation <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\"> on the probability distributions over some set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\"> of \"outcomes\", and this relation satisfies Independence and Continuity, then you have a utility function, and your genie will be maximizing expected utility.</p>\n<p>Here's what that means. A utility function is a function <img src=\"http://www.codecogs.com/png.latex?u : \\mathcal{O}\\to\\mathbb{R}\" alt=\"\"> which assigns a numerical \"utility\" to every outcome. Given a probability distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> over&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\">, we can compute the expected value of&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> under&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\">,&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\textstyle\\sum_{i\\in\\mathcal{O}} u(i)\\,x_i\" alt=\"\">; this is called the <em>expected utility</em>. We can prove that there is some utility function such that for all&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, we have <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\"> if and only if the expected utility under <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> is greater than the expected utility under <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">.</p>\n<p>In other words: <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\"> is <em>completely</em> described by <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\">; if you know&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\">, you know <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\">. Instead of programming your genie with a function that takes two outcomes and says which one is better, you might as well program it with a function that takes one outcome and returns its utility. Any coherent direction for steering the world which happens to satisfy Continuity can be reduced to a function that takes outcomes and assigns them numerical ratings.</p>\n<p>In fact, it turns out that the <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> for a given&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\"> is \"almost\" unique: Given two utility functions <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?v\" alt=\"\"> that describe the same <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\">, there are numbers <img src=\"http://www.codecogs.com/png.latex?a>0\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?b\\in\\mathbb{R}\" alt=\"\"> such that for all&nbsp;<img src=\"http://www.codecogs.com/png.latex?i\\in\\mathcal{O}\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?v(i) = au(i) + b\" alt=\"\">; this is called an \"affine transformation\". On the other hand, it's not hard to see that for any such <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\">,</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\sum_{i\\in\\mathcal{O}} u(i)\\,x_i > \\sum_{i\\in\\mathcal{O}} u(i)\\,y_i \\;\\iff\\; \\sum_{i\\in\\mathcal{O}} \\big(au(i) + b\\big)\\,x_i > \\sum_{i\\in\\mathcal{O}} \\big(au(i) + b\\big)\\,y_i,\" alt=\"\"></p>\n<p>so two utility functions represent the same preference relation if and only if they are related in this way.</p>\n<p align=\"center\">*</p>\n<p>You shouldn't read <em>too</em> much into this conception of utility. For example, it doesn't make sense to see a fundamental distinction between outcomes with \"positive\" and with \"negative\" von Neumann-Morgenstern utility \u2014 because adding the right <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\"> can make any negative utility positive and any positive utility negative, without changing the underlying preference relation. The numbers that have real meaning are ratios between differences between utilities, <img src=\"http://www.codecogs.com/png.latex?\\textstyle\\frac{u(i) - u(j)}{u(k) - u(\\ell)}\" alt=\"\">, because these don't change under affine transformations (the <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\">'s cancel when you take the difference, and the <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"\">'s cancel when you take the ratio). <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/#pre\">Academian's post</a> has more about misunderstandings of VNM utility.</p>\n<p>In my view, what VNM utilities represent is not <em>necessarily</em> how <em>good</em> each outcome is; what they represent is what trade-offs between probability distributions you are willing to accept. Now, if you strongly felt that the difference between <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?j\" alt=\"\"> was about the same as the difference between <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?\\ell\" alt=\"\">, then you should have <em>a very good reason</em> before you make your <img src=\"http://www.codecogs.com/png.latex?%5Ctextstyle%5Cfrac%7Bu%28i%29%20-%20u%28j%29%7D%7Bu%28k%29%20-%20u%28%5Cell%29%7D\" alt=\"\"> a huge number. But on the other hand, I think it's ultimately your responsibility to decide what trade-offs you are willing to make; I don't think you can get away with \"<a href=\"/lw/cod/a_novel_approach_to_axiomatic_decision_theory/6onu\">stating how much you value different outcomes</a>\" and outsourcing the rest of the job to decision theory, without ever <em>considering</em> what these valuations should mean in terms of probabilistic trade-offs.</p>\n<p align=\"center\">*</p>\n<h2 id=\"Doing_without_Continuity\">Doing without Continuity<br></h2>\n<p>What happens if your preferences do <em>not</em> satisfy Continuity? Say, you want to save human lives, but you're not willing to incur <em>any</em> probability, no matter how small, of <em>infinitely</em> many people getting tortured infinitely long for this?</p>\n<p>I do not see a good argument that this couldn't add up to a coherent direction for steering the world. I do, however, see an argument that in this case you care so little about finite numbers of human lives that in practice, you can probably neglect this concern entirely. (As a result, I doubt that your reflective equilibrium would want to adopt such preferences. But I don't think they're <em>in</em><em>coherent</em>.)</p>\n<p>I'll assume that your morality can still distinguish only a finite number of outcomes, and you can choose only between a finite number of decisions. It's not obvious that these assumptions are justified if we want to take into account the <em>possibility</em> that the true laws of physics might turn out to allow for infinite computations, but even in this case <em>you</em> and any AI <em>you</em> build will probably still be finite (though <em>it</em> might build a successor that isn't), so I do in fact think there is a good chance that results derived under this assumption have relevance in the real world.</p>\n<p>In this case, it turns out that you <em>still</em> have a utility function, in a certain sense. (Proofs for non-standard results can be found in the <a href=\"/r/discussion/lw/fq0/math_appendix_for_why_you_must_maximize_expected/\">math appendix</a> to this post. I did the work myself, but I don't expect these results to be new.) This utility function describes only the concern most important to you: in our example, only the probability of infinite torture makes a difference to expected utility; any change in the probability of saving a finite number of lives leaves expected utility unchanged.</p>\n<p>Let's define a relation <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\">, read \"<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> is <em>much better</em> than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">\", which says that there is nothing you wouldn't give up a little probability of in order to get <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> instead of <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> \u2014 in our example: <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> doesn't merely save lives compared to <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, it makes infinite torture less likely. Formally, we define <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\"> to mean that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\"> for all <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\"> \"close enough\" to <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> respectively; more precisely: <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\"> if there is an <img src=\"http://www.codecogs.com/png.latex?\\epsilon>0\" alt=\"\"> such that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\"> for all <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\"> with</p>\n<p align=\"center\"><img src=\"http://www.codecogs.com/png.latex?\\sum_{i\\in\\mathcal{O}}|x'_i - x_i| \\;<\\; \\epsilon \\qquad\\text{and}\\qquad \\sum_{i\\in\\mathcal{O}} |y'_i - y_i| \\;<\\; \\epsilon.\" alt=\"\"></p>\n<p>(Or equivalently: if there are open sets&nbsp;<img src=\"http://www.codecogs.com/png.latex?U\" alt=\"\"> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?V\" alt=\"\"> around <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, respectively, such that <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\"> for all <img src=\"http://www.codecogs.com/png.latex?x'\\in U\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y'\\in V\" alt=\"\">.)</p>\n<p>It turns out that if <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\"> is a preference relation satisfying Independence, then <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\"> is a preference relation satisfying Independence and Continuity, and there is a utility function <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> such that <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\"> iff the expected utility under <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> is larger than the expected utility under <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">. Obviously, <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\"> implies <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\">, so whenever two options have different expected utilities, you prefer the one with the larger expected utility. Your genie is <em>still</em> an expected utility maximizer.</p>\n<p>Furthermore, unless <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\"> for <em>all</em> <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> isn't constant \u2014 that is, there are <em>some</em> <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> with <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\">. (If this weren't the case, the result above obviously wouldn't tell us very much about <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\">!) Being indifferent between all possible actions doesn't make for a particularly interesting direction for steering the world, if it can be called one at all, so from now on let's assume that you are not.</p>\n<p align=\"center\">*</p>\n<p>It <em>can</em> happen that there are two distributions <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> with the same expected utility, but <img src=\"http://www.codecogs.com/png.latex?x\\succ y\" alt=\"\">. (<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> saves more lives, but the probability of eternal torture is the same.) Thus, if your genie happens to face a choice between two actions that lead to the <em>same</em> expected utility, it must do more work to figure out which of the actions it should take. But there is some reason to expect that such situations should be <em>rare</em>.</p>\n<p>If there are <img src=\"http://www.codecogs.com/png.latex?N\" alt=\"\"> possible outcomes, then <a href=\"http://en.wikipedia.org/wiki/Simplex#The_standard_simplex\">the set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\"> of probability distributions over <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\"></a> is <img src=\"http://www.codecogs.com/png.latex?(N-1)\" alt=\"\">-dimensional (because the probabilities must add up to 1, so if you know <img src=\"http://www.codecogs.com/png.latex?N-1\" alt=\"\"> of them, you can figure out the last one). For example, if there are three outcomes, <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\"> is a triangle, and if there are four outcomes, it's a tetrahedron. On the other hand, it turns out that for any <img src=\"http://www.codecogs.com/png.latex?r\\in\\mathbb{R}\" alt=\"\">, the set of all <img src=\"http://www.codecogs.com/png.latex?x\\in\\Delta_N\" alt=\"\"> for which the expected utility equals <img src=\"http://www.codecogs.com/png.latex?r\" alt=\"\"> has dimension <img src=\"http://www.codecogs.com/png.latex?N-2\" alt=\"\"> or smaller: if <img src=\"http://www.codecogs.com/png.latex?N=3\" alt=\"\">, it's a line (or a point or the empty set); if <img src=\"http://www.codecogs.com/png.latex?N=4\" alt=\"\">, it's a plane (or a line or a point or the empty set).</p>\n<p>Thus, in order to have the same expected utility, <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\"> must lie on the same hyperplane \u2014 not just on a plane <em>very close by</em>, but on <em>exactly</em> the same plane. That's not just a small target to hit, that's an infinitely small target. If you use, say, a <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff prior</a>, then it seems <em>very</em> unlikely that two of your finitely many options just <em>happen</em> to lead to probability distributions which yield the same expected utility.</p>\n<p>But we are bounded rationalists, not perfect Bayesians with uncomputable Solomonoff priors. We assign heads and tails exactly the same probability, not because there is no information that would make one or the other more likely (we could try to arrive at a best guess about which side is a little heavier than the other?), but because the problem is so complicated that we simply give up on it. What if it turns out that because of this, all the <em>difficult</em> decisions we need to make turn out to be between actions that happen to have the same expected utility?</p>\n<p>If you do your imperfect calculation and find that two of your options seem to yield exactly the same probability of eternal hell for infinitely many people, you <em>could</em> then try to figure out which of them is more likely to save a finite number of lives. But it seems to me that this is <em>not</em> the best approximation of an ideal Bayesian with your stated preferences. Shouldn't you spend those computational resources on doing a <em>better</em> calculation of which option is more likely to lead to eternal hell?</p>\n<p>For you <em>might</em> arrive at a new estimate under which the probabilities of hell are at least slightly different. Even if you <em>suspect</em> that the new calculation will again come out with the probabilities exactly equal, you don't <em>know</em> that. And therefore, can you truly in good conscience argue that doing the new calculation does not improve the odds of avoiding hell \u2014</p>\n<p>\u2014 <em>at least a teeny tiny incredibly super-small for all ordinary intents and purposes completely irrelevant bit?</em></p>\n<p>Even if it <em>should</em> be the case that to a <em>perfect</em> Bayesian, the expected utilities under a Solomonoff prior were exactly the same, <em>you</em> don't know that, so how can you possibly justify stopping the calculation and saving a mere finite number of lives?</p>\n<p align=\"center\">*</p>\n<p>So there you have it. In order to have a coherent direction in which you want to steer the world, you must have a set of outcomes and a preference relation over the probability distributions over these outcomes, and this relation must satisfy Independence \u2014 or so it seems to me, anyway. And if you do, then you have a utility function, and a perfect Bayesian maximizing your preferences will always maximize expected utility.</p>\n<p>It <em>could</em> happen that two options have exactly the same expected utility, and in this case the utility function doesn't tell you which of these is better, under your preferences; but as a bounded rationalist, you can never <em>know</em> this, so if you have any computational resources left that you could spend on figuring out what your true preferences have to say, you should spend them on a better calculation of the expected utilities instead.</p>\n<p>Given this, we might as well just talk about <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\">, which satisfies Continuity as well as Independence, instead of <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\">; and you might as well program your genie with your utility function, which only reflects <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\">, instead of with your true preferences.</p>\n<p><em>(Note: I am not literally saying that you should not try to understand the whole topic better than this if you are </em>actually<em> going to program a Friendly AI. This is still meant as a metaphor. I </em>am,<em> however, saying that expected utility theory, even with boring old real numbers as utilities</em>,<em> is not to be discarded </em>lightly<em>.)</em></p>\n<p align=\"center\">*</p>\n<h2 id=\"Next_post__Dealing_with_time\">Next post: Dealing with time</h2>\n<p>So far, we've always pretended that you only face <em>one</em> choice, at <em>one</em> point in time. But not only is there a way to apply our theory to repeated interactions with the environment \u2014 there are two!</p>\n<p>One way is to say that at each point in time, you should apply decision theory to set of actions you can perform <em>at that point</em>. Now, the actual outcome depends of course not only on what you do now, but also on what you do later; but you know that you'll still use decision theory later, so you can <em>foresee</em> what you will do in any possible future situation, and take it into account when computing what action you should choose now.</p>\n<p>The second way is to make a choice only once, not between the actions you can take at that point in time, but between complete <em>plans</em> \u2014 giant lookup tables \u2014 which specify how you <em>will</em> behave in any situation you might possibly face. Thus, you simply do your expected utility calculation <em>once</em>, and then stick with the plan you have decided on.</p>\n<p><a href=\"/lw/fu1/why_you_must_maximize_expected_utility/81h9\"><strong>Meditation:</strong></a> <em>Which of these is the <strong>right</strong> thing to do, if you have a perfect Bayesian genie and you want steer the future in some particular direction?</em> <em>(Does it even make a difference which one you use?)</em></p>\n<p>&nbsp;</p>\n<p><strong id=\"__To_the_mathematical_appendix\"><a href=\"/r/discussion/lw/fq0/math_appendix_for_why_you_must_maximize_expected/\">\u00bb To the mathematical appendix</a></strong></p>\n<p>&nbsp;</p>\n<div style=\"font-size: smaller\">\n<p><strong id=\"Notes\">Notes</strong></p>\n<p><sup>1 </sup>The accounts of decision theory I've read use the term \"outcome\", or \"consequence\", but leave it mostly undefined; in a lottery, it's the prize you get at the end, but clearly nobody is saying decision theory should <em>only</em> apply to lotteries. I'm not changing its role in the mathematics, and I think my explanation of it is what the term always <em>wanted</em> to mean; I expect that other people have explained it in similar ways, though I'm not sure how similar precisely.</p>\n</div>", "sections": [{"title": "Setting the stage", "anchor": "Setting_the_stage", "level": 1}, {"title": "Outcomes", "anchor": "Outcomes", "level": 1}, {"title": "Preference relations", "anchor": "Preference_relations", "level": 1}, {"title": "The Axiom of Independence", "anchor": "The_Axiom_of_Independence", "level": 1}, {"title": "The Axiom of Continuity", "anchor": "The_Axiom_of_Continuity", "level": 1}, {"title": "The VNM theorem", "anchor": "The_VNM_theorem", "level": 1}, {"title": "Doing without Continuity", "anchor": "Doing_without_Continuity", "level": 1}, {"title": "Next post: Dealing with time", "anchor": "Next_post__Dealing_with_time", "level": 1}, {"title": "\u00bb To the mathematical appendix", "anchor": "__To_the_mathematical_appendix", "level": 2}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "76 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3dmpfGgXZzB28JZdS", "RcvyJjPQwimAeapNg", "zJZvoiwydJ5zvzTHK", "f6ZLxEWaankRZ2Crv", "YCMfQoqqi2o9Tjwoa", "5J34FAKyEmqKaT7jt", "hN8Ld8YdqFsui2xgc", "bkSkRwo9SRYxJMiSY", "wqDRRx9RqwKLzWt7R", "n5ucT5ZbPdhfGNLtP", "tGhz4aKyNzXjvnWhX", "jiBFC7DcCrZjGmZnJ", "r5MSQ83gtbjWRBDWJ", "KcayfY2KHE9jmN5oL", "Kyc5dFDzBg4WccrbK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T01:11:20.015Z", "modifiedAt": null, "url": null, "title": "Math appendix for: \"Why you must maximize expected utility\"", "slug": "math-appendix-for-why-you-must-maximize-expected-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:31.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KcayfY2KHE9jmN5oL/math-appendix-for-why-you-must-maximize-expected-utility", "pageUrlRelative": "/posts/KcayfY2KHE9jmN5oL/math-appendix-for-why-you-must-maximize-expected-utility", "linkUrl": "https://www.lesswrong.com/posts/KcayfY2KHE9jmN5oL/math-appendix-for-why-you-must-maximize-expected-utility", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Math%20appendix%20for%3A%20%22Why%20you%20must%20maximize%20expected%20utility%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMath%20appendix%20for%3A%20%22Why%20you%20must%20maximize%20expected%20utility%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKcayfY2KHE9jmN5oL%2Fmath-appendix-for-why-you-must-maximize-expected-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Math%20appendix%20for%3A%20%22Why%20you%20must%20maximize%20expected%20utility%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKcayfY2KHE9jmN5oL%2Fmath-appendix-for-why-you-must-maximize-expected-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKcayfY2KHE9jmN5oL%2Fmath-appendix-for-why-you-must-maximize-expected-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1079, "htmlBody": "<p>This is a mathematical appendix to my post \"<a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Why you must maximize expected utility</a>\", giving precise statements and proofs of some results about von Neumann-Morgenstern utility theory without the Axiom of Continuity. I wish I had the time to make this post more easily readable, giving more intuition; the ideas are rather straight-forward and I hope they won't get lost in the line noise!</p>\n<p>The work here is my own (though closely based on the standard proof of the VNM theorem), but I don't expect the results to be new.</p>\n<p align=\"center\">*</p>\n<p>I represent preference relations as <a href=\"http://en.wikipedia.org/wiki/Total_preorder#Total_preorders\">total preorders</a> <img src=\"http://www.codecogs.com/png.latex?%5Cpreccurlyeq\" alt=\"\" /> on a <a href=\"http://en.wikipedia.org/wiki/Simplex#The_standard_simplex\">simplex</a> <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" />; define <img src=\"http://www.codecogs.com/png.latex?\\prec\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?\\sim\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?\\succcurlyeq\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" /> in the obvious ways (e.g., <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" /> iff both <img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\\preccurlyeq x\" alt=\"\" />, and <img src=\"http://www.codecogs.com/png.latex?x\\prec y\" alt=\"\" /> iff <img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq y\" alt=\"\" /> but <em>not</em> <img src=\"http://www.codecogs.com/png.latex?y\\preccurlyeq x\" alt=\"\" />). Write <img src=\"http://www.codecogs.com/png.latex?e^i\" alt=\"\" /> for the <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" />'th unit vector in <img src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^N\" alt=\"\" />.</p>\n<p>In the following, I will always assume that <img src=\"http://www.codecogs.com/png.latex?%5Cpreccurlyeq\" alt=\"\" /> <em>satisfies the independence axiom</em>: that is, for all <img src=\"http://www.codecogs.com/png.latex?x,y,z\\in\\Delta_N\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?p%5Cin(0,1]\" alt=\"\" />, we have&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\\prec y\" alt=\"\" /> if and only if&nbsp;<img src=\"http://www.codecogs.com/png.latex?px + (1-p)z \\prec py + (1-p)z\" alt=\"\" />. Note that the analogous statement with weak preferences follows from this: <img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq y\" alt=\"\" /> holds iff <img src=\"http://www.codecogs.com/png.latex?y\\not\\prec x\" alt=\"\" />, which by independence is equivalent to <img src=\"http://www.codecogs.com/png.latex?py + (1-p)z \\not\\prec px + (1-p)z\" alt=\"\" />, which is just <img src=\"http://www.codecogs.com/png.latex?px + (1-p)z \\preccurlyeq py + (1-p)z\" alt=\"\" />.</p>\n<p><strong>Lemma 1 </strong>(more of a good thing is always better)<strong>.</strong> <em>If </em><img src=\"http://www.codecogs.com/png.latex?x\\prec y\" alt=\"\" /><em> and <img src=\"http://www.codecogs.com/png.latex?0\\le p &lt; q \\le 1\" alt=\"\" />, </em><em>then <img src=\"http://www.codecogs.com/png.latex?(1-p)x + py\\prec (1-q)x + qy\" alt=\"\" />.</em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>Let <img src=\"http://www.codecogs.com/png.latex?r := q-p\" alt=\"\" />. Then, <img src=\"http://www.codecogs.com/png.latex?(1-p)x + py = \\big((1-q)x + py\\big) + rx\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?(1-q)x + qy = \\big((1-q)x + py\\big) + ry\" alt=\"\" />. Thus, the result follows from independence applied to <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" />,&nbsp;<img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?\\textstyle\\frac{1}{1-r}\\big((1-q)x + py\\big)\" alt=\"\" />, and <img src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" />.<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>\n<p><strong>Lemma 2.</strong> <em>If&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq y\\preccurlyeq z\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?x\\prec z\" alt=\"\" />, then there</em><em> is a unique <img src=\"http://www.codecogs.com/png.latex?p\\in[0,1]\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?(1-q)x + qz \\prec y\" alt=\"\" /> for <img src=\"http://www.codecogs.com/png.latex?q\\in[0,p)\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\\prec (1-q)x + qz\" alt=\"\" /> for <img src=\"http://www.codecogs.com/png.latex?q\\in(p,1]\" alt=\"\" />.</em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>Let <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" /> be the supremum of all <img src=\"http://www.codecogs.com/png.latex?r\\in[0,1]\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?(1-r)x + rz\\preccurlyeq y\" alt=\"\" /> (note that by assumption, this condition holds for <img src=\"http://www.codecogs.com/png.latex?r=0\" alt=\"\" />). Suppose that <img src=\"http://www.codecogs.com/png.latex?0\\le q&lt;p\" alt=\"\" />. Then there is an <img src=\"http://www.codecogs.com/png.latex?r\\in(q,p]\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?(1-r)x%20+%20rz%5Cpreccurlyeq%20y\" alt=\"\" />. By Lemma 1, we have <img src=\"http://www.codecogs.com/png.latex?(1-q)x + qz \\prec (1-r)x + rz\" alt=\"\" />, and the first assertion follows.</p>\n<p style=\"padding-left: 30px;\">Suppose now that <img src=\"http://www.codecogs.com/png.latex?p &lt; q \\le 1\" alt=\"\" />. Then by definition of&nbsp;<img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" />, we do <em><strong>not</strong></em> have <img src=\"http://www.codecogs.com/png.latex?(1-q)x%20+%20qz%5Cpreccurlyeq%20y\" alt=\"\" />, which means that we have <img src=\"http://www.codecogs.com/png.latex?(1-q)x + qz\\succ y\" alt=\"\" />, which was the second assertion.</p>\n<p style=\"padding-left: 30px;\">Finally, uniqueness is obvious, because if both <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?p'\" alt=\"\" /> satisfied the condition, we would have <img src=\"http://www.codecogs.com/png.latex?\\textstyle y \\prec \\big(1 - \\frac{p+p'}2\\big)x + \\frac{p+p'}2z \\prec y\" alt=\"\" />.<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>\n<p><strong>Definition 3.</strong> <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is <em>much better</em> than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, notation <img src=\"http://www.codecogs.com/png.latex?x\\succ_* y\" alt=\"\" /> or <img src=\"http://www.codecogs.com/png.latex?y\\prec_* x\" alt=\"\" />, if there are neighbourhoods <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" /> of <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?V\" alt=\"\" /> of <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> (in the relative topology of <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" />) such that we have <img src=\"http://www.codecogs.com/png.latex?x' \\succ y'\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x'\\in U\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\\in V\" alt=\"\" />. (In other words, the graph of <img src=\"http://www.codecogs.com/png.latex?\\succ_*\" alt=\"\" /> is the interior of the graph of <img src=\"http://www.codecogs.com/png.latex?\\succ\" alt=\"\" />.) Write&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq_* y\" alt=\"\" /> or <img src=\"http://www.codecogs.com/png.latex?y\\succcurlyeq_* x\" alt=\"\" /> when <img src=\"http://www.codecogs.com/png.latex?x\\nsucc_* y\" alt=\"\" /> (<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is <em>not much better</em> than <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />), and <img src=\"http://www.codecogs.com/png.latex?x\\sim_* y\" alt=\"\" /> (<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> is <em>about as good</em> as <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />) when both <img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq_* y\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?x\\succcurlyeq_* y\" alt=\"\" />.</p>\n<p><strong>Theorem 4</strong> (existence of a utility function). <em>There is a </em><img src=\"http://www.codecogs.com/png.latex?u\\in\\mathbb{R}^N\" alt=\"\" /><em> such that for all </em><img src=\"http://www.codecogs.com/png.latex?x,y\\in\\Delta_N\" alt=\"\" />,</p>\n<p align=\"center\"><em><img src=\"http://www.codecogs.com/png.latex?%5Csum_i%20x_i%5C,u_i%20%5C;%3C%5C;%20%5Csum_i%20y_i%5C,u_i%5C;%5C;%5Ciff%5C;%5C;%20x%5Cprec_*%20y%5C;%5C;%5Cimplies%5C;%5C;x%5Cprec%20y.\" alt=\"\" /></em></p>\n<p><em>Unless <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, there are&nbsp;<img src=\"http://www.codecogs.com/png.latex?i,j\\in\\{1,\\dotsc,N\\}\" alt=\"\" /> such that <img src=\"http://www.codecogs.com/png.latex?u_i\\neq u_j\" alt=\"\" />.<br /></em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>Let <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" /> be a worst and <img src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" /> a best outcome, i.e. let <img src=\"http://www.codecogs.com/png.latex?i,j\\in\\{1,\\dotsc,N\\}\" alt=\"\" /> be such that <img src=\"http://www.codecogs.com/png.latex?e^i\\preccurlyeq e^k\\preccurlyeq e^j\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?k\\in\\{1,\\dotsc,N\\}\" alt=\"\" />. If <img src=\"http://www.codecogs.com/png.latex?e^i\\sim e^j\" alt=\"\" />, then&nbsp;<img src=\"http://www.codecogs.com/png.latex?e^i \\sim e^k\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\" />, and by repeated applications of independence we get <img src=\"http://www.codecogs.com/png.latex?x\\sim e^i\\sim y\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x,y\\in\\Delta_N\" alt=\"\" />, and therefore <img src=\"http://www.codecogs.com/png.latex?x\\sim_* y\" alt=\"\" /> again for all <img src=\"http://www.codecogs.com/png.latex?x,y\\in\\Delta_N\" alt=\"\" />, and we can simply choose <img src=\"http://www.codecogs.com/png.latex?u=0\" alt=\"\" />.</p>\n<p style=\"padding-left: 30px;\">Thus, suppose that <img src=\"http://www.codecogs.com/png.latex?e^i\\prec e^j\" alt=\"\" />. In this case, let <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> be such that for every <img src=\"http://www.codecogs.com/png.latex?k\\in\\{1,\\dotsc,N\\}\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?u_k\" alt=\"\" /> equals the unique <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" /> provided by Lemma 2 applied to <img src=\"http://www.codecogs.com/png.latex?e^i\\preccurlyeq e^k\\preccurlyeq e^j\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?e^i\\prec e^j\" alt=\"\" />. Because of Lemma 1, <img src=\"http://www.codecogs.com/png.latex?u_i = 0 \\neq 1 = u_j\" alt=\"\" />. Let <img src=\"http://www.codecogs.com/png.latex?f(r) := (1-r)e^i + re^j\" alt=\"\" />.</p>\n<p style=\"padding-left: 30px;\">We first show that <img src=\"http://www.codecogs.com/png.latex?\\textstyle p := \\sum_k x_k\\,u_k &lt; \\sum_k y_k\\,u_k =: q\" alt=\"\" /> implies <img src=\"http://www.codecogs.com/png.latex?x\\prec y\" alt=\"\" />. For every <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\" />, we either have <img src=\"http://www.codecogs.com/png.latex?u_k%20%3C%201\" alt=\"\" />, in which case by Lemma 2 we have <img src=\"http://www.codecogs.com/png.latex?e%5Ek%20%5Cprec%20f%28u_k%20+%20%5Cepsilon_k%29\" alt=\"\" /> for arbitrarily small <img src=\"http://www.codecogs.com/png.latex?\\epsilon_k &gt; 0\" alt=\"\" />, or we have <img src=\"http://www.codecogs.com/png.latex?u_k = 1\" alt=\"\" />, in which case we set&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\epsilon_k := 0\" alt=\"\" /> and find <img src=\"http://www.codecogs.com/png.latex?e^k\\preccurlyeq e^j = f(u_k + \\epsilon_k)\" alt=\"\" />. Set <img src=\"http://www.codecogs.com/png.latex?\\textstyle \\epsilon := \\sum_k x_k\\,\\epsilon_k\" alt=\"\" />. Now, by independence applied <img src=\"http://www.codecogs.com/png.latex?N-1\" alt=\"\" /> times, we have <img src=\"http://www.codecogs.com/png.latex?%5Ctextstyle%20x%20=%20%5Csum_k%20x_k\\,e%5Ek%20%5Cpreccurlyeq \\sum_k x_k f(u_k + \\epsilon_k) = f(p+\\epsilon)\" alt=\"\" />; analogously, we obtain <img src=\"http://www.codecogs.com/png.latex?y \\succcurlyeq f(q-\\delta)\" alt=\"\" /> for arbitrarily small <img src=\"http://www.codecogs.com/png.latex?\\delta &gt; 0\" alt=\"\" />. Thus, using <img src=\"http://www.codecogs.com/png.latex?p&lt;q\" alt=\"\" /> and Lemma 1, <img src=\"http://www.codecogs.com/png.latex?x\\preccurlyeq f(p+\\epsilon)\\prec f(q-\\delta)\\preccurlyeq y\" alt=\"\" /> and therefore&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\\prec y\" alt=\"\" /> as claimed. Now note that if <img src=\"http://www.codecogs.com/png.latex?\\textstyle%5Csum_k%20x_k%5C,u_k%20%3C%20%5Csum_k%20y_k%5C,u_k\" alt=\"\" />, then this continues to hold for <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\" /> in a sufficiently small neighbourhood of <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" />, and therefore we have <img src=\"http://www.codecogs.com/png.latex?x\\prec_* y\" alt=\"\" />.</p>\n<p style=\"padding-left: 30px;\">Now suppose that <img src=\"http://www.codecogs.com/png.latex?\\textstyle \\sum_k x_k\\,u_k \\ge \\sum_k y_k\\,u_k\" alt=\"\" />. Since we have&nbsp;<img src=\"http://www.codecogs.com/png.latex?u_i = 0\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?u_j = 1\" alt=\"\" />, we can find points <img src=\"http://www.codecogs.com/png.latex?x'\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y'\" alt=\"\" /> arbitrarily close to <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?y\" alt=\"\" /> such that the inequality becomes strict (either the left-hand side is smaller than one and we can increase it, or the right-hand side is greater than zero and we can decrease it, or else the inequality is already strict). Then, <img src=\"http://www.codecogs.com/png.latex?x'\\succ y'\" alt=\"\" /> by the preceding paragraph. But this implies that <img src=\"http://www.codecogs.com/png.latex?x\\not\\prec_* y\" alt=\"\" />, which completes the proof.<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>\n<p><strong>Corollary 5.</strong> <em><img src=\"http://www.codecogs.com/png.latex?\\preccurlyeq_*\" alt=\"\" /> is a preference relation (i.e., a total preorder) that satisfies independence and the von Neumann-Morgenstern continuity axiom.<br /></em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>It is well-known (and straightforward to check) that this follows from the assertion of the theorem.<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>\n<p><strong>Corollary 6.</strong> <em><img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> is unique up to affine transformations.<br /></em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>Since&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> is a VNM utility function for <img src=\"http://www.codecogs.com/png.latex?\\preccurlyeq_*\" alt=\"\" />, this follows from the analogous result for that case.<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>\n<p><strong>Corollary 7.</strong> <em>Unless <img src=\"http://www.codecogs.com/png.latex?x\\sim y\" alt=\"\" /> for all <img src=\"http://www.codecogs.com/png.latex?x,y\\in\\Delta_N\" alt=\"\" />, for all <img src=\"http://www.codecogs.com/png.latex?r\\in\\mathbb{R}\" alt=\"\" /> the set <img src=\"http://www.codecogs.com/png.latex?\\textstyle \\{x\\in\\Delta_N : \\sum_i x_i\\,u_i = r\\}\" alt=\"\" /> has lower dimension than <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" /> (i.e., it is the intersection of <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" /> with a lower-dimensional subspace of <img src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^N\" alt=\"\" />).</em></p>\n<p style=\"padding-left: 30px;\"><em>Proof. </em>First, note that the assumption implies that <img src=\"http://www.codecogs.com/png.latex?N\\ge 2\" alt=\"\" />. Let <img src=\"http://www.codecogs.com/png.latex?v\\in\\mathbb{R}^N\" alt=\"\" /> be given by <img src=\"http://www.codecogs.com/png.latex?v_i = 1\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?\\forall i\" alt=\"\" />, and note that <img src=\"http://www.codecogs.com/png.latex?\\Delta_N\" alt=\"\" /> is the intersection of the hyperplane <img src=\"http://www.codecogs.com/png.latex?A := \\{x\\in\\mathbb{R}^N : x\\cdot v = 1\\}\" alt=\"\" /> with the closed positive orthant <img src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^N_+\" alt=\"\" />. By the theorem, <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> is not parallel to <img src=\"http://www.codecogs.com/png.latex?v\" alt=\"\" />, so the hyperplane <img src=\"http://www.codecogs.com/png.latex?B_r := \\{x\\in\\mathbb{R}^N : x\\cdot u = r\\}\" alt=\"\" /> is not parallel to <img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" />. It follows that <img src=\"http://www.codecogs.com/png.latex?A\\cap B_r\" alt=\"\" /> has dimension <img src=\"http://www.codecogs.com/png.latex?N-2\" alt=\"\" />, and therefore <img src=\"http://www.codecogs.com/png.latex?\\textstyle\\{x\\in\\Delta_N : \\sum_i x_i\\,u_i = r\\} \\;=\\; A\\cap B_r\\cap\\mathbb{R}^N_+\" alt=\"\" /> can have at most this dimension. (It can have smaller dimension or be the empty set if <img src=\"http://www.codecogs.com/png.latex?A\\cap B_r\" alt=\"\" /> only touches or lies entirely outside the positive orthant.)<span style=\"display: block; float: right;\"><img src=\"http://www.codecogs.com/png.latex?%5Csquare\" alt=\"\" /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KcayfY2KHE9jmN5oL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 1.0567803787147274e-06, "legacy": true, "legacyId": "20376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["F46jPraqp258q67nE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T07:53:29.193Z", "modifiedAt": null, "url": null, "title": "Noisy Reasoners", "slug": "noisy-reasoners", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h6mNG2nC56sP88w3D/noisy-reasoners", "pageUrlRelative": "/posts/h6mNG2nC56sP88w3D/noisy-reasoners", "linkUrl": "https://www.lesswrong.com/posts/h6mNG2nC56sP88w3D/noisy-reasoners", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Noisy%20Reasoners&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANoisy%20Reasoners%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6mNG2nC56sP88w3D%2Fnoisy-reasoners%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Noisy%20Reasoners%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6mNG2nC56sP88w3D%2Fnoisy-reasoners", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6mNG2nC56sP88w3D%2Fnoisy-reasoners", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>One of the more interesting papers at this year's <a href=\"http://agi-conference.org/2012/\">AGI-12 conference</a> was Finton Costello's <a href=\"http://www.mindmakers.org/attachments/download/250/paper_61.pdf\">Noisy Reasoners</a>. I think it will be of interest to Less Wrong:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>This paper examines reasoning under uncertainty in the case&nbsp;where the AI reasoning mechanism is itself subject to random error or&nbsp;noise in its own processes. The main result is a demonstration that systematic, directed biases naturally arise if there is random noise in a&nbsp;reasoning process that follows the normative rules of probability theory.&nbsp;A number of reliable errors in human reasoning under uncertainty can&nbsp;be explained as the consequence of these systematic biases due to noise.&nbsp;Since AI systems are subject to noise, we should expect to see the same&nbsp;biases and errors in AI reasoning systems based on probability theory.</p>\n<p>&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h6mNG2nC56sP88w3D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 1.057013358242212e-06, "legacy": true, "legacyId": "20631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T09:00:48.804Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Friendly Projects vs. Products", "slug": "seq-rerun-friendly-projects-vs-products", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qDw9RRSNXG97WCwbf/seq-rerun-friendly-projects-vs-products", "pageUrlRelative": "/posts/qDw9RRSNXG97WCwbf/seq-rerun-friendly-projects-vs-products", "linkUrl": "https://www.lesswrong.com/posts/qDw9RRSNXG97WCwbf/seq-rerun-friendly-projects-vs-products", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Friendly%20Projects%20vs.%20Products&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Friendly%20Projects%20vs.%20Products%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDw9RRSNXG97WCwbf%2Fseq-rerun-friendly-projects-vs-products%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Friendly%20Projects%20vs.%20Products%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDw9RRSNXG97WCwbf%2Fseq-rerun-friendly-projects-vs-products", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDw9RRSNXG97WCwbf%2Fseq-rerun-friendly-projects-vs-products", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/friendly-projec.html\">Friendly Projects vs. Products</a> was originally published on December 5, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Friendliness can take two forms. Friendliness can refer to whether or not your project will only benefit its creators, or whether that project will benefit competitors as well, or it can refer to how the project will treat its creators.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fwb/seq_rerun_sustained_strong_recursion/\">Sustained Strong Recursion</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qDw9RRSNXG97WCwbf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0570523716011227e-06, "legacy": true, "legacyId": "20634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cs6WdZCre3nmmyuYJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T11:27:35.739Z", "modifiedAt": null, "url": null, "title": "Negative karma is a bad design", "slug": "negative-karma-is-a-bad-design", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.207Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sanxiyn", "createdAt": "2010-02-01T02:59:28.963Z", "isAdmin": false, "displayName": "sanxiyn"}, "userId": "zevyuFg7BE9dBBFMq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BGaAofZu5FsTiiDan/negative-karma-is-a-bad-design", "pageUrlRelative": "/posts/BGaAofZu5FsTiiDan/negative-karma-is-a-bad-design", "linkUrl": "https://www.lesswrong.com/posts/BGaAofZu5FsTiiDan/negative-karma-is-a-bad-design", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Negative%20karma%20is%20a%20bad%20design&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANegative%20karma%20is%20a%20bad%20design%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGaAofZu5FsTiiDan%2Fnegative-karma-is-a-bad-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Negative%20karma%20is%20a%20bad%20design%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGaAofZu5FsTiiDan%2Fnegative-karma-is-a-bad-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGaAofZu5FsTiiDan%2Fnegative-karma-is-a-bad-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>It came to my attention that when you receive downvotes for your comments, your karma goes negative and you need to \"pay back\" to be able to post to Discussion or to Main.</p>\n<p>Since new users start with zero karma, having negative karma seems to just encourage those with negative karma to create a new account. We don't want to encourage people to create superfluous accounts, do we? Therefore I think LessWrong codebase should be patched so that karma does not go below zero even with lots of downvotes.</p>\n<p>What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BGaAofZu5FsTiiDan", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": -13, "extendedScore": null, "score": -2.8e-05, "legacy": true, "legacyId": "20638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T16:09:58.698Z", "modifiedAt": null, "url": null, "title": "Why (anthropic) probability isn't enough", "slug": "why-anthropic-probability-isn-t-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eso8pwW6pZRxZRwGw/why-anthropic-probability-isn-t-enough", "pageUrlRelative": "/posts/eso8pwW6pZRxZRwGw/why-anthropic-probability-isn-t-enough", "linkUrl": "https://www.lesswrong.com/posts/eso8pwW6pZRxZRwGw/why-anthropic-probability-isn-t-enough", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20(anthropic)%20probability%20isn't%20enough&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20(anthropic)%20probability%20isn't%20enough%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feso8pwW6pZRxZRwGw%2Fwhy-anthropic-probability-isn-t-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20(anthropic)%20probability%20isn't%20enough%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feso8pwW6pZRxZRwGw%2Fwhy-anthropic-probability-isn-t-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feso8pwW6pZRxZRwGw%2Fwhy-anthropic-probability-isn-t-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p><a href=\"http://www.fhi.ox.ac.uk/anthropics-why-probability-isnt-enough.pdf\">A technical report</a> of the Future of Humanity Institute (authored by me), on why anthropic probability isn't enough to reach decisions in anthropic situations. You also have to choose your decision theory, and take into account your altruism towards your copies. And these components can co-vary while leaving your ultimate decision the same - typically, EDT agents using SSA will reach the same decisions as CDT agents using SIA, and&nbsp;altruistic&nbsp;causal agents may decide the same way as selfish evidential agents.</p>\n<p>&nbsp;</p>\n<h2><a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0004/27940/2012-2.pdf\">Anthropics: why probability isn't enough</a></h2>\n<p style=\"padding-left: 30px;\">This paper argues that the current treatment of anthropic and self-locating&nbsp;problems over-emphasises the importance of anthropic probabilities, and ignores&nbsp;other relevant and important factors, such as whether the various copies of the&nbsp;agents in question consider that they are acting in a linked fashion and whether&nbsp;they are mutually altruistic towards each other. These issues, generally irrelevant&nbsp;for non-anthropic problems, come to the forefront in anthropic situations and are&nbsp;at least as important as the anthropic probabilities: indeed they can erase the&nbsp;difference between different theories of anthropic probability, or increase their&nbsp;divergence. These help to reinterpret the decisions, rather than probabilities, as the&nbsp;fundamental objects of interest in anthropic problems.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eso8pwW6pZRxZRwGw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 33, "extendedScore": null, "score": 1.0573011164923043e-06, "legacy": true, "legacyId": "20639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T19:52:53.861Z", "modifiedAt": null, "url": null, "title": "Meetup : Cleveland Ohio Meetup", "slug": "meetup-cleveland-ohio-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:34.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raelifin", "createdAt": "2011-12-25T18:17:26.827Z", "isAdmin": false, "displayName": "Raelifin"}, "userId": "L5eSHjQcjZWhf39fG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WPePenHxPgJbHrtA4/meetup-cleveland-ohio-meetup", "pageUrlRelative": "/posts/WPePenHxPgJbHrtA4/meetup-cleveland-ohio-meetup", "linkUrl": "https://www.lesswrong.com/posts/WPePenHxPgJbHrtA4/meetup-cleveland-ohio-meetup", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cleveland%20Ohio%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cleveland%20Ohio%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWPePenHxPgJbHrtA4%2Fmeetup-cleveland-ohio-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cleveland%20Ohio%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWPePenHxPgJbHrtA4%2Fmeetup-cleveland-ohio-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWPePenHxPgJbHrtA4%2Fmeetup-cleveland-ohio-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gy'>Cleveland Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 December 2012 02:52:45PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cleveland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The LW group for Cleveland Ohio will be meeting at Gypsy Bean Coffee (65th and Detroit) on the 22nd or 23rd. (Probably the 23rd, but it's still up in the air.)</p>\n\n<p>If you're interested in coming, or just interested in our discussion, please subscribe to the mailing list: https://groups.google.com/forum/?fromgroups=#!forum/less-wrong-cleveland</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gy'>Cleveland Ohio Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WPePenHxPgJbHrtA4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "20640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup\">Discussion article for the meetup : <a href=\"/meetups/gy\">Cleveland Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 December 2012 02:52:45PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cleveland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The LW group for Cleveland Ohio will be meeting at Gypsy Bean Coffee (65th and Detroit) on the 22nd or 23rd. (Probably the 23rd, but it's still up in the air.)</p>\n\n<p>If you're interested in coming, or just interested in our discussion, please subscribe to the mailing list: https://groups.google.com/forum/?fromgroups=#!forum/less-wrong-cleveland</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/gy\">Cleveland Ohio Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cleveland Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cleveland Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T20:10:33.800Z", "modifiedAt": null, "url": null, "title": "Meetup :  Paderborn Meetup, December 19th", "slug": "meetup-paderborn-meetup-december-19th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.813Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5NHtkyevpuXrYz3Mc/meetup-paderborn-meetup-december-19th", "pageUrlRelative": "/posts/5NHtkyevpuXrYz3Mc/meetup-paderborn-meetup-december-19th", "linkUrl": "https://www.lesswrong.com/posts/5NHtkyevpuXrYz3Mc/meetup-paderborn-meetup-december-19th", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Paderborn%20Meetup%2C%20December%2019th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Paderborn%20Meetup%2C%20December%2019th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NHtkyevpuXrYz3Mc%2Fmeetup-paderborn-meetup-december-19th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Paderborn%20Meetup%2C%20December%2019th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NHtkyevpuXrYz3Mc%2Fmeetup-paderborn-meetup-december-19th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NHtkyevpuXrYz3Mc%2Fmeetup-paderborn-meetup-december-19th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gz'> Paderborn Meetup, December 19th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 December 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced\nEpistemology 101 for Beginners\", how to beat psychological reactance, and bioethics.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gz'> Paderborn Meetup, December 19th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5NHtkyevpuXrYz3Mc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0574406054255873e-06, "legacy": true, "legacyId": "20641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Paderborn_Meetup__December_19th\">Discussion article for the meetup : <a href=\"/meetups/gz\"> Paderborn Meetup, December 19th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 December 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced\nEpistemology 101 for Beginners\", how to beat psychological reactance, and bioethics.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Paderborn_Meetup__December_19th1\">Discussion article for the meetup : <a href=\"/meetups/gz\"> Paderborn Meetup, December 19th</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Paderborn Meetup, December 19th", "anchor": "Discussion_article_for_the_meetup____Paderborn_Meetup__December_19th", "level": 1}, {"title": "Discussion article for the meetup :  Paderborn Meetup, December 19th", "anchor": "Discussion_article_for_the_meetup____Paderborn_Meetup__December_19th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T20:25:40.330Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 21-23", "slug": "meetup-durham-hpmor-discussion-chapters-21-23", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xaHvEQkoDAb9DHu8e/meetup-durham-hpmor-discussion-chapters-21-23", "pageUrlRelative": "/posts/xaHvEQkoDAb9DHu8e/meetup-durham-hpmor-discussion-chapters-21-23", "linkUrl": "https://www.lesswrong.com/posts/xaHvEQkoDAb9DHu8e/meetup-durham-hpmor-discussion-chapters-21-23", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2021-23&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2021-23%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxaHvEQkoDAb9DHu8e%2Fmeetup-durham-hpmor-discussion-chapters-21-23%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2021-23%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxaHvEQkoDAb9DHu8e%2Fmeetup-durham-hpmor-discussion-chapters-21-23", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxaHvEQkoDAb9DHu8e%2Fmeetup-durham-hpmor-discussion-chapters-21-23", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h0'>Durham HPMoR Discussion, chapters 21-23</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet and discuss Harry Potter and the Methods of Rationality, chapters 21-23 (about 53 pages). There will likely be Zendo and/or continued off-topic discussion afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h0'>Durham HPMoR Discussion, chapters 21-23</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xaHvEQkoDAb9DHu8e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0574493664536945e-06, "legacy": true, "legacyId": "20642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_21_23\">Discussion article for the meetup : <a href=\"/meetups/h0\">Durham HPMoR Discussion, chapters 21-23</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet and discuss Harry Potter and the Methods of Rationality, chapters 21-23 (about 53 pages). There will likely be Zendo and/or continued off-topic discussion afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_21_231\">Discussion article for the meetup : <a href=\"/meetups/h0\">Durham HPMoR Discussion, chapters 21-23</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 21-23", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_21_23", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 21-23", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_21_231", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-13T23:32:45.555Z", "modifiedAt": null, "url": null, "title": "Bad news for uploading", "slug": "bad-news-for-uploading", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.001Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kcEdqZqF98eCaZpfQ/bad-news-for-uploading", "pageUrlRelative": "/posts/kcEdqZqF98eCaZpfQ/bad-news-for-uploading", "linkUrl": "https://www.lesswrong.com/posts/kcEdqZqF98eCaZpfQ/bad-news-for-uploading", "postedAtFormatted": "Thursday, December 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bad%20news%20for%20uploading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABad%20news%20for%20uploading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkcEdqZqF98eCaZpfQ%2Fbad-news-for-uploading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bad%20news%20for%20uploading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkcEdqZqF98eCaZpfQ%2Fbad-news-for-uploading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkcEdqZqF98eCaZpfQ%2Fbad-news-for-uploading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>Recently, the Blue Brain Project published a paper arguing that human neurons don't form synapses at locations determined by learning, but just wherever they bump into each other.&nbsp; See video and article <a href=\"http://actu.epfl.ch/news/blue-brain-project-accurately-predicts-connections/\">here</a>.</p>\n<p>For those people hoping to upload their brains by mapping out and virtually duplicating all the synapses&mdash;this means that won't work.&nbsp; The synapse locations do not differ from human to human in any useful way.&nbsp; Learning must be encoded in some modulation of each synapse's function.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "ksdiAMKfgSyEeKMo6": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kcEdqZqF98eCaZpfQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 19, "extendedScore": null, "score": 1.0575578617921191e-06, "legacy": true, "legacyId": "20643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-14T04:03:23.595Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is That Your True Rejection?", "slug": "seq-rerun-is-that-your-true-rejection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q3CewTkyZi9yxEAAt/seq-rerun-is-that-your-true-rejection", "pageUrlRelative": "/posts/Q3CewTkyZi9yxEAAt/seq-rerun-is-that-your-true-rejection", "linkUrl": "https://www.lesswrong.com/posts/Q3CewTkyZi9yxEAAt/seq-rerun-is-that-your-true-rejection", "postedAtFormatted": "Friday, December 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20That%20Your%20True%20Rejection%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20That%20Your%20True%20Rejection%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3CewTkyZi9yxEAAt%2Fseq-rerun-is-that-your-true-rejection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20That%20Your%20True%20Rejection%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3CewTkyZi9yxEAAt%2Fseq-rerun-is-that-your-true-rejection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3CewTkyZi9yxEAAt%2Fseq-rerun-is-that-your-true-rejection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"/lw/wj/is_that_your_true_rejection/\">Is That Your True Rejection?</a> was originally published on 06 December 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Is_That_Your_True_Rejection.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People's stated reason for a rejection may not be the same as the actual reason for that rejection.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/fx6/seq_rerun_friendly_projects_vs_products/\">Friendly Projects vs. Products</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q3CewTkyZi9yxEAAt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0577148424412187e-06, "legacy": true, "legacyId": "20650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TGux5Fhcd7GmTfNGC", "qDw9RRSNXG97WCwbf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-14T15:49:51.580Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin; Atlanta; Cleveland; Montreal; Sofia, Bulgaria; Toronto; Vancouver; Washington DC", "slug": "weekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XujsYztoM2FQYLG9G/weekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "pageUrlRelative": "/posts/XujsYztoM2FQYLG9G/weekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "linkUrl": "https://www.lesswrong.com/posts/XujsYztoM2FQYLG9G/weekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "postedAtFormatted": "Friday, December 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%3B%20Atlanta%3B%20Cleveland%3B%20Montreal%3B%20Sofia%2C%20Bulgaria%3B%20Toronto%3B%20Vancouver%3B%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%3B%20Atlanta%3B%20Cleveland%3B%20Montreal%3B%20Sofia%2C%20Bulgaria%3B%20Toronto%3B%20Vancouver%3B%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXujsYztoM2FQYLG9G%2Fweekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%3B%20Atlanta%3B%20Cleveland%3B%20Montreal%3B%20Sofia%2C%20Bulgaria%3B%20Toronto%3B%20Vancouver%3B%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXujsYztoM2FQYLG9G%2Fweekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXujsYztoM2FQYLG9G%2Fweekly-lw-meetups-austin-atlanta-cleveland-montreal-sofia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 523, "htmlBody": "<p><strong>This summary was posted to LW Main on Dec 7th. The following week's summary is <a href=\"/lw/fxs/weekly_lw_meetups_austin_brussels_durham_london/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ge\">Moscow: Applied Rationality and Cognitive Biases:&nbsp;<span class=\"date\">08 December 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/gn\">Vancouver!:&nbsp;<span class=\"date\">09 December 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/gn\"></a><a href=\"/meetups/go\">Atlanta LessWrong Meetups REBOOT:&nbsp;<span class=\"date\">09 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/gq\">Washington DC meetup- new sequence?:&nbsp;<span class=\"date\">09 December 2012 11:55PM</span></a></li>\n<li><a href=\"/meetups/gp\">Montreal LessWrong Meetup - More Biases and Biased Board Gaming:&nbsp;<span class=\"date\">10 December 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/gc\">Brussels meetup:&nbsp;<span class=\"date\">15 December 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/gs\">16/12 London Meetup:&nbsp;<span class=\"date\">16 December 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">08 December 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/gt\">Less Wrong: Cleveland:&nbsp;<span class=\"date\">09 December 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/gu\">Toronto THINK:&nbsp;<span class=\"date\">12 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XujsYztoM2FQYLG9G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0581248212737598e-06, "legacy": true, "legacyId": "20523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MjqNFGwCpoXkvxuAC", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-15T07:24:04.926Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Shared AI Wins", "slug": "seq-rerun-shared-ai-wins", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iAsLvBk9adNo3Nrds/seq-rerun-shared-ai-wins", "pageUrlRelative": "/posts/iAsLvBk9adNo3Nrds/seq-rerun-shared-ai-wins", "linkUrl": "https://www.lesswrong.com/posts/iAsLvBk9adNo3Nrds/seq-rerun-shared-ai-wins", "postedAtFormatted": "Saturday, December 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Shared%20AI%20Wins&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Shared%20AI%20Wins%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiAsLvBk9adNo3Nrds%2Fseq-rerun-shared-ai-wins%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Shared%20AI%20Wins%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiAsLvBk9adNo3Nrds%2Fseq-rerun-shared-ai-wins", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiAsLvBk9adNo3Nrds%2Fseq-rerun-shared-ai-wins", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/shared-ai-wins.html\">Shared AI Wins</a> was originally published on December 6, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>AIs do have enormous difficulties in communicating. That's why decreasing those difficulties is important. Collections of AIs that are able to share data, like ems, are more likely to succeed at making new developments.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/fxm/seq_rerun_is_that_your_true_rejection/\">Is That Your True Rejection?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iAsLvBk9adNo3Nrds", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 1.0586674040097594e-06, "legacy": true, "legacyId": "20666", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q3CewTkyZi9yxEAAt", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-15T17:04:24.308Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnuPpM6hvKq4Pq4mB/meetup-brussels-meetup-1", "pageUrlRelative": "/posts/YnuPpM6hvKq4Pq4mB/meetup-brussels-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/YnuPpM6hvKq4Pq4mB/meetup-brussels-meetup-1", "postedAtFormatted": "Saturday, December 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnuPpM6hvKq4Pq4mB%2Fmeetup-brussels-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnuPpM6hvKq4Pq4mB%2Fmeetup-brussels-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnuPpM6hvKq4Pq4mB%2Fmeetup-brussels-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h1'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 January 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h1'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnuPpM6hvKq4Pq4mB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0590046951392835e-06, "legacy": true, "legacyId": "20670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/h1\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 January 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/h1\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-15T17:18:30.297Z", "modifiedAt": null, "url": null, "title": "Meetup : DC meetup Dec 16", "slug": "meetup-dc-meetup-dec-16", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "W55i9XXdye9ETAyu5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zA995jom7xcFoPGay/meetup-dc-meetup-dec-16", "pageUrlRelative": "/posts/zA995jom7xcFoPGay/meetup-dc-meetup-dec-16", "linkUrl": "https://www.lesswrong.com/posts/zA995jom7xcFoPGay/meetup-dc-meetup-dec-16", "postedAtFormatted": "Saturday, December 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20meetup%20Dec%2016&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20meetup%20Dec%2016%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA995jom7xcFoPGay%2Fmeetup-dc-meetup-dec-16%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20meetup%20Dec%2016%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA995jom7xcFoPGay%2Fmeetup-dc-meetup-dec-16", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA995jom7xcFoPGay%2Fmeetup-dc-meetup-dec-16", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h2'>DC meetup Dec 16</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 December 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The current plan is to meet at the Gallery courtyard, and walk to the US Botanic Garden to see the <a href=\"http://www.usbg.gov/exhibits\" rel=\"nofollow\">Season&#39;s Greenings</a> display. Visiting a museum or two afterwards is possible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h2'>DC meetup Dec 16</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zA995jom7xcFoPGay", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0590128924803406e-06, "legacy": true, "legacyId": "20671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_meetup_Dec_16\">Discussion article for the meetup : <a href=\"/meetups/h2\">DC meetup Dec 16</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 December 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The current plan is to meet at the Gallery courtyard, and walk to the US Botanic Garden to see the <a href=\"http://www.usbg.gov/exhibits\" rel=\"nofollow\">Season's Greenings</a> display. Visiting a museum or two afterwards is possible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_meetup_Dec_161\">Discussion article for the meetup : <a href=\"/meetups/h2\">DC meetup Dec 16</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC meetup Dec 16", "anchor": "Discussion_article_for_the_meetup___DC_meetup_Dec_16", "level": 1}, {"title": "Discussion article for the meetup : DC meetup Dec 16", "anchor": "Discussion_article_for_the_meetup___DC_meetup_Dec_161", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-15T23:50:25.709Z", "modifiedAt": null, "url": null, "title": "[Link] New prize on causality in statistics education", "slug": "link-new-prize-on-causality-in-statistics-education", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LbymJ5RqKTwmYPZp6/link-new-prize-on-causality-in-statistics-education", "pageUrlRelative": "/posts/LbymJ5RqKTwmYPZp6/link-new-prize-on-causality-in-statistics-education", "linkUrl": "https://www.lesswrong.com/posts/LbymJ5RqKTwmYPZp6/link-new-prize-on-causality-in-statistics-education", "postedAtFormatted": "Saturday, December 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20New%20prize%20on%20causality%20in%20statistics%20education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20New%20prize%20on%20causality%20in%20statistics%20education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbymJ5RqKTwmYPZp6%2Flink-new-prize-on-causality-in-statistics-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20New%20prize%20on%20causality%20in%20statistics%20education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbymJ5RqKTwmYPZp6%2Flink-new-prize-on-causality-in-statistics-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbymJ5RqKTwmYPZp6%2Flink-new-prize-on-causality-in-statistics-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>\"[Judea]&nbsp;Pearl is setting up a contest to help advance the teaching of causal inference in introductory statistics courses\" (<a href=\"http://andrewgelman.com/2012/12/new-prize-on-causality-in-statstistics-education/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+StatisticalModelingCausalInferenceAndSocialScience+%28Statistical+Modeling%2C+Causal+Inference%2C+and+Social+Science%29\">link</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LbymJ5RqKTwmYPZp6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.0592407941239765e-06, "legacy": true, "legacyId": "20674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-16T05:35:21.975Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Artificial Mysterious Intelligence", "slug": "seq-rerun-artificial-mysterious-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fGGBth2PG6duQBNNB/seq-rerun-artificial-mysterious-intelligence", "pageUrlRelative": "/posts/fGGBth2PG6duQBNNB/seq-rerun-artificial-mysterious-intelligence", "linkUrl": "https://www.lesswrong.com/posts/fGGBth2PG6duQBNNB/seq-rerun-artificial-mysterious-intelligence", "postedAtFormatted": "Sunday, December 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Artificial%20Mysterious%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Artificial%20Mysterious%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGGBth2PG6duQBNNB%2Fseq-rerun-artificial-mysterious-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Artificial%20Mysterious%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGGBth2PG6duQBNNB%2Fseq-rerun-artificial-mysterious-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGGBth2PG6duQBNNB%2Fseq-rerun-artificial-mysterious-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/wk/artificial_mysterious_intelligence/\">Artificial Mysterious Intelligence</a> was originally published on 07 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Artificial_Mysterious_Intelligence\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Attempting to create an intelligence without actually understanding what intelligence is, is a common failure mode. If you want to make actual progress, you need to truly understand what it is that you are trying to make.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fy2/seq_rerun_shared_ai_wins/\">Shared AI Wins</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fGGBth2PG6duQBNNB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0594414455959189e-06, "legacy": true, "legacyId": "20676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fKofLyepu446zRgPP", "iAsLvBk9adNo3Nrds", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-16T06:26:14.512Z", "modifiedAt": null, "url": null, "title": "Open Thread, December 16-31, 2012", "slug": "open-thread-december-16-31-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:26.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tigtWNi3vS9pAyJmB/open-thread-december-16-31-2012", "pageUrlRelative": "/posts/tigtWNi3vS9pAyJmB/open-thread-december-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/tigtWNi3vS9pAyJmB/open-thread-december-16-31-2012", "postedAtFormatted": "Sunday, December 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20December%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20December%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtigtWNi3vS9pAyJmB%2Fopen-thread-december-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20December%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtigtWNi3vS9pAyJmB%2Fopen-thread-december-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtigtWNi3vS9pAyJmB%2Fopen-thread-december-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tigtWNi3vS9pAyJmB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0594710457796405e-06, "legacy": true, "legacyId": "20677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 160, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-16T19:08:36.565Z", "modifiedAt": null, "url": null, "title": "Consistence of reciprocity?", "slug": "consistence-of-reciprocity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:37.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yttrium", "createdAt": "2011-11-26T23:30:29.876Z", "isAdmin": false, "displayName": "yttrium"}, "userId": "BBzE4abSkhSfkQ89D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MQo3dPWhGpgpY8wsf/consistence-of-reciprocity", "pageUrlRelative": "/posts/MQo3dPWhGpgpY8wsf/consistence-of-reciprocity", "linkUrl": "https://www.lesswrong.com/posts/MQo3dPWhGpgpY8wsf/consistence-of-reciprocity", "postedAtFormatted": "Sunday, December 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consistence%20of%20reciprocity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsistence%20of%20reciprocity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQo3dPWhGpgpY8wsf%2Fconsistence-of-reciprocity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consistence%20of%20reciprocity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQo3dPWhGpgpY8wsf%2Fconsistence-of-reciprocity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQo3dPWhGpgpY8wsf%2Fconsistence-of-reciprocity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>Many people see themselves in various groups (member of the population of their home country, or their social network), and feel justified in caring more about the well-being of people in this group than about that of others. They will argue with reciprocity: \"Those people pay taxes in our country, they are entitled to more support from 'us' than others!\" My question is: Is this inconsistent with some rationality axioms that seem obvious? What often-adopted or reasonable axioms are there that make this inconsistent?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MQo3dPWhGpgpY8wsf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 1.0599147781857925e-06, "legacy": true, "legacyId": "20678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-16T19:55:17.981Z", "modifiedAt": null, "url": null, "title": "Meetup : First Buffalo Meetup", "slug": "meetup-first-buffalo-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vWw2DTt4A9cfdj9zP/meetup-first-buffalo-meetup", "pageUrlRelative": "/posts/vWw2DTt4A9cfdj9zP/meetup-first-buffalo-meetup", "linkUrl": "https://www.lesswrong.com/posts/vWw2DTt4A9cfdj9zP/meetup-first-buffalo-meetup", "postedAtFormatted": "Sunday, December 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Buffalo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Buffalo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvWw2DTt4A9cfdj9zP%2Fmeetup-first-buffalo-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Buffalo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvWw2DTt4A9cfdj9zP%2Fmeetup-first-buffalo-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvWw2DTt4A9cfdj9zP%2Fmeetup-first-buffalo-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/h3\">First Buffalo Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 December 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:&nbsp;</strong><span style=\"background-color: #e5e3df; font-family: arial, sans-serif; font-size: 13px;\">765 Elmwood Avenue&nbsp;</span><span style=\"background-color: #e5e3df; font-family: arial, sans-serif; font-size: 13px;\">Buffalo, NY 14222</span></p>\n<div style=\"margin: 0px; padding: 0px; font-family: arial, sans-serif; font-size: 13px; background-color: #e5e3df;\"><br /></div>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Its the first ever Buffalo LW Meetup! It will be at Spot Coffee on Elmwood. Lets meet at 6:30pm, I'll get there earlier and have some seating by the couches with a LW sign.</p>\n<p>Since this is the first meeting, we will be getting to know each other and talking about what we want to get out of the group meetups. We can discuss the locations, times, and frequency of future meetups. Also, I thought some games and group activities might be a fun way to get know each each other.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/h3\">First Buffalo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vWw2DTt4A9cfdj9zP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.059941964709446e-06, "legacy": true, "legacyId": "20679", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Buffalo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/h3\">First Buffalo Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 December 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:&nbsp;</strong><span style=\"background-color: #e5e3df; font-family: arial, sans-serif; font-size: 13px;\">765 Elmwood Avenue&nbsp;</span><span style=\"background-color: #e5e3df; font-family: arial, sans-serif; font-size: 13px;\">Buffalo, NY 14222</span></p>\n<div style=\"margin: 0px; padding: 0px; font-family: arial, sans-serif; font-size: 13px; background-color: #e5e3df;\"><br></div>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Its the first ever Buffalo LW Meetup! It will be at Spot Coffee on Elmwood. Lets meet at 6:30pm, I'll get there earlier and have some seating by the couches with a LW sign.</p>\n<p>Since this is the first meeting, we will be getting to know each other and talking about what we want to get out of the group meetups. We can discuss the locations, times, and frequency of future meetups. Also, I thought some games and group activities might be a fun way to get know each each other.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___First_Buffalo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/h3\">First Buffalo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___First_Buffalo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___First_Buffalo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-16T22:18:52.514Z", "modifiedAt": null, "url": null, "title": "Ask LW: \u03c9-self-aware systems", "slug": "ask-lw-o-self-aware-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:36.468Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Technoguyrob", "createdAt": "2010-12-23T01:08:03.374Z", "isAdmin": false, "displayName": "Technoguyrob"}, "userId": "WxPBN5cdcazmWbcr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SJ79LiWvbseNbLZ7J/ask-lw-o-self-aware-systems", "pageUrlRelative": "/posts/SJ79LiWvbseNbLZ7J/ask-lw-o-self-aware-systems", "linkUrl": "https://www.lesswrong.com/posts/SJ79LiWvbseNbLZ7J/ask-lw-o-self-aware-systems", "postedAtFormatted": "Sunday, December 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20LW%3A%20%CF%89-self-aware%20systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20LW%3A%20%CF%89-self-aware%20systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJ79LiWvbseNbLZ7J%2Fask-lw-o-self-aware-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20LW%3A%20%CF%89-self-aware%20systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJ79LiWvbseNbLZ7J%2Fask-lw-o-self-aware-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJ79LiWvbseNbLZ7J%2Fask-lw-o-self-aware-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 293, "htmlBody": "<p>I was having a conversation with a religious friend of mine who argued that even if materialism was true, we would never be able to fully understand or replicate human intelligence because a physical system cannot understand itself--it would need to use the resources contained within it to perform that understanding, excluding the possibility of full understanding.</p>\n<p>I countered with the following argument. Assume you are what your neurons are doing, and suppose you wish to extend your consciousness to fully grasp yourself (be aware of the larger systems functioning of your neuronal circuits, as well as possibly the smaller biochemical details, and the larger conceptual maps). Since consciousness offers us gestalt parallel information processing, and we will assume it can be extended to arbitrarily large concurrent information flow, one could create a (much larger) co-brain which consciously perceives all the functioning of your original brain. Now you can identify with your old consciousness and the newly added (much more expansive) co-consciousness.</p>\n<p>The problem is that now you do not understand the full brain &amp; co-brain system. But you can perform the process again, adding a co-co-brain which gives a realtime gestalt understanding of the co-brain consciousness. Since this process may be performed to arbitrarily large nesting levels, we can say that any physical system that is like the brain is&nbsp;&omega;-self-aware, with <em>n</em>-self-aware referring to the nesting level <em>n</em>. Since we do not expect the neural structures required to encode an&nbsp;<em>n</em>-self-aware system in an&nbsp;<em>n+1</em>-self-aware system to be any functionally different, we can say we've satisfactorily produced a physical system with full understanding of itself. Denying this would be equivalent to claiming we do not understand the natural numbers because we have not written every one of them down.</p>\n<p>Does anyone see any trouble with this argument?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SJ79LiWvbseNbLZ7J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 0, "extendedScore": null, "score": 1.0600255727166725e-06, "legacy": true, "legacyId": "20681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T00:58:17.443Z", "modifiedAt": null, "url": null, "title": "a utility-maximizing varient of AIXI", "slug": "a-utility-maximizing-varient-of-aixi-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rY4g6d3ac5aRkRvqa/a-utility-maximizing-varient-of-aixi-0", "pageUrlRelative": "/posts/rY4g6d3ac5aRkRvqa/a-utility-maximizing-varient-of-aixi-0", "linkUrl": "https://www.lesswrong.com/posts/rY4g6d3ac5aRkRvqa/a-utility-maximizing-varient-of-aixi-0", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20a%20utility-maximizing%20varient%20of%20AIXI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aa%20utility-maximizing%20varient%20of%20AIXI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrY4g6d3ac5aRkRvqa%2Fa-utility-maximizing-varient-of-aixi-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=a%20utility-maximizing%20varient%20of%20AIXI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrY4g6d3ac5aRkRvqa%2Fa-utility-maximizing-varient-of-aixi-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrY4g6d3ac5aRkRvqa%2Fa-utility-maximizing-varient-of-aixi-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2437, "htmlBody": "<h2>Background</h2>\n<p><br />Here is the function implemented by finite-lifetime [AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;](http://www.hutter1.net/ai/aixigentle.pdf):<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" align=\"bottom\" /&gt;,<br /><br />where &lt;img src=\"http://www.codecogs.com/png.latex?m\" title=\"m\" align=\"bottom\" /&gt; is the number of steps in the lifetime of the agent, &lt;img src=\"http://www.codecogs.com/png.latex?k\" title=\"k\" align=\"bottom\" /&gt;<br />is the current step being computed, &lt;img src=\"http://www.codecogs.com/png.latex?X\" title=\"X\" align=\"bottom\" /&gt; is the set of possible observations,<br />&lt;img src=\"http://www.codecogs.com/png.latex?Y\" title=\"Y\" align=\"bottom\" /&gt; is the set of possible actions, &lt;img src=\"http://www.codecogs.com/png.latex?r\" title=\"r\" align=\"bottom\" /&gt; is a function that extracts<br />a reward value from an observation, a dot over a variable represents<br />that its value is known to be the true value of the action or observation<br />it represents, underlines represent that the variable is an input<br />to a probability distribution, and &lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; is a function that returns<br />the probability of a sequence of observations, given a certain known<br />history and sequence of actions, and starting from the Solomonoff<br />prior. More formally,<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}2^{-\\ell\\left(q\\right)}\\right)}\" title=\"{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}2^{-\\ell\\left(q\\right)}\\right)}\" align=\"bottom\" /&gt;,<br /><br />where &lt;img src=\"http://www.codecogs.com/png.latex?Q\" title=\"Q\" align=\"bottom\" /&gt; is the set of all programs, &lt;img src=\"http://www.codecogs.com/png.latex?\\ell\" title=\"\\ell\" align=\"bottom\" /&gt; is a function that<br />returns the length of a program in bits, and a program applied to<br />a sequence of actions returns the resulting sequence of observations.<br />Notice that the denominator is a constant, depending only on the already<br />known &lt;img src=\"http://www.codecogs.com/png.latex?\\dot{y}\\dot{x}_{&lt;k}\" title=\"\\dot{y}\\dot{x}_{&lt;k}\" align=\"bottom\" /&gt;, and multiplying by a positive constant<br />does not change the argmax, so we can pretend that the denominator<br />doesn't exist. If &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt; is a valid program, then any longer program<br />with &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt; as a prefix is not a valid program, so &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}2^{-\\ell\\left(q\\right)}\\leq1}\" title=\"{\\displaystyle \\sum_{q\\in Q}2^{-\\ell\\left(q\\right)}\\leq1}\" align=\"bottom\" /&gt;.<br /><br /></p>\n<h2>Problem</h2>\n<p><br />A problem with this is that it can only optimize over the input it<br />receives, not over aspects of the external world that it cannot observe.<br />Given the chance, AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; would hack its input channel so that it<br />would only observe good things, instead of trying to make good things<br />happen (in other words, it would [wirehead](http://lesswrong.com/lw/fkx/a\\_definition\\_of\\_wireheading/)<br />itself). Anja [specified](http://lesswrong.com/lw/feo/universal\\_agents\\_and\\_utility\\_functions/)<br />a variant of AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; in which she replaced the sum of rewards with<br />a single utility value and made the domain of the utility function<br />be the entire sequence of actions and observations instead of a single<br />observation, like so:<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}U\\left(\\dot{y}\\dot{x}_{&lt;k}yx_{k:m}\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}U\\left(\\dot{y}\\dot{x}_{&lt;k}yx_{k:m}\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" align=\"bottom\" /&gt;.<br /><br />This doesn't really solve the problem, because the utility function<br />still only takes what the agent can see, rather than what is actually<br />going on outside the agent. The situation is a little better because<br />the utility function also takes into account the agent's actions,<br />so it could punish actions that look like the agent is trying to wirehead<br />itself, but if there was a flaw in the instructions not to wirehead,<br />the agent would exploit it, so the incentive not to wirehead would<br />have to be perfect, and this formulation is not very enlightening<br />about how to do that.<br /><br /></p>\n<h2>Solution</h2>\n<p><br />Here's what I suggest instead: everything that happens is determined<br />by the program that the world is running on and the agent's actions,<br />so the domain of the utility function should be &lt;img src=\"http://www.codecogs.com/png.latex?Q\\times Y^{m}\" title=\"Q\\times Y^{m}\" align=\"bottom\" /&gt;.<br />The apparent problem with that is that the formula for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; does<br />not contain any mention of elements of &lt;img src=\"http://www.codecogs.com/png.latex?Q\" title=\"Q\" align=\"bottom\" /&gt;. If we just take the original<br />formula and replace &lt;img src=\"http://www.codecogs.com/png.latex?r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\" title=\"r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\" align=\"bottom\" /&gt;<br />with &lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)\" title=\"U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)\" align=\"bottom\" /&gt;, it wouldn't make any<br />sense. However, if we expand out &lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; in the original formula (excluding<br />the unnecessary denominator), we can move the sum of rewards inside<br />the sum over programs, like this:<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />Now it is easy to replace the sum of rewards with the desired utility<br />function.<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />With this formulation, there is no danger of the agent wireheading,<br />and all &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; has to do is compute everything that happens when the<br />agent performs a given sequence of actions in a given program, and<br />decide how desirable it is. If the range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is unbounded, then<br />this might not converge. Let's assume throughout this post that the<br />range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is bounded.<br /><br /></p>\n<h2>Extension to infinite lifetimes</h2>\n<p><br />The previous discussion assumed that the agent would only have the<br />opportunity to perform a finite number of actions. The situation gets<br />a little tricky when the agent is allowed to perform an unbounded<br />number of actions. Hutter uses a finite look-ahead approach for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;,<br />where on each step &lt;img src=\"http://www.codecogs.com/png.latex?k\" title=\"k\" align=\"bottom\" /&gt;, it pretends that it will only be performing<br />&lt;img src=\"http://www.codecogs.com/png.latex?m_{k}\" title=\"m_{k}\" align=\"bottom\" /&gt; actions, where &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\, m_{k}\\gg k\" title=\"\\forall k\\, m_{k}\\gg k\" align=\"bottom\" /&gt;.<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m_{k}}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m_{k}}\\right)}\" align=\"bottom\" /&gt;.<br /><br />If we make the same modification to the utility-based variant, we<br />get<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />This is unsatisfactory because the domain of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; was supposed to<br />consist of all the information necessary to determine everything that<br />happens, but here, it is missing all the actions after step &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}\" title=\"m_{k}\" align=\"bottom\" /&gt;.<br />One obvious thing to try is to set &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" title=\"m_{k}:=\\infty\" align=\"bottom\" /&gt;. This will be<br />easier to do using a compacted expression for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;:<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br /><br />where &lt;img src=\"http://www.codecogs.com/png.latex?P\" title=\"P\" align=\"bottom\" /&gt; is the set of policies that map sequences of observations<br />to sequences of actions and &lt;img src=\"http://www.codecogs.com/png.latex?x_{i}^{pq}\" title=\"x_{i}^{pq}\" align=\"bottom\" /&gt; is shorthand for the last<br />observation in the sequence returned by &lt;img src=\"http://www.codecogs.com/png.latex?q\\left(p\\left(\\dot{x}_{&lt;k}x_{k:i-1}^{pq}\\right)\\right)\" title=\"q\\left(p\\left(\\dot{x}_{&lt;k}x_{k:i-1}^{pq}\\right)\\right)\" align=\"bottom\" /&gt;.<br />If we take this compacted formulation, modify it to accommodate the<br />new utility function, set &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" title=\"m_{k}:=\\infty\" align=\"bottom\" /&gt;, and replace the maximum<br />with a supremum (since there's an infinite number of possible policies),<br />we get<br /><br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br /><br />where &lt;img src=\"http://www.codecogs.com/png.latex?y_{i}^{pq}\" title=\"y_{i}^{pq}\" align=\"bottom\" /&gt; is shorthand for the last action in the sequence<br />returned by &lt;img src=\"http://www.codecogs.com/png.latex?p\\left(q\\left(\\dot{y}_{&lt;k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\" title=\"p\\left(q\\left(\\dot{y}_{&lt;k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\" align=\"bottom\" /&gt;.<br /><br />But there is a problem with this, which I will illustrate with a toy<br />example. Suppose &lt;img src=\"http://www.codecogs.com/png.latex?Y:=\\left\\{ a,b\\right\\} \" title=\"Y:=\\left\\{ a,b\\right\\} \" align=\"bottom\" /&gt;, and &lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,y_{1:\\infty}\\right)=0\" title=\"U\\left(q,y_{1:\\infty}\\right)=0\" align=\"bottom\" /&gt;<br />when &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\, y_{k}=a\" title=\"\\forall k\\in\\mathbb{N}\\, y_{k}=a\" align=\"bottom\" /&gt;, and for any &lt;img src=\"http://www.codecogs.com/png.latex?n\\in\\mathbb{N}\" title=\"n\\in\\mathbb{N}\" align=\"bottom\" /&gt;,<br />&lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\" title=\"U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\" align=\"bottom\" /&gt; when &lt;img src=\"http://www.codecogs.com/png.latex?y_{n}=b\" title=\"y_{n}=b\" align=\"bottom\" /&gt; and<br />&lt;img src=\"http://www.codecogs.com/png.latex?\\forall k&lt;n\\, y_{k}=a\" title=\"\\forall k&lt;n\\, y_{k}=a\" align=\"bottom\" /&gt;. (&lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; does not depend on the program &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt;<br />in this example). An agent following the above formula would output<br />&lt;img src=\"http://www.codecogs.com/png.latex?a\" title=\"a\" align=\"bottom\" /&gt; on every step, and end up with a utility of &lt;img src=\"http://www.codecogs.com/png.latex?0\" title=\"0\" align=\"bottom\" /&gt;, when it could<br />have gotten arbitrarily close to &lt;img src=\"http://www.codecogs.com/png.latex?1\" title=\"1\" align=\"bottom\" /&gt; by eventually outputting &lt;img src=\"http://www.codecogs.com/png.latex?b\" title=\"b\" align=\"bottom\" /&gt;.<br /><br />To avoid problems like that, we could assume the reasonable-seeming<br />condition that if &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt; is an action sequence and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />is a sequence of action sequences that converges to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;<br />(by which I mean &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" title=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" align=\"bottom\" /&gt;),<br />then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" align=\"bottom\" /&gt;.<br />Under that assumption, the supremum is in fact a maximum, and the<br />formula gives you an action sequence that will reach that maximum<br />(proof below).<br /><br />If you don't like the condition I imposed on &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt;, you might not be<br />satisfied by this. But without it, there is not necessarily a best<br />policy. One thing you can do is, on step 1, pick some extremely small<br />&lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon&gt;0\" title=\"\\varepsilon&gt;0\" align=\"bottom\" /&gt;, pick any element from &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}&gt;\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\" title=\"{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}&gt;\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\" align=\"bottom\" /&gt;,<br />and then follow that policy for the rest of eternity, which will guarantee<br />that you do not miss out on more than &lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon\" title=\"\\varepsilon\" align=\"bottom\" /&gt; of expected utility.<br /><br /></p>\n<h2>Proof of criterion for supremum-chasing working</h2>\n<p><br />definition: If &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt; is an action sequence and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />is an infinite sequence of action sequences, and &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" title=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" align=\"bottom\" /&gt;,<br />then we say &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt; converges<br />to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;. If &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt; is a policy and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />is a sequence of policies, and &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\forall x_{&lt;k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, p\\left(x_{&lt;k}\\right)=p_{n}\\left(x_{&lt;k}\\right)\" title=\"\\forall k\\in\\mathbb{N}\\,\\forall x_{&lt;k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, p\\left(x_{&lt;k}\\right)=p_{n}\\left(x_{&lt;k}\\right)\" align=\"bottom\" /&gt;,<br />then we say &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt; converges to<br />&lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt;.<br /><br />assumption (for lemma 2 and theorem): If &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />converges to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;, then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" align=\"bottom\" /&gt;<br /><br />lemma 1: The agent described by &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;<br />follows a policy that is the limit of a sequence of policies &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />such that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />proof of lemma 1: Any policy can be completely described by the last<br />action it outputs for every finite observation sequence. Observations<br />are returned by a program, so the set of possible finite observation<br />sequences is countable. It is possible to fix the last action returned<br />on any particular finite observation sequence to be the argmax, and<br />still get arbitrarily close to the supremum with suitable choices<br />for the last action returned on the other finite observation sequences.<br />By induction, it is possible to get arbitrarily close to the supremum<br />while fixing the last action returned to be the argmax for any finite<br />set of finite observation sequences. Thus, there exists a sequence<br />of policies approaching the policy implemented by AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; whose expected<br />utilities approach the supremum.<br /><br />lemma 2: If &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt; is a policy and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />is a sequence of policies converging to &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt;, then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />proof of lemma 2: Let &lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon&gt;0\" title=\"\\varepsilon&gt;0\" align=\"bottom\" /&gt;. On any given sequence of inputs<br />&lt;img src=\"http://www.codecogs.com/png.latex?x_{1:\\infty}\\in X^{\\infty}\" title=\"x_{1:\\infty}\\in X^{\\infty}\" align=\"bottom\" /&gt;, &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />converges to &lt;img src=\"http://www.codecogs.com/png.latex?p\\left(x_{1:\\infty}\\right)\" title=\"p\\left(x_{1:\\infty}\\right)\" align=\"bottom\" /&gt;, so, by assumption, &lt;img src=\"http://www.codecogs.com/png.latex?\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\" title=\"\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\" align=\"bottom\" /&gt;.<br />For each &lt;img src=\"http://www.codecogs.com/png.latex?N\\in\\mathbb{N}\" title=\"N\\in\\mathbb{N}\" align=\"bottom\" /&gt;, let &lt;img src=\"http://www.codecogs.com/png.latex?Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\\right\\} \" title=\"Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\\right\\} \" align=\"bottom\" /&gt;.<br />The previous statement implies that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" title=\"{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" align=\"bottom\" /&gt;,<br />and each element of &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" title=\"\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" align=\"bottom\" /&gt; is<br />a subset of the next, so &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\" title=\"{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\" align=\"bottom\" /&gt;.<br />The range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is bounded, so &lt;img src=\"http://www.codecogs.com/png.latex?\\sup U\" title=\"\\sup U\" align=\"bottom\" /&gt; and &lt;img src=\"http://www.codecogs.com/png.latex?\\inf U\" title=\"\\inf U\" align=\"bottom\" /&gt; are defined.<br />This also implies that the difference in expected utility, given any<br />information, of any two policies, is bounded. More formally: &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\" title=\"{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\" align=\"bottom\" /&gt;,<br />so in particular, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2}}\" title=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2}}\" align=\"bottom\" /&gt;.<br />&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|&lt;\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" title=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|&lt;\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" align=\"bottom\" /&gt;.<br /><br />theorem: &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br />where, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br /><br />proof of theorem: Let's call the policy implemented by the agent &lt;img src=\"http://www.codecogs.com/png.latex?p^{*}\" title=\"p^{*}\" align=\"bottom\" /&gt;.<br />By lemma 1, there is a sequence of policies &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br />converging to &lt;img src=\"http://www.codecogs.com/png.latex?p^{*}\" title=\"p^{*}\" align=\"bottom\" /&gt; such that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br />By lemma 2, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rY4g6d3ac5aRkRvqa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "20682", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Background\">Background</h2>\n<p><br>Here is the function implemented by finite-lifetime [AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;](http://www.hutter1.net/ai/aixigentle.pdf):<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" align=\"bottom\" /&gt;,<br><br>where &lt;img src=\"http://www.codecogs.com/png.latex?m\" title=\"m\" align=\"bottom\" /&gt; is the number of steps in the lifetime of the agent, &lt;img src=\"http://www.codecogs.com/png.latex?k\" title=\"k\" align=\"bottom\" /&gt;<br>is the current step being computed, &lt;img src=\"http://www.codecogs.com/png.latex?X\" title=\"X\" align=\"bottom\" /&gt; is the set of possible observations,<br>&lt;img src=\"http://www.codecogs.com/png.latex?Y\" title=\"Y\" align=\"bottom\" /&gt; is the set of possible actions, &lt;img src=\"http://www.codecogs.com/png.latex?r\" title=\"r\" align=\"bottom\" /&gt; is a function that extracts<br>a reward value from an observation, a dot over a variable represents<br>that its value is known to be the true value of the action or observation<br>it represents, underlines represent that the variable is an input<br>to a probability distribution, and &lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; is a function that returns<br>the probability of a sequence of observations, given a certain known<br>history and sequence of actions, and starting from the Solomonoff<br>prior. More formally,<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}2^{-\\ell\\left(q\\right)}\\right)}\" title=\"{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}2^{-\\ell\\left(q\\right)}\\right)}\" align=\"bottom\" /&gt;,<br><br>where &lt;img src=\"http://www.codecogs.com/png.latex?Q\" title=\"Q\" align=\"bottom\" /&gt; is the set of all programs, &lt;img src=\"http://www.codecogs.com/png.latex?\\ell\" title=\"\\ell\" align=\"bottom\" /&gt; is a function that<br>returns the length of a program in bits, and a program applied to<br>a sequence of actions returns the resulting sequence of observations.<br>Notice that the denominator is a constant, depending only on the already<br>known &lt;img src=\"http://www.codecogs.com/png.latex?\\dot{y}\\dot{x}_{&lt;k}\" title=\"\\dot{y}\\dot{x}_{&lt;k}\" align=\"bottom\" /&gt;, and multiplying by a positive constant<br>does not change the argmax, so we can pretend that the denominator<br>doesn't exist. If &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt; is a valid program, then any longer program<br>with &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt; as a prefix is not a valid program, so &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}2^{-\\ell\\left(q\\right)}\\leq1}\" title=\"{\\displaystyle \\sum_{q\\in Q}2^{-\\ell\\left(q\\right)}\\leq1}\" align=\"bottom\" /&gt;.<br><br></p>\n<h2 id=\"Problem\">Problem</h2>\n<p><br>A problem with this is that it can only optimize over the input it<br>receives, not over aspects of the external world that it cannot observe.<br>Given the chance, AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; would hack its input channel so that it<br>would only observe good things, instead of trying to make good things<br>happen (in other words, it would [wirehead](http://lesswrong.com/lw/fkx/a\\_definition\\_of\\_wireheading/)<br>itself). Anja [specified](http://lesswrong.com/lw/feo/universal\\_agents\\_and\\_utility\\_functions/)<br>a variant of AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; in which she replaced the sum of rewards with<br>a single utility value and made the domain of the utility function<br>be the entire sequence of actions and observations instead of a single<br>observation, like so:<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}U\\left(\\dot{y}\\dot{x}_{&lt;k}yx_{k:m}\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}U\\left(\\dot{y}\\dot{x}_{&lt;k}yx_{k:m}\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" align=\"bottom\" /&gt;.<br><br>This doesn't really solve the problem, because the utility function<br>still only takes what the agent can see, rather than what is actually<br>going on outside the agent. The situation is a little better because<br>the utility function also takes into account the agent's actions,<br>so it could punish actions that look like the agent is trying to wirehead<br>itself, but if there was a flaw in the instructions not to wirehead,<br>the agent would exploit it, so the incentive not to wirehead would<br>have to be perfect, and this formulation is not very enlightening<br>about how to do that.<br><br></p>\n<h2 id=\"Solution\">Solution</h2>\n<p><br>Here's what I suggest instead: everything that happens is determined<br>by the program that the world is running on and the agent's actions,<br>so the domain of the utility function should be &lt;img src=\"http://www.codecogs.com/png.latex?Q\\times Y^{m}\" title=\"Q\\times Y^{m}\" align=\"bottom\" /&gt;.<br>The apparent problem with that is that the formula for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; does<br>not contain any mention of elements of &lt;img src=\"http://www.codecogs.com/png.latex?Q\" title=\"Q\" align=\"bottom\" /&gt;. If we just take the original<br>formula and replace &lt;img src=\"http://www.codecogs.com/png.latex?r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\" title=\"r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\" align=\"bottom\" /&gt;<br>with &lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)\" title=\"U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)\" align=\"bottom\" /&gt;, it wouldn't make any<br>sense. However, if we expand out &lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; in the original formula (excluding<br>the unnecessary denominator), we can move the sum of rewards inside<br>the sum over programs, like this:<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>Now it is easy to replace the sum of rewards with the desired utility<br>function.<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>With this formulation, there is no danger of the agent wireheading,<br>and all &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; has to do is compute everything that happens when the<br>agent performs a given sequence of actions in a given program, and<br>decide how desirable it is. If the range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is unbounded, then<br>this might not converge. Let's assume throughout this post that the<br>range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is bounded.<br><br></p>\n<h2 id=\"Extension_to_infinite_lifetimes\">Extension to infinite lifetimes</h2>\n<p><br>The previous discussion assumed that the agent would only have the<br>opportunity to perform a finite number of actions. The situation gets<br>a little tricky when the agent is allowed to perform an unbounded<br>number of actions. Hutter uses a finite look-ahead approach for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;,<br>where on each step &lt;img src=\"http://www.codecogs.com/png.latex?k\" title=\"k\" align=\"bottom\" /&gt;, it pretends that it will only be performing<br>&lt;img src=\"http://www.codecogs.com/png.latex?m_{k}\" title=\"m_{k}\" align=\"bottom\" /&gt; actions, where &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\, m_{k}\\gg k\" title=\"\\forall k\\, m_{k}\\gg k\" align=\"bottom\" /&gt;.<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m_{k}}\\right)}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m_{k}}\\right)}\" align=\"bottom\" /&gt;.<br><br>If we make the same modification to the utility-based variant, we<br>get<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>This is unsatisfactory because the domain of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; was supposed to<br>consist of all the information necessary to determine everything that<br>happens, but here, it is missing all the actions after step &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}\" title=\"m_{k}\" align=\"bottom\" /&gt;.<br>One obvious thing to try is to set &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" title=\"m_{k}:=\\infty\" align=\"bottom\" /&gt;. This will be<br>easier to do using a compacted expression for AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt;:<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br><br>where &lt;img src=\"http://www.codecogs.com/png.latex?P\" title=\"P\" align=\"bottom\" /&gt; is the set of policies that map sequences of observations<br>to sequences of actions and &lt;img src=\"http://www.codecogs.com/png.latex?x_{i}^{pq}\" title=\"x_{i}^{pq}\" align=\"bottom\" /&gt; is shorthand for the last<br>observation in the sequence returned by &lt;img src=\"http://www.codecogs.com/png.latex?q\\left(p\\left(\\dot{x}_{&lt;k}x_{k:i-1}^{pq}\\right)\\right)\" title=\"q\\left(p\\left(\\dot{x}_{&lt;k}x_{k:i-1}^{pq}\\right)\\right)\" align=\"bottom\" /&gt;.<br>If we take this compacted formulation, modify it to accommodate the<br>new utility function, set &lt;img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" title=\"m_{k}:=\\infty\" align=\"bottom\" /&gt;, and replace the maximum<br>with a supremum (since there's an infinite number of possible policies),<br>we get<br><br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br><br>where &lt;img src=\"http://www.codecogs.com/png.latex?y_{i}^{pq}\" title=\"y_{i}^{pq}\" align=\"bottom\" /&gt; is shorthand for the last action in the sequence<br>returned by &lt;img src=\"http://www.codecogs.com/png.latex?p\\left(q\\left(\\dot{y}_{&lt;k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\" title=\"p\\left(q\\left(\\dot{y}_{&lt;k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\" align=\"bottom\" /&gt;.<br><br>But there is a problem with this, which I will illustrate with a toy<br>example. Suppose &lt;img src=\"http://www.codecogs.com/png.latex?Y:=\\left\\{ a,b\\right\\} \" title=\"Y:=\\left\\{ a,b\\right\\} \" align=\"bottom\" /&gt;, and &lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,y_{1:\\infty}\\right)=0\" title=\"U\\left(q,y_{1:\\infty}\\right)=0\" align=\"bottom\" /&gt;<br>when &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\, y_{k}=a\" title=\"\\forall k\\in\\mathbb{N}\\, y_{k}=a\" align=\"bottom\" /&gt;, and for any &lt;img src=\"http://www.codecogs.com/png.latex?n\\in\\mathbb{N}\" title=\"n\\in\\mathbb{N}\" align=\"bottom\" /&gt;,<br>&lt;img src=\"http://www.codecogs.com/png.latex?U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\" title=\"U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\" align=\"bottom\" /&gt; when &lt;img src=\"http://www.codecogs.com/png.latex?y_{n}=b\" title=\"y_{n}=b\" align=\"bottom\" /&gt; and<br>&lt;img src=\"http://www.codecogs.com/png.latex?\\forall k&lt;n\\, y_{k}=a\" title=\"\\forall k&lt;n\\, y_{k}=a\" align=\"bottom\" /&gt;. (&lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; does not depend on the program &lt;img src=\"http://www.codecogs.com/png.latex?q\" title=\"q\" align=\"bottom\" /&gt;<br>in this example). An agent following the above formula would output<br>&lt;img src=\"http://www.codecogs.com/png.latex?a\" title=\"a\" align=\"bottom\" /&gt; on every step, and end up with a utility of &lt;img src=\"http://www.codecogs.com/png.latex?0\" title=\"0\" align=\"bottom\" /&gt;, when it could<br>have gotten arbitrarily close to &lt;img src=\"http://www.codecogs.com/png.latex?1\" title=\"1\" align=\"bottom\" /&gt; by eventually outputting &lt;img src=\"http://www.codecogs.com/png.latex?b\" title=\"b\" align=\"bottom\" /&gt;.<br><br>To avoid problems like that, we could assume the reasonable-seeming<br>condition that if &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt; is an action sequence and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>is a sequence of action sequences that converges to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;<br>(by which I mean &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" title=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" align=\"bottom\" /&gt;),<br>then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" align=\"bottom\" /&gt;.<br>Under that assumption, the supremum is in fact a maximum, and the<br>formula gives you an action sequence that will reach that maximum<br>(proof below).<br><br>If you don't like the condition I imposed on &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt;, you might not be<br>satisfied by this. But without it, there is not necessarily a best<br>policy. One thing you can do is, on step 1, pick some extremely small<br>&lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon&gt;0\" title=\"\\varepsilon&gt;0\" align=\"bottom\" /&gt;, pick any element from &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}&gt;\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\" title=\"{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}&gt;\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\" align=\"bottom\" /&gt;,<br>and then follow that policy for the rest of eternity, which will guarantee<br>that you do not miss out on more than &lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon\" title=\"\\varepsilon\" align=\"bottom\" /&gt; of expected utility.<br><br></p>\n<h2 id=\"Proof_of_criterion_for_supremum_chasing_working\">Proof of criterion for supremum-chasing working</h2>\n<p><br>definition: If &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt; is an action sequence and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>is an infinite sequence of action sequences, and &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" title=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" align=\"bottom\" /&gt;,<br>then we say &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt; converges<br>to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;. If &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt; is a policy and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>is a sequence of policies, and &lt;img src=\"http://www.codecogs.com/png.latex?\\forall k\\in\\mathbb{N}\\,\\forall x_{&lt;k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, p\\left(x_{&lt;k}\\right)=p_{n}\\left(x_{&lt;k}\\right)\" title=\"\\forall k\\in\\mathbb{N}\\,\\forall x_{&lt;k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, p\\left(x_{&lt;k}\\right)=p_{n}\\left(x_{&lt;k}\\right)\" align=\"bottom\" /&gt;,<br>then we say &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt; converges to<br>&lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt;.<br><br>assumption (for lemma 2 and theorem): If &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>converges to &lt;img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" title=\"y_{1:\\infty}\" align=\"bottom\" /&gt;, then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" align=\"bottom\" /&gt;<br><br>lemma 1: The agent described by &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;<br>follows a policy that is the limit of a sequence of policies &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>such that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>proof of lemma 1: Any policy can be completely described by the last<br>action it outputs for every finite observation sequence. Observations<br>are returned by a program, so the set of possible finite observation<br>sequences is countable. It is possible to fix the last action returned<br>on any particular finite observation sequence to be the argmax, and<br>still get arbitrarily close to the supremum with suitable choices<br>for the last action returned on the other finite observation sequences.<br>By induction, it is possible to get arbitrarily close to the supremum<br>while fixing the last action returned to be the argmax for any finite<br>set of finite observation sequences. Thus, there exists a sequence<br>of policies approaching the policy implemented by AI&lt;img src=\"http://www.codecogs.com/png.latex?\\xi\" title=\"\\xi\" align=\"bottom\" /&gt; whose expected<br>utilities approach the supremum.<br><br>lemma 2: If &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt; is a policy and &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>is a sequence of policies converging to &lt;img src=\"http://www.codecogs.com/png.latex?p\" title=\"p\" align=\"bottom\" /&gt;, then &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>proof of lemma 2: Let &lt;img src=\"http://www.codecogs.com/png.latex?\\varepsilon&gt;0\" title=\"\\varepsilon&gt;0\" align=\"bottom\" /&gt;. On any given sequence of inputs<br>&lt;img src=\"http://www.codecogs.com/png.latex?x_{1:\\infty}\\in X^{\\infty}\" title=\"x_{1:\\infty}\\in X^{\\infty}\" align=\"bottom\" /&gt;, &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>converges to &lt;img src=\"http://www.codecogs.com/png.latex?p\\left(x_{1:\\infty}\\right)\" title=\"p\\left(x_{1:\\infty}\\right)\" align=\"bottom\" /&gt;, so, by assumption, &lt;img src=\"http://www.codecogs.com/png.latex?\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\" title=\"\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\" align=\"bottom\" /&gt;.<br>For each &lt;img src=\"http://www.codecogs.com/png.latex?N\\in\\mathbb{N}\" title=\"N\\in\\mathbb{N}\" align=\"bottom\" /&gt;, let &lt;img src=\"http://www.codecogs.com/png.latex?Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\\right\\} \" title=\"Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\\right\\} \" align=\"bottom\" /&gt;.<br>The previous statement implies that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" title=\"{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" align=\"bottom\" /&gt;,<br>and each element of &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" title=\"\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" align=\"bottom\" /&gt; is<br>a subset of the next, so &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\" title=\"{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\" align=\"bottom\" /&gt;.<br>The range of &lt;img src=\"http://www.codecogs.com/png.latex?U\" title=\"U\" align=\"bottom\" /&gt; is bounded, so &lt;img src=\"http://www.codecogs.com/png.latex?\\sup U\" title=\"\\sup U\" align=\"bottom\" /&gt; and &lt;img src=\"http://www.codecogs.com/png.latex?\\inf U\" title=\"\\inf U\" align=\"bottom\" /&gt; are defined.<br>This also implies that the difference in expected utility, given any<br>information, of any two policies, is bounded. More formally: &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\" title=\"{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\" align=\"bottom\" /&gt;,<br>so in particular, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2}}\" title=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2}}\" align=\"bottom\" /&gt;.<br>&lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|&lt;\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" title=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|&lt;\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" align=\"bottom\" /&gt;.<br><br>theorem: &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;,<br>where, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br><br>proof of theorem: Let's call the policy implemented by the agent &lt;img src=\"http://www.codecogs.com/png.latex?p^{*}\" title=\"p^{*}\" align=\"bottom\" /&gt;.<br>By lemma 1, there is a sequence of policies &lt;img src=\"http://www.codecogs.com/png.latex?\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" title=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" align=\"bottom\" /&gt;<br>converging to &lt;img src=\"http://www.codecogs.com/png.latex?p^{*}\" title=\"p^{*}\" align=\"bottom\" /&gt; such that &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.<br>By lemma 2, &lt;img src=\"http://www.codecogs.com/png.latex?{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" title=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" align=\"bottom\" /&gt;.</p>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Problem", "anchor": "Problem", "level": 1}, {"title": "Solution", "anchor": "Solution", "level": 1}, {"title": "Extension to infinite lifetimes", "anchor": "Extension_to_infinite_lifetimes", "level": 1}, {"title": "Proof of criterion for supremum-chasing working", "anchor": "Proof_of_criterion_for_supremum_chasing_working", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T01:12:19.518Z", "modifiedAt": null, "url": null, "title": "A utility-maximizing varient of AIXI", "slug": "a-utility-maximizing-varient-of-aixi-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sqB9aJaBkzc3ASYsT/a-utility-maximizing-varient-of-aixi-1", "pageUrlRelative": "/posts/sqB9aJaBkzc3ASYsT/a-utility-maximizing-varient-of-aixi-1", "linkUrl": "https://www.lesswrong.com/posts/sqB9aJaBkzc3ASYsT/a-utility-maximizing-varient-of-aixi-1", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20utility-maximizing%20varient%20of%20AIXI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20utility-maximizing%20varient%20of%20AIXI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqB9aJaBkzc3ASYsT%2Fa-utility-maximizing-varient-of-aixi-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20utility-maximizing%20varient%20of%20AIXI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqB9aJaBkzc3ASYsT%2Fa-utility-maximizing-varient-of-aixi-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqB9aJaBkzc3ASYsT%2Fa-utility-maximizing-varient-of-aixi-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1611, "htmlBody": "<p>Background<br /><br />Here is the function implemented by finite-lifetime [AI![](http://www.codecogs.com/png.latex?\\\\xi)](http://www.hutter1.net/ai/aixigentle.pdf):<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\}\\\\in%20X\\}\\\\left\\(r\\\\left\\(x\\_\\{k\\}\\\\right\\)\\+\\.\\.\\.\\+r\\\\left\\(x\\_\\{m\\}\\\\right\\)\\\\right\\)\\\\cdot\\\\xi\\\\left\\(\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}y\\\\underline\\{x\\}\\_\\{k:m\\}\\\\right\\)\\}),<br /><br />where ![](http://www.codecogs.com/png.latex?m) is the number of steps in the lifetime of the agent, ![](http://www.codecogs.com/png.latex?k)<br />is the current step being computed, ![](http://www.codecogs.com/png.latex?X) is the set of possible observations,<br />![](http://www.codecogs.com/png.latex?Y) is the set of possible actions, ![](http://www.codecogs.com/png.latex?r) is a function that extracts<br />a reward value from an observation, a dot over a variable represents<br />that its value is known to be the true value of the action or observation<br />it represents, underlines represent that the variable is an input<br />to a probability distribution, and ![](http://www.codecogs.com/png.latex?\\\\xi) is a function that returns<br />the probability of a sequence of observations, given a certain known<br />history and sequence of actions, and starting from the Solomonoff<br />prior. More formally,<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\xi\\\\left\\(\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}y\\\\underline\\{x\\}\\_\\{k:m\\}\\\\right\\)=\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(y\\_\\{\\\\leq%20m\\}\\\\right\\)=x\\_\\{\\\\leq%20m\\}\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\diagup\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{x\\}\\_\\{%3Ck\\}\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\}),<br /><br />where ![](http://www.codecogs.com/png.latex?Q) is the set of all programs, ![](http://www.codecogs.com/png.latex?\\\\ell) is a function that<br />returns the length of a program in bits, and a program applied to<br />a sequence of actions returns the resulting sequence of observations.<br />Notice that the denominator is a constant, depending only on the already<br />known ![](http://www.codecogs.com/png.latex?\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}), and multiplying by a positive constant<br />does not change the argmax, so we can pretend that the denominator<br />doesn't exist. If ![](http://www.codecogs.com/png.latex?q) is a valid program, then any longer program<br />with ![](http://www.codecogs.com/png.latex?q) as a prefix is not a valid program, so ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\sum\\_\\{q\\\\in%20Q\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\leq1\\}).<br /><br /><br />Problem<br /><br />A problem with this is that it can only optimize over the input it<br />receives, not over aspects of the external world that it cannot observe.<br />Given the chance, AI![](http://www.codecogs.com/png.latex?\\\\xi) would hack its input channel so that it<br />would only observe good things, instead of trying to make good things<br />happen (in other words, it would [wirehead](http://lesswrong.com/lw/fkx/a\\_definition\\_of\\_wireheading/)<br />itself). Anja [specified](http://lesswrong.com/lw/feo/universal\\_agents\\_and\\_utility\\_functions/)<br />a variant of AI![](http://www.codecogs.com/png.latex?\\\\xi) in which she replaced the sum of rewards with<br />a single utility value and made the domain of the utility function<br />be the entire sequence of actions and observations instead of a single<br />observation, like so:<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\}\\\\in%20X\\}U\\\\left\\(\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}yx\\_\\{k:m\\}\\\\right\\)\\\\cdot\\\\xi\\\\left\\(\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}y\\\\underline\\{x\\}\\_\\{k:m\\}\\\\right\\)\\}).<br /><br />This doesn't really solve the problem, because the utility function<br />still only takes what the agent can see, rather than what is actually<br />going on outside the agent. The situation is a little better because<br />the utility function also takes into account the agent's actions,<br />so it could punish actions that look like the agent is trying to wirehead<br />itself, but if there was a flaw in the instructions not to wirehead,<br />the agent would exploit it, so the incentive not to wirehead would<br />have to be perfect, and this formulation is not very enlightening<br />about how to do that.<br /><br /><br />Solution<br /><br />Here's what I suggest instead: everything that happens is determined<br />by the program that the world is running on and the agent's actions,<br />so the domain of the utility function should be ![](http://www.codecogs.com/png.latex?Q\\\\times%20Y%5E\\{m\\}).<br />The apparent problem with that is that the formula for AI![](http://www.codecogs.com/png.latex?\\\\xi) does<br />not contain any mention of elements of ![](http://www.codecogs.com/png.latex?Q). If we just take the original<br />formula and replace ![](http://www.codecogs.com/png.latex?r\\\\left\\(x\\_\\{k\\}\\\\right\\)\\+\\.\\.\\.\\+r\\\\left\\(x\\_\\{m\\}\\\\right\\))<br />with ![](http://www.codecogs.com/png.latex?U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k:m\\}\\\\right\\)), it wouldn't make any<br />sense. However, if we expand out ![](http://www.codecogs.com/png.latex?\\\\xi) in the original formula (excluding<br />the unnecessary denominator), we can move the sum of rewards inside<br />the sum over programs, like this:<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\}\\\\in%20X\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(y\\_\\{\\\\leq%20m\\}\\\\right\\)=x\\_\\{\\\\leq%20m\\}\\}\\\\left\\(r\\\\left\\(x\\_\\{k\\}\\\\right\\)\\+\\.\\.\\.\\+r\\\\left\\(x\\_\\{m\\}\\\\right\\)\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />Now it is easy to replace the sum of rewards with the desired utility<br />function.<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\}\\\\in%20X\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(y\\_\\{\\\\leq%20m\\}\\\\right\\)=x\\_\\{\\\\leq%20m\\}\\}U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k:m\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />With this formulation, there is no danger of the agent wireheading,<br />and all ![](http://www.codecogs.com/png.latex?U) has to do is compute everything that happens when the<br />agent performs a given sequence of actions in a given program, and<br />decide how desirable it is. If the range of ![](http://www.codecogs.com/png.latex?U) is unbounded, then<br />this might not converge. Let's assume throughout this post that the<br />range of ![](http://www.codecogs.com/png.latex?U) is bounded.<br /><br /><br />Extension to infinite lifetimes<br /><br />The previous discussion assumed that the agent would only have the<br />opportunity to perform a finite number of actions. The situation gets<br />a little tricky when the agent is allowed to perform an unbounded<br />number of actions. Hutter uses a finite look-ahead approach for AI![](http://www.codecogs.com/png.latex?\\\\xi),<br />where on each step ![](http://www.codecogs.com/png.latex?k), it pretends that it will only be performing<br />![](http://www.codecogs.com/png.latex?m\\_\\{k\\}) actions, where ![](http://www.codecogs.com/png.latex?\\\\forall%20k\\\\,%20m\\_\\{k\\}\\\\gg%20k).<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\_\\{k\\}\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\_\\{k\\}\\}\\\\in%20X\\}\\\\left\\(r\\\\left\\(x\\_\\{k\\}\\\\right\\)\\+\\.\\.\\.\\+r\\\\left\\(x\\_\\{m\\_\\{k\\}\\}\\\\right\\)\\\\right\\)\\\\cdot\\\\xi\\\\left\\(\\\\dot\\{y\\}\\\\dot\\{x\\}\\_\\{%3Ck\\}y\\\\underline\\{x\\}\\_\\{k:m\\_\\{k\\}\\}\\\\right\\)\\}).<br /><br />If we make the same modification to the utility-based variant, we<br />get<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\}\\\\in%20X\\}\\\\max\\_\\{y\\_\\{k\\+1\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{k\\+1\\}\\\\in%20X\\}\\.\\.\\.\\\\max\\_\\{y\\_\\{m\\_\\{k\\}\\}\\\\in%20Y\\}\\\\sum\\_\\{x\\_\\{m\\_\\{k\\}\\}\\\\in%20X\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(y\\_\\{\\\\leq%20m\\_\\{k\\}\\}\\\\right\\)=x\\_\\{\\\\leq%20m\\_\\{k\\}\\}\\}U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k:m\\_\\{k\\}\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />This is unsatisfactory because the domain of ![](http://www.codecogs.com/png.latex?U) was supposed to<br />consist of all the information necessary to determine everything that<br />happens, but here, it is missing all the actions after step ![](http://www.codecogs.com/png.latex?m\\_\\{k\\}).<br />One obvious thing to try is to set ![](http://www.codecogs.com/png.latex?m\\_\\{k\\}:=\\\\infty). This will be<br />easier to do using a compacted expression for AI![](http://www.codecogs.com/png.latex?\\\\xi):<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\max\\_\\{p\\\\in%20P:p\\\\left\\(\\\\dot\\{x\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{x\\}\\_\\{%3Ck\\}\\}\\\\left\\(r\\\\left\\(x\\_\\{k\\}%5E\\{pq\\}\\\\right\\)\\+\\.\\.\\.\\+r\\\\left\\(x\\_\\{m\\_\\{k\\}\\}%5E\\{pq\\}\\\\right\\)\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}),<br /><br />where ![](http://www.codecogs.com/png.latex?P) is the set of policies that map sequences of observations<br />to sequences of actions and ![](http://www.codecogs.com/png.latex?x\\_\\{i\\}%5E\\{pq\\}) is shorthand for the last<br />observation in the sequence returned by ![](http://www.codecogs.com/png.latex?q\\\\left\\(p\\\\left\\(\\\\dot\\{x\\}\\_\\{%3Ck\\}x\\_\\{k:i\\-1\\}%5E\\{pq\\}\\\\right\\)\\\\right\\)).<br />If we take this compacted formulation, modify it to accommodate the<br />new utility function, set ![](http://www.codecogs.com/png.latex?m\\_\\{k\\}:=\\\\infty), and replace the maximum<br />with a supremum (since there's an infinite number of possible policies),<br />we get<br /><br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sup\\_\\{p\\\\in%20P:p\\\\left\\(\\\\dot\\{x\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{x\\}\\_\\{%3Ck\\}\\}U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}y\\_\\{k\\+1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}),<br /><br />where ![](http://www.codecogs.com/png.latex?y\\_\\{i\\}%5E\\{pq\\}) is shorthand for the last action in the sequence<br />returned by ![](http://www.codecogs.com/png.latex?p\\\\left\\(q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}y\\_\\{k\\+1:i\\-1\\}%5E\\{pq\\}\\\\right\\)\\\\right\\)).<br /><br />But there is a problem with this, which I will illustrate with a toy<br />example. Suppose ![](http://www.codecogs.com/png.latex?Y:=\\\\left\\\\\\{%20a,b\\\\right\\\\\\}%20), and ![](http://www.codecogs.com/png.latex?U\\\\left\\(q,y\\_\\{1:\\\\infty\\}\\\\right\\)=0)<br />when ![](http://www.codecogs.com/png.latex?\\\\forall%20k\\\\in\\\\mathbb\\{N\\}\\\\,%20y\\_\\{k\\}=a), and for any ![](http://www.codecogs.com/png.latex?n\\\\in\\\\mathbb\\{N\\}),<br />![](http://www.codecogs.com/png.latex?U\\\\left\\(q,y\\_\\{1:\\\\infty\\}\\\\right\\)=1\\-\\\\frac\\{1\\}\\{n\\}) when ![](http://www.codecogs.com/png.latex?y\\_\\{n\\}=b) and<br />![](http://www.codecogs.com/png.latex?\\\\forall%20k%3Cn\\\\,%20y\\_\\{k\\}=a). (![](http://www.codecogs.com/png.latex?U) does not depend on the program ![](http://www.codecogs.com/png.latex?q)<br />in this example). An agent following the above formula would output<br />![](http://www.codecogs.com/png.latex?a) on every step, and end up with a utility of ![](http://www.codecogs.com/png.latex?0), when it could<br />have gotten arbitrarily close to ![](http://www.codecogs.com/png.latex?1) by eventually outputting ![](http://www.codecogs.com/png.latex?b).<br /><br />To avoid problems like that, we could assume the reasonable-seeming<br />condition that if ![](http://www.codecogs.com/png.latex?y\\_\\{1:\\\\infty\\}) is an action sequence and ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />is a sequence of action sequences that converges to ![](http://www.codecogs.com/png.latex?y\\_\\{1:\\\\infty\\})<br />(by which I mean ![](http://www.codecogs.com/png.latex?\\\\forall%20k\\\\in\\\\mathbb\\{N\\}\\\\,\\\\exists%20N\\\\in\\\\mathbb\\{N\\}\\\\,\\\\forall%20n%3EN\\\\,%20y\\_\\{k\\}%5E\\{n\\}=y\\_\\{k\\})),<br />then ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\lim\\_\\{n\\\\rightarrow\\\\infty\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\)=U\\\\left\\(q,y\\_\\{1:\\\\infty\\}\\\\right\\)\\}).<br />Under that assumption, the supremum is in fact a maximum, and the<br />formula gives you an action sequence that will reach that maximum<br />(proof below).<br /><br />If you don't like the condition I imposed on ![](http://www.codecogs.com/png.latex?U), you might not be<br />satisfied by this. But without it, there is not necessarily a best<br />policy. One thing you can do is, on step 1, pick some extremely small<br />![](http://www.codecogs.com/png.latex?\\\\varepsilon%3E0), pick any element from ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\left\\\\\\{%20p%5E\\{\\*\\}\\\\in%20P:\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p%5E\\{\\*\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}%3E\\\\left\\(\\\\sup\\_\\{p\\\\in%20P\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\-\\\\varepsilon\\\\right\\\\\\}%20\\}),<br />and then follow that policy for the rest of eternity, which will guarantee<br />that you do not miss out on more than ![](http://www.codecogs.com/png.latex?\\\\varepsilon) of expected utility.<br /><br /><br />Proof of criterion for supremum-chasing working<br /><br />definition: If ![](http://www.codecogs.com/png.latex?y\\_\\{1:\\\\infty\\}) is an action sequence and ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />is an infinite sequence of action sequences, and ![](http://www.codecogs.com/png.latex?\\\\forall%20k\\\\in\\\\mathbb\\{N\\}\\\\,\\\\exists%20N\\\\in\\\\mathbb\\{N\\}\\\\,\\\\forall%20n%3EN\\\\,%20y\\_\\{k\\}%5E\\{n\\}=y\\_\\{k\\}),<br />then we say ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\}) converges<br />to ![](http://www.codecogs.com/png.latex?y\\_\\{1:\\\\infty\\}). If ![](http://www.codecogs.com/png.latex?p) is a policy and ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />is a sequence of policies, and ![](http://www.codecogs.com/png.latex?\\\\forall%20k\\\\in\\\\mathbb\\{N\\}\\\\,\\\\forall%20x\\_\\{%3Ck\\}\\\\in%20X%5E\\{k\\}\\\\,\\\\exists%20N\\\\in\\\\mathbb\\{N\\}\\\\,\\\\forall%20n%3EN\\\\,%20p\\\\left\\(x\\_\\{%3Ck\\}\\\\right\\)=p\\_\\{n\\}\\\\left\\(x\\_\\{%3Ck\\}\\\\right\\)),<br />then we say ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\}) converges to<br />![](http://www.codecogs.com/png.latex?p).<br /><br />assumption (for lemma 2 and theorem): If ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />converges to ![](http://www.codecogs.com/png.latex?y\\_\\{1:\\\\infty\\}), then ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\lim\\_\\{n\\\\rightarrow\\\\infty\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{n\\}\\\\right\\)=U\\\\left\\(q,y\\_\\{1:\\\\infty\\}\\\\right\\)\\})<br /><br />lemma 1: The agent described by ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sup\\_\\{p\\\\in%20P:p\\\\left\\(\\\\dot\\{x\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{x\\}\\_\\{%3Ck\\}\\}U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}y\\_\\{k\\+1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\})<br />follows a policy that is the limit of a sequence of policies ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />such that ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\lim\\_\\{n\\\\rightarrow\\\\infty\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{n\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}=\\\\sup\\_\\{p\\\\in%20P\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />proof of lemma 1: Any policy can be completely described by the last<br />action it outputs for every finite observation sequence. Observations<br />are returned by a program, so the set of possible finite observation<br />sequences is countable. It is possible to fix the last action returned<br />on any particular finite observation sequence to be the argmax, and<br />still get arbitrarily close to the supremum with suitable choices<br />for the last action returned on the other finite observation sequences.<br />By induction, it is possible to get arbitrarily close to the supremum<br />while fixing the last action returned to be the argmax for any finite<br />set of finite observation sequences. Thus, there exists a sequence<br />of policies approaching the policy implemented by AI![](http://www.codecogs.com/png.latex?\\\\xi) whose expected<br />utilities approach the supremum.<br /><br />lemma 2: If ![](http://www.codecogs.com/png.latex?p) is a policy and ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />is a sequence of policies converging to ![](http://www.codecogs.com/png.latex?p), then ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}=\\\\lim\\_\\{n\\\\rightarrow\\\\infty\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{n\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />proof of lemma 2: Let ![](http://www.codecogs.com/png.latex?\\\\varepsilon%3E0). On any given sequence of inputs<br />![](http://www.codecogs.com/png.latex?x\\_\\{1:\\\\infty\\}\\\\in%20X%5E\\{\\\\infty\\}), ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\left\\(x\\_\\{1:\\\\infty\\}\\\\right\\)\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />converges to ![](http://www.codecogs.com/png.latex?p\\\\left\\(x\\_\\{1:\\\\infty\\}\\\\right\\)), so, by assumption, ![](http://www.codecogs.com/png.latex?\\\\forall%20q\\\\in%20Q\\\\,\\\\exists%20N\\\\in\\\\mathbb\\{N\\}\\\\,\\\\forall%20n\\\\geq%20N\\\\,\\\\left|U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)\\-U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{n\\}q\\}\\\\right\\)\\\\right|%3C\\\\frac\\{\\\\varepsilon\\}\\{2\\}).<br />For each ![](http://www.codecogs.com/png.latex?N\\\\in\\\\mathbb\\{N\\}), let ![](http://www.codecogs.com/png.latex?Q\\_\\{N\\}:=\\\\left\\\\\\{%20q\\\\in%20Q:\\\\forall%20n\\\\geq%20N\\\\,\\\\left|U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)\\-U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{n\\}q\\}\\\\right\\)\\\\right|%3C\\\\frac\\{\\\\varepsilon\\}\\{2\\}\\\\right\\\\\\}%20).<br />The previous statement implies that ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\bigcup\\_\\{N\\\\in\\\\mathbb\\{N\\}\\}Q\\_\\{N\\}=Q\\}),<br />and each element of ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20Q\\_\\{N\\}\\\\right\\\\\\}%20\\_\\{N\\\\in\\\\mathbb\\{N\\}\\}) is<br />a subset of the next, so ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\exists%20N\\\\in\\\\mathbb\\{N\\}\\\\,\\\\sum\\_\\{q\\\\in%20Q\\\\setminus%20Q\\_\\{N\\}\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}%3C\\\\frac\\{\\\\varepsilon\\}\\{2\\\\left\\(\\\\sup%20U\\-\\\\inf%20U\\\\right\\)\\}\\}).<br />The range of ![](http://www.codecogs.com/png.latex?U) is bounded, so ![](http://www.codecogs.com/png.latex?\\\\sup%20U) and ![](http://www.codecogs.com/png.latex?\\\\inf%20U) are defined.<br />This also implies that the difference in expected utility, given any<br />information, of any two policies, is bounded. More formally: ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\forall%20Q'\\\\subset%20Q\\\\,\\\\forall%20p',p''\\\\in%20P\\\\,\\\\left|\\\\left\\(\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q'\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p'q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\diagup\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q'\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right\\)\\-\\\\left\\(\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q'\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p''q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\diagup\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q'\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right\\)\\\\right|\\\\leq\\\\sup%20U\\-\\\\inf%20U\\}),<br />so in particular, ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\left|\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\\\setminus%20Q\\_\\{N\\}\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\-\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\\\setminus%20Q\\_\\{N\\}\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{N\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right|\\\\leq\\\\left\\(\\\\sup%20U\\-\\\\inf%20U\\\\right\\)\\\\sum\\_\\{q\\\\in%20Q\\\\setminus%20Q\\_\\{N\\}\\}2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}%3C\\\\frac\\{\\\\varepsilon\\}\\{2\\}\\}).<br />![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\left|\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\-\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{N\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right|\\\\leq\\\\left|\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\_\\{N\\}\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\-\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\_\\{N\\}\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{N\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right|\\+\\\\left|\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\\\setminus%20Q\\_\\{N\\}\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\-\\\\left\\(\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{N\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\\\right\\)\\\\right|%3C\\\\frac\\{\\\\varepsilon\\}\\{2\\}\\+\\\\frac\\{\\\\varepsilon\\}\\{2\\}=\\\\varepsilon\\}).<br /><br />theorem: ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\sum\\_\\{\\\\dot\\{q\\}\\\\in%20Q\\}U\\\\left\\(\\\\dot\\{q\\},\\\\dot\\{y\\}\\_\\{1:\\\\infty\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(\\\\dot\\{q\\}\\\\right\\)\\}=\\\\sup\\_\\{p\\\\in%20P\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}),<br />where, ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\dot\\{y\\}\\_\\{k\\}:=\\\\arg\\\\max\\_\\{y\\_\\{k\\}\\\\in%20Y\\}\\\\sup\\_\\{p\\\\in%20P:p\\\\left\\(\\\\dot\\{x\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}\\}\\\\sum\\_\\{q\\\\in%20Q:q\\\\left\\(\\\\dot\\{y\\}\\_\\{%3Ck\\}\\\\right\\)=\\\\dot\\{x\\}\\_\\{%3Ck\\}\\}U\\\\left\\(q,\\\\dot\\{y\\}\\_\\{%3Ck\\}y\\_\\{k\\}y\\_\\{k\\+1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br /><br />proof of theorem: Let's call the policy implemented by the agent ![](http://www.codecogs.com/png.latex?p%5E\\{\\*\\}).<br />By lemma 1, there is a sequence of policies ![](http://www.codecogs.com/png.latex?\\\\left\\\\\\{%20p\\_\\{n\\}\\\\right\\\\\\}%20\\_\\{n=1\\}%5E\\{\\\\infty\\})<br />converging to ![](http://www.codecogs.com/png.latex?p%5E\\{\\*\\}) such that ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\lim\\_\\{n\\\\rightarrow\\\\infty\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p\\_\\{n\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}=\\\\sup\\_\\{p\\\\in%20P\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).<br />By lemma 2, ![](http://www.codecogs.com/png.latex?\\{\\\\displaystyle%20\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{p%5E\\{\\*\\}q\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}=\\\\sup\\_\\{p\\\\in%20P\\}\\\\sum\\_\\{q\\\\in%20Q\\}U\\\\left\\(q,y\\_\\{1:\\\\infty\\}%5E\\{pq\\}\\\\right\\)2%5E\\{\\-\\\\ell\\\\left\\(q\\\\right\\)\\}\\}).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sqB9aJaBkzc3ASYsT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "20684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T03:48:05.173Z", "modifiedAt": null, "url": null, "title": "A utility-maximizing varient of AIXI", "slug": "a-utility-maximizing-varient-of-aixi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LGSotF4bQ2pRSbB8a/a-utility-maximizing-varient-of-aixi", "pageUrlRelative": "/posts/LGSotF4bQ2pRSbB8a/a-utility-maximizing-varient-of-aixi", "linkUrl": "https://www.lesswrong.com/posts/LGSotF4bQ2pRSbB8a/a-utility-maximizing-varient-of-aixi", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20utility-maximizing%20varient%20of%20AIXI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20utility-maximizing%20varient%20of%20AIXI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLGSotF4bQ2pRSbB8a%2Fa-utility-maximizing-varient-of-aixi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20utility-maximizing%20varient%20of%20AIXI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLGSotF4bQ2pRSbB8a%2Fa-utility-maximizing-varient-of-aixi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLGSotF4bQ2pRSbB8a%2Fa-utility-maximizing-varient-of-aixi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1545, "htmlBody": "<p><em>Response to: <a href=\"/lw/feo/universal_agents_and_utility_functions/\">Universal agents and utility functions</a></em></p>\n<p><em>Related approaches: <a href=\"http://versita.metapress.com/content/q5523w34gk767041/?genre=article&amp;id=doi%3a10.2478%2fv10229-011-0013-5\">Hibbard (2012)</a>,&nbsp; <a href=\"http://www.cs.auckland.ac.nz/~nickjhay/honours.revamped.pdf\">Hay (2005)</a><br /></em></p>\n<h2>Background</h2>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Here is the function implemented by finite-lifetime <a href=\"http://www.hutter1.net/ai/aixigentle.pdf\">AI<img src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\\xi\" width=\"8\" height=\"17\" /></a>:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where&nbsp;<img src=\"http://www.codecogs.com/png.latex?m\" alt=\"m\" width=\"17\" height=\"10\" /> is the number of steps in the lifetime of the agent,&nbsp;<img src=\"http://www.codecogs.com/png.latex?k\" alt=\"k\" width=\"8\" height=\"13\" /> is the current step being computed,&nbsp;<img src=\"http://www.codecogs.com/png.latex?X\" alt=\"X\" width=\"16\" height=\"13\" /> is the set of possible observations, <img src=\"http://www.codecogs.com/png.latex?Y\" alt=\"Y\" width=\"14\" height=\"13\" /> is the set of possible actions,&nbsp;<img src=\"http://www.codecogs.com/png.latex?r\" alt=\"r\" width=\"10\" height=\"10\" /> is a function that extracts a reward value from an observation, a dot over a variable represents that its value is known to be the true value of the action or observation it represents, underlines represent that the variable is an input to a probability distribution, and <img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" /> is a function that returns the probability of a sequence of observations, given a certain known history and sequence of actions, and starting from the Solomonoff prior. More formally,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29=\\left%28\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}2^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}2^{-\\ell\\left%28q\\right%29}\\right%29}\" alt=\"{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}2^{-\\ell\\left(q\\right)}\\right)}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"Q\" width=\"15\" height=\"18\" /> is the set of all programs, <img src=\"http://www.codecogs.com/png.latex?\\ell\" alt=\"\\ell\" width=\"7\" height=\"13\" /> is a function that returns the length of a program in bits, and a program applied to a sequence of actions returns the resulting sequence of observations. Notice that the denominator is a constant, depending only on the already known <img src=\"http://www.codecogs.com/png.latex?\\dot{y}\\dot{x}_{%3Ck}\" alt=\"\\dot{y}\\dot{x}_{%3Ck}\" width=\"37\" height=\"17\" />, and multiplying by a positive constant does not change the argmax, so we can pretend that the denominator doesn't exist. If <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\" /> is a valid program, then any longer program with <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\" /> as a prefix is not a valid program, so <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}2^{-\\ell\\left%28q\\right%29}\\leq1}\" alt=\"{\\displaystyle%20\\sum_{q\\in%20Q}2^{-\\ell\\left%28q\\right%29}\\leq1}\" width=\"102\" height=\"43\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Problem</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">A problem with this is that it can only optimize over the input it receives, not over aspects of the external world that it cannot observe. Given the chance, AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" /> would hack its input channel so that it would only observe good things, instead of trying to make good things happen (in other words, it would <a href=\"/lw/fkx/a\\_definition\\_of\\_wireheading/\">wirehead</a> itself). Anja <a href=\"/lw/feo/universal\\_agents\\_and\\_utility\\_functions/\">specified</a> a variant of AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" /> in which she replaced the sum of rewards with a single utility value and made the domain of the utility function be the entire sequence of actions and observations instead of a single observation, like so:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}U\\left%28\\dot{y}\\dot{x}_{%3Ck}yx_{k:m}\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" alt=\"{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}U\\left%28\\dot{y}\\dot{x}_{%3Ck}yx_{k:m}\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" width=\"575\" height=\"44\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">This doesn't really solve the problem, because the utility function still only takes what the agent can see, rather than what is actually going on outside the agent. The situation is a little better because the utility function also takes into account the agent's actions, so it could punish actions that look like the agent is trying to wirehead itself, but if there was a flaw in the instructions not to wirehead, the agent would exploit it, so the incentive not to wirehead would have to be perfect, and this formulation is not very enlightening about how to do that.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><em>[Edit: Hibbard (2012) also presents a solution to this problem. I haven't read all of it yet, but it appears to be fairly different from what I suggest in the next section.]</em></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Solution</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Here's what I suggest instead: everything that happens is determined by the program that the world is running on and the agent's actions, so the domain of the utility function should be <img src=\"http://www.codecogs.com/png.latex?Q\\times%20Y^{m}\" alt=\"Q\\times%20Y^{m}\" width=\"62\" height=\"17\" />. The apparent problem with that is that the formula for AI<img src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\\xi\" width=\"8\" height=\"17\" /> does not contain any mention of elements of <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"Q\" width=\"15\" height=\"18\" />. If we just take the original formula and replace <img src=\"http://www.codecogs.com/png.latex?r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\" alt=\"r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\" /></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">with <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,\\dot{y}_{%3Ck}y_{k:m}\\right%29\" alt=\"U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)\" width=\"103\" height=\"19\" />, it wouldn't make any sense. However, if we expand out <img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" /> in the original formula (excluding the unnecessary denominator), we can move the sum of rewards inside the sum over programs, like this:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Now it is easy to replace the sum of rewards with the desired utility function.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}U\\left%28q,\\dot{y}_{%3Ck}y_{k:m}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">With this formulation, there is no danger of the agent wireheading, and all <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> has to do is compute everything that happens when the agent performs a given sequence of actions in a given program, and decide how desirable it is. If the range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> is unbounded, then this might not converge. Let's assume throughout this post that the range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> is bounded.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><em>[Edit: Hay (2005) presents a similar formulation to this.]</em></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Extension to infinite lifetimes</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">The previous discussion assumed that the agent would only have the opportunity to perform a finite number of actions. The situation gets a little tricky when the agent is allowed to perform an unbounded number of actions. Hutter uses a finite look-ahead approach for AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" />, where on each step <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"k\" width=\"8\" height=\"13\" />, it pretends that it will only be performing <img src=\"http://www.codecogs.com/png.latex?m_{k}\" alt=\"m_{k}\" width=\"23\" height=\"11\" /> actions, where <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\,%20m_{k}\\gg%20k\" alt=\"\\forall%20k\\,%20m_{k}\\gg%20k\" width=\"84\" height=\"16\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m_{k}}\\in%20Y}\\sum_{x_{m_{k}}\\in%20X}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m_{k}}\\right%29\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m_{k}}\\right%29}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{&lt;k}y\\underline{x}_{k:m_{k}}\\right)}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">If we make the same modification to the utility-based variant, we get</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m_{k}}\\in%20Y}\\sum_{x_{m_{k}}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m_{k}}\\right%29=x_{\\leq%20m_{k}}}U\\left%28q,\\dot{y}_{%3Ck}y_{k:m_{k}}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{&lt;k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">This is unsatisfactory because the domain of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> was supposed to consist of all the information necessary to determine everything that happens, but here, it is missing all the actions after step <img src=\"http://www.codecogs.com/png.latex?m_%7Bk%7D\" alt=\"m_{k}\" width=\"23\" height=\"11\" />. One obvious thing to try is to set <img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" alt=\"m_{k}:=\\infty\" width=\"70\" height=\"11\" />. This will be easier to do using a compacted expression for AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\" />:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\max_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}\\left%28r\\left%28x_{k}^{pq}\\right%29+...+r\\left%28x_{m_{k}}^{pq}\\right%29\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?P\" alt=\"P\" width=\"14\" height=\"13\" /> is the set of policies that map sequences of observations to sequences of actions and <img src=\"http://www.codecogs.com/png.latex?x_{i}^{pq}\" alt=\"x_{i}^{pq}\" width=\"23\" height=\"19\" /> is shorthand for the last observation in the sequence returned by <img src=\"http://www.codecogs.com/png.latex?q\\left%28p\\left%28\\dot{x}_{%3Ck}x_{k:i-1}^{pq}\\right%29\\right%29\" alt=\"q\\left(p\\left(\\dot{x}_{&lt;k}x_{k:i-1}^{pq}\\right)\\right)\" />. If we take this compacted formulation, modify it to accommodate the new utility function, set <img src=\"http://www.codecogs.com/png.latex?m_%7Bk%7D:=%5Cinfty\" alt=\"m_{k}:=\\infty\" width=\"70\" height=\"11\" />, and replace the maximum with a supremum (since there's an infinite number of possible policies), we get</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?y_{i}^{pq}\" alt=\"y_{i}^{pq}\" width=\"23\" height=\"19\" /> is shorthand for the last action in the sequence returned by <img src=\"http://www.codecogs.com/png.latex?p\\left%28q\\left%28\\dot{y}_{%3Ck}y_{k}y_{k+1:i-1}^{pq}\\right%29\\right%29\" alt=\"p\\left(q\\left(\\dot{y}_{&lt;k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">But there is a problem with this, which I will illustrate with a toy example. Suppose <img src=\"http://www.codecogs.com/png.latex?Y:=\\left\\{%20a,b\\right\\}\" alt=\"Y:=\\left\\{ a,b\\right\\} \" width=\"86\" height=\"19\" />, and <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,y_{1:\\infty}\\right%29=0\" alt=\"U\\left(q,y_{1:\\infty}\\right)=0\" /> when <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,%20y_{k}=a\" alt=\"\\forall k\\in\\mathbb{N}\\, y_{k}=a\" width=\"108\" height=\"17\" />, and for any <img src=\"http://www.codecogs.com/png.latex?n\\in\\mathbb{N}\" alt=\"n\\in\\mathbb{N}\" width=\"46\" height=\"14\" />, <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,y_{1:\\infty}\\right%29=1-\\frac{1}{n}\" alt=\"U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\" /> when <img src=\"http://www.codecogs.com/png.latex?y_{n}=b\" alt=\"y_{n}=b\" width=\"49\" height=\"17\" /> and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k%3Cn\\,%20y_{k}=a\" alt=\"\\forall k&lt;n\\, y_{k}=a\" width=\"108\" height=\"17\" />. (<img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> does not depend on the program <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\" /> in this example). An agent following the above formula would output <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"a\" width=\"11\" height=\"10\" /> on every step, and end up with a utility of <img src=\"http://www.codecogs.com/png.latex?0\" alt=\"0\" width=\"8\" height=\"13\" />, when it could have gotten arbitrarily close to <img src=\"http://www.codecogs.com/png.latex?1\" alt=\"1\" width=\"7\" height=\"12\" /> by eventually outputting <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"b\" width=\"7\" height=\"13\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">To avoid problems like that, we could assume the reasonable-seeming condition that if <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\" /> is an action sequence and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\" /> is a sequence of action sequences that converges to <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\" /> (by which I mean <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20y_{k}^{n}=y_{k}\" alt=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" width=\"246\" height=\"18\" />), then <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}U\\left%28q,y_{1:\\infty}^{n}\\right%29=U\\left%28q,y_{1:\\infty}\\right%29}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Under that assumption, the supremum is in fact a maximum, and the formula gives you an action sequence that will reach that maximum (proof below).</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p>If you don't like the condition I imposed on <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" />, you might not be satisfied by this. But without it, there is not necessarily a best policy. One thing you can do is, on step 1, pick some extremely small <img src=\"http://www.codecogs.com/png.latex?\\varepsilon%3E0\" alt=\"\\varepsilon&gt;0\" width=\"41\" height=\"13\" />, pick any element from</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left\\{%20p^{*}\\in%20P:\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p^{*}q}\\right%292^{-\\ell\\left%28q\\right%29}%3E\\left%28\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\varepsilon\\right\\}%20}\" alt=\"{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}&gt;\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\" />, and then follow that policy for the rest of eternity, which will guarantee that you do not miss out on more than&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\varepsilon\" alt=\"\\varepsilon\" width=\"10\" height=\"12\" /> of expected utility.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Proof of criterion for supremum-chasing working</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">definition: If <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\" /> is an action sequence and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\" /> is an infinite sequence of action sequences, and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20y_{k}^{n}=y_{k}\" alt=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, y_{k}^{n}=y_{k}\" width=\"246\" height=\"18\" />, then we say <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\" /> converges to <img src=\"http://www.codecogs.com/png.latex?y_%7B1:%5Cinfty%7D\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\" />. If <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\" /> is a policy and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\" /> is a sequence of policies, and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\forall%20x_{%3Ck}\\in%20X^{k}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20p\\left%28x_{%3Ck}\\right%29=p_{n}\\left%28x_{%3Ck}\\right%29\" alt=\"\\forall k\\in\\mathbb{N}\\,\\forall x_{&lt;k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n&gt;N\\, p\\left(x_{&lt;k}\\right)=p_{n}\\left(x_{&lt;k}\\right)\" />, then we say <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\" /> converges to <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">assumption (for lemma 2 and theorem): If <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20y_%7B1:%5Cinfty%7D%5E%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\" /> converges to <img src=\"http://www.codecogs.com/png.latex?y_%7B1:%5Cinfty%7D\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\" />, then <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}U\\left%28q,y_{1:\\infty}^{n}\\right%29=U\\left%28q,y_{1:\\infty}\\right%29}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" width=\"219\" height=\"25\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">lemma 1: The agent described by</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"516\" height=\"44\" /> follows a policy that is the limit of a sequence of policies <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\" /> such that</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of lemma 1: Any policy can be completely described by the last action it outputs for every finite observation sequence. Observations are returned by a program, so the set of possible finite observation sequences is countable. It is possible to fix the last action returned on any particular finite observation sequence to be the argmax, and still get arbitrarily close to the supremum with suitable choices for the last action returned on the other finite observation sequences. By induction, it is possible to get arbitrarily close to the supremum while fixing the last action returned to be the argmax for any finite set of finite observation sequences. Thus, there exists a sequence of policies approaching the policy that the agent implements whose expected utilities approach the supremum.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">lemma 2: If <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\" /> is a policy and <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\" /> is a sequence of policies converging to <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\" />, then</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of lemma 2: Let <img src=\"http://www.codecogs.com/png.latex?%5Cvarepsilon%3E0\" alt=\"\\varepsilon&gt;0\" width=\"41\" height=\"13\" />. On any given sequence of inputs <img src=\"http://www.codecogs.com/png.latex?x_{1:\\infty}\\in%20X^{\\infty}\" alt=\"x_{1:\\infty}\\in X^{\\infty}\" width=\"85\" height=\"16\" />, <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\left%28x_{1:\\infty}\\right%29\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\" /> converges to <img src=\"http://www.codecogs.com/png.latex?p\\left%28x_{1:\\infty}\\right%29\" alt=\"p\\left(x_{1:\\infty}\\right)\" />, so, by assumption, <img src=\"http://www.codecogs.com/png.latex?\\forall%20q\\in%20Q\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n\\geq%20N\\,\\left|U\\left%28q,y_{1:\\infty}^{pq}\\right%29-U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%29\\right|%3C\\frac{\\varepsilon}{2}\" alt=\"\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">For each <img src=\"http://www.codecogs.com/png.latex?N\\in\\mathbb{N}\" alt=\"N\\in\\mathbb{N}\" width=\"52\" height=\"14\" />, let <img src=\"http://www.codecogs.com/png.latex?Q_{N}:=\\left\\{%20q\\in%20Q:\\forall%20n\\geq%20N\\,\\left|U\\left%28q,y_{1:\\infty}^{pq}\\right%29-U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%29\\right|%3C\\frac{\\varepsilon}{2}\\right\\}\" alt=\"Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|&lt;\\frac{\\varepsilon}{2}\\right\\} \" />. The previous statement implies that <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" alt=\"{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" width=\"96\" height=\"41\" />, and each element of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20Q_{N}\\right\\}%20_{N\\in\\mathbb{N}}\" alt=\"\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" width=\"74\" height=\"21\" /> is a subset of the next, so</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\exists%20N\\in\\mathbb{N}\\,\\sum_{q\\in%20Q\\setminus%20Q_{N}}2^{-\\ell\\left%28q\\right%29}%3C\\frac{\\varepsilon}{2\\left%28\\sup%20U-\\inf%20U\\right%29}}\" alt=\"{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\" />. The range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\" /> is bounded, so&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\sup%20U\" alt=\"\\sup U\" width=\"44\" height=\"17\" /> and <img src=\"http://www.codecogs.com/png.latex?\\inf%20U\" alt=\"\\inf U\" width=\"39\" height=\"14\" /> are defined. This also implies that the difference in expected utility, given any information, of any two policies, is bounded. More formally:<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\forall%20Q%27\\subset%20Q\\,\\forall%20p%27,p%27%27\\in%20P\\,\\left|\\left%28\\left%28\\sum_{q\\in%20Q%27}U\\left%28q,y_{1:\\infty}^{p%27q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q%27}2^{-\\ell\\left%28q\\right%29}\\right%29\\right%29-\\left%28\\left%28\\sum_{q\\in%20Q%27}U\\left%28q,y_{1:\\infty}^{p%27%27q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q%27}2^{-\\ell\\left%28q\\right%29}\\right%29\\right%29\\right|\\leq\\sup%20U-\\inf%20U}\" alt=\"{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">so in particular,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left|\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|\\leq\\left%28\\sup%20U-\\inf%20U\\right%29\\sum_{q\\in%20Q\\setminus%20Q_{N}}2^{-\\ell\\left%28q\\right%29}%3C\\frac{\\varepsilon}{2}}\" alt=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}&lt;\\frac{\\varepsilon}{2}}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left|\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|\\leq\\left|\\left%28\\sum_{q\\in%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q_{N}}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|+\\left|\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|%3C\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" alt=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|&lt;\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">theorem:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{\\dot{q}\\in%20Q}U\\left%28\\dot{q},\\dot{y}_{1:\\infty}\\right%292^{-\\ell\\left%28\\dot{q}\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" />,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{&lt;k}\\right)=\\dot{y}_{&lt;k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{&lt;k}\\right)=\\dot{x}_{&lt;k}}U\\left(q,\\dot{y}_{&lt;k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"516\" height=\"44\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of theorem: Let's call the policy implemented by the agent <img src=\"http://www.codecogs.com/png.latex?p^{*}\" alt=\"p^{*}\" width=\"16\" height=\"17\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">By lemma 1, there is a sequence of policies <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\" /></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">converging to <img src=\"http://www.codecogs.com/png.latex?p%5E%7B*%7D\" alt=\"p^{*}\" width=\"16\" height=\"17\" /> such that</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"397\" height=\"43\" />.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">By lemma 2,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p^{*}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" />.</p>\n<h3 style=\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><br /></h3>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LGSotF4bQ2pRSbB8a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 1.0602173253750728e-06, "legacy": true, "legacyId": "20691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Response to: <a href=\"/lw/feo/universal_agents_and_utility_functions/\">Universal agents and utility functions</a></em></p>\n<p><em>Related approaches: <a href=\"http://versita.metapress.com/content/q5523w34gk767041/?genre=article&amp;id=doi%3a10.2478%2fv10229-011-0013-5\">Hibbard (2012)</a>,&nbsp; <a href=\"http://www.cs.auckland.ac.nz/~nickjhay/honours.revamped.pdf\">Hay (2005)</a><br></em></p>\n<h2 id=\"Background\">Background</h2>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Here is the function implemented by finite-lifetime <a href=\"http://www.hutter1.net/ai/aixigentle.pdf\">AI<img src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\\xi\" width=\"8\" height=\"17\"></a>:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{<k}y\\underline{x}_{k:m}\\right)}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where&nbsp;<img src=\"http://www.codecogs.com/png.latex?m\" alt=\"m\" width=\"17\" height=\"10\"> is the number of steps in the lifetime of the agent,&nbsp;<img src=\"http://www.codecogs.com/png.latex?k\" alt=\"k\" width=\"8\" height=\"13\"> is the current step being computed,&nbsp;<img src=\"http://www.codecogs.com/png.latex?X\" alt=\"X\" width=\"16\" height=\"13\"> is the set of possible observations, <img src=\"http://www.codecogs.com/png.latex?Y\" alt=\"Y\" width=\"14\" height=\"13\"> is the set of possible actions,&nbsp;<img src=\"http://www.codecogs.com/png.latex?r\" alt=\"r\" width=\"10\" height=\"10\"> is a function that extracts a reward value from an observation, a dot over a variable represents that its value is known to be the true value of the action or observation it represents, underlines represent that the variable is an input to a probability distribution, and <img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\"> is a function that returns the probability of a sequence of observations, given a certain known history and sequence of actions, and starting from the Solomonoff prior. More formally,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29=\\left%28\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}2^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}2^{-\\ell\\left%28q\\right%29}\\right%29}\" alt=\"{\\displaystyle \\xi\\left(\\dot{y}\\dot{x}_{<k}y\\underline{x}_{k:m}\\right)=\\left(\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q:q\\left(\\dot{y}_{<k}\\right)=\\dot{x}_{<k}}2^{-\\ell\\left(q\\right)}\\right)}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"Q\" width=\"15\" height=\"18\"> is the set of all programs, <img src=\"http://www.codecogs.com/png.latex?\\ell\" alt=\"\\ell\" width=\"7\" height=\"13\"> is a function that returns the length of a program in bits, and a program applied to a sequence of actions returns the resulting sequence of observations. Notice that the denominator is a constant, depending only on the already known <img src=\"http://www.codecogs.com/png.latex?\\dot{y}\\dot{x}_{%3Ck}\" alt=\"\\dot{y}\\dot{x}_{%3Ck}\" width=\"37\" height=\"17\">, and multiplying by a positive constant does not change the argmax, so we can pretend that the denominator doesn't exist. If <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\"> is a valid program, then any longer program with <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\"> as a prefix is not a valid program, so <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}2^{-\\ell\\left%28q\\right%29}\\leq1}\" alt=\"{\\displaystyle%20\\sum_{q\\in%20Q}2^{-\\ell\\left%28q\\right%29}\\leq1}\" width=\"102\" height=\"43\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\" id=\"Problem\">Problem</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">A problem with this is that it can only optimize over the input it receives, not over aspects of the external world that it cannot observe. Given the chance, AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\"> would hack its input channel so that it would only observe good things, instead of trying to make good things happen (in other words, it would <a href=\"/lw/fkx/a\\_definition\\_of\\_wireheading/\">wirehead</a> itself). Anja <a href=\"/lw/feo/universal\\_agents\\_and\\_utility\\_functions/\">specified</a> a variant of AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\"> in which she replaced the sum of rewards with a single utility value and made the domain of the utility function be the entire sequence of actions and observations instead of a single observation, like so:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}U\\left%28\\dot{y}\\dot{x}_{%3Ck}yx_{k:m}\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" alt=\"{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}U\\left%28\\dot{y}\\dot{x}_{%3Ck}yx_{k:m}\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m}\\right%29}\" width=\"575\" height=\"44\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">This doesn't really solve the problem, because the utility function still only takes what the agent can see, rather than what is actually going on outside the agent. The situation is a little better because the utility function also takes into account the agent's actions, so it could punish actions that look like the agent is trying to wirehead itself, but if there was a flaw in the instructions not to wirehead, the agent would exploit it, so the incentive not to wirehead would have to be perfect, and this formulation is not very enlightening about how to do that.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><em>[Edit: Hibbard (2012) also presents a solution to this problem. I haven't read all of it yet, but it appears to be fairly different from what I suggest in the next section.]</em></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\" id=\"Solution\">Solution</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Here's what I suggest instead: everything that happens is determined by the program that the world is running on and the agent's actions, so the domain of the utility function should be <img src=\"http://www.codecogs.com/png.latex?Q\\times%20Y^{m}\" alt=\"Q\\times%20Y^{m}\" width=\"62\" height=\"17\">. The apparent problem with that is that the formula for AI<img src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\\xi\" width=\"8\" height=\"17\"> does not contain any mention of elements of <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"Q\" width=\"15\" height=\"18\">. If we just take the original formula and replace <img src=\"http://www.codecogs.com/png.latex?r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\" alt=\"r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\"></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">with <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,\\dot{y}_{%3Ck}y_{k:m}\\right%29\" alt=\"U\\left(q,\\dot{y}_{<k}y_{k:m}\\right)\" width=\"103\" height=\"19\">, it wouldn't make any sense. However, if we expand out <img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\"> in the original formula (excluding the unnecessary denominator), we can move the sum of rewards inside the sum over programs, like this:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m}\\right%29\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m}\\right)\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Now it is easy to replace the sum of rewards with the desired utility function.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m}\\in%20Y}\\sum_{x_{m}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m}\\right%29=x_{\\leq%20m}}U\\left%28q,\\dot{y}_{%3Ck}y_{k:m}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m}\\in Y}\\sum_{x_{m}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m}\\right)=x_{\\leq m}}U\\left(q,\\dot{y}_{<k}y_{k:m}\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">With this formulation, there is no danger of the agent wireheading, and all <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> has to do is compute everything that happens when the agent performs a given sequence of actions in a given program, and decide how desirable it is. If the range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> is unbounded, then this might not converge. Let's assume throughout this post that the range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> is bounded.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><em>[Edit: Hay (2005) presents a similar formulation to this.]</em></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\" id=\"Extension_to_infinite_lifetimes\">Extension to infinite lifetimes</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">The previous discussion assumed that the agent would only have the opportunity to perform a finite number of actions. The situation gets a little tricky when the agent is allowed to perform an unbounded number of actions. Hutter uses a finite look-ahead approach for AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\">, where on each step <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"k\" width=\"8\" height=\"13\">, it pretends that it will only be performing <img src=\"http://www.codecogs.com/png.latex?m_{k}\" alt=\"m_{k}\" width=\"23\" height=\"11\"> actions, where <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\,%20m_{k}\\gg%20k\" alt=\"\\forall%20k\\,%20m_{k}\\gg%20k\" width=\"84\" height=\"16\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m_{k}}\\in%20Y}\\sum_{x_{m_{k}}\\in%20X}\\left%28r\\left%28x_{k}\\right%29+...+r\\left%28x_{m_{k}}\\right%29\\right%29\\cdot\\xi\\left%28\\dot{y}\\dot{x}_{%3Ck}y\\underline{x}_{k:m_{k}}\\right%29}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\left(r\\left(x_{k}\\right)+...+r\\left(x_{m_{k}}\\right)\\right)\\cdot\\xi\\left(\\dot{y}\\dot{x}_{<k}y\\underline{x}_{k:m_{k}}\\right)}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">If we make the same modification to the utility-based variant, we get</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sum_{x_{k}\\in%20X}\\max_{y_{k+1}\\in%20Y}\\sum_{x_{k+1}\\in%20X}...\\max_{y_{m_{k}}\\in%20Y}\\sum_{x_{m_{k}}\\in%20X}\\sum_{q\\in%20Q:q\\left%28y_{\\leq%20m_{k}}\\right%29=x_{\\leq%20m_{k}}}U\\left%28q,\\dot{y}_{%3Ck}y_{k:m_{k}}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sum_{x_{k}\\in X}\\max_{y_{k+1}\\in Y}\\sum_{x_{k+1}\\in X}...\\max_{y_{m_{k}}\\in Y}\\sum_{x_{m_{k}}\\in X}\\sum_{q\\in Q:q\\left(y_{\\leq m_{k}}\\right)=x_{\\leq m_{k}}}U\\left(q,\\dot{y}_{<k}y_{k:m_{k}}\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">This is unsatisfactory because the domain of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> was supposed to consist of all the information necessary to determine everything that happens, but here, it is missing all the actions after step <img src=\"http://www.codecogs.com/png.latex?m_%7Bk%7D\" alt=\"m_{k}\" width=\"23\" height=\"11\">. One obvious thing to try is to set <img src=\"http://www.codecogs.com/png.latex?m_{k}:=\\infty\" alt=\"m_{k}:=\\infty\" width=\"70\" height=\"11\">. This will be easier to do using a compacted expression for AI<img src=\"http://www.codecogs.com/png.latex?%5Cxi\" alt=\"\\xi\" width=\"8\" height=\"17\">:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\max_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}\\left%28r\\left%28x_{k}^{pq}\\right%29+...+r\\left%28x_{m_{k}}^{pq}\\right%29\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\max_{p\\in P:p\\left(\\dot{x}_{<k}\\right)=\\dot{y}_{<k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{<k}\\right)=\\dot{x}_{<k}}\\left(r\\left(x_{k}^{pq}\\right)+...+r\\left(x_{m_{k}}^{pq}\\right)\\right)2^{-\\ell\\left(q\\right)}}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?P\" alt=\"P\" width=\"14\" height=\"13\"> is the set of policies that map sequences of observations to sequences of actions and <img src=\"http://www.codecogs.com/png.latex?x_{i}^{pq}\" alt=\"x_{i}^{pq}\" width=\"23\" height=\"19\"> is shorthand for the last observation in the sequence returned by <img src=\"http://www.codecogs.com/png.latex?q\\left%28p\\left%28\\dot{x}_{%3Ck}x_{k:i-1}^{pq}\\right%29\\right%29\" alt=\"q\\left(p\\left(\\dot{x}_{<k}x_{k:i-1}^{pq}\\right)\\right)\">. If we take this compacted formulation, modify it to accommodate the new utility function, set <img src=\"http://www.codecogs.com/png.latex?m_%7Bk%7D:=%5Cinfty\" alt=\"m_{k}:=\\infty\" width=\"70\" height=\"11\">, and replace the maximum with a supremum (since there's an infinite number of possible policies), we get</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{<k}\\right)=\\dot{y}_{<k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{<k}\\right)=\\dot{x}_{<k}}U\\left(q,\\dot{y}_{<k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?y_{i}^{pq}\" alt=\"y_{i}^{pq}\" width=\"23\" height=\"19\"> is shorthand for the last action in the sequence returned by <img src=\"http://www.codecogs.com/png.latex?p\\left%28q\\left%28\\dot{y}_{%3Ck}y_{k}y_{k+1:i-1}^{pq}\\right%29\\right%29\" alt=\"p\\left(q\\left(\\dot{y}_{<k}y_{k}y_{k+1:i-1}^{pq}\\right)\\right)\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">But there is a problem with this, which I will illustrate with a toy example. Suppose <img src=\"http://www.codecogs.com/png.latex?Y:=\\left\\{%20a,b\\right\\}\" alt=\"Y:=\\left\\{ a,b\\right\\} \" width=\"86\" height=\"19\">, and <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,y_{1:\\infty}\\right%29=0\" alt=\"U\\left(q,y_{1:\\infty}\\right)=0\"> when <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,%20y_{k}=a\" alt=\"\\forall k\\in\\mathbb{N}\\, y_{k}=a\" width=\"108\" height=\"17\">, and for any <img src=\"http://www.codecogs.com/png.latex?n\\in\\mathbb{N}\" alt=\"n\\in\\mathbb{N}\" width=\"46\" height=\"14\">, <img src=\"http://www.codecogs.com/png.latex?U\\left%28q,y_{1:\\infty}\\right%29=1-\\frac{1}{n}\" alt=\"U\\left(q,y_{1:\\infty}\\right)=1-\\frac{1}{n}\"> when <img src=\"http://www.codecogs.com/png.latex?y_{n}=b\" alt=\"y_{n}=b\" width=\"49\" height=\"17\"> and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k%3Cn\\,%20y_{k}=a\" alt=\"\\forall k<n\\, y_{k}=a\" width=\"108\" height=\"17\">. (<img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> does not depend on the program <img src=\"http://www.codecogs.com/png.latex?q\" alt=\"q\" width=\"8\" height=\"12\"> in this example). An agent following the above formula would output <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"a\" width=\"11\" height=\"10\"> on every step, and end up with a utility of <img src=\"http://www.codecogs.com/png.latex?0\" alt=\"0\" width=\"8\" height=\"13\">, when it could have gotten arbitrarily close to <img src=\"http://www.codecogs.com/png.latex?1\" alt=\"1\" width=\"7\" height=\"12\"> by eventually outputting <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"b\" width=\"7\" height=\"13\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">To avoid problems like that, we could assume the reasonable-seeming condition that if <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\"> is an action sequence and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\"> is a sequence of action sequences that converges to <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\"> (by which I mean <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20y_{k}^{n}=y_{k}\" alt=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n>N\\, y_{k}^{n}=y_{k}\" width=\"246\" height=\"18\">), then <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}U\\left%28q,y_{1:\\infty}^{n}\\right%29=U\\left%28q,y_{1:\\infty}\\right%29}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">Under that assumption, the supremum is in fact a maximum, and the formula gives you an action sequence that will reach that maximum (proof below).</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p>If you don't like the condition I imposed on <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\">, you might not be satisfied by this. But without it, there is not necessarily a best policy. One thing you can do is, on step 1, pick some extremely small <img src=\"http://www.codecogs.com/png.latex?\\varepsilon%3E0\" alt=\"\\varepsilon>0\" width=\"41\" height=\"13\">, pick any element from</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left\\{%20p^{*}\\in%20P:\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p^{*}q}\\right%292^{-\\ell\\left%28q\\right%29}%3E\\left%28\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\varepsilon\\right\\}%20}\" alt=\"{\\displaystyle \\left\\{ p^{*}\\in P:\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}>\\left(\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\varepsilon\\right\\} }\">, and then follow that policy for the rest of eternity, which will guarantee that you do not miss out on more than&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\varepsilon\" alt=\"\\varepsilon\" width=\"10\" height=\"12\"> of expected utility.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<h2 style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\" id=\"Proof_of_criterion_for_supremum_chasing_working\">Proof of criterion for supremum-chasing working</h2>\n<p>&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">definition: If <img src=\"http://www.codecogs.com/png.latex?y_{1:\\infty}\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\"> is an action sequence and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\"> is an infinite sequence of action sequences, and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20y_{k}^{n}=y_{k}\" alt=\"\\forall k\\in\\mathbb{N}\\,\\exists N\\in\\mathbb{N}\\,\\forall n>N\\, y_{k}^{n}=y_{k}\" width=\"246\" height=\"18\">, then we say <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20y_{1:\\infty}^{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\"> converges to <img src=\"http://www.codecogs.com/png.latex?y_%7B1:%5Cinfty%7D\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\">. If <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\"> is a policy and <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\"> is a sequence of policies, and <img src=\"http://www.codecogs.com/png.latex?\\forall%20k\\in\\mathbb{N}\\,\\forall%20x_{%3Ck}\\in%20X^{k}\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n%3EN\\,%20p\\left%28x_{%3Ck}\\right%29=p_{n}\\left%28x_{%3Ck}\\right%29\" alt=\"\\forall k\\in\\mathbb{N}\\,\\forall x_{<k}\\in X^{k}\\,\\exists N\\in\\mathbb{N}\\,\\forall n>N\\, p\\left(x_{<k}\\right)=p_{n}\\left(x_{<k}\\right)\">, then we say <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\"> converges to <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">assumption (for lemma 2 and theorem): If <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20y_%7B1:%5Cinfty%7D%5E%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ y_{1:\\infty}^{n}\\right\\} _{n=1}^{\\infty}\" width=\"74\" height=\"20\"> converges to <img src=\"http://www.codecogs.com/png.latex?y_%7B1:%5Cinfty%7D\" alt=\"y_{1:\\infty}\" width=\"31\" height=\"12\">, then <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}U\\left%28q,y_{1:\\infty}^{n}\\right%29=U\\left%28q,y_{1:\\infty}\\right%29}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}U\\left(q,y_{1:\\infty}^{n}\\right)=U\\left(q,y_{1:\\infty}\\right)}\" width=\"219\" height=\"25\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">lemma 1: The agent described by</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{<k}\\right)=\\dot{y}_{<k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{<k}\\right)=\\dot{x}_{<k}}U\\left(q,\\dot{y}_{<k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"516\" height=\"44\"> follows a policy that is the limit of a sequence of policies <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\"> such that</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of lemma 1: Any policy can be completely described by the last action it outputs for every finite observation sequence. Observations are returned by a program, so the set of possible finite observation sequences is countable. It is possible to fix the last action returned on any particular finite observation sequence to be the argmax, and still get arbitrarily close to the supremum with suitable choices for the last action returned on the other finite observation sequences. By induction, it is possible to get arbitrarily close to the supremum while fixing the last action returned to be the argmax for any finite set of finite observation sequences. Thus, there exists a sequence of policies approaching the policy that the agent implements whose expected utilities approach the supremum.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">lemma 2: If <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\"> is a policy and <img src=\"http://www.codecogs.com/png.latex?%5Cleft%5C%7B%20p_%7Bn%7D%5Cright%5C%7D%20_%7Bn=1%7D%5E%7B%5Cinfty%7D\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\"> is a sequence of policies converging to <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"p\">, then</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}=\\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of lemma 2: Let <img src=\"http://www.codecogs.com/png.latex?%5Cvarepsilon%3E0\" alt=\"\\varepsilon>0\" width=\"41\" height=\"13\">. On any given sequence of inputs <img src=\"http://www.codecogs.com/png.latex?x_{1:\\infty}\\in%20X^{\\infty}\" alt=\"x_{1:\\infty}\\in X^{\\infty}\" width=\"85\" height=\"16\">, <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\left%28x_{1:\\infty}\\right%29\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\left(x_{1:\\infty}\\right)\\right\\} _{n=1}^{\\infty}\"> converges to <img src=\"http://www.codecogs.com/png.latex?p\\left%28x_{1:\\infty}\\right%29\" alt=\"p\\left(x_{1:\\infty}\\right)\">, so, by assumption, <img src=\"http://www.codecogs.com/png.latex?\\forall%20q\\in%20Q\\,\\exists%20N\\in\\mathbb{N}\\,\\forall%20n\\geq%20N\\,\\left|U\\left%28q,y_{1:\\infty}^{pq}\\right%29-U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%29\\right|%3C\\frac{\\varepsilon}{2}\" alt=\"\\forall q\\in Q\\,\\exists N\\in\\mathbb{N}\\,\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|<\\frac{\\varepsilon}{2}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">For each <img src=\"http://www.codecogs.com/png.latex?N\\in\\mathbb{N}\" alt=\"N\\in\\mathbb{N}\" width=\"52\" height=\"14\">, let <img src=\"http://www.codecogs.com/png.latex?Q_{N}:=\\left\\{%20q\\in%20Q:\\forall%20n\\geq%20N\\,\\left|U\\left%28q,y_{1:\\infty}^{pq}\\right%29-U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%29\\right|%3C\\frac{\\varepsilon}{2}\\right\\}\" alt=\"Q_{N}:=\\left\\{ q\\in Q:\\forall n\\geq N\\,\\left|U\\left(q,y_{1:\\infty}^{pq}\\right)-U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)\\right|<\\frac{\\varepsilon}{2}\\right\\} \">. The previous statement implies that <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" alt=\"{\\displaystyle \\bigcup_{N\\in\\mathbb{N}}Q_{N}=Q}\" width=\"96\" height=\"41\">, and each element of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20Q_{N}\\right\\}%20_{N\\in\\mathbb{N}}\" alt=\"\\left\\{ Q_{N}\\right\\} _{N\\in\\mathbb{N}}\" width=\"74\" height=\"21\"> is a subset of the next, so</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\exists%20N\\in\\mathbb{N}\\,\\sum_{q\\in%20Q\\setminus%20Q_{N}}2^{-\\ell\\left%28q\\right%29}%3C\\frac{\\varepsilon}{2\\left%28\\sup%20U-\\inf%20U\\right%29}}\" alt=\"{\\displaystyle \\exists N\\in\\mathbb{N}\\,\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}<\\frac{\\varepsilon}{2\\left(\\sup U-\\inf U\\right)}}\">. The range of <img src=\"http://www.codecogs.com/png.latex?U\" alt=\"U\" width=\"13\" height=\"14\"> is bounded, so&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\sup%20U\" alt=\"\\sup U\" width=\"44\" height=\"17\"> and <img src=\"http://www.codecogs.com/png.latex?\\inf%20U\" alt=\"\\inf U\" width=\"39\" height=\"14\"> are defined. This also implies that the difference in expected utility, given any information, of any two policies, is bounded. More formally:<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\forall%20Q%27\\subset%20Q\\,\\forall%20p%27,p%27%27\\in%20P\\,\\left|\\left%28\\left%28\\sum_{q\\in%20Q%27}U\\left%28q,y_{1:\\infty}^{p%27q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q%27}2^{-\\ell\\left%28q\\right%29}\\right%29\\right%29-\\left%28\\left%28\\sum_{q\\in%20Q%27}U\\left%28q,y_{1:\\infty}^{p%27%27q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\diagup\\left%28\\sum_{q\\in%20Q%27}2^{-\\ell\\left%28q\\right%29}\\right%29\\right%29\\right|\\leq\\sup%20U-\\inf%20U}\" alt=\"{\\displaystyle \\forall Q'\\subset Q\\,\\forall p',p''\\in P\\,\\left|\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p'q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)-\\left(\\left(\\sum_{q\\in Q'}U\\left(q,y_{1:\\infty}^{p''q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\diagup\\left(\\sum_{q\\in Q'}2^{-\\ell\\left(q\\right)}\\right)\\right)\\right|\\leq\\sup U-\\inf U}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">so in particular,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left|\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|\\leq\\left%28\\sup%20U-\\inf%20U\\right%29\\sum_{q\\in%20Q\\setminus%20Q_{N}}2^{-\\ell\\left%28q\\right%29}%3C\\frac{\\varepsilon}{2}}\" alt=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left(\\sup U-\\inf U\\right)\\sum_{q\\in Q\\setminus Q_{N}}2^{-\\ell\\left(q\\right)}<\\frac{\\varepsilon}{2}}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left|\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|\\leq\\left|\\left%28\\sum_{q\\in%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q_{N}}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|+\\left|\\left%28\\sum_{q\\in%20Q\\setminus%20Q_{N}}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29-\\left%28\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{N}q}\\right%292^{-\\ell\\left%28q\\right%29}\\right%29\\right|%3C\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\" alt=\"{\\displaystyle \\left|\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|\\leq\\left|\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q_{N}}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|+\\left|\\left(\\sum_{q\\in Q\\setminus Q_{N}}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}\\right)-\\left(\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{N}q}\\right)2^{-\\ell\\left(q\\right)}\\right)\\right|<\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon}\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">theorem:</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{\\dot{q}\\in%20Q}U\\left%28\\dot{q},\\dot{y}_{1:\\infty}\\right%292^{-\\ell\\left%28\\dot{q}\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{\\dot{q}\\in Q}U\\left(\\dot{q},\\dot{y}_{1:\\infty}\\right)2^{-\\ell\\left(\\dot{q}\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\">,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">where <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\dot{y}_{k}:=\\arg\\max_{y_{k}\\in%20Y}\\sup_{p\\in%20P:p\\left%28\\dot{x}_{%3Ck}\\right%29=\\dot{y}_{%3Ck}y_{k}}\\sum_{q\\in%20Q:q\\left%28\\dot{y}_{%3Ck}\\right%29=\\dot{x}_{%3Ck}}U\\left%28q,\\dot{y}_{%3Ck}y_{k}y_{k+1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\dot{y}_{k}:=\\arg\\max_{y_{k}\\in Y}\\sup_{p\\in P:p\\left(\\dot{x}_{<k}\\right)=\\dot{y}_{<k}y_{k}}\\sum_{q\\in Q:q\\left(\\dot{y}_{<k}\\right)=\\dot{x}_{<k}}U\\left(q,\\dot{y}_{<k}y_{k}y_{k+1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"516\" height=\"44\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">&nbsp;</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">proof of theorem: Let's call the policy implemented by the agent <img src=\"http://www.codecogs.com/png.latex?p^{*}\" alt=\"p^{*}\" width=\"16\" height=\"17\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">By lemma 1, there is a sequence of policies <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20p_{n}\\right\\}%20_{n=1}^{\\infty}\" alt=\"\\left\\{ p_{n}\\right\\} _{n=1}^{\\infty}\" width=\"59\" height=\"20\"></p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">converging to <img src=\"http://www.codecogs.com/png.latex?p%5E%7B*%7D\" alt=\"p^{*}\" width=\"16\" height=\"17\"> such that</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\lim_{n\\rightarrow\\infty}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p_{n}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\lim_{n\\rightarrow\\infty}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p_{n}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\" width=\"397\" height=\"43\">.</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\">By lemma 2,</p>\n<p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{p^{*}q}\\right%292^{-\\ell\\left%28q\\right%29}=\\sup_{p\\in%20P}\\sum_{q\\in%20Q}U\\left%28q,y_{1:\\infty}^{pq}\\right%292^{-\\ell\\left%28q\\right%29}}\" alt=\"{\\displaystyle \\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{p^{*}q}\\right)2^{-\\ell\\left(q\\right)}=\\sup_{p\\in P}\\sum_{q\\in Q}U\\left(q,y_{1:\\infty}^{pq}\\right)2^{-\\ell\\left(q\\right)}}\">.</p>\n<h3 style=\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"><br></h3>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Problem", "anchor": "Problem", "level": 1}, {"title": "Solution", "anchor": "Solution", "level": 1}, {"title": "Extension to infinite lifetimes", "anchor": "Extension_to_infinite_lifetimes", "level": 1}, {"title": "Proof of criterion for supremum-chasing working", "anchor": "Proof_of_criterion_for_supremum_chasing_working", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q3mZNmvqBtnG2nQre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T04:08:27.854Z", "modifiedAt": null, "url": null, "title": "Parallelizing Rationality: How Should Rationalists Think in Groups?", "slug": "parallelizing-rationality-how-should-rationalists-think-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:34.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "almkglor", "createdAt": "2012-11-24T01:59:31.415Z", "isAdmin": false, "displayName": "almkglor"}, "userId": "uzR74doZZhkQtjYTG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4nDJM3nBvKxeoHzgo/parallelizing-rationality-how-should-rationalists-think-in", "pageUrlRelative": "/posts/4nDJM3nBvKxeoHzgo/parallelizing-rationality-how-should-rationalists-think-in", "linkUrl": "https://www.lesswrong.com/posts/4nDJM3nBvKxeoHzgo/parallelizing-rationality-how-should-rationalists-think-in", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Parallelizing%20Rationality%3A%20How%20Should%20Rationalists%20Think%20in%20Groups%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParallelizing%20Rationality%3A%20How%20Should%20Rationalists%20Think%20in%20Groups%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4nDJM3nBvKxeoHzgo%2Fparallelizing-rationality-how-should-rationalists-think-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Parallelizing%20Rationality%3A%20How%20Should%20Rationalists%20Think%20in%20Groups%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4nDJM3nBvKxeoHzgo%2Fparallelizing-rationality-how-should-rationalists-think-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4nDJM3nBvKxeoHzgo%2Fparallelizing-rationality-how-should-rationalists-think-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3281, "htmlBody": "<p>Consider the following statement: <strong>two heads are better than one</strong>.</p>\n<p>It seems obvious to me that several rationalists working together can, effectively, bring more precious brainpower to bear on a problem, than one rationalist working alone (otherwise, what would be the point of having a Less Wrong forum community? You might as well just leave it as a curated community blog and excise the discussion forums.). Further, due to various efforts (<a href=\"http://hpmor.com/\">HPMOR</a> especially) it appears that <a href=\"/lw/fqj/poll_is_endless_september_a_threat_to_lw_and_what/\">LW is inevitably growing</a>. This makes it not only <em>desirable</em> to find ways to effectively get <a href=\"/lw/cz/the_craft_and_the_community/\">groups of rationalists</a> to <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">think together</a>, but also increasingly <em>necessary</em>.</p>\n<p>It is also <em>desirable</em> that methods of getting groups to think should be feasibly doable over the Internet. <small>(I am aware that real-life meetups and stuff exist, but please be reminded that some people in the world do have to live in shitty little third-world countries and might not at all find it economically feasible to go to first-world countries with atrociously high costs-of-living)</small></p>\n<p>So first, let us start with the current \"best methods\" of getting groups of Traditional Rationalists to coordinate and think, while avoiding <a href=\"/lw/lm/affective_death_spirals/\">groupthink effects</a> that diminish our aggregate rationality. Hopefully, we can then use it as the basis of part of <a href=\"/lw/c4/go_forth_and_create_the_art/\">the art of rationalist group thinking</a>. So I'll discuss:</p>\n<ul>\n<li><strong>Disputation Arenas</strong> - procedures with <strong>centrifugal</strong> and <strong>centripetal</strong> phases <ol>\n<li><strong>Delphi Method</strong> - members secretly answer questions, non-member summarizer anonymizes answers, members read anonymized summary, repeat</li>\n<li><strong>Prediction Market</strong> - members place stock bets on market, market settles on price, members react to price signal, repeat</li>\n<li><strong>Nominal Group Technique</strong> - members write down ideas privately, non-member facilitator guides members in sharing ideas, non-member facilitator guides members in paring down and cleaning up ideas, members vote</li>\n</ol></li>\n<li><strong>Conclusion</strong> - strengths and weaknesses of the various disputation arenas shown here</li>\n</ul>\n<h1>Disputation Arenas</h1>\n<p>One of my favorite SF authors, David Brin, talks a bit about <a href=\"http://www.davidbrin.com/disputation.html\">what he calls \"disputation arenas\"</a>. I won't discuss his ideas here, since his concept of \"disputation arena\" is actually a relatively \"new\", raw, and relatively untested procedure - what I intend to discuss for now are things that have at least been studied more rigorously than just a bunch of blog posts (or personal website pages, whatever).</p>\n<p>However, I do want to co-opt the term \"Disputation Arena\" for any process that tries to achieve the following:</p>\n<ul>\n<li><strong>Avoid groupthink</strong> - actively search for information without settling too early on an option considered desirable by certain influential members</li>\n<li><strong>Achieve consensus</strong> - designate some choice as the best, given current information known by the group members</li>\n</ul>\n<p>We want our group rationality process to avoid groupthink (possibly at some expense of efficiency) because actual, real-world rationalists are not perfect Bayesian reasoners - two words: <a href=\"http://en.wikipedia.org/wiki/Robert_Aumann#Torah_codes_controversy\">Robert Aumann</a>. Because rationalists are not perfect, we do not expect a clear consensus to form after the end of the process (i.e. <a href=\"http://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem\">Aumann's</a> does not necessarily apply), so the process we use <em>must</em> force some consensus to become visible.</p>\n<p>One thing that David Brin discusses is the general division of the procedures into two \"phases\":</p>\n<ul>\n<li><strong>Centrifugal</strong> phase - members of the group generate ideas separately.</li>\n<li><strong>Centripetal</strong> phase - ideas are judged by the group together.</li>\n</ul>\n<p>This seems to me to be a good way of labeling parts of any group-coordination process that attempts to <strong>avoid groupthink</strong> and <strong>achieve consensus</strong>.</p>\n<p>In my personal research, I've found three things that attempt to achieve those two goals (avoid groupthink, achieve consensus) and which might (perhaps with a stretch) be considered as approximately having two phases (centrifugal and centripetal).</p>\n<h2>Delphi Method</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Delphi_method\">Wikipedia:Delphi method</a></em></dd>\n<p>The Delphi Method was originally developed by <a href=\"http://en.wikipedia.org/wiki/RAND_Corporation\">RAND Corporation</a> (Project RAND at that time, and no relation to <a href=\"/lw/m1/guardians_of_ayn_rand/\">Ayn Rand</a>) in order to get better predictions on future military hardware. It is currently used to get better utilization of current human wetware. <tt>(^.^)v</tt></p>\n<h3>Delphi Method: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>A panel of experts is chosen, and a questionnaire is prepared.</li>\n<li>Experts answer the questionnaire, giving the answers and also justifications and reasons for those answers.</li>\n<li>Summarizer provides anonymous summaries of the expert's answers and justifications.</li>\n<li>Experts read the summary, and may revise their answers/justifications. The process is repeated (with the same experts and questionnaire) for a set number of rounds, or until everyone gets bored, or the military bunker everyone is in gets nuked.</li>\n</ol>\n<p>Internet version (<em><a href=\"http://en.wikipedia.org/wiki/Real-time_Delphi\">Wikipedia:Real-time Delphi</a></em>):</p>\n<ol>\n<li>Username/passwords are chosen and emailed, and a questionnaire is prepared. The questionnaire in the online version is somewhat restricted, however. Here are some ideas:    \n<ul>\n<li>Use a \"poll\" question. Experts select one of the choices given.</li>\n<li>Use \"multiple-choice\" questions. Experts then select from a range of (e.g.) 1 for (strongly disagree) to 10 (strongly agree) for each possible choice.</li>\n<li>Use \"multiple-choice\" questions, and also give different aspects such as \"feasibility\", \"desirability\", \"good side-effects\", etc. Experts answer 1 to 10 for each combination of choice and aspect.</li>\n</ul>\n</li>\n<li>Experts answer the online questionnaire. Aside from the numerical or selection (quantitative) answer, experts should also supply a short sentence or two justifying each answer (qualitative).</li>\n<li>After the expert submits his or her answers, he or she is shown the current averages (for scoring-type questionnaires) or current poll results - this is the quantitative group answer. Expert is also shown (or provided links to) randomly-sorted (and randomly-chosen, if groups are very large and the number of answers may overwhelm a typical human) qualitative answers for each poll item / choice / choice+aspect - the qualitative group answers - for each score or aspect.    \n<ul>\n<li><strong>IMPORTANT</strong>: individual qualitative answers should <em>not</em> show the username of the expert who gave them!</li>\n<li>In effect, the average (or poll results) plus the randomized sample of qualitative answers are a simple, anonymous, machine-generated summary.</li>\n</ul>\n</li>\n<li>Experts may change their own quantitative and/or qualitative answers at any time, and see the current quantitative group answers and qualitative group answers at any time after they have submitted their own answers.</li>\n<li>The questionnaire is kept open until some specified time, or somebody hacks the server to put LOLcats instead.</li>\n</ol>\n<h3>Delphi Method: Analysis</h3>\n<p>Delphi methods <strong>avoid groupthink</strong> largely by anonymity: this avoids the bandwagon effect, the halo effect, and <a href=\"/lw/gw/politics_is_the_mindkiller/\">the mind-killer</a>. Anonymity and constant feedback also encourage people to revise their positions in light of new information from their peers (by reducing consistency pressure): in non-anonymous face-to-face meetings, people tend to stick to their previously stated opinions, and to conform to the meeting leader(s) or their own bosses in the meeting. A lot of those effects is reduced by anonymity. Pen-and-paper form makes anonymity much easier, since the summary gets the tone and language patterns of the summarizer; some amount of anonymity is lost in the online version (since language patterns might theoretically be analyzed) but hopefully the small sample size (just a short sentence or two) can make language pattern analysis difficult. Note that randomizing the order of the comments in the online version is important, as this reduces the effects of anchoring; sorting by time or karma may increase groupthink due to anchoring on earlier comments, but if each expert sees different \"first comments\", then this bias gets randomized (hopefully into irrelevancy).</p>\n<p>Delphi methods <strong>achieve consensus</strong> by the summary (which often serves as the \"final output\" when the process is finished). Arguably, the pen-and-paper version is better at achieving consensus due to the \"turn-based\" arrival of the summary, which makes the expert pay more attention to the summary, compared to the real-time online system.</p>\n<p>The Delphi method's <strong>centrifugal phase</strong> is the expert's private answering of the questionnaire: each expert makes this decision, and provides the justification, without other's knowledge or help.</p>\n<p>The Delphi method's <strong>centripetal phase</strong> is the act of summarizing, and having the experts read the summary.</p>\n<h3>Delphi Method: Other Thoughts</h3>\n<p>I think that forum polls, in general, can be easily adapted into online real-time Delphis by adding the following:</p>\n<ul>\n<li>Members will be required to give a short sentence or two (probably limited to say 200 chars or so) justifying their poll choice.</li>\n<li>Members should be allowed to change their poll choice and their justification at any time until the poll closes or the forum is hacked by LOLcat vandals.</li>\n<li>Members should be able to click on a poll choice on the poll results page to get a random anonymous sampling of the justifications that the other members have made in choosing that poll choice.</li>\n</ul>\n<p>The procedure says \"experts\" but I think that in something more democratic than the military you're supposed to read that as \"anyone who bothers to participate\".</p>\n<h2>Prediction Market</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Prediction_market\">Wikipedia:Prediction market</a></em></dd>\n<p>Prediction markets are speculation markets built around bets on what things will happen in the future. They are also the core of Robin Hanson's <a href=\"http://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>, and which you can see somewhere in the background of <a href=\"/lw/y4/three_worlds_collide_08/\">LW's favorite tentacle alien porn novella</a> <tt>(O.o);;</tt>.</p>\n<h3>Prediction Market: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>Convince a trusted monetary institution to sell you \"X is gonna happen\" stock and \"X is not gonna happen\" stock for $1 a pair (i.e. $1 for a pair of contracts, one that says \"If X happens, Monetary Institution pays you $1\" and another that says \"If X doesn't happen, Monetary Institution pays you $1\", so the pair costs $1 total since X can't both happen and not happen). You may need to pay some sort of additional fee or something if the monetary institution is for-profit.</li>\n<li>Sell the stock (i.e. the contract) you think is false for as high as you can get on the open market. Buy more stock of what you think is true from others who are willing to sell to you.</li>\n<li>Just buy and sell stocks depending on what you think is the best prediction, based on what you hear on the news, gossip you hear from neighbors, and predictions from the tea leaves. Keep doing this until X definitely happens or X definitely does not happen (in which case you cash in your stock contracts, if you bet correctly on what happened), or a market crash results because someone discovers that the weak nuclear force actually allows you to make nuclear bombs out of orange juice, and Einstein and the gang were lying about it and distracting you by talking about dice-playing gods.</li>\n</ol>\n<p><small>(what I described above is the simplest and most basic form I found; refer to the Wikipedia article for better elaborations)</small></p>\n<p>Internet version:</p>\n<ol>\n<li>Hack <a href=\"http://www.intrade.com/\">Intrade</a> so that the topic you want to bet on is in their list of markets. Or better yet just hack Intrade and put one million dollars into your account.</li>\n</ol>\n<h3>Prediction Market: Analysis</h3>\n<p>Prediction markets <strong>avoid groupthink</strong> by utilizing the invisible hand. Someone selling you a stock might be an idiot who can't read the tea leaves properly. Or the seller might have knowledge you do not possess, so maybe buying the stock wasn't such a good idea after all? Remember: if you can't find who the sucker on the table is, that sucker <em>is you!!</em> You can't simply assume that what your neighbor says is true and you should sell as many stock of X as possible: maybe he or she is trying to take advantage of you to get your hard-earned cash. Groupthink in such a mistrusting environment gets hard to sustain. Prediction markets work better with very large groups of people, so that you get practical anonymity (although not perfect, in theory you or anyone else can keep track of who's selling to who; online versions are also likely to hide user identities). Anonymity in the prediction market also has the advantages previously described under Delphi Method above.</p>\n<p>Prediction markets <strong>achieve consensus</strong> by utilizing the invisible hand. The price point of any sale serves as an approximate judgment of the epistemic probability of X occurring (or not occurring, depending on the contract that got sold). This gives a real-time signal on what the group of traders as a whole think the probability of X occurring is.</p>\n<p>The prediction market's <strong>centrifugal phase</strong> is each individual trader's thought process as he or she considers whether to buy or sell stock, and at what price.</p>\n<p>The prediction market's <strong>centripetal phase</strong> is any actual sale at any actual price point.</p>\n<h3>Prediction Market: Other Thoughts</h3>\n<p>Prediction markets are well-represented online; Intrade is just one of the more famous online prediction markets. Prediction markets appear to be the most popular and widely-known of the disputation arenas I've researched. These all tend to suggest that prediction markets are one of the better disputation arenas - but then remember that the Internet itself has no protection against groupthink.</p>\n<h2>Nominal Group Technique</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Nominal_group_technique\">Wikipedia: Nominal group technique</a></em></dd>\n<p>Nominal group technique is a group decision-making process, appropriate for groups of many different sizes. This procedure's pen-and-paper form is faster than the pen-and-paper forms of the other disputation arenas discussed here.</p>\n<h3>Nominal Group Technique: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>The facilitator informs the group of the issue to be discussed.</li>\n<li>Silent idea generation: group members are provided pen and paper, and are told to write down all ideas they can think of about the issue on the paper. They are given a fixed amount of time to do this (usually 10 to 15 minutes).    \n<ul>\n<li><strong>IMPORTANT</strong>: members are not allowed to discuss, show, or otherwise share their ideas with others during this stage. There's a reason it's called \"silent\".</li>\n</ul>\n</li>\n<li>Idea sharing: the facilitator asks group members, one at a time, to discuss their own ideas, until all members have shared their ideas. The facilitator writes the shared ideas into a whiteboard, or a similar location visible to all members.    \n<ul>\n<li><strong>IMPORTANT</strong>: debate is not allowed at this stage; only one member at a time can speak at this stage.</li>\n<li>Group members may also add additional ideas and notes to their written-down ideas while waiting for their turn.</li>\n</ul>\n</li>\n<li>Group discussion: members may ask for clarification or further details about particular ideas shared by other group members. The group may agree to split ideas, or merge ideas, or group ideas into categories.    \n<ul>\n<li><strong>IMPORTANT</strong>: the facilitator must ensure that (1) all members participate, and (2) no single idea gets too much attention (i.e. all ideas must be discussed).</li>\n<li>The discussion should be as neutral as possible, avoiding judgment or criticism.</li>\n<li>The final result of this stage should be a set of options to be chosen among.</li>\n</ul>\n</li>\n<li>Ranking: members secretly rank the options from 1 (best) to N (worst), where N is the number of options generated in the previous stage. The facilitator then tallies the (anonymous) rankings (by adding the rankings for each option) and declares the option with the lowest total as the group consensus.</li>\n</ol>\n<p>Unlike the previous procedures, which have been extrapolated into Internet versions, there is currently no Internet version of nominal group technique.</p>\n<h3>Nominal Group Technique: Analysis</h3>\n<p>Nominal group techniques <strong>avoid groupthink</strong> by the two \"secret\" steps: silent idea generation, and the secret ranking.  Having members write down their ideas in the silent idea generation step helps them precommit to those ideas in the idea sharing step, even though more influential group members may present opposite or incompatible ideas.  Although the group discussion step disallows explicit criticism of ideas, those criticisms are implicitly expressed during the secret ranking step (i.e. if you have a criticism of an idea, then you should rank it lower).</p>\n<p>Nominal group techniques <strong>achieve consensus</strong> by the idea sharing, group discussion, and ranking steps.  In particular, tallying of option rankings is the final consensus-achieving step.</p>\n<p>The nominal group technique's <strong>centrifugal phase</strong> is largely the silent idea generation step, and is the most explicit centrifugal phase among the disputation arenas discussed here.</p>\n<p>The nominal group technique's <strong>centripetal phase</strong> is largely the rest of the procedure.</p>\n<h3>Nominal Group Technique: Other Thoughts</h3>\n<p>A modified form of nominal group technique eats up a quarter of my recently-finished novel, Judge on a Boat, <a href=\"/lw/3m/rationalist_fiction/7ye1\">which I talked about on LessWrong here</a>, and <a href=\"http://raw.github.com/AmkG/judge-on-a-boat/master/judge-on-a-boat.txt\">whose latest raw text source you can read online</a>.  Yes, this entire article is just a self-serving advertisement to garner interest in my novel <tt>o(^.^o)(o^.^)o</tt>.</p>\n<p>Compared to the other procedures here, nominal group technique is more complicated and much more dependent on the centralized facilitator; the extreme dependency on the facilitator makes it difficult to create an automated online version.  On the other hand, a small group of say 5 to 10 people can finish the nominal group technique in 1 to 2 hours; the other procedures tend to work better when done over several days, and are largely impossible (in pen-and-paper form) to do in a similar time frame.  Even the real-time online versions of the other procedures are difficult to do within 2 hours.  Prediction markets in particular tend to fail badly if too thin (i.e. not enough participants); for small groups with tight schedules, nominal group technique tends to be the best.</p>\n<h1>Conclusion</h1>\n<p>For small groups that need to make a decision within one or two hours, use nominal group technique.  It's relatively unwieldy compared to the other disputation arenas, and is less ideal (it has fewer protections against groupthink, in particular), but is fast compared to the others.  Also, one might consider parallelizing nominal group technique: split a large group into smaller, randomly-selected sub-groups, have each perform the procedure independently, and then have them send a representative that performs the idea sharing, group discussion, and ranking steps with other representatives (i.e. each sub-group's nominal group technique serves as the silent idea generation of the super-group of representatives).  This tends to bias the super-group towards the agendas of the chosen representatives, but if <em><strong>speed</strong></em> is absolutely necessary for a large group, this may be the best you can do.</p>\n<p>As mentioned above, prediction markets tend to fail badly if there are too few participants in the speculation market; use it only for extremely large groups that are impossible to coordinate otherwise.  In addition, using prediction markets for policy decisions is effectively futarchy; you may want to see the (defunct?) <a href=\"http://tech.groups.yahoo.com/group/futarchy_discuss/messages/1?l=1\"><tt>futarchy_discuss</tt> Yahoo! group's message archives</a>.  In particular the earlier messages in the archive tend to discuss the general principles of prediction markets.  Prediction markets are the most famous of the disputation arenas here, but remember that the Internet is not a decent disputation arena.</p>\n<p>The Delphi methods seem to be a \"dark horse\" of sorts.  I don't see much discussion online about Delphi methods; I'm not sure whether it's because it's been tried and rejected, or if it simply isn't well known enough to actually be tried by most people.  I tend to suspect the latter, since if the universe were in the former case I would at least see some \"Delphi Methods suck!!\" blog posts.</p>\n<p>Both prediction markets and Delphi methods are continuously repeated methods.  At any time, the procedure may be stopped or repeated in order to make decisions.  However, unlike the nominal group techniques, both are targeted more towards generating advice for decision-makers, rather than making actual decisions themselves.</p>\n<p>It may be possible to organize a large, hierarchical group (say a company) with a prediction market for the rank-and-file, some key experts (who should be aware of the prediction market's results) running a Delphi method, and the key decision-making individuals (who read the Delphi method's report) at the top who form a decision using nominal group technique.  For more democratic processes, a \"poll-style\" real-time online Delphi method by itself may work.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4nDJM3nBvKxeoHzgo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "20657", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Consider the following statement: <strong>two heads are better than one</strong>.</p>\n<p>It seems obvious to me that several rationalists working together can, effectively, bring more precious brainpower to bear on a problem, than one rationalist working alone (otherwise, what would be the point of having a Less Wrong forum community? You might as well just leave it as a curated community blog and excise the discussion forums.). Further, due to various efforts (<a href=\"http://hpmor.com/\">HPMOR</a> especially) it appears that <a href=\"/lw/fqj/poll_is_endless_september_a_threat_to_lw_and_what/\">LW is inevitably growing</a>. This makes it not only <em>desirable</em> to find ways to effectively get <a href=\"/lw/cz/the_craft_and_the_community/\">groups of rationalists</a> to <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">think together</a>, but also increasingly <em>necessary</em>.</p>\n<p>It is also <em>desirable</em> that methods of getting groups to think should be feasibly doable over the Internet. <small>(I am aware that real-life meetups and stuff exist, but please be reminded that some people in the world do have to live in shitty little third-world countries and might not at all find it economically feasible to go to first-world countries with atrociously high costs-of-living)</small></p>\n<p>So first, let us start with the current \"best methods\" of getting groups of Traditional Rationalists to coordinate and think, while avoiding <a href=\"/lw/lm/affective_death_spirals/\">groupthink effects</a> that diminish our aggregate rationality. Hopefully, we can then use it as the basis of part of <a href=\"/lw/c4/go_forth_and_create_the_art/\">the art of rationalist group thinking</a>. So I'll discuss:</p>\n<ul>\n<li><strong>Disputation Arenas</strong> - procedures with <strong>centrifugal</strong> and <strong>centripetal</strong> phases <ol>\n<li><strong>Delphi Method</strong> - members secretly answer questions, non-member summarizer anonymizes answers, members read anonymized summary, repeat</li>\n<li><strong>Prediction Market</strong> - members place stock bets on market, market settles on price, members react to price signal, repeat</li>\n<li><strong>Nominal Group Technique</strong> - members write down ideas privately, non-member facilitator guides members in sharing ideas, non-member facilitator guides members in paring down and cleaning up ideas, members vote</li>\n</ol></li>\n<li><strong>Conclusion</strong> - strengths and weaknesses of the various disputation arenas shown here</li>\n</ul>\n<h1 id=\"Disputation_Arenas\">Disputation Arenas</h1>\n<p>One of my favorite SF authors, David Brin, talks a bit about <a href=\"http://www.davidbrin.com/disputation.html\">what he calls \"disputation arenas\"</a>. I won't discuss his ideas here, since his concept of \"disputation arena\" is actually a relatively \"new\", raw, and relatively untested procedure - what I intend to discuss for now are things that have at least been studied more rigorously than just a bunch of blog posts (or personal website pages, whatever).</p>\n<p>However, I do want to co-opt the term \"Disputation Arena\" for any process that tries to achieve the following:</p>\n<ul>\n<li><strong>Avoid groupthink</strong> - actively search for information without settling too early on an option considered desirable by certain influential members</li>\n<li><strong>Achieve consensus</strong> - designate some choice as the best, given current information known by the group members</li>\n</ul>\n<p>We want our group rationality process to avoid groupthink (possibly at some expense of efficiency) because actual, real-world rationalists are not perfect Bayesian reasoners - two words: <a href=\"http://en.wikipedia.org/wiki/Robert_Aumann#Torah_codes_controversy\">Robert Aumann</a>. Because rationalists are not perfect, we do not expect a clear consensus to form after the end of the process (i.e. <a href=\"http://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem\">Aumann's</a> does not necessarily apply), so the process we use <em>must</em> force some consensus to become visible.</p>\n<p>One thing that David Brin discusses is the general division of the procedures into two \"phases\":</p>\n<ul>\n<li><strong>Centrifugal</strong> phase - members of the group generate ideas separately.</li>\n<li><strong>Centripetal</strong> phase - ideas are judged by the group together.</li>\n</ul>\n<p>This seems to me to be a good way of labeling parts of any group-coordination process that attempts to <strong>avoid groupthink</strong> and <strong>achieve consensus</strong>.</p>\n<p>In my personal research, I've found three things that attempt to achieve those two goals (avoid groupthink, achieve consensus) and which might (perhaps with a stretch) be considered as approximately having two phases (centrifugal and centripetal).</p>\n<h2 id=\"Delphi_Method\">Delphi Method</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Delphi_method\">Wikipedia:Delphi method</a></em></dd>\n<p>The Delphi Method was originally developed by <a href=\"http://en.wikipedia.org/wiki/RAND_Corporation\">RAND Corporation</a> (Project RAND at that time, and no relation to <a href=\"/lw/m1/guardians_of_ayn_rand/\">Ayn Rand</a>) in order to get better predictions on future military hardware. It is currently used to get better utilization of current human wetware. <tt>(^.^)v</tt></p>\n<h3 id=\"Delphi_Method__How_To_Do_It_\">Delphi Method: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>A panel of experts is chosen, and a questionnaire is prepared.</li>\n<li>Experts answer the questionnaire, giving the answers and also justifications and reasons for those answers.</li>\n<li>Summarizer provides anonymous summaries of the expert's answers and justifications.</li>\n<li>Experts read the summary, and may revise their answers/justifications. The process is repeated (with the same experts and questionnaire) for a set number of rounds, or until everyone gets bored, or the military bunker everyone is in gets nuked.</li>\n</ol>\n<p>Internet version (<em><a href=\"http://en.wikipedia.org/wiki/Real-time_Delphi\">Wikipedia:Real-time Delphi</a></em>):</p>\n<ol>\n<li>Username/passwords are chosen and emailed, and a questionnaire is prepared. The questionnaire in the online version is somewhat restricted, however. Here are some ideas:    \n<ul>\n<li>Use a \"poll\" question. Experts select one of the choices given.</li>\n<li>Use \"multiple-choice\" questions. Experts then select from a range of (e.g.) 1 for (strongly disagree) to 10 (strongly agree) for each possible choice.</li>\n<li>Use \"multiple-choice\" questions, and also give different aspects such as \"feasibility\", \"desirability\", \"good side-effects\", etc. Experts answer 1 to 10 for each combination of choice and aspect.</li>\n</ul>\n</li>\n<li>Experts answer the online questionnaire. Aside from the numerical or selection (quantitative) answer, experts should also supply a short sentence or two justifying each answer (qualitative).</li>\n<li>After the expert submits his or her answers, he or she is shown the current averages (for scoring-type questionnaires) or current poll results - this is the quantitative group answer. Expert is also shown (or provided links to) randomly-sorted (and randomly-chosen, if groups are very large and the number of answers may overwhelm a typical human) qualitative answers for each poll item / choice / choice+aspect - the qualitative group answers - for each score or aspect.    \n<ul>\n<li><strong>IMPORTANT</strong>: individual qualitative answers should <em>not</em> show the username of the expert who gave them!</li>\n<li>In effect, the average (or poll results) plus the randomized sample of qualitative answers are a simple, anonymous, machine-generated summary.</li>\n</ul>\n</li>\n<li>Experts may change their own quantitative and/or qualitative answers at any time, and see the current quantitative group answers and qualitative group answers at any time after they have submitted their own answers.</li>\n<li>The questionnaire is kept open until some specified time, or somebody hacks the server to put LOLcats instead.</li>\n</ol>\n<h3 id=\"Delphi_Method__Analysis\">Delphi Method: Analysis</h3>\n<p>Delphi methods <strong>avoid groupthink</strong> largely by anonymity: this avoids the bandwagon effect, the halo effect, and <a href=\"/lw/gw/politics_is_the_mindkiller/\">the mind-killer</a>. Anonymity and constant feedback also encourage people to revise their positions in light of new information from their peers (by reducing consistency pressure): in non-anonymous face-to-face meetings, people tend to stick to their previously stated opinions, and to conform to the meeting leader(s) or their own bosses in the meeting. A lot of those effects is reduced by anonymity. Pen-and-paper form makes anonymity much easier, since the summary gets the tone and language patterns of the summarizer; some amount of anonymity is lost in the online version (since language patterns might theoretically be analyzed) but hopefully the small sample size (just a short sentence or two) can make language pattern analysis difficult. Note that randomizing the order of the comments in the online version is important, as this reduces the effects of anchoring; sorting by time or karma may increase groupthink due to anchoring on earlier comments, but if each expert sees different \"first comments\", then this bias gets randomized (hopefully into irrelevancy).</p>\n<p>Delphi methods <strong>achieve consensus</strong> by the summary (which often serves as the \"final output\" when the process is finished). Arguably, the pen-and-paper version is better at achieving consensus due to the \"turn-based\" arrival of the summary, which makes the expert pay more attention to the summary, compared to the real-time online system.</p>\n<p>The Delphi method's <strong>centrifugal phase</strong> is the expert's private answering of the questionnaire: each expert makes this decision, and provides the justification, without other's knowledge or help.</p>\n<p>The Delphi method's <strong>centripetal phase</strong> is the act of summarizing, and having the experts read the summary.</p>\n<h3 id=\"Delphi_Method__Other_Thoughts\">Delphi Method: Other Thoughts</h3>\n<p>I think that forum polls, in general, can be easily adapted into online real-time Delphis by adding the following:</p>\n<ul>\n<li>Members will be required to give a short sentence or two (probably limited to say 200 chars or so) justifying their poll choice.</li>\n<li>Members should be allowed to change their poll choice and their justification at any time until the poll closes or the forum is hacked by LOLcat vandals.</li>\n<li>Members should be able to click on a poll choice on the poll results page to get a random anonymous sampling of the justifications that the other members have made in choosing that poll choice.</li>\n</ul>\n<p>The procedure says \"experts\" but I think that in something more democratic than the military you're supposed to read that as \"anyone who bothers to participate\".</p>\n<h2 id=\"Prediction_Market\">Prediction Market</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Prediction_market\">Wikipedia:Prediction market</a></em></dd>\n<p>Prediction markets are speculation markets built around bets on what things will happen in the future. They are also the core of Robin Hanson's <a href=\"http://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>, and which you can see somewhere in the background of <a href=\"/lw/y4/three_worlds_collide_08/\">LW's favorite tentacle alien porn novella</a> <tt>(O.o);;</tt>.</p>\n<h3 id=\"Prediction_Market__How_To_Do_It_\">Prediction Market: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>Convince a trusted monetary institution to sell you \"X is gonna happen\" stock and \"X is not gonna happen\" stock for $1 a pair (i.e. $1 for a pair of contracts, one that says \"If X happens, Monetary Institution pays you $1\" and another that says \"If X doesn't happen, Monetary Institution pays you $1\", so the pair costs $1 total since X can't both happen and not happen). You may need to pay some sort of additional fee or something if the monetary institution is for-profit.</li>\n<li>Sell the stock (i.e. the contract) you think is false for as high as you can get on the open market. Buy more stock of what you think is true from others who are willing to sell to you.</li>\n<li>Just buy and sell stocks depending on what you think is the best prediction, based on what you hear on the news, gossip you hear from neighbors, and predictions from the tea leaves. Keep doing this until X definitely happens or X definitely does not happen (in which case you cash in your stock contracts, if you bet correctly on what happened), or a market crash results because someone discovers that the weak nuclear force actually allows you to make nuclear bombs out of orange juice, and Einstein and the gang were lying about it and distracting you by talking about dice-playing gods.</li>\n</ol>\n<p><small>(what I described above is the simplest and most basic form I found; refer to the Wikipedia article for better elaborations)</small></p>\n<p>Internet version:</p>\n<ol>\n<li>Hack <a href=\"http://www.intrade.com/\">Intrade</a> so that the topic you want to bet on is in their list of markets. Or better yet just hack Intrade and put one million dollars into your account.</li>\n</ol>\n<h3 id=\"Prediction_Market__Analysis\">Prediction Market: Analysis</h3>\n<p>Prediction markets <strong>avoid groupthink</strong> by utilizing the invisible hand. Someone selling you a stock might be an idiot who can't read the tea leaves properly. Or the seller might have knowledge you do not possess, so maybe buying the stock wasn't such a good idea after all? Remember: if you can't find who the sucker on the table is, that sucker <em>is you!!</em> You can't simply assume that what your neighbor says is true and you should sell as many stock of X as possible: maybe he or she is trying to take advantage of you to get your hard-earned cash. Groupthink in such a mistrusting environment gets hard to sustain. Prediction markets work better with very large groups of people, so that you get practical anonymity (although not perfect, in theory you or anyone else can keep track of who's selling to who; online versions are also likely to hide user identities). Anonymity in the prediction market also has the advantages previously described under Delphi Method above.</p>\n<p>Prediction markets <strong>achieve consensus</strong> by utilizing the invisible hand. The price point of any sale serves as an approximate judgment of the epistemic probability of X occurring (or not occurring, depending on the contract that got sold). This gives a real-time signal on what the group of traders as a whole think the probability of X occurring is.</p>\n<p>The prediction market's <strong>centrifugal phase</strong> is each individual trader's thought process as he or she considers whether to buy or sell stock, and at what price.</p>\n<p>The prediction market's <strong>centripetal phase</strong> is any actual sale at any actual price point.</p>\n<h3 id=\"Prediction_Market__Other_Thoughts\">Prediction Market: Other Thoughts</h3>\n<p>Prediction markets are well-represented online; Intrade is just one of the more famous online prediction markets. Prediction markets appear to be the most popular and widely-known of the disputation arenas I've researched. These all tend to suggest that prediction markets are one of the better disputation arenas - but then remember that the Internet itself has no protection against groupthink.</p>\n<h2 id=\"Nominal_Group_Technique\">Nominal Group Technique</h2>\n<dd><em><a href=\"http://en.wikipedia.org/wiki/Nominal_group_technique\">Wikipedia: Nominal group technique</a></em></dd>\n<p>Nominal group technique is a group decision-making process, appropriate for groups of many different sizes. This procedure's pen-and-paper form is faster than the pen-and-paper forms of the other disputation arenas discussed here.</p>\n<h3 id=\"Nominal_Group_Technique__How_To_Do_It_\">Nominal Group Technique: How To Do It!</h3>\n<p>Pen and paper version:</p>\n<ol>\n<li>The facilitator informs the group of the issue to be discussed.</li>\n<li>Silent idea generation: group members are provided pen and paper, and are told to write down all ideas they can think of about the issue on the paper. They are given a fixed amount of time to do this (usually 10 to 15 minutes).    \n<ul>\n<li><strong>IMPORTANT</strong>: members are not allowed to discuss, show, or otherwise share their ideas with others during this stage. There's a reason it's called \"silent\".</li>\n</ul>\n</li>\n<li>Idea sharing: the facilitator asks group members, one at a time, to discuss their own ideas, until all members have shared their ideas. The facilitator writes the shared ideas into a whiteboard, or a similar location visible to all members.    \n<ul>\n<li><strong>IMPORTANT</strong>: debate is not allowed at this stage; only one member at a time can speak at this stage.</li>\n<li>Group members may also add additional ideas and notes to their written-down ideas while waiting for their turn.</li>\n</ul>\n</li>\n<li>Group discussion: members may ask for clarification or further details about particular ideas shared by other group members. The group may agree to split ideas, or merge ideas, or group ideas into categories.    \n<ul>\n<li><strong>IMPORTANT</strong>: the facilitator must ensure that (1) all members participate, and (2) no single idea gets too much attention (i.e. all ideas must be discussed).</li>\n<li>The discussion should be as neutral as possible, avoiding judgment or criticism.</li>\n<li>The final result of this stage should be a set of options to be chosen among.</li>\n</ul>\n</li>\n<li>Ranking: members secretly rank the options from 1 (best) to N (worst), where N is the number of options generated in the previous stage. The facilitator then tallies the (anonymous) rankings (by adding the rankings for each option) and declares the option with the lowest total as the group consensus.</li>\n</ol>\n<p>Unlike the previous procedures, which have been extrapolated into Internet versions, there is currently no Internet version of nominal group technique.</p>\n<h3 id=\"Nominal_Group_Technique__Analysis\">Nominal Group Technique: Analysis</h3>\n<p>Nominal group techniques <strong>avoid groupthink</strong> by the two \"secret\" steps: silent idea generation, and the secret ranking.  Having members write down their ideas in the silent idea generation step helps them precommit to those ideas in the idea sharing step, even though more influential group members may present opposite or incompatible ideas.  Although the group discussion step disallows explicit criticism of ideas, those criticisms are implicitly expressed during the secret ranking step (i.e. if you have a criticism of an idea, then you should rank it lower).</p>\n<p>Nominal group techniques <strong>achieve consensus</strong> by the idea sharing, group discussion, and ranking steps.  In particular, tallying of option rankings is the final consensus-achieving step.</p>\n<p>The nominal group technique's <strong>centrifugal phase</strong> is largely the silent idea generation step, and is the most explicit centrifugal phase among the disputation arenas discussed here.</p>\n<p>The nominal group technique's <strong>centripetal phase</strong> is largely the rest of the procedure.</p>\n<h3 id=\"Nominal_Group_Technique__Other_Thoughts\">Nominal Group Technique: Other Thoughts</h3>\n<p>A modified form of nominal group technique eats up a quarter of my recently-finished novel, Judge on a Boat, <a href=\"/lw/3m/rationalist_fiction/7ye1\">which I talked about on LessWrong here</a>, and <a href=\"http://raw.github.com/AmkG/judge-on-a-boat/master/judge-on-a-boat.txt\">whose latest raw text source you can read online</a>.  Yes, this entire article is just a self-serving advertisement to garner interest in my novel <tt>o(^.^o)(o^.^)o</tt>.</p>\n<p>Compared to the other procedures here, nominal group technique is more complicated and much more dependent on the centralized facilitator; the extreme dependency on the facilitator makes it difficult to create an automated online version.  On the other hand, a small group of say 5 to 10 people can finish the nominal group technique in 1 to 2 hours; the other procedures tend to work better when done over several days, and are largely impossible (in pen-and-paper form) to do in a similar time frame.  Even the real-time online versions of the other procedures are difficult to do within 2 hours.  Prediction markets in particular tend to fail badly if too thin (i.e. not enough participants); for small groups with tight schedules, nominal group technique tends to be the best.</p>\n<h1 id=\"Conclusion\">Conclusion</h1>\n<p>For small groups that need to make a decision within one or two hours, use nominal group technique.  It's relatively unwieldy compared to the other disputation arenas, and is less ideal (it has fewer protections against groupthink, in particular), but is fast compared to the others.  Also, one might consider parallelizing nominal group technique: split a large group into smaller, randomly-selected sub-groups, have each perform the procedure independently, and then have them send a representative that performs the idea sharing, group discussion, and ranking steps with other representatives (i.e. each sub-group's nominal group technique serves as the silent idea generation of the super-group of representatives).  This tends to bias the super-group towards the agendas of the chosen representatives, but if <em><strong>speed</strong></em> is absolutely necessary for a large group, this may be the best you can do.</p>\n<p>As mentioned above, prediction markets tend to fail badly if there are too few participants in the speculation market; use it only for extremely large groups that are impossible to coordinate otherwise.  In addition, using prediction markets for policy decisions is effectively futarchy; you may want to see the (defunct?) <a href=\"http://tech.groups.yahoo.com/group/futarchy_discuss/messages/1?l=1\"><tt>futarchy_discuss</tt> Yahoo! group's message archives</a>.  In particular the earlier messages in the archive tend to discuss the general principles of prediction markets.  Prediction markets are the most famous of the disputation arenas here, but remember that the Internet is not a decent disputation arena.</p>\n<p>The Delphi methods seem to be a \"dark horse\" of sorts.  I don't see much discussion online about Delphi methods; I'm not sure whether it's because it's been tried and rejected, or if it simply isn't well known enough to actually be tried by most people.  I tend to suspect the latter, since if the universe were in the former case I would at least see some \"Delphi Methods suck!!\" blog posts.</p>\n<p>Both prediction markets and Delphi methods are continuously repeated methods.  At any time, the procedure may be stopped or repeated in order to make decisions.  However, unlike the nominal group techniques, both are targeted more towards generating advice for decision-makers, rather than making actual decisions themselves.</p>\n<p>It may be possible to organize a large, hierarchical group (say a company) with a prediction market for the rank-and-file, some key experts (who should be aware of the prediction market's results) running a Delphi method, and the key decision-making individuals (who read the Delphi method's report) at the top who form a decision using nominal group technique.  For more democratic processes, a \"poll-style\" real-time online Delphi method by itself may work.</p>", "sections": [{"title": "Disputation Arenas", "anchor": "Disputation_Arenas", "level": 1}, {"title": "Delphi Method", "anchor": "Delphi_Method", "level": 2}, {"title": "Delphi Method: How To Do It!", "anchor": "Delphi_Method__How_To_Do_It_", "level": 3}, {"title": "Delphi Method: Analysis", "anchor": "Delphi_Method__Analysis", "level": 3}, {"title": "Delphi Method: Other Thoughts", "anchor": "Delphi_Method__Other_Thoughts", "level": 3}, {"title": "Prediction Market", "anchor": "Prediction_Market", "level": 2}, {"title": "Prediction Market: How To Do It!", "anchor": "Prediction_Market__How_To_Do_It_", "level": 3}, {"title": "Prediction Market: Analysis", "anchor": "Prediction_Market__Analysis", "level": 3}, {"title": "Prediction Market: Other Thoughts", "anchor": "Prediction_Market__Other_Thoughts", "level": 3}, {"title": "Nominal Group Technique", "anchor": "Nominal_Group_Technique", "level": 2}, {"title": "Nominal Group Technique: How To Do It!", "anchor": "Nominal_Group_Technique__How_To_Do_It_", "level": 3}, {"title": "Nominal Group Technique: Analysis", "anchor": "Nominal_Group_Technique__Analysis", "level": 3}, {"title": "Nominal Group Technique: Other Thoughts", "anchor": "Nominal_Group_Technique__Other_Thoughts", "level": 3}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yrKxWvjm6S3uWhSH6", "YdcF6WbBmJhaaDqoD", "7FzD7pNm9X68Gp5ZC", "XrzQW69HpidzvBxGr", "aFEsqd6ofwnkNqaXo", "96TBXaHwLbFyeAxrg", "9weLK2AJ9JEt2Tt8f", "HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T04:33:13.684Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Wrapping Up", "slug": "seq-rerun-wrapping-up", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/exaaKBSqBmHwhryvG/seq-rerun-wrapping-up", "pageUrlRelative": "/posts/exaaKBSqBmHwhryvG/seq-rerun-wrapping-up", "linkUrl": "https://www.lesswrong.com/posts/exaaKBSqBmHwhryvG/seq-rerun-wrapping-up", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Wrapping%20Up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Wrapping%20Up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexaaKBSqBmHwhryvG%2Fseq-rerun-wrapping-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Wrapping%20Up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexaaKBSqBmHwhryvG%2Fseq-rerun-wrapping-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexaaKBSqBmHwhryvG%2Fseq-rerun-wrapping-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/wrapping-up.html\">Wrapping Up</a> was originally published on December 7, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>A summary of what Hanson believes to be the key unresolved issues.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fyc/seq_rerun_artificial_mysterious_intelligence/\">Artificial Mysterious Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "exaaKBSqBmHwhryvG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0602436235033923e-06, "legacy": true, "legacyId": "20692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fGGBth2PG6duQBNNB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T05:51:08.987Z", "modifiedAt": null, "url": null, "title": "Wanted: Rationalist Pushback (link)", "slug": "wanted-rationalist-pushback-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aronwall", "createdAt": "2012-12-17T01:34:03.658Z", "isAdmin": false, "displayName": "aronwall"}, "userId": "d4KHcLTkSGr6rNuRN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fBaCxEWLCCsvq98YB/wanted-rationalist-pushback-link", "pageUrlRelative": "/posts/fBaCxEWLCCsvq98YB/wanted-rationalist-pushback-link", "linkUrl": "https://www.lesswrong.com/posts/fBaCxEWLCCsvq98YB/wanted-rationalist-pushback-link", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wanted%3A%20Rationalist%20Pushback%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWanted%3A%20Rationalist%20Pushback%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBaCxEWLCCsvq98YB%2Fwanted-rationalist-pushback-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wanted%3A%20Rationalist%20Pushback%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBaCxEWLCCsvq98YB%2Fwanted-rationalist-pushback-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBaCxEWLCCsvq98YB%2Fwanted-rationalist-pushback-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>Recently I started a new blog, named \"<span class=\"il\">Undivided</span> <span class=\"il\">Looking</span>: comments on physics and theology\". &nbsp;You can find it here:<br /> <br /> <a href=\"http://www.wall.org/%7Earon/blog/\" target=\"_blank\">http://www.wall.org/~aron/blog/</a><br /> <br /> My main goals are to:</p>\n<ul>\n<li>discuss science and/or religion issues</li>\n<li>popularize some fundamental physics ideas to folks who aren't afraid of math, but don't necessarily know much physics</li>\n</ul>\n<p>(My day job is researching quantum gravity at UC Santa Barbara, so I actually know what I'm talking about when it comes to the science.)</p>\n<p>I'm posting this to Less Wrong in order to solicit comments from intelligent and civil members of the rationalist community.&nbsp; I'm a Christian, but I want to avoid <a href=\"http://wiki.lesswrong.com/wiki/Groupthink\">groupthink</a> in the comments section, since I believe ideas should be developed and tested around people with multiple viewpoints.&nbsp; So if anyone is willing to come and provide some friendly pushback from a rationalist perspective, that would be much appreciated.</p>\n<p>Since the Less Wrong community is particularly interested in discussing epistemic norms, I'd be especially happy to get feedback on this series of posts:</p>\n<p><a href=\"http://www.wall.org/~aron/blog/pillars-of-science-summary/\">Pillars of Science: Summary and Questions</a></p>\n<p>in which I identify 6 different features of scientific inquiry which help account for its phenomenal success.&nbsp; (Religion is only mentioned tangentially in this particular series, so even if you aren't interested in rehashing religious debates, you could still make a valuable contribution there.)</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fBaCxEWLCCsvq98YB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -1, "extendedScore": null, "score": 1.0602890208699679e-06, "legacy": true, "legacyId": "20693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T07:19:33.096Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 17, chapter 86", "slug": "harry-potter-and-the-methods-of-rationality-discussion-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:43.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alsadius", "createdAt": "2009-10-23T06:53:40.578Z", "isAdmin": false, "displayName": "Alsadius"}, "userId": "kCc9vGRdkuGo2PWbo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4sY9rqAqty8rHWGSW/harry-potter-and-the-methods-of-rationality-discussion-4", "pageUrlRelative": "/posts/4sY9rqAqty8rHWGSW/harry-potter-and-the-methods-of-rationality-discussion-4", "linkUrl": "https://www.lesswrong.com/posts/4sY9rqAqty8rHWGSW/harry-potter-and-the-methods-of-rationality-discussion-4", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2017%2C%20chapter%2086&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2017%2C%20chapter%2086%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sY9rqAqty8rHWGSW%2Fharry-potter-and-the-methods-of-rationality-discussion-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2017%2C%20chapter%2086%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sY9rqAqty8rHWGSW%2Fharry-potter-and-the-methods-of-rationality-discussion-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sY9rqAqty8rHWGSW%2Fharry-potter-and-the-methods-of-rationality-discussion-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 253, "htmlBody": "<p>Edit: <a href=\"/r/discussion/lw/g1q/harry_potter_and_the_methods_of_rationality/\">New thread posted here</a>.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://www.fanfiction.net/s/5782108/86/Harry-Potter-and-the-Methods-of-Rationality\">chapter</a></span>&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://hpmor.com/chapter/86\">86</a></span>.&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">The previous thread</a></span>&nbsp; has long passed 500 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.&nbsp; Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>, <a href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">As a reminder, it&rsquo;s often useful to start your comment by indicating which chapter you are commenting on.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4sY9rqAqty8rHWGSW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "20695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 609, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["35GjH7tDvNJWSHQ3H", "QkhX5YeuYHzPW7Waz", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T17:23:06.676Z", "modifiedAt": null, "url": null, "title": "Statistical checks on some social science", "slug": "statistical-checks-on-some-social-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RWhY5WcsJAGdx2pJb/statistical-checks-on-some-social-science", "pageUrlRelative": "/posts/RWhY5WcsJAGdx2pJb/statistical-checks-on-some-social-science", "linkUrl": "https://www.lesswrong.com/posts/RWhY5WcsJAGdx2pJb/statistical-checks-on-some-social-science", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Statistical%20checks%20on%20some%20social%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStatistical%20checks%20on%20some%20social%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWhY5WcsJAGdx2pJb%2Fstatistical-checks-on-some-social-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Statistical%20checks%20on%20some%20social%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWhY5WcsJAGdx2pJb%2Fstatistical-checks-on-some-social-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWhY5WcsJAGdx2pJb%2Fstatistical-checks-on-some-social-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p><a href=\"http://www.theatlantic.com/magazine/archive/2012/12/the-data-vigilante/309172\">Simonsohn, a social scientist, investigates bad use of statistics in his field.</a></p>\n<p>A few good quotes:</p>\n<blockquote>The three social psychologists set up a test experiment, then played by current academic methodologies and widely permissible statistical rules. By going on what amounted to a fishing expedition (that is, by recording many, many variables but reporting only the results that came out to their liking); by failing to establish in advance the number of human subjects in an experiment; and by analyzing the data as they went, so they could end the experiment when the results suited them, they produced a howler of a result, a truly absurd finding. They then ran a series of computer simulations using other experimental data to show that these methods could increase the odds of a false-positive result&mdash;a statistical fluke, basically&mdash;to nearly two-thirds.</blockquote>\n<p>Laugh or cry?:\"He prefers psychology&rsquo;s close-up focus on the quirks of actual human minds to the sweeping theory and deduction involved in economics.\"</p>\n<blockquote>Last summer, not long after Sanna and Smeesters left their respective universities, Simonsohn laid out his approach to fraud-busting in an online article called <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2114571\">&ldquo;Just Post It: The Lesson From Two Cases of Fabricated Data Detected by Statistics Alone&rdquo;</a>. Afterward, his inbox was flooded with tips from strangers. People wanted him to investigate election results, drug trials, the work of colleagues they&rsquo;d long doubted. He has not replied to these messages. Making a couple of busts is one thing. Assuming the mantle of the social sciences&rsquo; full-time Grand Inquisitor would be quite another.</blockquote>\n<p>This looks like a clue that there's work available for anyone who knows statistics. Eventually, there will be an additional line of work for how to tell whether a forensic statistician is competent.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RWhY5WcsJAGdx2pJb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 30, "extendedScore": null, "score": 1.060692310131662e-06, "legacy": true, "legacyId": "20705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T18:51:04.956Z", "modifiedAt": null, "url": null, "title": "[Link] Rethinking the way colleges teach critical thinking", "slug": "link-rethinking-the-way-colleges-teach-critical-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.451Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Despard", "createdAt": "2012-06-11T21:21:52.973Z", "isAdmin": false, "displayName": "Despard"}, "userId": "inrBDZgvL5FzK6584", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vJBiJrxs4kZKawHbb/link-rethinking-the-way-colleges-teach-critical-thinking", "pageUrlRelative": "/posts/vJBiJrxs4kZKawHbb/link-rethinking-the-way-colleges-teach-critical-thinking", "linkUrl": "https://www.lesswrong.com/posts/vJBiJrxs4kZKawHbb/link-rethinking-the-way-colleges-teach-critical-thinking", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Rethinking%20the%20way%20colleges%20teach%20critical%20thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Rethinking%20the%20way%20colleges%20teach%20critical%20thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJBiJrxs4kZKawHbb%2Flink-rethinking-the-way-colleges-teach-critical-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Rethinking%20the%20way%20colleges%20teach%20critical%20thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJBiJrxs4kZKawHbb%2Flink-rethinking-the-way-colleges-teach-critical-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJBiJrxs4kZKawHbb%2Flink-rethinking-the-way-colleges-teach-critical-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>\"Though critical thinking is universally regarded as a pillar of higher education ... results show that students are not developing their critical thinking skills to the extent we expect.\"</p>\n<p>&nbsp;</p>\n<p>http://blogs.scientificamerican.com/guest-blog/2012/12/14/re-thinking-the-way-colleges-teach-critical-thinking/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vJBiJrxs4kZKawHbb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.0607436010722432e-06, "legacy": true, "legacyId": "20707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T19:46:33.980Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: How Robot Cars Are Near", "slug": "meetup-berkeley-how-robot-cars-are-near", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JKfbidNSWCRKD9YWF/meetup-berkeley-how-robot-cars-are-near", "pageUrlRelative": "/posts/JKfbidNSWCRKD9YWF/meetup-berkeley-how-robot-cars-are-near", "linkUrl": "https://www.lesswrong.com/posts/JKfbidNSWCRKD9YWF/meetup-berkeley-how-robot-cars-are-near", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20How%20Robot%20Cars%20Are%20Near&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20How%20Robot%20Cars%20Are%20Near%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKfbidNSWCRKD9YWF%2Fmeetup-berkeley-how-robot-cars-are-near%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20How%20Robot%20Cars%20Are%20Near%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKfbidNSWCRKD9YWF%2Fmeetup-berkeley-how-robot-cars-are-near", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKfbidNSWCRKD9YWF%2Fmeetup-berkeley-how-robot-cars-are-near", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h4'>Berkeley: How Robot Cars Are Near</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 December 2012 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location and time for this Wednesday's meetup are confirmed! It will be at 7:30pm (not 7pm) at Zendo. I will not be there, but Michael Keenan will be giving a talk about robot cars.</p>\n\n<p>Michael Keenan is an entrepreneur, activist and futurist who works with The Seasteading Institute and the Center For Applied Rationality. His talk, How Robot Cars Are Near, describes how robot cars will save millions of lives, billions of hours and trillions of dollars.</p>\n\n<p>Michael will also be speaking at the Extreme Futurist Fest on December 22.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h4'>Berkeley: How Robot Cars Are Near</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JKfbidNSWCRKD9YWF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0607759526520319e-06, "legacy": true, "legacyId": "20708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__How_Robot_Cars_Are_Near\">Discussion article for the meetup : <a href=\"/meetups/h4\">Berkeley: How Robot Cars Are Near</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 December 2012 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location and time for this Wednesday's meetup are confirmed! It will be at 7:30pm (not 7pm) at Zendo. I will not be there, but Michael Keenan will be giving a talk about robot cars.</p>\n\n<p>Michael Keenan is an entrepreneur, activist and futurist who works with The Seasteading Institute and the Center For Applied Rationality. His talk, How Robot Cars Are Near, describes how robot cars will save millions of lives, billions of hours and trillions of dollars.</p>\n\n<p>Michael will also be speaking at the Extreme Futurist Fest on December 22.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__How_Robot_Cars_Are_Near1\">Discussion article for the meetup : <a href=\"/meetups/h4\">Berkeley: How Robot Cars Are Near</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: How Robot Cars Are Near", "anchor": "Discussion_article_for_the_meetup___Berkeley__How_Robot_Cars_Are_Near", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: How Robot Cars Are Near", "anchor": "Discussion_article_for_the_meetup___Berkeley__How_Robot_Cars_Are_Near1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-17T20:59:49.000Z", "modifiedAt": null, "url": null, "title": "More Cryonics Probability Estimates", "slug": "more-cryonics-probability-estimates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TK5McpcF584e9mFCy/more-cryonics-probability-estimates", "pageUrlRelative": "/posts/TK5McpcF584e9mFCy/more-cryonics-probability-estimates", "linkUrl": "https://www.lesswrong.com/posts/TK5McpcF584e9mFCy/more-cryonics-probability-estimates", "postedAtFormatted": "Monday, December 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20Cryonics%20Probability%20Estimates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20Cryonics%20Probability%20Estimates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTK5McpcF584e9mFCy%2Fmore-cryonics-probability-estimates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20Cryonics%20Probability%20Estimates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTK5McpcF584e9mFCy%2Fmore-cryonics-probability-estimates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTK5McpcF584e9mFCy%2Fmore-cryonics-probability-estimates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 940, "htmlBody": "<p>There are a lot of steps that all need to go correctly for cryonics to work. People who had gone through the potential problems, assigning probabilities, had come up with odds of success between 1:4 and 1:435. About <a href=\"http://www.jefftk.com/news/2011-09-22\">a year ago</a> I went through and collected estimates, finding other people's and making my own. I've been maintaining these in a <a href=\"https://docs.google.com/spreadsheet/ccc?key=0Ajn1LpstEUO_dE00ZVVfa3pzX2Y2dk9mWWRKOUVkWlE#gid=0\">googledoc</a>.</p>\n<p>Yesterday, on the bus back from the <a href=\"/lw/etn/meetup_winter_solstice_megameetup_nyc/\">NYC mega-meetup</a> with a group of people from the <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/\">Cambridge LessWrong meetup</a>, I got more people to give estimates for these probabilities. We started with my potential problems, I explained the model and how independence works in it [1]. For each question everyone decided on their own answer and then we went around and shared our answers (to reduce anchoring). Because there's still going to be some people adjusting to others based on their answers I tried to randomize the order in which I asked people their estimates. My notes are <a href=\"/2012-12-16--cryonics-probabilities-notes.txt\">here</a>. [2]</p>\n<p>The questions were:</p>\n<ul>\n<li>You die suddenly or in a circumstance where you would not be able to be frozen in time. </li>\n<li>You die of something where the brain is degraded at death. </li>\n<li>You die in a hospital that refuses access to you by the cryonics people. </li>\n<li>After death your relatives reject your wishes and don't let the cryonics people freeze you. </li>\n<li>Some law is passed that prohibits cryonics before you die. </li>\n<li>The cryonics people make a mistake in freezing you. </li>\n<li>Not all of what makes you you is encoded in the physical state of the brain (or whatever you would have preserved). </li>\n<li>The current cryonics process is insufficient to preserve everything (even when perfectly executed). </li>\n<li>All people die (existential risks). </li>\n<li>Society falls apart (global catastrophic non-existential risks). </li>\n<li>Some time after you die cryonics is outlawed. </li>\n<li>All cryonics companies go out of business. </li>\n<li>The cryonics company you chose goes out of business. </li>\n<li>Your cryonics company screws something up and you are defrosted. </li>\n<li>It is impossible to extract all the information preserved in the frozen brain. </li>\n<li>The technology is never developed to extract the information. </li>\n<li>No one is interested in your brain's information. </li>\n<li>It is too expensive to extract your brain's information. </li>\n<li>Reviving people in simulation is impossible. </li>\n<li>The technology is never developed to run people in simulation. </li>\n<li>Running people in simulation is outlawed. </li>\n<li>No one is interested running you in simulation. </li>\n<li>It is too expensive to run you in simulation. </li>\n<li>Other. </li>\n</ul>\n<p>To see people's detailed responses have a look at the <a href=\"https://docs.google.com/spreadsheet/ccc?key=0Ajn1LpstEUO_dE00ZVVfa3pzX2Y2d k9mWWRKOUVkWlE#gid=0\">googledoc</a>, but bottom line numbers were:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>person</td>\n<td>chance of failure</td>\n<td>odds of success</td>\n</tr>\n<tr>\n<td>Kelly</td>\n<td>35%</td>\n<td>1:2</td>\n</tr>\n<tr>\n<td>Jim</td>\n<td>80%</td>\n<td>1:5</td>\n</tr>\n<tr>\n<td>Mick</td>\n<td>89%</td>\n<td>1:9</td>\n</tr>\n<tr>\n<td>Julia</td>\n<td>96%</td>\n<td>1:23</td>\n</tr>\n<tr>\n<td>Ben</td>\n<td>98%</td>\n<td>1:44</td>\n</tr>\n<tr>\n<td>Jeff</td>\n<td>100%</td>\n<td>1:1500</td>\n</tr>\n</tbody>\n</table>\n<p>(These are all rounded, but one of the two should have enough resolution for each person.)</p>\n<p>The most significant way my estimate differs from others turned out to be for \"the current cryonics process is insufficient to preserve everything\". On that question alone we have:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>person</td>\n<td>chance of failure</td>\n</tr>\n<tr>\n<td>Kelly</td>\n<td>0%</td>\n</tr>\n<tr>\n<td>Jim</td>\n<td>35%</td>\n</tr>\n<tr>\n<td>Mick</td>\n<td>15%</td>\n</tr>\n<tr>\n<td>Julia</td>\n<td>60%</td>\n</tr>\n<tr>\n<td>Ben</td>\n<td>33%</td>\n</tr>\n<tr>\n<td>Jeff</td>\n<td>95%</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>My estimate for this used to be more positive, but it was significantly brought down by reading <a href=\"/r/discussion/lw/8f4/neil_degrasse_tyson_on_cryonics/6krm\">this lesswrong comment</a>:</p>\n<blockquote>Let me give you a fuller view: I am a neuroscientist, and I specialize in the biochemistry/biophysics of the synapse (and interactions with ER and mitochondria there). I also work on membranes and the effect on lipid composition in the opposing leaflets for all the organelles involved.\n<p>Looking at what happens during cryonics, I do not see any physically possible way this damage could ever be repaired. Reading the structure and \"downloading it\" is impossible, since many aspects of synaptic strength and connectivity are irretrievably lost as soon as the synaptic membrane gets distorted. You can't simply replace unfolded proteins, since their relative position and concentration (and modification, and current status in several different signalling pathways) determines what happens to the signals that go through that synapse; you would have to replace them manually, which is a) impossible to do without destroying surrounding membrane, and b) would take thousands of years at best, even if you assume maximally efficient robots doing it (during which period molecular drift would undo the previous work).</p>\n<p>Etc, etc. I can't even begin to cover complications I see as soon as I look at what's happening here. I'm all for life extension, I just don't think cryonics is a viable way to accomplish it.</p>\n</blockquote>\n<p>In the responses to their comment they go into more detail.</p>\n<p>Should I be giving this information this much weight? \"many aspects of synaptic strength and connectivity are irretrievably lost as soon as the synaptic membrane gets distorted\" seems critical.</p>\n<p>Other questions on which I was substantially more pessimistic than others were \"all cryonics companies go out of business\", \"the technology is never developed to extract the information\", \"no one is interested in your brain's information\", and \"it is too expensive to extract your brain's information\".</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2012-12-17\">on my blog</a></em></small></p>\n<p><br /> [1] Specifically, each question is asking you \"the chance that X happens and this keeps you from being revived, assuming that all of the previous steps all succeeded\". So if both A and B would keep you from being successfully revived, and I ask them in that order, but you think they're basically the same question, then A basically only A gets a probability while B gets 0 or close to it (because B is technically \"B given not-A\")./p&gt;</p>\n<p>&nbsp;</p>\n<p>[2] For some reason I was writing \".000000001\" when people said \"impossible\". For the purposes of this model '0' is fine, and that's what I put on the <a href=\"https://docs.google.com/spreadsheet/ccc?key=0Ajn1LpstEUO_dE00ZVVfa3pzX2Y2d k9mWWRKOUVkWlE#gid=0\">googledoc</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TK5McpcF584e9mFCy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 6.7e-05, "legacy": true, "legacyId": "20709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fbSEMZhmuBFJ8xn69"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T01:19:00.272Z", "modifiedAt": null, "url": null, "title": "CFAR\u2019s Inaugural Fundraising Drive", "slug": "cfar-s-inaugural-fundraising-drive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YE7HuqEjxAqXuKtc/cfar-s-inaugural-fundraising-drive", "pageUrlRelative": "/posts/7YE7HuqEjxAqXuKtc/cfar-s-inaugural-fundraising-drive", "linkUrl": "https://www.lesswrong.com/posts/7YE7HuqEjxAqXuKtc/cfar-s-inaugural-fundraising-drive", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%E2%80%99s%20Inaugural%20Fundraising%20Drive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%E2%80%99s%20Inaugural%20Fundraising%20Drive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YE7HuqEjxAqXuKtc%2Fcfar-s-inaugural-fundraising-drive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%E2%80%99s%20Inaugural%20Fundraising%20Drive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YE7HuqEjxAqXuKtc%2Fcfar-s-inaugural-fundraising-drive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YE7HuqEjxAqXuKtc%2Fcfar-s-inaugural-fundraising-drive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>http://appliedrationality.org/fundraising/</p>\n<p>(interested in hearing how other donors frame allocation between SI and CFAR)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YE7HuqEjxAqXuKtc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 1.0609698280241965e-06, "legacy": true, "legacyId": "20713", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T04:16:56.345Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] True Sources of Disagreement", "slug": "seq-rerun-true-sources-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FFBJRBqD65k4eEvLP/seq-rerun-true-sources-of-disagreement", "pageUrlRelative": "/posts/FFBJRBqD65k4eEvLP/seq-rerun-true-sources-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/FFBJRBqD65k4eEvLP/seq-rerun-true-sources-of-disagreement", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20True%20Sources%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20True%20Sources%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFBJRBqD65k4eEvLP%2Fseq-rerun-true-sources-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20True%20Sources%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFBJRBqD65k4eEvLP%2Fseq-rerun-true-sources-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFBJRBqD65k4eEvLP%2Fseq-rerun-true-sources-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Today's post, <a href=\"/lw/wl/true_sources_of_disagreement/\">True Sources of Disagreement</a> was originally published on 08 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#True_Sources_of_Disagreement\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Yudkowsky's guesses about what the key sticking points in the AI FOOM debate are.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fys/seq_rerun_wrapping_up/\">Wrapping Up</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FFBJRBqD65k4eEvLP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0610736238262902e-06, "legacy": true, "legacyId": "20715", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3pyLbH3BqevetQros", "exaaKBSqBmHwhryvG", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T08:12:37.103Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Dark Arts", "slug": "meetup-west-la-meetup-dark-arts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "APXAS", "createdAt": "2011-10-24T06:37:29.643Z", "isAdmin": false, "displayName": "APXAS"}, "userId": "JKgoNQb2rWTJ28Mao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TnL28CzhBrGhW8J58/meetup-west-la-meetup-dark-arts", "pageUrlRelative": "/posts/TnL28CzhBrGhW8J58/meetup-west-la-meetup-dark-arts", "linkUrl": "https://www.lesswrong.com/posts/TnL28CzhBrGhW8J58/meetup-west-la-meetup-dark-arts", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Dark%20Arts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Dark%20Arts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnL28CzhBrGhW8J58%2Fmeetup-west-la-meetup-dark-arts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Dark%20Arts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnL28CzhBrGhW8J58%2Fmeetup-west-la-meetup-dark-arts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnL28CzhBrGhW8J58%2Fmeetup-west-la-meetup-dark-arts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h5'>West LA Meetup - Dark Arts</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 December 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, December 19th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we'll discuss the <a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Dark Arts</a> including (but not limited to) what the constitutes Dark Arts, how to realize when they might be employed aganst you, and we'll also discuss ways to counter them.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We can be identified by a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h5'>West LA Meetup - Dark Arts</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TnL28CzhBrGhW8J58", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0612111318320211e-06, "legacy": true, "legacyId": "20724", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Dark_Arts\">Discussion article for the meetup : <a href=\"/meetups/h5\">West LA Meetup - Dark Arts</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 December 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, December 19th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we'll discuss the <a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Dark Arts</a> including (but not limited to) what the constitutes Dark Arts, how to realize when they might be employed aganst you, and we'll also discuss ways to counter them.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We can be identified by a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Dark_Arts1\">Discussion article for the meetup : <a href=\"/meetups/h5\">West LA Meetup - Dark Arts</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Dark Arts", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Dark_Arts", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Dark Arts", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Dark_Arts1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T12:29:12.642Z", "modifiedAt": null, "url": null, "title": "That Thing That Happened", "slug": "that-thing-that-happened", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.493Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xz7BxXs4TBjtCkqxH/that-thing-that-happened", "pageUrlRelative": "/posts/Xz7BxXs4TBjtCkqxH/that-thing-that-happened", "linkUrl": "https://www.lesswrong.com/posts/Xz7BxXs4TBjtCkqxH/that-thing-that-happened", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20Thing%20That%20Happened&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20Thing%20That%20Happened%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz7BxXs4TBjtCkqxH%2Fthat-thing-that-happened%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20Thing%20That%20Happened%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz7BxXs4TBjtCkqxH%2Fthat-thing-that-happened", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz7BxXs4TBjtCkqxH%2Fthat-thing-that-happened", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<p>I am emotionally excited and/or deeply hurt by what <strong>st_rev</strong> <a href=\"http://st-rev.livejournal.com/385195.html\">wrote</a> recently. You better take me seriously because you've spent a lot of time reading my posts already and feel invested in our common tribe. Anecdote about how people are tribal thinkers.</p>\n<blockquote>\n<p>That thing that happened shows that everything I was already advocating for is correct and necessary. Indeed it is time for everyone to put their differences aside and come together to carry out my recommended course of action. If you continue to deny what both you and I know in our hearts to be correct, you want everyone to die and I am defriending you.</p>\n</blockquote>\n<p>I don't even know where to begin. This is what blueist ideology has been workign towards for decades if not millennia, but to see it written here is hard to stomach even for one as used to the depravity caused by such delusions as I am. The lack of socially admired virtues among its adherents is frightening. Here I introduce an elaborate explanation of how blueist domination is not just completely obvious and a constant thorn in the side of all who wish more goodness but is achieved by the most questionable means often citing a particular blogger or public intellectual who I read in order to show how smart I am and because people I admire read him too. Followed by an appeal to the plot of a movie. Anecdote from my personal life. If you are familiar with the obscure work of an academic taken out of context and this does not convince you then you are clearly an intolerant sexual deviant engaging in motivated cognition.</p>\n<blockquote>\n<p>Consider well: do you want to be on the wrong side of history? If you persist, millions or billions of people you will never meet will be simultaneously mystified and appalled that an issue so obvious caused such needless contention. They will argue whether you were motivated more by stupidity, malice, raw interest, or if you were a helpless victim of the times in which you lived. Characters in fiction set in your era will inevitably be on (or at worst, join) the right side unless they are unredeemable villains. (Including historical figures who were on the other side, lest they lose all audience sympathy.).<br /><br />Remember: it's much more important what hypothetical future people will consider right than what you or current people you respect do. And you and I both know they'll agree with me.</p>\n</blockquote>\n<p>While sympathetic to this criticism I must signal my world-weariness and sophistication by writing several long paragraphs about how this is much too optimistic and we are in grave danger of a imminent and eternal takeover by our opponents. The only solution is to begin work on an organization dedicated to preventing this which happens to give me access to material resources and attractive females.</p>\n<p>Ciphergoth proves to be the lone voice of reason by encouraging us to recall what we all learned on 9/11:</p>\n<blockquote>\n<p>However, we must also consider if this is not also a lesson to us all; a lesson that my political views are correct.<br /><br /><a rel=\"nofollow\" href=\"http://www.adequacy.org/stories/2001.9.12.102423.271.html\">http://www.adequacy.org/stories/2001.9.12.102423.271.html</a></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BtQRRKTPxagBH6KrG": 1, "DdgSyQoZXjj3KnF4N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xz7BxXs4TBjtCkqxH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 33, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "20725", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T15:13:33.311Z", "modifiedAt": null, "url": null, "title": "Caring about what happens after you die", "slug": "caring-about-what-happens-after-you-die", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xjSNkYoMkpmYv8Edq/caring-about-what-happens-after-you-die", "pageUrlRelative": "/posts/xjSNkYoMkpmYv8Edq/caring-about-what-happens-after-you-die", "linkUrl": "https://www.lesswrong.com/posts/xjSNkYoMkpmYv8Edq/caring-about-what-happens-after-you-die", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Caring%20about%20what%20happens%20after%20you%20die&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACaring%20about%20what%20happens%20after%20you%20die%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjSNkYoMkpmYv8Edq%2Fcaring-about-what-happens-after-you-die%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Caring%20about%20what%20happens%20after%20you%20die%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjSNkYoMkpmYv8Edq%2Fcaring-about-what-happens-after-you-die", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjSNkYoMkpmYv8Edq%2Fcaring-about-what-happens-after-you-die", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>More than once, I've had a conversation roughly similar to the following:</p>\n<p>Me: \"I want to live forever, of course; but even if I don't, I'd still like for some sort of sapience to keep on living.\"</p>\n<p>Someone else: \"Yeah, so? You'll be dead, so how/why should you care?\"</p>\n<p>&nbsp;</p>\n<p>I've tried describing how it's the me-of-the-present who's caring about which sort of future comes to pass, but I haven't been able to do so in a way that doesn't fall flat. Might you have any thoughts on how to better frame this idea?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xjSNkYoMkpmYv8Edq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 16, "extendedScore": null, "score": 1.0614568077049553e-06, "legacy": true, "legacyId": "20727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T16:38:36.380Z", "modifiedAt": null, "url": null, "title": "Matrix and Inspirational religious fiction", "slug": "matrix-and-inspirational-religious-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:31.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnXz67YevqnekyDkJ/matrix-and-inspirational-religious-fiction", "pageUrlRelative": "/posts/YnXz67YevqnekyDkJ/matrix-and-inspirational-religious-fiction", "linkUrl": "https://www.lesswrong.com/posts/YnXz67YevqnekyDkJ/matrix-and-inspirational-religious-fiction", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Matrix%20and%20Inspirational%20religious%20fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMatrix%20and%20Inspirational%20religious%20fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnXz67YevqnekyDkJ%2Fmatrix-and-inspirational-religious-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Matrix%20and%20Inspirational%20religious%20fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnXz67YevqnekyDkJ%2Fmatrix-and-inspirational-religious-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnXz67YevqnekyDkJ%2Fmatrix-and-inspirational-religious-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 905, "htmlBody": "<p><strong>Preface</strong><br /><br />This post is entirely intended to be inspirational and entertaining. Which though is no excuse for the logical flaws in it. None of this is particularly plausible nor am I trying to support religion in some weird way, anyway I hope anyone doesn't find this offensive, but I don't think this is the most sensitive website around religious subjects anyway, so.</p>\n<p>&nbsp;</p>\n<p><br /><strong>Religion Sci-fi</strong> <strong>in style of Dr. Emmet Brown</strong><br /><br />Suppose that mankind succesfully produces a Friendly AI in the future and also adopts some ethical doctrine that cares about people, goes on making technological advancements and prolonging the lives and subjective experiences of humans, or something else along those lines. Then also suppose that humans learn how to <em>travel back in time</em>. It would present an ethical dilemma of what do about all those lost beings that died long ago. Considering there had been a <em>finite</em> number of such cases, it would be plausible to consume a finite amount of resources to bend the future of humanity backwards in time, or travel back in time, to fetch all those people just before they perish and allow them to join the eudaimonic future society of humans.<br /><br />Now consider the possibility that the vast majority of futures for mankind do not contain these outcomes where a Friendly Ai is succesfully established where people care about people and have not succesfully established any flexible, sophisticated or complex morality at all. And in those futures where humanity achieved all those things, it could be seen as a problem, that some futures had consisted of such tragic waste. So we decide to alter the past futures of humanity, and travel backwards in time - yet again, and establish religion, appear on the mountain of Sinai, give out the 10 commandments, and speak of this being superior to humans that takes care of everybody. Which would be the friendly AI, a bostromian singleton, or similar. However they would not do this just to steer the possible future outcomes of humanity away from unwanted futures, but also to create a new parallel future, to see if the course of recursive moral advancements would take an entirely different form after injecting similar goals or ideals <em>prior</em> to the development of the Friendly AI - that is 'God'&nbsp; (I'd prefer <em>Mother</em> over Father though) in heaven - the future society.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><br /><strong>The Matrix movie, Reductionism and Mathematical Universe Hypothesis</strong><br /><br />Suppose that the universe humanity lives in happened to be a computer simulation - akin to the fictional setting of the Matrix movie - instead of a \"real\" universe. The notion of \"real\" would then have a very obscure meaning, but for the sake of argument, let's consider this possibility. (There's no particular reason to think of simulated people as not real)</p>\n<address><a href=\"http://wiki.lesswrong.com/wiki/Meditation\" target=\"_blank\">Meditation</a>: How would you expect a simulated universe to differ from any 'actual' universe?</address>\n<p><br />Well for one, you could say that if the observable universe is reducible to, or entirely describable by, mathematical formulas and axioms this would seem to increase the plausibility of a simulated universe, because in contrast, if the universe was not reducible to mathematically describable structures that would in some sense rule out a computational environment of finite accuracy<sup>1</sup> or information.<br /><br />According to the <a href=\"http://en.wikipedia.org/wiki/Mathematical_universe_hypothesis\" target=\"_blank\">Mathematical Universe Hypothesis</a> this would be the case - not that the universe is a simulation, but - that the universe is reducible to physics with mathematical descriptions. The physics underlying observed reality can be generalized to fields and formulas, correlations and so forth, which technically could be running in a limited computational space, with limited memory if not computational speed.<br /><br />Another rather interesting point is to consider the likelihood of any sentient being (ie. selected at random) existing in reality vs existing inside a simulation occurring within that/some reality. So if you would have intelligent beings creating simulations containtaining sentient beings, it could be said that <em>maybe</em> in some of the possible universes there are far more sentient beings inside simulations than there are actual 'physical' sentient beings. And if you knew you were in one of those universes <strong>(</strong><em>though in that case \"a universe\" could be something that exists within(/is the) a simulation, but since our concept of simulations exists inside this universe, the \"think outside the box\" kind of becomes impossible to do</em><strong>)</strong> then how likely would it be to be in a simulation without being aware of that?</p>\n<p>On the other hand there's <sup>1: </sup><a href=\"http://en.wikipedia.org/wiki/Uncertainty_principle\" target=\"_blank\">Heisenberg uncertainity principle</a>, and since our minds are in constant interaction with the environment <em>not just via sensory information</em> but also through trauma, metabolism, disease, etc. it does seem like humans have their brains located in the really real world which we can observe, rather than some dentist's chair in some fictional dystopia.</p>\n<p>&nbsp;</p>\n<p>Ps. Although now that I think about it these two subjects though serving the same purpose are mostly distinct. Anyway it would be good to treat the \"Religion Sci-Fi\" as an <em>exercise</em> of rationality, since it takes so many \"leaps of faith\", includes <a href=\"/lw/ju/rationalization/\" target=\"_blank\">rationalization</a> and triggers <a href=\"/lw/ji/conjunction_fallacy/\" target=\"_blank\">conjunction fallacy</a>. For an example: What probability do you assign for future time travel? On the other hand the simulation scenario is of different nature, and it's more about actual reasoning and collecting evidence from the environment for and against. Why do you think this reality can/can't be a simulation?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnXz67YevqnekyDkJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -3, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "20728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SFZoEBpLo9frSJGkc", "QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T17:32:39.150Z", "modifiedAt": "2021-02-17T04:21:12.226Z", "url": null, "title": "Ontological Crisis in Humans", "slug": "ontological-crisis-in-humans", "viewCount": null, "lastCommentedAt": "2019-01-05T04:06:39.836Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KLaJjNdENsHhKhG5m/ontological-crisis-in-humans", "pageUrlRelative": "/posts/KLaJjNdENsHhKhG5m/ontological-crisis-in-humans", "linkUrl": "https://www.lesswrong.com/posts/KLaJjNdENsHhKhG5m/ontological-crisis-in-humans", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ontological%20Crisis%20in%20Humans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOntological%20Crisis%20in%20Humans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLaJjNdENsHhKhG5m%2Fontological-crisis-in-humans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ontological%20Crisis%20in%20Humans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLaJjNdENsHhKhG5m%2Fontological-crisis-in-humans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLaJjNdENsHhKhG5m%2Fontological-crisis-in-humans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1162, "htmlBody": "<p>Imagine a robot that was designed to find and collect spare change around its owner's house. It had a world model where macroscopic everyday objects are ontologically primitive and ruled by high-school-like physics and (for humans and their pets)&nbsp;rudimentary&nbsp;psychology and animal behavior. Its goals were expressed as a utility function over this world model, which was sufficient for its designed purpose. All went well until one day, a prankster decided to \"upgrade\" the robot's world model to be based on modern particle physics. This unfortunately caused the robot's utility function to instantly throw a <a href=\"http://stackoverflow.com/questions/641064/what-is-a-domain-error\">domain error</a> exception (since its inputs are no longer the expected list of macroscopic objects and associated properties like shape and color), thus crashing the controlling AI.</p>\n<p>According to Peter de Blanc, who used the phrase \"<a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crisis</a>\" to describe this kind of problem,</p>\n<blockquote>\n<p>Human beings also confront ontological crises. We should find out what cognitive algorithms humans use to solve the same problems described in this paper. If we wish to build agents that maximize human values, this may be aided by knowing how humans re-interpret their values in new ontologies.</p>\n</blockquote>\n<p>I recently realized that a couple of problems that I've been thinking over&nbsp;(the&nbsp;<a href=\"/lw/8gk/where_do_selfish_values_come_from\">nature of selfishness</a>&nbsp;and the&nbsp;<a href=\"/lw/4qg/a_thought_experiment_on_pain_as_a_moral_disvalue/\">nature of pain/pleasure/suffering/happiness</a>)&nbsp;can be considered instances of ontological crises in humans (although I'm not so sure we necessarily have the cognitive algorithms to solve them).&nbsp;I started thinking in this direction after writing <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/6hef\">this comment</a>:</p>\n<blockquote>\n<p>This formulation or variant of TDT requires that before a decision problem is handed to it, the world is divided into the agent itself (X), other agents (Y), and \"dumb matter\" (G). I think this is misguided, since the world doesn't really divide cleanly into these 3 parts.</p>\n</blockquote>\n<p>What struck me is that even though the world doesn't divide cleanly into these 3 parts, <em>our models</em>&nbsp;of the world actually do. In the world models that we humans use on a day to day basis, and over which our utility functions seem to be defined (<a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">to the extent</a> that we can be said to have utility functions at all), we do take the Self, Other People, and various Dumb Matter to be ontologically primitive entities. Our world models, like the coin collecting robot's, consist of these macroscopic objects ruled by a hodgepodge of heuristics and prediction algorithms, rather than&nbsp;microscopic particles governed&nbsp;by a coherent set of laws of physics.</p>\n<p>For example, the amount of pain someone is experiencing doesn't seem to exist in the real world as an XML tag attached to some \"person entity\", but that's pretty much how our models of the world work, and perhaps more importantly, that's what our utility functions expect their inputs to look like (as opposed to, say, a list of particles and their positions and velocities). Similarly, a human can be selfish just by treating the object labeled \"SELF\" in its world model differently from other objects, whereas an AI with a world model consisting of microscopic particles would need to somehow inherit or learn a detailed description of itself in order to&nbsp;be selfish.</p>\n<p>To fully confront the ontological crisis that we face, we would have to upgrade our world model to be based on actual physics, and simultaneously translate our utility functions so that their domain is the set of possible states of the new model. We currently have little idea how to accomplish this, and instead what we do in practice is, as far as I can tell, keep our ontologies intact and utility functions unchanged, but just add some new&nbsp;heuristics that in certain limited circumstances call out to new physics formulas to better update/extrapolate our models. This is actually rather clever, because it lets us make use of updated understandings of physics without ever having to, for instance, decide exactly what patterns of particle movements constitute pain or pleasure, or&nbsp;what&nbsp;patterns constitute oneself. Nevertheless, this approach hardly seems capable of being extended to work in a future where many people may have&nbsp;nontraditional&nbsp;mind architectures, or have a zillion copies of themselves running on all kinds of strange substrates, or be merged into amorphous group minds with no clear boundaries between individuals.</p>\n<p>By the way, I think nihilism often gets short&nbsp;changed <a href=\"/lw/sc/existential_angst_factory/\">around</a> <a href=\"/lw/5i7/on_being_okay_with_the_truth/\">here</a>. Given that we do not actually have at hand a solution to ontological crises in general or to the specific crisis that we face, what's wrong with saying that the solution set may just be null? Given that evolution doesn't constitute a particularly benevolent and farsighted designer, perhaps we may not be able to do much better than that poor spare-change collecting robot? If Eliezer is <a href=\"/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/7pqr\">worried</a> that actual AIs facing actual ontological crises could do worse&nbsp;than just crash, should we be very sanguine that for humans everything must \"add up to moral normality\"?</p>\n<p>To expand a bit more on this possibility, many people have an aversion against moral arbitrariness, so we need at a minimum a utility translation scheme that's principled enough to pass that filter. But our existing world models are a hodgepodge put together by evolution so there may not be any such sufficiently&nbsp;principled&nbsp;scheme, which (if other approaches to solving moral philosophy also don't pan out) would leave us with legitimate&nbsp;feelings of \"existential angst\" and nihilism. One could perhaps still argue that any <em>current</em> such feelings are premature, but maybe some people have stronger intuitions than others that these problems are unsolvable?</p>\n<p>Do we have any examples of humans successfully navigating an ontological crisis? The LessWrong Wiki <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">mentions</a> loss of faith in God:</p>\n<blockquote>\n<p>In the human context, a clear example of an ontological crisis is a believer&rsquo;s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.</p>\n</blockquote>\n<p>But I don't think loss of faith in God actually constitutes an ontological crisis, or if it does, certainly not a very severe one. An ontology consisting of Gods, Self, Other People, and Dumb Matter just isn't very different from one consisting of Self, Other People, and Dumb Matter (the latter could just be considered a special case of the former with quantity of Gods being 0), especially when you compare either&nbsp;ontology&nbsp;to one made of microscopic particles or even <a href=\"http://en.wikipedia.org/wiki/Loop_quantum_gravity\">less</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/String_theory\">familiar</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Ultimate_ensemble\">entities</a>.</p>\n<p>But to end on a more positive note, realizing that seemingly unrelated problems are actually instances of a more general problem gives some hope that by \"going meta\" we can find a solution to all of these problems at once. Maybe we can solve many ethical problems simultaneously by discovering some generic algorithm that can be used by an agent to transition from any ontology to another?&nbsp;</p>\n<p>(Note that I'm not saying this <em>is</em>&nbsp;the right way to understand one's real preferences/morality, but just drawing attention to it as a possible alternative to other more \"object level\" or \"purely&nbsp;philosophical\" approaches. See also <a href=\"/lw/6ha/the_blueminimizing_robot/4gi2\">this previous discussion</a>, which I recalled after writing most of the above.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 2, "nSHiKwWyMZFdZg5qt": 2, "ouT6wKhACJRouGokM": 2, "5f5c37ee1b5cdee568cfb2c2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KLaJjNdENsHhKhG5m", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 68, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "20675", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Nz62ZurRkGPigAxMK", "w5M5oMLLHyink4ak9", "fa5o2tg9EfJE77jEQ", "8rdoea3g6QGhWQtmx", "3v24wGePcdSGB3i8a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-18T17:32:39.150Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T18:38:13.094Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham LW discussion", "slug": "meetup-durham-lw-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xt53wARYBrJHusHip/meetup-durham-lw-discussion", "pageUrlRelative": "/posts/Xt53wARYBrJHusHip/meetup-durham-lw-discussion", "linkUrl": "https://www.lesswrong.com/posts/Xt53wARYBrJHusHip/meetup-durham-lw-discussion", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20LW%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20LW%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt53wARYBrJHusHip%2Fmeetup-durham-lw-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20LW%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt53wARYBrJHusHip%2Fmeetup-durham-lw-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt53wARYBrJHusHip%2Fmeetup-durham-lw-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h6'>Durham LW discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's cafe, 706B 9th St Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Regular Thursday discussion meetup. Topic TBD, to be decided on the email list or possibly at the meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h6'>Durham LW discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xt53wARYBrJHusHip", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0615762937411545e-06, "legacy": true, "legacyId": "20729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_LW_discussion\">Discussion article for the meetup : <a href=\"/meetups/h6\">Durham LW discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's cafe, 706B 9th St Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Regular Thursday discussion meetup. Topic TBD, to be decided on the email list or possibly at the meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_LW_discussion1\">Discussion article for the meetup : <a href=\"/meetups/h6\">Durham LW discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham LW discussion", "anchor": "Discussion_article_for_the_meetup___Durham_LW_discussion", "level": 1}, {"title": "Discussion article for the meetup : Durham LW discussion", "anchor": "Discussion_article_for_the_meetup___Durham_LW_discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T19:47:32.327Z", "modifiedAt": null, "url": null, "title": "Bounding the impact of AGI", "slug": "bounding-the-impact-of-agi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:33.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CeZgKf6uTaPZb4x3u/bounding-the-impact-of-agi", "pageUrlRelative": "/posts/CeZgKf6uTaPZb4x3u/bounding-the-impact-of-agi", "linkUrl": "https://www.lesswrong.com/posts/CeZgKf6uTaPZb4x3u/bounding-the-impact-of-agi", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bounding%20the%20impact%20of%20AGI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABounding%20the%20impact%20of%20AGI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCeZgKf6uTaPZb4x3u%2Fbounding-the-impact-of-agi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bounding%20the%20impact%20of%20AGI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCeZgKf6uTaPZb4x3u%2Fbounding-the-impact-of-agi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCeZgKf6uTaPZb4x3u%2Fbounding-the-impact-of-agi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1551, "htmlBody": "<p><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">For those of you interested,&nbsp;</span><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">Andr&aacute;s Kornai's paper \"Bounding the impact of AGI\" from this year's </span></span><a style=\"font-family: CMR12; font-size: 16px;\" href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">AGI-Impacts conference</a><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"> at Oxford had a few interesting ideas (which I've&nbsp;excerpted&nbsp;below).</span></span></span></span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">Summary:</span></span></p>\n<ol>\n<li>Acceptable risk tolerances for AGI design can be determined using standard safety engineering techniques from other fields</li>\n<li>Mathematical proof is the only available tool to secure the tolerances required to prevent intolerable increases in xrisk</li>\n<li>Automated theorem proving will be required so that the proof can reasonably be checked by multiple human minds</li>\n</ol>\n<div><br /></div>\n<blockquote>\n<div class=\"column\">\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\"><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"><strong>Safety engineering</strong></span></span></p>\n<p><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">Since the original approach of Yudkowsky (2006) to friendly AI, which sought mathematical guarantees of friendliness, was met with considerable skepticism, we revisit the issue of why such guarantees are essential. In designing radioactive equipment, a reasonable guideline is to limit emissions to several orders of magnitude below the natural background radiation level, so that human-caused dangers are lost in the noise compared to the pre-existing threat we must live with anyway. In the full paper, we take the &ldquo;big five&rdquo; extinction events that occurred within the past half billion years as background, and argue that we need to design systems with a failure rate below 10<sup>&minus;63</sup> per logical operation. </span></p>\n<p><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">What needs to be emphasized in the face of this requirement is that the very best physical measurements have only one part in 10<sup>17</sup> precision, not to speak of social and psychological phenomena where our understanding is considerably weaker. What this means is that guarantees of the requisite sort can only be expected from mathematics, where our measurement precision is already considerably better.</span></p>\n</div>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p><strong style=\"font-family: CMR12; font-size: 16px;\">How reliable is mathematics?</strong></p>\n<p style=\"font-size: 16px;\"><span style=\"font-family: CMR12;\">The period since World War II has brought incredible advances in mathematics, such as the Four&nbsp;Color Theorem (Appel and Haken 1976), Fermat&rsquo;s Last Theorem (Wiles 1995), the classi\ufb01cation of \ufb01nite simple groups (Gorenstein 1982, Aschbacher 2004), and the Poincare conjecture (Perelman 1994). While the community of mathematicians is entirely convinced of the correctness of these results, few&nbsp;individual mathematicians are, as the complexity of the proofs, both in terms of knowledge assumed&nbsp;from various branches of mathematics and in terms of the length of the deductive chain, is generally&nbsp;beyond our ken. Instead of a personal understanding of the matter, most of us now rely on argumentum ad verecundiam: well Faltings and Ribet now think that the Wiles-Taylor proof is correct, and&nbsp;even if I don&rsquo;t know Faltings or Ribet at least I know and respect people who know and respect them,&nbsp;and if that&rsquo;s not good enough I can go and devote a few years of my life to understand the proof for&nbsp;good. Unfortunately, the communal checking of proofs often takes years, and sometimes errors are&nbsp;discovered only after a decade has passed: the hole in the original proof of the Four Color Theorem&nbsp;(Kempe 1879) was detected by Heawood in 1890. Tomonaga in his Nobel lecture (1965) describes&nbsp;how his team&rsquo;s work in 1947 uncovered a major problem in Dancoff (1939):</span></p>\n<p style=\"font-size: 16px; padding-left: 60px;\"><span style=\"font-family: CMR12;\">Our new method of calculation was not at all different in its contents from Dancoff&rsquo;s&nbsp;perturbation method, but had the advantage of making the calculation more clear. In fact,&nbsp;what took a few months in the Dancoff type of calculation could be done in a few weeks.&nbsp;And it was by this method that a mistake was discovered in Dancoff&rsquo;s calculation; we had&nbsp;also made the same mistake in the beginning.</span></p>\n<p style=\"font-size: 16px;\"><span style=\"font-family: CMR12;\">To see that such long-hidden errors are are by no means a thing of the past, and to observe the &lsquo;web&nbsp;of trust&rsquo; method in action, consider the following example from Mohr (2012).</span></p>\n<p style=\"font-size: 16px; padding-left: 60px;\"><span style=\"font-family: CMR12;\">The eighth-order coef\ufb01cient A<sub>1</sub><sup>(8)</sup>&nbsp;arises from 891 Feynman diagrams of which only a few&nbsp;are known analytically. Evaluation of this coef\ufb01cient numerically by Kinoshita and coworkers has been underway for many years (Kinoshita, 2010). The value used in the 2006&nbsp;adjustment is&nbsp;A<sub>1</sub><sup>(8)</sup>&nbsp;= -1.7283(35) as reported by Kinoshita and Nio (2006). However, (...)&nbsp;it was discovered by Aoyama et al. (2007) that a signi\ufb01cant error had been made in the&nbsp;calculation. In particular, 2 of the 47 integrals representing 518 diagrams that had not been&nbsp;con\ufb01rmed independently required a corrected treatment of infrared divergences. (...) The&nbsp;new value is (Aoyama et al., 2007)&nbsp;A<sub>1</sub><sup>(8)</sup>&nbsp;= 1.9144(35); (111) details of the calculation are&nbsp;given by Aoyama et al. (2008). In view of the extensive effort made by these workers to&nbsp;ensure that the result in Eq. (111) is reliable, the Task Group adopts both its value and&nbsp;quoted uncertainty for use in the 2010 adjustment.</span></p>\n<p style=\"font-size: 16px;\">&nbsp;</p>\n<p style=\"font-size: 16px;\"><span style=\"font-family: CMR12;\">Assuming no more than three million mathematics and physics papers published since the beginnings of scienti\ufb01c publishing, and no less than the three errors documented above, we can safely&nbsp;conclude that the overall error rate of the reasoning used in these \ufb01elds is at least 10<sup>-6</sup>&nbsp;per paper.</span></p>\n<p style=\"font-size: 16px;\">&nbsp;</p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"><strong>The role of automated theorem-proving</strong></span></span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">That human reasoning, much like manual arithmetic, is a signi\ufb01cantly error-prone process comes as&nbsp;</span><span style=\"font-size: 16px;\">no surprise. Starting with de Bruijn&rsquo;s Automath (see Nederpelt et al 1994) logicians and computer&nbsp;</span><span style=\"font-size: 16px;\">scientists have invested signi\ufb01cant effort in mechanized proof checking, and it is indeed only through&nbsp;</span><span style=\"font-size: 16px;\">such efforts, in particular through the Coq veri\ufb01cation (Gonthier 2008) of the entire logic behind the&nbsp;</span><span style=\"font-size: 16px;\">Appel and Haken proof that all lingering doubts about the Four Color Theorem were laid to rest. The&nbsp;</span><span style=\"font-size: 16px;\">error in&nbsp;</span><span style=\"font-size: 16px;\">A</span><sub>1</sub><sup>(8)</sup><span style=\"font-size: 16px;\">&nbsp;was also identi\ufb01ed by using FORTRAN code generated by an automatic code generator&nbsp;</span><span style=\"font-size: 16px;\">(Mohr et al 2012).</span></span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">To gain an appreciation of the state of the art, consider the theorem that \ufb01nite groups of odd&nbsp;</span><span style=\"font-size: 16px;\">order are solvable (Feit and Thompson 1963). The proof, which took two humans about two years&nbsp;</span><span style=\"font-size: 16px;\">to work out, takes up an entire issue of the Paci\ufb01c Journal of Mathematics (255 pages), and it was&nbsp;</span><span style=\"font-size: 16px;\">only this year that a fully formal proof was completed by Gonthier&rsquo;s team (see Knies 2012). The&nbsp;</span><span style=\"font-size: 16px;\">effort, \u0018170,000 lines, \u001815,000 de\ufb01nitions, \u00184,200 theorems in Coq terms, took person-decades of&nbsp;</span><span style=\"font-size: 16px;\">human assistance (15 people working six years, though many of them part-time) even after the toil of&nbsp;</span><span style=\"font-size: 16px;\">Bender and Glauberman (1995) and Peterfalvi (2000), who have greatly cleaned up and modularized&nbsp;</span><span style=\"font-size: 16px;\">the original proof, in which elementary group-theoretic and character-theoretic argumentation was&nbsp;</span><span style=\"font-size: 16px;\">completely intermixed.</span></span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">The classi\ufb01cation of simple \ufb01nite groups is two orders of magnitude bigger: the effort involved&nbsp;</span><span style=\"font-size: 16px;\">about 100 humans, the original proof is scattered among 20,000 pages of papers, the largest (Aschbacher and Smith 2004a,b) taking up two volumes totaling some 1,200 pages. While everybody&nbsp;</span><span style=\"font-size: 16px;\">capable of rendering meaningful judgment considers the proof to be complete and correct, it must be&nbsp;</span><span style=\"font-size: 16px;\">somewhat worrisome at the 10<sup>-</sup></span><span style=\"font-size: 16px;\"><sup>64&nbsp;</sup></span><span style=\"font-size: 16px;\">level that there are no more than a couple of hundred such people, and most of them have something of a vested interest in that they themselves contributed to the&nbsp;</span><span style=\"font-size: 16px;\">proof. Let us suppose that people who are convinced that the classi\ufb01cation is bug-free are offered the&nbsp;</span><span style=\"font-size: 16px;\">following bet by some superior intelligence that knows the answer. You must enter a room with as&nbsp;</span><span style=\"font-size: 16px;\">many people you can convince to come with you and push a button: if the classi\ufb01cation is bug-free&nbsp;</span><span style=\"font-size: 16px;\">you will each receive $100, if not, all of you will immediately die. Perhaps fools rush in where angels&nbsp;</span><span style=\"font-size: 16px;\">fear to tread, but on the whole we still wouldn&rsquo;t expect too many takers.</span></span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\">Whether the classi\ufb01cation of \ufb01nite simple groups is complete and correct is very hard to say &ndash; the&nbsp;</span><span style=\"font-size: 16px;\">planned second generation proof will still be 5,000 pages, and mechanized proof is not yet in sight.&nbsp;</span><span style=\"font-size: 16px;\">But this is not to say that gaining mathematical knowledge of the required degree of reliability is&nbsp;</span><span style=\"font-size: 16px;\">hopeless, it&rsquo;s just that instead of monumental chains of abstract reasoning we need to retreat to considerably simpler ones.&nbsp;</span><span style=\"font-size: 16px;\">Take, for example, the \ufb01rst Sylow Theorem, that if the order of a \ufb01nite group G is divisible by&nbsp;</span><span style=\"font-size: 16px;\">some prime power p<sup>n</sup></span><span style=\"font-size: 16px;\">, G will have a subgroup H of this order. We are <em>absolutely certain</em> about this.&nbsp;</span><span style=\"font-size: 16px;\">Argumentum ad verecundiam of course is still available, but it is not needed: anybody can join the&nbsp;hive-mind by studying the proof. The Coq veri\ufb01cation contains 350 lines, 15 de\ufb01nitions, 90 theorems,&nbsp;</span><span style=\"font-size: 16px;\">and took 2 people 2 weeks to produce. The number of people capable of rendering meaningful&nbsp;</span><span style=\"font-size: 16px;\">judgment is at least three orders of magnitude larger, and the vast majority of those who know the&nbsp;</span><span style=\"font-size: 16px;\">proof would consider betting their lives on the truth of this theorem an easy way of winning $100&nbsp;</span><span style=\"font-size: 16px;\">with no downside risk.</span></span></p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\"><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"><strong>Further remarks</strong></span></span></p>\n<p><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">Not only do we have to prove that the planned AGI will be friendly, the proof itself has to be short enough to be verifiable by humans. Consider, for example, the fundamental theorem of algebra. Could it be the case that we, humans, are all deluded into thinking that an&nbsp;</span><span style=\"font-size: 12.000000pt; font-family: 'CMMI12'\">n-th&nbsp;</span><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">degree polynomial will have&nbsp;</span><span style=\"font-size: 12.000000pt; font-family: 'CMMI12'\">n&nbsp;</span><span style=\"font-size: 12.000000pt; font-family: 'CMR12'\">roots? Yes, but this is unlikely in the extreme. If this so-called theorem is really a trap laid by a superior intelligence we are doomed anyway, humanity can find its way around it no more than a bee can find its way around the windowpane. Now consider the four-color theorem, which is still outside the human-verifiable range. It is fair to say that it would be unwise to create AIs whose friendliness critically depends on design limits implied by the truth of this theorem, while AIs whose friendliness is guaranteed by the fundamental theorem of algebra represent a tolerable level of risk.&nbsp;</span></p>\n<p><span style=\"font-family: CMR12;\"><span style=\"font-size: 16px;\"><span style=\"font-family: CMR12;\">Recently, Goertzel and Pitt (2012) have laid out a plan to endow AGI with morality by means of carefully controlled machine learning. Much as we are in agreement with their goals, we remain skeptical about their plan meeting the plain failure engineering criteria laid out above.</span></span></span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CeZgKf6uTaPZb4x3u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 27, "extendedScore": null, "score": 1.061616769752788e-06, "legacy": true, "legacyId": "20712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-18T20:39:05.260Z", "modifiedAt": null, "url": null, "title": "Meetup : Montpellier: Tentative first meetup", "slug": "meetup-montpellier-tentative-first-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:58.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ShXXnjkxF2WX46r3A/meetup-montpellier-tentative-first-meetup", "pageUrlRelative": "/posts/ShXXnjkxF2WX46r3A/meetup-montpellier-tentative-first-meetup", "linkUrl": "https://www.lesswrong.com/posts/ShXXnjkxF2WX46r3A/meetup-montpellier-tentative-first-meetup", "postedAtFormatted": "Tuesday, December 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montpellier%3A%20Tentative%20first%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montpellier%3A%20Tentative%20first%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShXXnjkxF2WX46r3A%2Fmeetup-montpellier-tentative-first-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montpellier%3A%20Tentative%20first%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShXXnjkxF2WX46r3A%2Fmeetup-montpellier-tentative-first-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShXXnjkxF2WX46r3A%2Fmeetup-montpellier-tentative-first-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/h7\">Montpellier: Tentative first meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">03 January 2013 03:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Bagel House, 6 Rue Loys, Montpellier, France</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>I'm going to be in Montpellier for the holidays <del>and I noticed that there isn't a single LW group in France</del>. So this is a one-shot experimental meetup. I'll be there with a sign and a book, hoping for interesting people in the area to drop by.</p>\n<p>The location is the best I could find through internet search, since I don't know the city all that well. It's open to suggestions.</p>\n<p>The meetup could be held in French if everyone who comes is fluent.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ShXXnjkxF2WX46r3A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0616468707219665e-06, "legacy": true, "legacyId": "20730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T04:02:49.448Z", "modifiedAt": null, "url": null, "title": "Notes on Psychopathy", "slug": "notes-on-psychopathy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:37:03.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ktr39MFWpTqmzuKxQ/notes-on-psychopathy", "pageUrlRelative": "/posts/ktr39MFWpTqmzuKxQ/notes-on-psychopathy", "linkUrl": "https://www.lesswrong.com/posts/ktr39MFWpTqmzuKxQ/notes-on-psychopathy", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20Psychopathy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20Psychopathy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fktr39MFWpTqmzuKxQ%2Fnotes-on-psychopathy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20Psychopathy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fktr39MFWpTqmzuKxQ%2Fnotes-on-psychopathy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fktr39MFWpTqmzuKxQ%2Fnotes-on-psychopathy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10453, "htmlBody": "<blockquote>\n<p>This is some old work I did for SI. See also <a href=\"/lw/dtg/notes_on_the_psychology_of_power/\">Notes on the Psychology of Power</a>.</p>\n</blockquote>\n<p>Deviant but not necessarily diseased or dysfunctional minds can demonstrate resistance to all treatment and attempts to change their mind (think <a href=\"/lw/rn/no_universally_compelling_arguments/\">No Universally Compelling Arguments</a>; the premier example are probably <a href=\"http://en.wikipedia.org/wiki/Psychopathy\">psychopaths</a> - no drug treatments are at all useful nor are there any therapies with solid evidence of even marginal effectiveness (one widely cited chapter, <a title=\"Harris &amp; Rice 2006\" href=\"http://dl.dropbox.com/u/85192141/2006-harris.pdf\">&ldquo;Treatment of psychopathy: A review of empirical findings&rdquo;</a>, concludes that some attempted therapies merely made them more effective manipulators! We&rsquo;ll look at that later.) While some psychopath traits bear resemblance to general characteristic of the powerful, they&rsquo;re still a pretty unique group and worth looking at.</p>\n<p>The main focus of my excerpts is on whether they are treatable, their effectiveness, possible evolutionary bases, and what other issues they have or don&rsquo;t have which might lead one to not simply write them off as &ldquo;broken&rdquo; and of no relevance to AI.</p>\n<p>(For example, if we were to discover that psychopaths were healthy human beings who were not universally mentally retarded or ineffective in gaining wealth/power and were destructive and amoral, despite being completely human and often socialized normally, then what does this say about the fragility of human values and how likely an AI will just be nice to us?)</p>\n<p><a id=\"more\"></a></p>\n<p id=\"psychopathy\">As usual in my 'notes' articles, the following is a series of excerpts and citations; if any interest you, leave a comment and I will try to jailbreak a copy for you or failing that, post a request on the <a href=\"/lw/eto/lesswrong_help_desk_free_paper_downloads_and_more/\">research help page</a>.</p>\n<h1><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Psychopathy</a></h1>\n<p><a href=\"http://libgen.info/view.php?id=467096\"><em>The Psychopath: Emotion and the brain</em></a>, Blair et al 2005:</p>\n<blockquote>\n<p>There are now a relatively large number of studies indicating that individuals with psychopathy reoffend at higher rates than non-psychopathic individuals. For example, in an early study, the PCL-R was administered to 231 offenders prior to release from prisons (Hart et al., 1988). Within 3 years, 25% of non-psychopathic individuals had been re-incarcerated. In sharp contrast, 80% of the individuals with psychopathy had breached the terms of their release. In another study Serin and Amos (1995) followed 299 offenders, and within 3 years, 65% of individuals with psychopathy versus only 25% of the non-psychopathic individuals were convicted of a new offence. Such results have been found in European studies also. Thus, in a Swedish sample of forensic patients, Grann et al. (1999) found that individuals scoring above 25 on the PCL-R violently reoffended at a rate of 66% versus only 18% for those with a score less than 26. In Belgium, the reconviction rates of psychopathic, middle scoring, and low scoring individuals were 44% , 21% , and 11% , respectively (Hare et al., 2000)&hellip;An international study of 278 offenders is of particular interest. This found that 82% of the individuals with psychopathy but only 40% of non- psychopathic individuals were reconvicted of an offence (Hare et al., 2000). In the same group, 38% of the high psychopathy group committed a violent offence, but only 2.7% of those with a low PCL-R score did. Interestingly, both the individuals with psychopathy and the non-psychopathic individuals failed to show attenuated reconviction rates following treatment after controlling for age and criminal history. However, the pattern of results changes when Factor 1 scores [interpersonal/affective: glib charm, grandiosity, lying, callosity, cunning] are carefully examined. Participants with high Factor 1 scores reoffended at higher rates if they had been treated: 86% as opposed to 59% ! Similarly striking results have been seen when examining participants who engage in educational and vocational training programs. Here offenders with low Factor 1 scores show an improvement in recidivism rate following the course. However, offenders with high Factor 1 scores are reconvicted at higher rates if they take part in these programs rather than if they do not. In what is perhaps the most comprehensive review and meta-analysis to date, Hemphill and colleagues (1998) examined nine available published and unpublished prospective studies of psychopathy and recidivism. The length of followup for the studies reviewed ranged from 1 to 10.5 years. The authors determined that within a year of release, individuals with psychopathy are three times more likely to recidivate, and four times more likely to recidivate violently. In fact, the relative risk for reoffending (the proportion of psychopathic individuals who reoffend divided by the proportion of non-psychopathic offenders who reoffend) ranged from 1.7 to as high as 6.5 across studies. Taken together, at a 1-year follow-up, the general recidivism rate for individuals with psychopathy was three times higher than that of non-psychopathic individuals and the violent recidivism rate was three to five times higher. Psychopathy is associated with both general and violent recidivism at follow-up lengths of as little as a year, or as long as more than 10 years.</p>\n<ul>\n<li>Hart S. D., Kropp P. R., Hare R. D. (1988). Performance of male psychopaths following conditional release from prison. Journal of Consulting and Clinical Psychology, 56, 227-232</li>\n<li>Serin R. C., Amos N. L. (1995). The role of psychopathy in the assessment of dangerousness. International Journal of Law and Psychiatry, 18, 231-238</li>\n<li>Grann M., Langstrom N., Tengstrom A., Kullgren G. (1999). Psychopathy (P. C. L-R) predicts violent recidivism among criminal offenders with personality disorders in Sweden. Law and Human Behavior, 23, 205-217.</li>\n<li>Hare R. D., Clark D., Grann M., Thornton D. (2000). Psychopathy and the predictive validity of the P. C. L-R: an international perspective. Behavioral Sciences and the Law, 18, 623-645</li>\n<li>Hemphill J. F., Hare R. D., Wong S. (1998). Psychopathy and recidivism: a review. Legal and Criminological Psychology, 3, 139-170</li>\n</ul>\n<p>However, preliminary work conducted by Paul Frick using the APSD has examined the incidence rate of psychopathic tendencies in community samples involving children. As discussed in chapter 1, we have used a score of 27 on the APSD as our cut-off point for a classification of psychopathic tendencies in many of our studies (Blair et al., 2001a, b). Using this cut-off results in a prevalence rate of psychopathic tendencies of between 1.23% and 3.46% (Frick, personal communication); i.e., approximately one quarter of the incidence rate of CD in community samples. Moreover, epidemiological studies examining the prevalence of psychopathy in forensic samples have been conducted. These reveal that while up to 80% of US inmates reach diagnostic criteria for ASPD, only 15-25% of US inmates meet criteria for psychopathy according to the criteria laid down by the PCL-R (Hare, 1996). In other words, approximately one quarter of those receiving the DSM-IV diagnosis of ASPD meet the criteria for psychopathy. Based on these findings and the 3% community incidence rate of ASPD suggested by the DSM-IV, the prevalence of psychopathy can be inferred. If we assume approximately 25% of those with a diagnosis of ASPD might meet criteria for psychopathy, we can estimate an incidence rate for psychopathy in males in the community of 0.75%.</p>\n<ul>\n<li>Blair R. J. R., Colledge E., Mitchell D. G. (2001a). Somatic markers and response reversal: is there orbitofrontal cortex dysfunction in boys with psychopathic tendencies? Journal of Abnormal Child Psychology, 29(6), 499-511.</li>\n<li>Blair R. J. R., Colledge E., Murray L., Mitchell D. G. (2001b). A selective impairment in the processing of sad and fearful expressions in children with psychopathic tendencies. Journal of Abnormal Child Psychology, 29(6), 491-498</li>\n<li>Hare R. D. (1996). Psychopathy: a clinical construct whose time has come. Criminal Justice and Behavior, 23, 25-54</li>\n</ul>\n<p>Hare and colleagues, using the Wechsler Adult Intelligence Scale, found little correlation between IQ and both PCL-R total scores and emotional dysfunction (Factor 1) scores. However, there was a modest negative correlation with antisocial behavior (Factor 2 [Impulsive/antisocial lifestyle: delinquency, parasitism, no long-term goals]) scores (Hare, 1991); i.e., lower IQ was associated with higher levels of antisocial behavior. Similar findings have been reported with children with psychopathic tendencies (Frick et al., 1994). Similarly, Hare, using a battery of tests that measure over 20 mental abilities, found no correlation between cognitive functioning and Factor 1 or total PCL-R scores, but did find a significant negative correlation (r = &minus;0.46) between Factor 2 and &ldquo;crystallized intelligence&rdquo; (Hare, 2003). Crystallized intelligence can be considered a measure of accumulated knowledge. It is highly influenced by an individual&rsquo;s experience (i.e., their schooling and involvement in cultural activities). Moreover, Hare, in a comprehensive review, reports a consistent, though modest, negative correlation between education and Factor 2, but not Factor 1, scores (Hare, 2003). Overall, then, there is no evidence to suggest that individuals with psychopathy have superior IQ compared to individuals with no psychopathy. However, antisocial behavior does appear to be linked with lower intelligence and lower level of schooling.</p>\n<ul>\n<li>Hare R. D. (1991). <em>The Hare Psychopathy Checklist - Revised</em>. Toronto, Ontario: Multi-Health Systems</li>\n<li>Frick P. J., O&rsquo;Brien B. S., Wootton J. M., McBurnett K. (1994). Psychopathy and conduct problems in children. Journal of Abnormal Psychology, 103, 700-707</li>\n<li>Hare R. D. (2003). <em>The Hare Psychopathy Checklist - Revised (PCL-R)</em>, 2nd edn. Toronto, Ontario: Multi-Health Systems.</li>\n</ul>\n</blockquote>\n<ul>\n<li>psychopathy uncorrelated with schizophrenia; <em>negatively</em> correlated with depression &amp; anxiety/fear in general (interestingly, positively correlated with Neuroticism, see &ldquo;Psychopathy and Personality&rdquo; in the <em>Handbook</em>); positively correlated with substance abuse of various sorts, and with ADHD</li>\n</ul>\n<blockquote>\n<p>In this chapter, we also reported that age, SES, and IQ are all inversely related to antisocial behavior. The older an individual is (after the age of 20 years), the higher their SES, and the higher their IQ, the less likely they are to engage in antisocial behavior. Moreover, we also reported that all of these variables are inversely associated with the antisocial behavior (Factor 2) component of psychopathy. However, it was interesting to note that none of these variables are associated with the emotional dysfunction (Factor 1) component of psychopathy&hellip;Much antisocial behavior shown by individuals with psychopathy is instrumental in nature - it has the goal of gaining another&rsquo;s money, sexual favors, or &ldquo;respect&rdquo; (Cornell et al., 1996; Williamson et al., 1987). Individuals can attempt to achieve these goals through a variety of means. Having a higher SES (or for that matter intelligence) enables a wider choice of available routes for achieving these goals than having a lower SES (or intelligence). We suggest that a reason for the inverse relationship between SES and IQ with the antisocial behavior component of psychopathy is that lower SES/IQ limits the behavioral options available so that antisocial behavior appears a useful route to the goal. A healthy individual of limited SES/IQ may also have a narrow range of behavioral options but will exclude antisocial behavior because of aversion to this behavior formed during socialization (see chapter 8). In contrast, individuals with psychopathy may entertain the antisocial option because they do not find the required antisocial behavior aversive&hellip;In other words, we anticipate that there are individuals of higher SES who do not present with the full psychopathic syndrome even though their emotional dysfunction is of an equivalent degree to other individuals who present with both the emotional and behavioral components of the disorder.</p>\n<ul>\n<li>Cornell D. G., Warren J., Hawk G., Stafford E., Oram G., Pine D. (1996). Psychopathy in instrumental and reactive violent offenders. Journal of Consulting and Clinical Psychology, 64, 783-790</li>\n<li>Williamson S., Hare R. D., Wong S. (1987). Violence: criminal psychopaths and their victims. Canadian Journal of Behavioral Science, 19, 454-462</li>\n</ul>\n<p>One extinction task that has been used with individuals with psychopathy is a card playing task originally developed by Joe Newman and colleagues (Newman et al., 1987). In this task, the participant has to decide whether to play a card. Initially, the participant&rsquo;s choice to play is always reinforcing; if the participant plays the card he or she will win points or money. However, as the participant progresses through the pack of cards, the probability of reward decreases. Thus, initially ten out of ten cards are rewarded, then nine out of ten, then eight out of ten continuing on until zero out of ten cards are rewarded. The participant should stop playing the cards when playing means that more cards are associated with punishment rather than reward. That is, they should stop playing the cards when only four out of ten cards are associated with reward. Children with psychopathic tendencies and adult individuals with psychopathy have considerable difficulty with this task; they continue to play the cards even when they are being repeatedly punished and may end up losing all the points that they had gained (Fisher and Blair, 1998; Newman et al., 1987; O&rsquo;Brien and Frick, 1996).</p>\n<ul>\n<li>Fisher L., Blair R. J. R. (1998). Cognitive impairment and its relationship to psychopathic tendencies in children with emotional and behavioural difficulties. Journal of Abnormal Child Psychology, 26, 511-519</li>\n<li>Newman J. P., Patterson C. M., Kosson D. S. (1987). Response perseveration in psychopaths. Journal of Abnormal Psychology, 96, 145-148</li>\n<li>O&rsquo;Brien B. S., Frick P. J. (1996). Reward dominance: associations with anxiety, conduct problems, and psychopathy in children. Journal of Abnormal Child Psychology, 24, 223-240</li>\n</ul>\n<p>There are several ways in which people differentiate between moral and conventional transgressions. Thus, first of all, people generally judge moral transgressions to be more serious than conventional transgressions (Nucci, 1981; Smetana and Braeges, 1990; Turiel, 1983). Second, people give different reasons for justifying why moral and conventional transgressions are wrong. Thus, for moral transgressions, people refer to the distress of the victim (i.e., it is wrong to hit someone because it will hurt them), but for conventional transgressions, people refer to the social disorder that may ensue (i.e., it is wrong to talk in class because you are there to learn) (Smetana, 1993; Turiel, 1983). Third, and more importantly, modifying the rule conditions (for example, by an authority figure removing the prohibition against the act) only affects the permissibility of conventional transgressions. Thus, even if there is no rule prohibiting the action, participants generally judge moral transgressions as non-permissible (i.e., they still think it is wrong to hit another individual even if there is no rule against it). In contrast, if there is no rule prohibiting a conventional transgression, participants generally judge the act as permissible (i.e., they think it is OK to talk in class if there is no rule against it). While participants do not always make the moral/conventional distinction in their seriousness judgments, they do always make the moral/conventional distinction in their modifiability judgments. Thus, children at certain ages have been found to judge some conventional and moral transgressions as equally serious (Stoddart and Turiel, 1985; Turiel, 1983). However, they still identify the moral transgressions as less rule contingent and less under authority jurisdiction than the conventional transgressions.</p>\n<ul>\n<li>Nucci L. P. (1981). Conceptions of personal issues: a domain distinct from moral or societal concepts. Child Development, 52, 114-121</li>\n<li>Smetana J. G., Braeges J. L. (1990). The development of toddlers&rsquo; moral and conventional judgments. Merrill-Palmer Quarterly, 36, 329-346</li>\n<li>Turiel E. (1983). The Development of Social Knowledge: Morality and convention. Cambridge: Cambridge University Press</li>\n<li>Smetana J. G. (1993). &ldquo;Understanding of social rules&rdquo;. In M. Bennett (ed.), <em>The Child as Psychologist: An introduction to the development of social cognition</em>, pp.&nbsp;111-141. New York: Harvester Wheatsheaf.</li>\n<li>Stoddart T., Turiel E. (1985). Children&rsquo;s concepts of cross-gender activities. Child Development, 56, 1241-1252</li>\n</ul>\n<p>Children with psychopathic tendencies and adults with psychopathy have considerable difficulty with the moral/conventional distinction task (Blair, 1995, 1997; Blair et al., 1995a, 2001c). In addition, similar difficulties have been observed with more general populations of children presenting with antisocial behavior (Arsenio and Fleiss, 1996; Dunn and Hughes, 2001; Hughes and Dunn, 2000; Nucci and Herman, 1982). Children with psychopathic tendencies, adults with psychopathy, and other antisocial populations do generally regard moral transgressions as more serious than conventional transgressions. However, such populations are far less likely than comparison individuals to make reference to the victim of the transgression when justifying why moral transgressions are bad (Arsenio and Fleiss, 1996; Blair, 1995; Blair et al., 2001c; Dunn and Hughes, 2001; Hughes and Dunn, 2000). In addition, when the rules prohibiting the transgressions are removed, such populations are far less likely to make the distinction between moral and conventional transgressions that is seen in healthy individuals (Blair, 1995; Blair et al., 2001c; Nucci and Herman, 1982)&hellip;while it has been repeatedly shown that the use of empathy-inducing positive parenting strategies by caregivers decreases the probability of antisocial behavior in healthy developing children, it does not decrease the probability of antisocial behavior in children who present with the emotional dysfunction of psychopathy (Wootton et al., 1997).</p>\n<ul>\n<li>Blair R. J. R. (1995). A cognitive developmental approach to morality: investigating the psychopath. Cognition, 57, 1-29</li>\n<li>Blair R. J. R. (1997). Moral reasoning in the child with psychopathic tendencies. Personality and Individual Differences, 22, 731-739</li>\n<li>Blair R. J. R., Jones L., Clark F., Smith M. (1995a). Is the psychopath &ldquo;morally insane&rdquo;? Personality and Individual Differences, 19, 741-752</li>\n<li>Blair R. J. R., Monson J., Frederickson N. (2001c). Moral reasoning and conduct problems in children with emotional and behavioural difficulties. Personality and Individual Differences, 31, 799-811</li>\n<li>Arsenio W. F., Fleiss K. (1996). Typical and behaviourally disruptive children&rsquo;s understanding of the emotion consequences of socio-moral events. British Journal of Developmental Psychology, 14, 173-186</li>\n<li>Dunn J., Hughes C. (2001). &ldquo;I got some swords and you&rsquo;re dead!&rdquo;: violent fantasy, antisocial behavior, friendship, and moral sensibility in young children. Child Development, 72(2), 491-505</li>\n<li>Hughes C., Dunn J. (2000). Hedonism or empathy? Hard-to-manage children&rsquo;s moral awareness and links with cognitive and maternal characteristics. British Journal of Developmental Psychology, 18, 227-245</li>\n<li>Nucci L. P., Herman S. (1982). Behavioral disordered children&rsquo;s conceptions of moral, conventional, and personal issues. Journal of Abnormal Child Psychology, 10, 411-425</li>\n<li>Wootton J. M., Frick P. J., Shelton K. K., Silverthorn P. (1997). Ineffective parenting and childhood conduct problems: the moderating role of callous - unemotional traits. Journal of Consulting and Clinical Psychology, 65, 292-300</li>\n</ul>\n</blockquote>\n<p>Long section summary:</p>\n<blockquote>\n<p>Data indicating that individuals with psychopathy present with relatively little or no impairment for functions known to require the integrity of the amygdala, such as the formation of stimulus-reward associations and aspects of social cognition, qualify the amygdala dysfunction position. They suggest that the genetic anomalies, which we assume are the fundamental causes of psychopathy, do not globally disrupt the functioning of the amygdala but rather have a more selective effect, perhaps by disrupting the functioning of specific neurotransmitter(s) involved in specific aspects of amygdala functioning. We suggest that the noradrenergic response to stress/threat stimuli may be disturbed in individuals with psychopathy.</p>\n</blockquote>\n<p>More on the ADHD correlation:</p>\n<blockquote>\n<p>ADHD is a conundrum because while there is high comorbidity of ADHD with psychopathic tendencies (Babinski et al., 1999; Barry et al., 2000; Colledge and Blair, 2001; Lynam, 1996), the neurocognitive impairments seen in children with ADHD are, to a large extent, not found in individuals with psychopathy. &hellip;Individuals with psychopathy show no impairment on classic measures of executive functioning such as the Wisconsin Card Sorting Task (LaPierre et al., 1995) or the ED-shift component of the ID/ED task (Mitchell et al., 2002). Individuals with ADHD show difficulty with both of these tasks (Pennington and Ozonoff, 1996; Williams et al., 2000). Individuals with psychopathy show no impairment, or even reduced interference (Newman et al., 1997), on Stroop, or Stroop-like, tasks (Blair et al., under revision; Smith et al., 1992). As described above, individuals with ADHD show striking difficulty with such tasks.</p>\n<ul>\n<li>Babinski L. M., Hartsough C. S., Lambert N. M. (1999). Childhood conduct problems, hyperactivity-impulsivity, and inattention as predictors of adult criminal activity. Journal of Child Psychology and Psychiatry and Allied Disciplines, 40, 347-355</li>\n<li>Barry C. T., Frick P. J., DeShazo T. M., McCoy M. G., Ellis M., Loney B. R. (2000). The importance of callous-unemotional traits for extending the concept of psychopathy to children. Journal of Abnormal Psychology, 109(2), 335-340</li>\n<li>Colledge E., Blair R. J. R. (2001). Relationship between attention-deficit-hyperactivity disorder and psychopathic tendencies in children. Personality and Individual Differences, 30, 1175-1187</li>\n<li>Lynam D. R. (1996). Early identification of chronic offenders: who is the fledgling psychopath? Psychological Bulletin, 120(2), 209-224</li>\n<li>LaPierre D., Braun C. M. J., Hodgins S. (1995). Ventral frontal deficits in psychopathy: neuropsychological test findings. Neuropsychologia, 33, 139-151</li>\n<li>Mitchell D. G. V., Colledge E., Leonard A., Blair R. J. R. (2002). Risky decisions and response reversal: is there evidence of orbitofrontal cortex dysfunction in psychopathic individuals? Neuropsychologia, 40, 2013-2022</li>\n<li>Pennington B. F., Ozonoff S. (1996). Executive functions and developmental psychopathology. Journal of Child Psychology and Psychiatry, 37, 51-87</li>\n<li>Williams D., Stott C. M., Goodyer I. M., Sahakian B. J. (2000). Specific language impairment with or without hyperactivity: neuropsychological evidence for frontostriatal dysfunction. Developmental Medicine and Child Neurology, 42(6), 368-375</li>\n<li>Newman J. P., Schmitt W. A., Voss W. D. (1997). The impact of motivationally neutral cues on psychopathic individuals: assessing the generality of the response modulation hypothesis. Journal of Abnormal Psychology, 106, 563-575</li>\n<li>Blair K. S., Newman C., Mitchell D. G., Richell R. A., Leonard A., Morton J., Blair R. J. R. (under revision). Differentiating among prefrontal substrates in psychopathy: neuropsychological test findings</li>\n<li>Smith S. S., Arnett P. A., Newman J. P. (1992). Neuropsychological differentiation of psychopathic and nonpsychopathic criminal offenders. Personality and Individual Differences, 13(11), 1233-1243</li>\n</ul>\n</blockquote>\n<p>A possible overall picture:</p>\n<blockquote>\n<p>No biologically based disorder other than psychopathy is associated with an increased risk of instrumental aggression Currently, there are no reasons to believe that there are any biologically-based disorders associated with a heightened risk of instrumental antisocial behavior other than psychopathy. There are other disorders associated with a heightened risk of instrumental antisocial behavior (e.g., adolescent-limited CD) but they are not biologically based. In chapter 8, we developed an account of psychopathy. In essence, this account suggests that genetic anomalies give rise to a disorder where there is reduced responsiveness of the amygdala to aversive stimuli in particular. This specific form of reduced emotional responsiveness interferes with socialization such that the individual is more likely to learn to use anti-social behavior to achieve goals.</p>\n</blockquote>\n<p><a href=\"http://libgen.info/view.php?id=510662\"><em>Snakes in Suits: When Psychopaths Go To Work</em></a>, Babiak &amp; Hare 2006:</p>\n<blockquote>\n<p>Psychopathy is a personality disorder described by the personality traits and behaviors that form the basis of this book. Psychopaths are without conscience and incapable of empathy, guilt, or loyalty to anyone but themselves. Sociopathy is not a formal psychiatric condition. It refers to patterns of attitudes and behaviors that are considered antisocial and criminal by society at large, but are seen as normal or necessary by the subculture or social environment in which they developed. Sociopaths may have a well-developed conscience and a normal capacity for empathy, guilt, and loyalty, but their sense of right and wrong is based on the norms and expectations of their subculture or group. Many criminals might be described as sociopaths. Antisocial personality disorder (APD) is a broad diagnostic category found in the American Psychiatric Association&rsquo;s Diagnostic and Statistical Manual of Mental Disorders, 4th edition (DSM-IV). Antisocial and criminal behaviors play a major role in its definition and, in this sense, APD is similar to sociopathy. Some of those with APD are psychopaths, but many are not. The difference between psychopathy and antisocial personality disorder is that the former includes personality traits such as lack of empathy, grandiosity, and shallow emotion that are not necessary for a diagnosis of APD. APD is three or four times more common than psychopathy in the general population and in prisons. The prevalence of those we would describe as sociopathic is unknown but likely is considerably higher than that of APD.</p>\n<p>Several recent twin studies provide convincing evidence that genetic factors play at least as important a role in the development of the core features of psychopathy as do environmental factors and forces. Researchers Blonigen, Carlson, Krueger &amp; Patrick stated that the results of their study of 271 adult twin pairs provided &ldquo;substantial evidence of genetic contributions to variance in the personality construct of psychopathy.&rdquo; Subsequently, researchers Larrson, Andershed &amp; Lichstenstien arrived at a similar conclusion in their study of 1090 adolescent twin pairs: &ldquo;A genetic factor explains most of the variation in the psychopathic personality.&rdquo; Viding, Blair, Moffitt &amp; Plomin studied 3687 seven-year-old twin pairs and also concluded that &ldquo;the core symptoms of psychopathy are strongly genetically determined.&rdquo; They reported that the genetic contribution was highest when callous-unemotional traits were combined with antisocial behaviors.</p>\n<ul>\n<li>Blonigen, D. M., Carlson, S. R., Krueger, R. F., &amp; Patrick, C. J. <a href=\"http://dl.dropbox.com/u/85192141/2003-blonigen.pdf\">&ldquo;A twin study of self-reported psychopathic personality traits&rdquo;</a>. <em>Personality and Individual Differences</em>, 35(1), 179-197, 2003.</li>\n<li>Larrson, H., Andershed, H., &amp; Lichstenstien, P. <a href=\"http://dl.dropbox.com/u/85192141/2006-larsson.pdf\">&ldquo;A genetic factor explains most of the variation in the psychopathic personality&rdquo;</a>. <em>Journal of Abnormal Psychology</em>, 115(2), 221-230, 2006</li>\n<li>Viding, E., Blair, R. J. R., Moffitt, T. E., &amp; Plomin, R. <a href=\"http://www.lscp.net/persons/ramus/fr/GDP1/papers/viding05.pdf\">&ldquo;Evidence for substantial genetic risk for psychopathy in 7-year-olds&rdquo;</a>. <em>Journal of Child Psychology and Psychiatry</em>, 46(6), 592-597, 2005.</li>\n</ul>\n<p>Unfortunately, no group is more surprised to learn that they have been psychologically manipulated than those who believe they are smarter and stronger than others, no matter how true this may be. Narcissistic managers, in particular, tend to rise to management positions in organizations in disproportionately large numbers. Being particularly self-absorbed, they are known to use (and abuse) their subordinates and play up to their superiors to assure their own personal career success. (See pages 40-41 for similarities and differences between narcissists and psychopaths.) We have spoken with a number of narcissistic managers who also felt victimized by corporate cons: much to their own surprise-and not easy for them to admit- they were outclassed and outgunned. Additionally, and this really plays into the hands of the corporate con, individuals with strong personalities, such as narcissism, are far less likely than most to seek assistance, guidance, or even personal feedback until it is too late, making them attractive long-term targets.</p>\n<p>PricewaterhouseCoopers (PWC) reported that in 2003, 37 percent of 3,600 companies in 50 countries had suffered from fraudulent acts, with an average company loss of more that $2 million. The actual average loss likely was much higher because of failures to detect or report frauds, or a tendency to write them off as a commercial loss. One quarter of the frauds were committed by senior managers and executives with a sophisticated understanding of the company&rsquo;s internal controls and risk management procedures. In spite of the public outrage at the recent spate of high-profile scandals in the corporate world, things are not getting any better. In 2004, the percentage of companies in the PWC global survey that experienced fraud rose from 37 to 44 and then to 45 in 2005.</p>\n<ul>\n<li>&ldquo;Corporate Fraud in the Boardroom&rdquo; Skalak, S., Nestler, C., &amp; Bussmann, K. Global Economic Crime Survey, 2005. PricewaterhouseCoopers</li>\n</ul>\n<p>In the journal <em>Psychology, Crime, and Law</em>, researchers Board and Fritzon administered a self-report personality inventory to a sample of British senior business managers and executives. They concluded that the prevalence of histrionic, narcissistic, and compulsive personality disorders was relatively high, and that many of the traits exhibited were consistent with psychopathy: superficial charm, insincerity, egocentricity, manipulativeness, grandiosity, lack of empathy, exploitativeness, independence, rigidity, stubbornness, and dictatorial tendencies.</p>\n<ul>\n<li>Board, B. J., &amp; Fritzon, K. Disordered personalities at work. Psychology, Crime and Law, 11(1), 17-32, 2005.</li>\n</ul>\n<p>In our original research working with almost 200 high-potential executives, we found about 3.5 percent who fit the profile of the psychopath as measured on the PCL: SV (pages 26-28). [Unable to find a paper about this original research]</p>\n<p>We now know that some organizations actively seek out and recruit individuals with at least a moderate dose of psychopathic features. Some executives have said to us, &ldquo;Many of the traits you describe to us seem to be valued by our company. Why shouldn&rsquo;t companies hire psychopaths to fill some jobs?&rdquo; A proper, scientific answer is that more research is needed to determine the impact of various doses of psychopathic characteristics on the performance of different types of jobs. The &ldquo;optimal&rdquo; number and severity of such characteristics presumably is higher for some jobs (such as stock promoter, politician, law enforcement, used-car salespeople, mercenaries, and lawyers) than for others (such as social workers, teachers, nurses, and ministers). Until such research is done, we can safely say that those who believe that &ldquo;psychopathy is good&rdquo; clearly have not had much exposure to the real thing.</p>\n<p>Are psychopaths particularly well suited for dangerous professions? David Cox, a psychology professor at Simon Fraser University, doesn&rsquo;t think so. He studied British bomb-disposal operations in Northern Ireland, beginning his research with the expectation that because psychopaths are &ldquo;cool under fire&rdquo; and have a strong &ldquo;need for excitement&rdquo; they would excel at the job. But he found that the soldiers who performed the exacting and dangerous task of defusing or dismantling IRA bombs referred to psychopaths as &ldquo;cowboys&rdquo;-unreliable and impulsive individuals who lacked the perfectionism and attention to detail needed to stay alive on the job. Most were filtered out during training, and those who slipped through didn&rsquo;t last long.</p>\n<p>We analyzed the succession plans of a few hundred North American executives and noted that the similarities between the developmental issues for some managers identified as &ldquo;high potentials&rdquo; and psychopathic-like features were startling. Our list of questionable characteristics-dysfunctional behaviors, attitudes, and judgments-was refined to form the B-Scan, a research instrument for use by companies as part of their evaluation for succession planning. We obtained clear differences between a group of successful, high-performing executives and a group of convicted white-collar or economic criminals (that is, individuals who defrauded their companies and other innocent victims). In a follow-up investigation, we also found predictable differences between the successful high performers and corporate psychopaths. Research on the B-Scan continues.</p>\n<ul>\n<li>\n<p>For more information about research with the B-Scan, see <a href=\"http://www.B-Scan.com\"><code class=\"url\">http://www.B-Scan.com</code></a>.</p>\n<p>[This research may be the research discussed in <a href=\"http://dl.dropbox.com/u/85192141/2010-babiak.pdf\">&ldquo;Corporate Psychopathy: Talking the Walk&rdquo;</a>, Babiak et al 2010.]</p>\n</li>\n</ul>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2006-larsson.pdf\">&ldquo;A Genetic Factor Explains Most of the Variation in the Psychopathic Personality&rdquo;</a>, Larsson et al 2006:</p>\n<blockquote>\n<p>The authors used a self-report questionnaire (The Youth Psychopathic Traits Inventory) to study the importance of genetic and environmental influences on psychopathic personality traits in a sample of 1,090 monozygotic and dizygotic twin pairs, aged 16 -17 years. Results showed a strong genetic influence behind the higher order &ldquo;psychopathic personality&rdquo; factor, underpinned by the three psychopathic personality dimensions. Over and above the effects to the higher order factor, significant unique genetic influences were also found in the callous/unemotional and in the impulsive/irresponsible dimension, but not in the grandiose/manipulative dimension.</p>\n<p>&hellip;there are only two published twin studies that have directly investigated the importance of genetic and environmental influences for psychopathic traits (Blonigen, Carlson, Krueger, &amp; Patrick, 2003; Taylor, Loney, Bobadilla, Iacono, &amp; McGue, 2003). One of these used a sample of adult male twins (Blonigen et al., 2003) and examined the genetic and environmental influence on psychopathic personality traits by using a self-report measure, the Psychopathic Personality Inventory (PPI; Lilienfeld &amp; Andrews, 1996). Individual differences in all eight dimensions measured by the PPI were associated with genetic and <em>nonshared environmental effects</em>. Genetic effects explained 29%-56% of the variation of the respective dimensions of the PPI. Shared environmental effects were not found for any of the PPI facets (Blonigen et al., 2003). Another recent study on male adolescent twins (Taylor et al., 2003) used a self-report measure that taps the impulsive/antisocial behavior and callous/unemotional interpersonal style of the psychopathic personality constellation (Minnesota Temperament Inventory; Loney, Taylor, Butler, &amp; Iacono, 2002). In this study, genetic effects accounted for approximately 40% of the variation in both the callous/unemotional and the impulsive/antisocial factors. <em>Nonshared environmental effects explained all of the remaining variance, whereas the influences of shared environment seemed to be of no importance</em> (Taylor el al., 2003). [emphasis added] &hellip;As would be expected, nonshared environmental influences were found [by us] to be significant in all of the analyses conducted. Nonshared environmental factors were shown to be important for explaining 37% of the variance in the latent psychopathic personality factor.</p>\n<p>&hellip;The results from the present study suggest that shared environmental factors produce a negligible contribution to the variance in the psychopathic personality constellation. These results replicate those of recent twin studies examining self-reported psychopathic traits in adolescent (Taylor et al., 2003) and adult (Blonigen et al., 2003) twins in finding no evidence of shared environmental influences in psychopathic traits. The results are also consistent with evidence reported from many behavioral genetic studies of psychopathology (Bouchard &amp; McGue, 2003) and personality (Bouchard &amp; Loehlin, 2001; Loehlin, 1992; McGuffin &amp; Thapar, 1992).</p>\n</blockquote>\n<p><a href=\"http://libgen.info/view.php?id=532288\"><em>Handbook of Psychopathy</em></a>, ed. Christopher Patrick 2005</p>\n<blockquote>\n<p>For example, in her important study of mental illness in primitive societies, Murphy (1976) found that the Yupic-speaking Eskimos in northwest Alaska have a name, <em>kunlangeta</em>, for the</p>\n<blockquote>\n<p>man who, for example, repeatedly lies and cheats and steals things and does not go hunting and, when the other men are out of the village, takes sexual advantage of many women-someone who does not pay attention to reprimands and who is always being brought to the elders for punishment. One Eskimo among the 499 on their island was called <em>kunlangeta</em>. When asked what would have happened to such a person traditionally, an Eskimo said that probably somebody would have pushed him off the ice when nobody else was looking. (p.&nbsp;1026)</p>\n</blockquote>\n</blockquote>\n<p>This is interesting since out of 500, the usual American base rates would predict not 1 but &gt;10 psychopaths. Is this all due to the tribal and closely knit nature of more aboriginal societies, or could Eskimo society really have been selecting against psychopaths while big modern societies give scope for their talents &amp; render them more evolutionarily fit? This may be unanswerable until the relevant genes are identified and samples of gene pools examined for the frequencies.</p>\n<blockquote>\n<p>Considering the primary facets and citing Church (1994), Benning and colleagues (2003) identified the following as the strongest associations between the MPQ and FFM that are relevant to psychopathy: FFM (low) Agreeableness with the MPQ Aggression and Alienation facets of NEM, FFM Neuroticism with the MPQ Stress Reaction Facet of NEM, and FFM Conscientiousness with both the MPQ Control component of Constraint and the achievement facet of PEM. Thus, the Antagonism end of Agreeableness includes elements of MPQ NEM having less to do with stress reactivity, and Conscientiousness includes achievement or agentic elements of MPQ PEM.</p>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2006-harris.pdf\">&ldquo;Treatment of Psychopathy: A Review of Empirical Findings&rdquo;</a>, Harris &amp; Rice 2006; from <em>Handbook of Psychopathy</em> 2005:</p>\n<blockquote>\n<p>The clinical literature has been quite pessimistic about the outcome of therapy for psychopaths. Hervey Cleckley, in his several editions of <a href=\"http://libgen.info/view.php?id=23550\"><em>The Mask of Sanity</em></a> (1941, 1982), described psychopaths as neither benefiting from treatment nor capable of forming the emotional bonds required for effective therapy. In contrast, some early studies claimed positive effects of psychotherapy (Beacher, 1962; Corsini, 1958; Rodgers, 1947; Rosow, 1955; Schmideberg, 1949; Showstack, 1956; Szurek, 1942; Thorne, 1959). However, all these were uncontrolled case reports. Reviewers before 1990 concluded, as had Cleckley, that there was no evidence for the efficacy of treatment with adult psychopaths (Hare, 1970; McCord, 1982).</p>\n<ul>\n<li>Hare, R. D. (1970). <em>Psychopathy: Theory and research</em>. New York: Wiley.</li>\n<li>McCord, J. (1982). Parental behavior in the cycle of aggression. Psychiatry, 51, 14-23</li>\n</ul>\n<p>Based on these, Rice, Harris, and Cormier (1992) evaluated an intensive therapeutic community for mentally disordered offenders thought to be especially suitable for psychopaths. It operated for over a decade in a maximum security psychiatric hospital and drew worldwide attention for its novelty. The program was described at length by Barker and colleagues&hellip;The results of a follow-up conducted an average of 10.5 years after completion of treatment showed that, compared to no program (in most cases, untreated offenders went to prison), treatment was associated with lower violent recidivism for non-psychopaths but <em>higher</em> violent recidivism for psychopaths. Psychopaths showed poorer adjustment in terms of problem behaviors while in the program, even though they were just as likely as nonpsychopaths to achieve positions of trust and early recommendations for release. Why did the therapeutic community program have such different effects on the two offender groups? We speculated that both the psychopaths and nonpsychopaths who participated in the program learned more about the feelings of others, taking others&rsquo; perspective, using emotional language, behaving in socially skilled ways, and delaying gratification.</p>\n<ul>\n<li>Rice, M. E., Harris, G. T., &amp; Cormier, C. (1992). A follow-up of rapists assessed in a maximum security psychiatric facility. Journal of Interpersonal Violence, 5, 435-448</li>\n</ul>\n<p>In another therapeutic community, Ogloff, Wong, and Greenwood (1990) reported on the behavior of psychopaths and nonpsychopaths defined by criteria outlined in an early version of the Psychopathy Checklist (Hare &amp; Frazelle, 1985). Compared to nonpsychopaths, psychopaths showed less motivation, were discharged earlier (usually because of lack of motivation or security concerns), and showed less improvement. Similar results were reported for a therapeutic community in England&rsquo;s Grendon prison in (Hobson, Shine, &amp; Roberts, 2000), where poor adjustment to the program was likewise associated with higher PCL-R scores. A recent study of a therapeutic community for female substance abusers (Richards, Casey, &amp; Lucente, 2003) reported that, although none of the offenders scored over 30 on the PCL-R, higher psychopathy scores were nevertheless associated with poorer treatment response indicated by failing to remain in the program, rule violations, avoiding urine tests, and sporadic attendance.</p>\n<ul>\n<li>Ogloff, J., Wong, S., &amp; Greenwood, A. (1990). Treating criminal psychopaths in a therapeutic community program. Behavioral Sciences and the Law, 8, 81-90</li>\n<li>Hobson, J., Shine, J., &amp; Roberts, R. (2000). How do psychopaths behave in a prison therapeutic community? Psychology, Crime and Law, 6, 139-154</li>\n<li>Richards, H. J., Casey, J. O., &amp; Lucente, S. W. (2003). Psychopathy and treatment response in incarcerated female substance abusers. Criminal Justice and Behavior, 30, 251-276</li>\n</ul>\n<p>Besides therapeutic communities, cognitive-behavioral therapy is often recommended for psychopathic offenders. Andrews and Bonta (1994), Brown and Gutsch (1985), Serin and Kurychik (1994), and Wong and Hare (2005) all suggested that intensive cognitive-behavioral programs targeting &ldquo;criminogenic needs&rdquo; (i.e., personal characteristics correlated with recidivism) might be effective. For example, Wong and Hare recommended relapse prevention in combination with cognitive-behavioral programs. However, doubts as to the efficacy of this treatment with psychopaths arose from an evaluation of a cognitive-behavioral and relapse prevention program for sex offenders conducted by Seto and Barbaree (1999). High psychopathy offenders who were rated as having shown the most improvement (as measured by conduct during the treatment sessions, quality of homework, and therapists&rsquo; ratings of motivation and change) were more likely to reoffend than other participants, particularly in violent ways&hellip;It was highly structured and cognitive-behavioral, best matching the learning style of most offenders, including psychopaths. Moreover, psychopaths are high-risk offenders with many criminogenic needs (Zinger &amp; Forth, 1998), and thus the program targeted deviant sexual preferences and antisocial attitudes (Barbaree, Peacock, Cortini, Marshall, &amp; Seto, 1998). In view of these features, the results pertaining to psychopaths are especially notable.</p>\n<ul>\n<li>Andrews, D. A., Zinger, I., Hoge, R. D., Bonta, J., Gendreau, P., &amp; Cullen, F. T. (1990). Does correctional treatment work? A clinically relevant and psychologically informed meta-analysis. Criminology, 28, 369-404</li>\n<li>Brown, H. J., &amp; Gutsch, K. U. (1985). Cognitions associated with a delay of gratification task: A study with psychopaths and normal prisoners. Criminal Justice and Behavior, 12, 453-462</li>\n<li>Serin, R. C., &amp; Kuriychuk, M. (1994). Social and cognitive processing deficits in violent offenders: Implications for treatment. International Journal of Law and Psychiatry, 17, 431-441</li>\n<li>Wong, S., &amp; Hare, R. D. (2005). <em>Guidelines for a psychopathy treatment program</em>. Toronto, ON, Canada: Multi-Health Systems</li>\n<li>Seto, M. C., &amp; Barbaree, H. (1999). Psychopathy, treatment behavior, and sex offender recidivism. Journal of Interpersonal Violence, 14, 1235-1248</li>\n<li>Zinger, I., &amp; Forth, A. E. (1998). Psychopathy and Canadian criminal proceedings: The potential for human rights abuses. Canadian Journal of Criminology, 40, 237-276.</li>\n<li>Barbaree, H. E., Peacock, E. J., Cortini, F., Marshall, W. L., &amp; Seto, M. (1998). Ontario penitentiaries&rsquo; program. In W. L. Marshall, Y. M. Fernandez, S. M. Hudson, &amp; T. Ward (Eds.), <em>Sourcebook of treatment programs for sexual offenders</em>. New York: Plenum Press</li>\n</ul>\n<p>In another study, Hare, Clark, Grann, and Thornton (2000) evaluated cognitive-behavioral prison programs for psychopathic and non-psychopathic offenders. After short-term anger management and social skills training, 24-month reconviction rates for 278 treated and untreated offenders yielded an interaction between psychopathy and treatment outcome similar to that reported by Rice and colleagues (1992). Whereas the program had no demonstrable effect on non-psychopaths, treated offenders who scored high on Factor 1 of the PCL-R had significantly higher rates of recidivism than high-scoring but untreated offenders.</p>\n<ul>\n<li>Hare, R. D., Clark, D., Grann, M., &amp; Thornton, D. (2000). Psychopathy and the predictive validity of the PCL-R: An international perspective. Behavioral Sciences and the Law, 18, 623-645</li>\n</ul>\n<p>How can we summarize these &ldquo;controlled&rdquo; studies of treatment outcome? We note that only one study (Rice et al., 1992) used the PCL-R, which is the contemporary standard (and most empirically valid) measure of psychopathy. Only two employed objective measures of criminal recidivism (Craft et al., 1964; Rice et al., 1992). Interestingly, our interpretation of both of these is that the treated group exhibited higher rates of recidivism than the control group. Our reading of the &ldquo;controlled&rdquo; studies in the Salekin meta-analysis is that there is absolutely no basis for optimism regarding treatment to reduce the risk of criminal or violent recidivism. Other problematic aspects of the meta-analysis cast further doubt on the author&rsquo;s optimistic conclusion. As mentioned earlier, most studies in the meta-analysis relied on therapists&rsquo; ratings to measure outcome [!]. We consider this inadequate, especially for psychopaths. Note that Seto and Barbaree (1999) examined the recidivism of sex offenders as a function of psychopathy and progress in treatment, with progress assessed via eight structured therapist ratings. Based on these ratings, which showed good interrater agreement and were undoubtedly more reliable than unstructured impressions of therapeutic progress, those offenders with better than average progress were more likely to recidivate violently, and this was especially true for psychopaths. In our opinion, therapists&rsquo; impressions of clinical progress cannot be defended as an index of treatment effectiveness for offenders, especially psychopaths. Independently measured criminal conduct must be at least part of the outcome for an evaluation of treatment for psychopaths. This requirement eliminates all but a handful of the studies in the Salekin meta-analysis.</p>\n</blockquote>\n<p>The later <em>Handbook</em> paper, &ldquo;Risk for Criminal Recidivism: The Role of Psychopathy&rdquo; (Douglas et al), also has useful critical comments on meta-analyses including the Salekin meta-analysis.</p>\n<p>Conclusion:</p>\n<blockquote>\n<p>We believe there is no evidence that any treatments yet applied to psychopaths have been shown to be effective in reducing violence or crime. In fact, some treatments that are effective for other offenders are actually harmful for psychopaths in that they appear to promote recidivism. We believe that the reason for these findings is that psychopaths are fundamentally different from other offenders and that there is nothing &ldquo;wrong&rdquo; with them in the manner of a deficit or impairment that therapy can &ldquo;fix.&rdquo; Instead, they exhibit an evolutionarily viable life strategy that involves lying, cheating, and manipulating others.</p>\n</blockquote>\n<p>The evolutionary hypothesis of psychopathy is striking (eg. it&rsquo;s partially hereditable; or, sex offenders who target post-pubertal women have the highest PCL-R scores compared to any other subdivision of sex offenders), but not immediately relevant. It&rsquo;s discussed a little skeptically in the chapter &ldquo;Theoretical and Empirical Foundations&rdquo; in the <em>Handbook</em>.</p>\n<p>&ldquo;Psychopathy and Personality&rdquo;, Lynam &amp; Derefinko, <em>Handbook</em>:</p>\n<blockquote>\n<p>In summary, effect sizes for N came from FFM N, Eysenck&rsquo;s N, and MPQ stress reaction. Effect sizes for E came from FFM E, Eysenck&rsquo;s E, and the average of MPQ well-being and social closeness. Effect sizes for A came from FFM A, Psychoticism (reversed), and the average of MPQ aggression (reversed) and social potency (reversed). Effect sizes for C came from FFM C, Eysenck&rsquo;s P (reversed), and Constraint. Results appear in Table 7.2. The weighted effect size for E is significantly different from zero but minuscule. N bears a small, positive relation to psychopathy with a weighted effect size of .14 and a 95% confidence interval ranging from .11 to .17. The effect for C is moderate to large and negative with weighted effect size of -.36 and a 95% confidence interval of -.38 to -.33. Finally, the relation between A and psychopathy is large and negative with a weighted average effect size of -.47 and a 95% confidence interval ranging from -.49 to -.44.</p>\n<p>Based on these descriptions, the psychopathic individual is interpersonally antagonistic (low A). At the facet level, he is suspicious (low in trust), deceptive (low in straightforwardness), exploitive, aggressive, arrogant, and tough-minded. This individual has trouble controlling his impulses and endorses nontraditional values and standards (low C). Running somewhat counter to Cleckley&rsquo;s original description is a tendency for the psychopathic individual to experience negative emotions (e.g., anger and cravingsrelated distress), although this relation is weaker than the relations to A and C. There is little evidence that the psychopathic individual is high or low in Extraversion.</p>\n<p>&hellip;Less consistent were the results for Neuroticism (N) and Extraversion (E), perhaps due to facets of these dimensions relating differentially to psychopathy. For example, expert ratings and the PCL-R translation both suggest that the psychopathic individual can be described as high in some elements of N (i.e., angry hostility and impulsiveness/urgency) but low in others (i.e., self-consciousness). These distinctions may get lost when one moves to the domain or higher-order factor level where N demonstrates a small, positive correlation with psychopathy in the meta-analyses. The case may be similar for E. Expert raters and the PCL-R translation agree that psychopathic individuals are low in some elements of E (i.e., warmth and positive emotions) but high in others (i.e., excitement seeking). Again, these distinctions are lost at the domain level where E demonstrates a small, negative relation with psychopathy.</p>\n</blockquote>\n<p>&ldquo;Psychopathy and DSM-IV Psychopathology&rdquo;, <em>Handbook</em>:</p>\n<blockquote>\n<p>The relationship of psychopathy to anxiety disorders has been controversial (Frick, Lilienfeld, Elllis, Loney, &amp; Silverthorn, 1999; Schmitt &amp; Newman, 1999). Cleckley (1941) included within his original criteria for psychopathy an &ldquo;absence of &lsquo;nervousness&rsquo; or psychoneurotic manifestations&rdquo; (p.&nbsp;206). Rather than be troubled by the presence of anxiety disorders it was suggested that &ldquo;it is highly typical for [psychopaths] not only to escape the abnormal anxiety and tension . . . but also to show a relative immunity from such anxiety and worry as might be judged normal or appropriate&rdquo; (Cleckley, 1941, p.&nbsp;206). Miller and colleagues (2001) surveyed 15 psychopathy researchers, asking them to describe the prototypic psychopath in terms of the domains and facets of the FFM description of general personality functioning. Their description included very low levels of anxiousness, inconsistent with the PCL-R assessment of psychopathy but consistent with the earlier description of this disorder by Cleckley (1941). In stark contrast, it is stated in DSM-IV that &ldquo;individuals with this disorder [APD] may also experience dysphoria, including complaints of tension, inability to tolerate boredom, and depressed mood&rdquo; (American Psychiatric Association, 2000, p.&nbsp;702). It is noted more specifically that &ldquo;they may have associated anxiety disorders [and] depressive disorders&rdquo; (American Psychiatric Association, 2000, p.&nbsp;702). The suggestion in DSM-IV that APD is associated with anxiety disorders can be attributed in part to the confinement of many of the APD studies to clinical populations (Lilienfeld, 1994). Anxiousness is common among persons in treatment for mental disorders.</p>\n</blockquote>\n<p>&ldquo;Neuroanatomical Bases of Psychopathy&rdquo;, <em>Handbook</em>; summary:</p>\n<blockquote>\n<p>Initial, preliminary structural imaging research on psychopathic groups has so far indicated (1) enlargement of the corpus callosum (Raine, Lencz, et al., 2003), (2) volume reduction in the posterior hippocampus (Laakso et al., 2001), (3) an exaggerated right &gt; left asymmetry to the anterior hippocampus (Raine et al., 2004), and (4) reduced prefrontal gray volume (Yang, Raine, Lencz, LaCasse, &amp; Colletti, 2005). Nevertheless, these latter two findings are specific to &ldquo;unsuccessful&rdquo; psychopaths and are not found in &ldquo;successful&rdquo; psychopaths&hellip;.From a theoretical standpoint, anatomical prefrontal impairments in psychopathic and antisocial populations could help explain the disinhibited, impulsive behavior of psychopaths and underpin the classic low arousal/fearlessness/conditioning theories of psychopathic behavior. Callosal structural abnormalities give rise to a &ldquo;faulty wiring&rdquo; hypothesis of psychopathy and in part account for social, autonomic, and emotional impairments observed in psychopaths. Hippocampal impairments may predispose to affect dysregulation and poor contextual fear conditioning in psychopaths, in part by disruption to prefrontal-hippocampal circuits. In this context, while research on single brain structures provides a starting point for understanding the neuroanatomical basis of psychopathy, future research needs to better understand impairments to more specific neural circuits which give rise to biobehavioral abnormalities which result in specific psychopathic symptoms. As such, the neuroanatomy of psychopathy is a research field in its infancy</p>\n</blockquote>\n<p>&ldquo;Understanding Psychopathy: The Cognitive Side&rdquo;</p>\n<blockquote>\n<p>Psychopathy has not traditionally been associated with cognitive dysfunction, at least with regard to intelligence, memory, and executive ability (e.g., Cleckley, 1982). Indeed, psychopaths are notorious for the contrast between their good explicit knowledge and their profound failures when put to the test of daily life. However, it is possible that the assumption of intact cognitive ability is based on an overly simplified model of cognitive and executive functions.</p>\n<p>Thus, both behavioral and physiological investigations of psychopaths&rsquo; attentional functioning are largely consistent with rigid task-focused attention that is poorly modulated by secondary or contextual information. In addition, these studies suggest that psychopaths&rsquo; attentional insensitivity to secondary or contextual information may be exacerbated by left-hemisphere activation. The possible contribution of left-hemisphere activation suggests a potential refinement of psychopaths&rsquo; difficulty accommodating unattended contextual information. We return to this possibility in the discussion section.</p>\n<p>Attempts to replicate Lykken&rsquo;s (1957) finding have produced mixed results and have revealed that psychopaths&rsquo; passive avoidance deficits are context dependent. Schmauk (1970) used a modified version of Lykken&rsquo;s task, and examined passive avoidance under conditions involving verbal punishment (&ldquo;wrong&rdquo;), tangible punishment (loss of 25 cents), or physical punishment (electric shocks). In the conditions involving verbal and physical punishment, low-anxious psychopaths displayed smaller skin conductance responses, poorer passive avoidance learning, and less awareness of the punishment contingencies than did nonincarcerated controls. However, the groups did not differ on any of these measures when the punishment contingency involved loss of money. Later studies by other researchers have found passive avoidance deficits even with loss of money (Newman &amp; Kosson, 1986; Newman, Patterson, &amp; Kosson, 1987; Siegel, 1978). Newman and colleagues (1990) argued that, unlike Schmauk (1970), each of the studies demonstrating poor passive avoidance under conditions of monetary loss also involved a competing reward contingency. They argued that a competing reward contingency is an important component of psychopaths&rsquo; poor passive avoidance.</p>\n<p>..Newman and Kosson (1986) presented participants with two versions of a passive avoidance task&hellip;Psychopaths made more passive avoidance (commission) errors than controls in the version involving competing reward and punishment contingencies but performed comparably to controls in the punishment-only condition. Newman and Kosson concluded that psychopaths show poor passive avoidance only in the presence of competing reward contingencies&hellip;Arnett, Smith, and Newman (1997) provided further evidence that psychopaths show normal avoidance of explicit punishment contingencies. &hellip;As with psychopaths&rsquo; attentional and language-processing abnormalities, psychopaths&rsquo; behavioral inhibition deficits reveal evidence of difficulty using information that occurs outside the primary focus of attention. Thus, poor accommodation of secondary or incidental information appears to be a consistent feature of psychopaths&rsquo; cognitive functioning. Although psychopaths&rsquo; hemispheric processing asymmetries have not been investigated within the domain of behavioral inhibition, it is worth noting that reward-seeking behaviors may differentially activate the left hemisphere (e.g., Davidson 1995; Miller &amp; Tomarken, 2001; Sobotka, Davidson, &amp; Senulis, 1992). The exacerbation of psychopaths&rsquo; disinhibition in the presence of a reward-seeking response set may therefore be consistent with Kosson&rsquo;s (1996, 1998) findings of attentional dysfunction under left-hemisphere activating conditions.</p>\n<p>The literature on psychopaths&rsquo; cognitive functioning is extensive and reveals a variety of consistent and compelling deficits &hellip;It is intriguing that psychopaths&rsquo; cognitive deficits do not fit established models of cognitive dysfunction, such as executive deficits or difficulty with sustained attention. Psychopaths appear to have adequate cognitive resources and capacity but difficulty maintaining an adaptive balance between top-down and bottom-up processing&hellip;Psychopaths&rsquo; deficits also indicate that context-specific failures in the appropriate, adaptive allocation of available resources can contribute to profound failures of self-regulation, despite the absence of traditional cognitive or executive deficits.</p>\n</blockquote>\n<p>[Irrational, or just higher valuing of rewards/lower fearing of injury?]</p>\n<p>&ldquo;The&rdquo;Successful\" Psychopath: Adaptive and Subclinical Manifestations of Psychopathy in the General Population\", Hall &amp; Benning, <em>Handbook</em></p>\n<blockquote>\n<p>Consistent with prior research, unsuccessful psychopaths demonstrated reduced cardiovascular reactivity during a social stressor task, relative to controls. The nonconvicted psychopaths, however, exhibited a pattern of increased cardiovascular response during the stressor, which consisted of giving a brief speech about one&rsquo;s faults. Relative to both comparison groups, these psychopaths also demonstrated a higher level of executive functioning, as measured by performance on the Wisconsin Card Sort Test (WCST; Heaton, Chelune, Talley, Kay, &amp; Kurtis, 1993). Ishikawa and colleagues speculated that heightened autonomic reactivity to stress and relatively higher levels of executive functioning might act as protective factors for &ldquo;successful&rdquo; psychopaths, enabling them to avoid the riskiest of criminal activities that might result in arrest, conviction, and incarceration.</p>\n<p>&hellip;To summarize, laboratory studies of psychopaths recruited from the community have demonstrated that these individuals tend to have higher arrest rates than the norm, but a slightly reduced rate of conviction relative to incarcerated psychopaths; they are psychometrically similar to incarcerated psychopaths in terms of self-reported personality (e.g., empathy, impulsivity, socialization, and MMPI profiles); and, like incarcerated psychopaths, they tend to demonstrate poor response modulation, careless motor behavior, and abnormal affective modulation of startle. Taken together, these findings point most notably to ways in which nonincarcerated psychopaths are phenotypically (and perhaps etiologically) similar to their incarcerated counterparts. Furthermore, these data suggest that at least a subset of nonincarcerated psychopaths manifest psychopathy at a reduced or subclinical level, insofar as their continued presence in the community is indicative of reduced severity of the process underlying their antisocial deviance. &hellip;Widom (1977) also noted that psychopaths from the community tend to come from higher socioeconomic backgrounds than incarcerated psychopaths.</p>\n</blockquote>\n<p>&ldquo;Psychopathy and Aggression&rdquo;, Porter &amp; Woodworth; <em>Handbook</em></p>\n<blockquote>\n<p>For example, in the Williamson and colleagues (1987) study, the majority of violent acts by psychopaths in the sample were not instrumental. This supports the idea that poor behavioral controls or impulsivity in psychopaths contributes to their violence (also see Dempster et al., 1996). Overall, these data established that psychopaths engage in both major forms of aggression, whereas violent nonpsychopaths are unlikely to engage in instrumental violence.</p>\n<p>&hellip;A potential moderator of the relationship between psychopathy and violence is intelligence. That is, more intelligent psychopaths may be less inclined to use aggression because they can they can use their cognitive resources to devise nonviolent means (such as conning and manipulation) to get what they want. Less intelligent psychopaths may resort to violence to compensate for their inferior abilities to manipulate others through language. Heilbrun (1982) found that past violent offending in a sample of 168 male inmates was influenced by the interaction of intellectual level and psychopathy. Less intelligent psychopaths were more likely to have a history of impulsive violence than more intelligent psychopaths (and than less intelligent nonpsychopaths). Heilbrun (1985) reported that the most dangerous offenders in a sample of 225 offenders were those with the following characteristics: psychopathic, low IQ, social withdrawal, and history of violence.</p>\n<p>While these early studies offered some evidence for intelligence as a moderator of psychopathy and violence, little research has addressed the issue in recent years, largely due to methodological obstacles. Specifically, the most intelligent psychopaths in society may succeed in corporate or political circles and/or use violence less frequently and thus may be less likely to wind up in prison. As such, they would be less likely to be studied by psychological researchers, whereas less intelligent psychopaths are available in disproportionate numbers for research.</p>\n<p>Another potential issue in this area is that psychopaths with higher cognitive functioning may be as likely to commit violence as other psychopaths but be much less likely to be apprehended for such acts. Ishikawa and colleagues (2001) tested a community sample of 16 &ldquo;unsuccessful&rdquo; and 13 &ldquo;successful&rdquo; psychopaths (classified based on their PCL-R scores and whether they had received criminal convictions) on measures of autonomic stress reactivity and executive functioning (referring to the capacity for initiation, planning, abstraction, decision making). The two groups had engaged in a substantial and similar amount of self-reported criminal behavior, including violence. The results indicated that the successful psychopaths exhibited greater autonomic reactivity to emotional stressors and stronger executive functioning than unsuccessful psychopaths. This suggested that psychopaths who are less likely to be caught and convicted for their violent acts have the capacity for better planning and decision making than their unsuccessful counterparts.</p>\n</blockquote>\n<p>&ldquo;Toward the Future: Translating Basic Research into Prevention and Treatment Strategies&rdquo;, Seto &amp; Quinsey:</p>\n<blockquote>\n<p>Meta-analytic studies demonstrating relatively few or no differences in the success of various psychotherapeutic approaches have stimulated research on the nonspecific factors that affect treatment outcome. There is good evidence that aspects of the therapeutic alliance are particularly important, with a recent meta-analysis of 79 studies finding a reliable, moderate relationship between measures of therapeutic alliance and outcome (Martin, Garske, &amp; Davis, 2000). According to Martin and colleagues, the common elements across different definitions of therapeutic alliance are the collaborative nature of the therapeutic relationship, the affective bond between therapist and client, and the therapist&rsquo;s and client&rsquo;s agreement on treatment goals and tasks. Developing a therapeutic alliance with psychopathic clients could be quite challenging because of their defining characteristics and because of therapists&rsquo; reaction to noncompliance; disruptive behavior; the nature of psychopaths&rsquo; offenses; and concerns about possible exploitation, manipulation, and deception. One could imagine that there is a great deal of potential for therapist mistrust, suspicion, and more confrontational or hostile interactions with psychopathic clients (these therapist behaviors are sometimes referred to as countertransference in the clinical literature). Consistent with this hypothesis, Taft, Murphy, Musser, and Remington (2004) found that self-reported psychopathic characteristics were significantly and negatively associated with therapeutic alliance in a sample of men in treatment for partner abuse. Psychopathic characteristics were also negatively associated with motivation for change, and motivation for change mediated the relationship between psychopathic characteristics and therapeutic alliance.</p>\n<p>Furthermore, although psychopaths have been described as affectively impoverished- for example, being less responsive to distress cues than nonpsychopaths (Blair, Jones, Clark, &amp; Smith, 1997)-they do not appear to have deficits in the recognition of emotional states in others. Book, Quinsey, Cooper, and Langford (2004) studied the relationship between psychopathy and accuracy in perceiving the emotional meaning of facial expressions and body language in a sample of 59 male prison inmates and 60 men recruited from the community. Psychopathy was measured by the Self-Report Psychopathy Scale (Levenson, Kiehl, &amp; Fitzpatrick, 1995) for all participants, and by the PCL-R for the inmates. The inmates&rsquo; PCL-R scores were not correlated with the number of errors in categorizing posed facial expressions and were positively but not significantly correlated with the inmates&rsquo; accuracy in rating emotional intensity of posed facial photographs. All participants rated the assertiveness of confederates from a brief, spontaneous videotaped social interaction between the confederate and one of the confederate&rsquo;s friends. The Self-Report Psychopathy Scale was positively correlated with the accuracy of participants&rsquo; ratings of the friend&rsquo;s level of assertiveness, as measured by both the confederate&rsquo;s rating and the friend&rsquo;s self-rating. In a companion study involving a subset of the same sample, Book, Quinsey, and Langford (2004) examined the relationship between psychopathy and the accuracy of posed facial expressions of emotion. Thirtyone inmates and 50 community volunteers agreed to be videotaped while attempting to mimic prototypical facial expressions (happy, sad, fearful, disgusted, and angry). PCL-R scores were positively associated with increased intensity of fear in the posed fearful faces, as measured by Ekman and Friesen&rsquo;s (1978) Facial Action Coding System. Undergraduate students gave higher believability and intensity ratings to fearful faces posed by participants who had higher scores on the Primary Psychopathy subscale of the Self-Report Psychopathy Scale. A similar trend was observed for Factor 1 of the PCL-R. Taken together with other research showing that psychopathy is associated with deceptiveness (Seto, Khattar, Lalumi&egrave;re, &amp; Quinsey, 1997), lack of response to distress cues (Blair et al., 1997), and an adequate theory of mind (Richell et al., 2003), these results indicate that psychopaths lack feelings for others but do understand their mental states; in other words, they know but they do not care. This does not seem to be much of a deficit if part of a socially manipulative and exploitative life history strategy.</p>\n<p>&hellip;Although schizophrenia is a genetically caused brain disease, the most effective treatment discovered to date for its most severe manifestations is a rigorously implemented and very carefully planned behavioral program (Paul &amp; Lentz, 1977). The thoroughness and integrity of implementation of this program seem to be the keys to its success. The implications for treatments of psychopathic offenders are clear (see also Harris &amp; Rice, Chapter 28, this volume). Interventions to reduce recidivism among psychopathic offenders will need to be provided on an ongoing basis, although the intensity of service may vary over time with changing circumstances. These interventions will likely involve high staff-to-client ratios in order to provide sufficient supervision, to protect therapists from being deceived or manipulated, and to help them refrain from negative reactions to psychopaths that might interfere with intervention efficacy. Moreover, the interventions will focus on shaping behavior in desired directions, rather than more abstract concepts such as responsibility, empathy, and relapse prevention, with substantial attention devoted to program fidelity and a reliance on measures other than self-report. Given the evidence for psychopaths&rsquo; dominant response styles and differing response thresholds, increasing the salience and consistency of punishments would be important elements in these interventions. Other important intervention targets would include increasing delay of gratification and compliance with program rules and reducing aggression and associations with antisocial peers.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"v4pviL33XGMuTpSNs": 2, "3uE2pXvbcnS9nnZRE": 2, "dBPou4ihoQNY4cquv": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ktr39MFWpTqmzuKxQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 42, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "20734", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ft2Cm9tWtcLNFLrMw", "PtoQdG7E8MxYJrigu", "2jfSi6HhTqWDKLQwm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T04:43:35.222Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, DC Meetup with Special Guest", "slug": "meetup-washington-dc-meetup-with-special-guest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LDAbPMttEYGDpCGGb/meetup-washington-dc-meetup-with-special-guest", "pageUrlRelative": "/posts/LDAbPMttEYGDpCGGb/meetup-washington-dc-meetup-with-special-guest", "linkUrl": "https://www.lesswrong.com/posts/LDAbPMttEYGDpCGGb/meetup-washington-dc-meetup-with-special-guest", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20DC%20Meetup%20with%20Special%20Guest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20DC%20Meetup%20with%20Special%20Guest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDAbPMttEYGDpCGGb%2Fmeetup-washington-dc-meetup-with-special-guest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20DC%20Meetup%20with%20Special%20Guest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDAbPMttEYGDpCGGb%2Fmeetup-washington-dc-meetup-with-special-guest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDAbPMttEYGDpCGGb%2Fmeetup-washington-dc-meetup-with-special-guest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h8'>Washington, DC Meetup with Special Guest</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 January 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After some of the DC group attended a talk by Robin Hanson, I asked him to come visit a meetup! And even better, he agreed!</p>\n\n<p>Those who didn't come to the talk can hear more about the idea of a future em economy, and what it would likely entail. (Or other ideas about what the future might hold?)</p>\n\n<p>We may also discuss issues of interest to contemporary economics, including prediction markets.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h8'>Washington, DC Meetup with Special Guest</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LDAbPMttEYGDpCGGb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0619298582947684e-06, "legacy": true, "legacyId": "20736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__DC_Meetup_with_Special_Guest\">Discussion article for the meetup : <a href=\"/meetups/h8\">Washington, DC Meetup with Special Guest</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 January 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After some of the DC group attended a talk by Robin Hanson, I asked him to come visit a meetup! And even better, he agreed!</p>\n\n<p>Those who didn't come to the talk can hear more about the idea of a future em economy, and what it would likely entail. (Or other ideas about what the future might hold?)</p>\n\n<p>We may also discuss issues of interest to contemporary economics, including prediction markets.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__DC_Meetup_with_Special_Guest1\">Discussion article for the meetup : <a href=\"/meetups/h8\">Washington, DC Meetup with Special Guest</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, DC Meetup with Special Guest", "anchor": "Discussion_article_for_the_meetup___Washington__DC_Meetup_with_Special_Guest", "level": 1}, {"title": "Discussion article for the meetup : Washington, DC Meetup with Special Guest", "anchor": "Discussion_article_for_the_meetup___Washington__DC_Meetup_with_Special_Guest1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T08:40:47.861Z", "modifiedAt": null, "url": null, "title": "LessWrong Survey Results: Do Ethical Theories Affect Behavior?", "slug": "lesswrong-survey-results-do-ethical-theories-affect-behavior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:37.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3PTNRJwaT9Nkekwui/lesswrong-survey-results-do-ethical-theories-affect-behavior", "pageUrlRelative": "/posts/3PTNRJwaT9Nkekwui/lesswrong-survey-results-do-ethical-theories-affect-behavior", "linkUrl": "https://www.lesswrong.com/posts/3PTNRJwaT9Nkekwui/lesswrong-survey-results-do-ethical-theories-affect-behavior", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Survey%20Results%3A%20Do%20Ethical%20Theories%20Affect%20Behavior%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Survey%20Results%3A%20Do%20Ethical%20Theories%20Affect%20Behavior%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PTNRJwaT9Nkekwui%2Flesswrong-survey-results-do-ethical-theories-affect-behavior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Survey%20Results%3A%20Do%20Ethical%20Theories%20Affect%20Behavior%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PTNRJwaT9Nkekwui%2Flesswrong-survey-results-do-ethical-theories-affect-behavior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PTNRJwaT9Nkekwui%2Flesswrong-survey-results-do-ethical-theories-affect-behavior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1560, "htmlBody": "<p>Awhile ago, Yvain released&nbsp;<a href=\"/lw/fp5/2012_survey_results/\">the results of the 2012 survey</a>&nbsp;(that I participated in), which contained responses from over 1000 readers. So I uploaded the data into STATA and fooled around looking for cool things.</p>\n<p>Mainly, I've heard some people argue that the moral theory you hold has little to no impact on your actual day-to-day behavior. I want to use these survey results to see if this is true -- are consequentialists more likely to donate money than deontologists? Are virtue ethics more likely to be vegetarian? We'll see!</p>\n<p>While I can't make any claims to the representativeness of this survey or the external validity of drawing conclusions from its results, at least among the people who did take the survey, <strong>ethical theories people endorse seem to have little impact on their actual self-reported behavior.</strong></p>\n<p>&nbsp;</p>\n<h4>The Ethical Theories</h4>\n<p>First, a breakdown of the actual ethical theories people endorse. Respondents were asked to categorize themselves into \"accept / lean towards consequentialism\", \"accept / lean towards deontology\", \"accept / lean towards virtue ethics\", or \"other / no answer\" (N = 1055):</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>Consequentialism</td>\n<td>63.41%</td>\n</tr>\n<tr>\n<td>Deontology</td>\n<td>3.98%</td>\n</tr>\n<tr>\n<td>Virtue Ethics</td>\n<td>14.41%</td>\n</tr>\n<tr>\n<td>Other / No Answer</td>\n<td>18.20%</td>\n</tr>\n</tbody>\n</table>\n<p>(Note that \"no answer\" are the people who specifically chose to note they had no answer. Other people ignored the question and didn't choose any answer at all, <em>truly</em> having no answer. Those people are not included in this analysis.)</p>\n<p>&nbsp;</p>\n<h4>Ethical Theories and Donation</h4>\n<p>So first up, I want to see if your ethical theory predicts how much money you are willing to donate, if any. Given the famous connection between utilitarianism and arguments like Peter Singer's who suggest you should donate all your money until it really hurts (you give so much money, you yourself become as poor as the people you're trying to help). Certainly consequentialism is not utilitarianism, but I would expect most consequentialists would endorse donation more than deontologists or virtue ethicists where donations aren't as mandatory.</p>\n<p>So I took the charity data and dropped all the non-numerical answers, and LessWrongers have donated an average of $445.15 (N = 879, SD = 1167.095, min = 0, max = 9000) to charity over the past year. To get a better proxy for \"effective charity\", LessWrongers donate $331.05 (N = 884, SD = 4087.303, min = 0, max = 110000) on average to SIAI and CFAR. (I don't know why the max is higher on the SIAI/CFAR question, but not the inclusive charity question...)</p>\n<p>Breaking down generic donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>$479.41</td>\n<td>0.266</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>$333.89</td>\n<td>0.561</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>$433.25</td>\n<td>0.887</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>$358.54</td>\n<td>0.323</td>\n</tr>\n</tbody>\n</table>\n<p>Breaking down SIAI/CFAR donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>$420.47</td>\n<td>0.391</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>$0.00</td>\n<td>0.629</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>$86.85</td>\n<td>0.459</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>$288.44</td>\n<td>0.886</td>\n</tr>\n</tbody>\n</table>\n<p>What we're seeing is that there are clear differences in the mean amount of money donated, with consequentialists giving the most. However, when we do t-tests (group compared to all not in the group), we find that none of these differences are statistically significant, which indicates the differences are probably due to chance.</p>\n<p>This would lead us to suspect that ethical theory has no influence on the amount of money donated...</p>\n<p>&nbsp;</p>\n<h4>Ethical Theories and Percent of Income Donated</h4>\n<p>However, I have one more trick in the bag -- these donations don't take into account the income people earn. Many LessWrongers are students, and therefore can't donate much, even if they wanted to. What if we adjusted the donation totals by income, and instead looked at percent of income donated?</p>\n<p>Overall, LessWrongers donate 1.75% of their income on average to generic charity (N = 523, SD = 5.70%, min = 0%, max = .88.24%) and 0.49% of their income to SIAI/CFAR (N = 523, SD = 3.11%, min = 0%, max = 52.38%).</p>\n<p>(For those keeping score at home, the average income was $49563.76, N = 602, SD = $59358.34, min = $0, max = $700000.)</p>\n<p>Breaking down percent of income spent on generic donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>2.09%</td>\n<td>0.069</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>1.09%</td>\n<td>0.568</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>1.36%</td>\n<td>0.490</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.91%</td>\n<td>0.161</td>\n</tr>\n</tbody>\n</table>\n<p>Breaking down percent of income spent on SIAI/CFAR donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>0.60%</td>\n<td>0.261</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>0.00%</td>\n<td>0.454</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>0.15%</td>\n<td>0.289</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.49%</td>\n<td>0.993</td>\n</tr>\n</tbody>\n</table>\n<p>A bit more can be made out of these results -- specifically, there's weak evidence that consequentialists actually donate more of their income (M = 2.09%) than non-consequentialists (M = 1.14%) with a p-value of 0.069, which is fairly significant. However, there are no differences across any other ethical theories or across SIAI/CFAR donations.</p>\n<p>(Though Unnamed concludes that <a href=\"/lw/g0b/lesswrong_survey_results_do_ethical_theories/82yc\">this might just be in-group bias</a>. &nbsp;Unnamed also does a <a href=\"/r/discussion/lw/g0b/lesswrong_survey_results_do_ethical_theories/836x\">more thorough analysis</a> and finds more correlations between consequentialism and donation.)</p>\n<p>&nbsp;</p>\n<h4>Vegetarianism</h4>\n<p>However, Peter Singer isn't just famous for consequentialist arguments for charity... he's also famous for consequentialist arguments for animal rights, which he argues necessitate veganism, or at least vegetarianism. Are consequentialists more likely to be vegetarian?</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Not a Vegetarian</strong></td>\n<td>Yes, Vegetarian</td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>84.39%</td>\n<td>15.61%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>83.78%</td>\n<td>16.22%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>88.97%</td>\n<td>11.03%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>90.00%</td>\n<td>10.00%</td>\n</tr>\n</tbody>\n</table>\n<p>Perhaps surprisingly, there is no statistically significant relationship (N = 958, chi2 = 4.73, p = 0.192). Your choice in ethical theory has no correlation with your choice to eat or not eat meat.</p>\n<p style=\"text-align: justify;\">(Though Unnamed <a href=\"/lw/g0b/lesswrong_survey_results_do_ethical_theories/833r\">finds somewhat contrary results</a>&nbsp;that are consistent with a connection -- consequentialists are more likely than non-consequentialists to be vegetarian [p=0.03], this effect holds up when looking at men<span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">&nbsp;[p&lt;0.01] but not for women, and as a single analysis, sex and consequentialism both predict vegetarianism [p=0.007 and p=0.02 respectively].)</span></span></p>\n<p>&nbsp;</p>\n<h4>Dust Specks</h4>\n<p>We can also take it into a more abstract realm of theory. Eliezer Yudkowsky in <a href=\"/lw/kn/torture_vs_dust_specks/\">\"Torture vs. Dust Specks\"</a> outlines a thought experiment:</p>\n<blockquote>[H]ere's the moral dilemma. If neither event is going to happen to you personally, but you still had to choose one or the other:</blockquote>\n<blockquote>Would you prefer that one person be horribly tortured for fifty years without hope or rest, or that 3^^^3 [an obnoxiously and unfathomably large number] people get dust specks in their eyes?</blockquote>\n<blockquote>I think the answer is obvious. How about you?</blockquote>\n<p>According to Yudkowsky, consequentialists (at least of the utilitarian variety) should choose torture over dust specks, since less total harm occurs. Does this turn to actually happen?</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Torture</td>\n<td>Dust Specks</td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>45.19%</td>\n<td>54.81%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>11.11%</td>\n<td>88.89%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>13.79%</td>\n<td>86.21%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>35.85%</td>\n<td>64.15%</td>\n</tr>\n</tbody>\n</table>\n<p>Here, we see another statistically significant relationship (N = 636, chi2 = 39.31, p &lt; 0.001), and it goes exactly as expected. &nbsp;People's ethical theories seem to influence their choice of theory in this scenario (or the other way around, or a third variable).</p>\n<p>&nbsp;</p>\n<h4>Politics</h4>\n<p>Going back into the practical realm, let's look at politics.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Communist</strong></td>\n<td><strong>Conservative</strong></td>\n<td><strong>Liberal</strong></td>\n<td><strong>Libertarian</strong></td>\n<td><strong>Socialist</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>0.30%</td>\n<td>1.82%</td>\n<td>41.03%</td>\n<td>29.64%</td>\n<td>27.20%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>2.38%</td>\n<td>16.67%</td>\n<td>21.43%</td>\n<td>26.19%</td>\n<td>33.33%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>1.99%</td>\n<td>5.30%</td>\n<td>34.44%</td>\n<td>28.48%</td>\n<td>29.80%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.00%</td>\n<td>2.69%</td>\n<td>31.18%</td>\n<td>34.41%</td>\n<td>31.72%</td>\n</tr>\n</tbody>\n</table>\n<p>Here, there is a statistically significant relationship (N = 1037, Chi2 = 50.91, p &lt; 0.001) between ethical theories and political beliefs -- the plurality of consequentialists and virtue ethicists are liberal, the plurality of deontologists are socialist, and the plurality of others or no answers are libertarians.</p>\n<p>&nbsp;</p>\n<h4>Religion</h4>\n<p>Continuing along a similar path, next let's look at religion:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Agnostic</strong></td>\n<td><strong>Atheist, nonpiritual</strong></td>\n<td><strong>Atheist, spiritual</strong></td>\n<td><strong>Theist, committed</strong></td>\n<td><strong>Deist/Pantheist</strong></td>\n<td><strong>Theist, lukewarm</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>5.71%</td>\n<td>81.35%</td>\n<td>8.72%</td>\n<td>1.35%</td>\n<td>1.35%</td>\n<td>1.50%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>11.90%</td>\n<td>54.76%</td>\n<td>7.14%</td>\n<td>11.90%</td>\n<td>7.14%</td>\n<td>7.14%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>12.50%</td>\n<td>61.18%</td>\n<td>13.16%</td>\n<td>5.26%</td>\n<td>3.95%</td>\n<td>3.95%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>10.00%</td>\n<td>75.79%</td>\n<td>6.84%</td>\n<td>3.16%</td>\n<td>1.05%</td>\n<td>3.16%</td>\n</tr>\n</tbody>\n</table>\n<p>Again, another statistically significant relationship (N = 1049, Chi2 = 64.74, p &lt; 0.001) between choice of ethical theory and religious beliefs -- consequentialists were more likely than deontologists and virtue ethics to not be religious.</p>\n<p>&nbsp;</p>\n<h4>Sequences and Meetups</h4>\n<p>And for a bit of bonus material, here's another interesting finding -- there is also a statistically significant relationship (N = 1052, chi2 = 128.43, p &lt; 0.001) between ethical theory endorsed and amount of sequences read. Whether sequences convince people to adopt more consequentialist theories, whether people with consequentialist theories are more likely to enjoy and therefore keep reading the sequences, or some hidden third variable at work, I cannot figure out with the current data.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>~1/4 of Sequences Read</strong></td>\n<td><strong>1/2 of Sequences</strong></td>\n<td><strong>3/4 of Sequences</strong></td>\n<td><strong>Never looked</strong></td>\n<td><strong>Never heard of 'em</strong></td>\n<td><strong>Nearly all</strong></td>\n<td><strong>Some, but &lt;1/4</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>11.83%</td>\n<td>14.82%</td>\n<td>21.71%</td>\n<td>0.60%</td>\n<td>31.44%</td>\n<td>4.49%</td>\n<td>15.12%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>14.29%</td>\n<td>7.14%</td>\n<td>9.52%</td>\n<td>4.76%</td>\n<td>19.05%</td>\n<td>11.90%</td>\n<td>33.33%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>11.26%</td>\n<td>12.58%</td>\n<td>10.60%</td>\n<td>1.99%</td>\n<td>15.23%</td>\n<td>20.53%</td>\n<td>27.81%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>14.66%</td>\n<td>14.14%</td>\n<td>10.99%</td>\n<td>6.28%</td>\n<td>16.75%</td>\n<td>11.52%</td>\n<td>25.65%</td>\n</tr>\n</tbody>\n</table>\n<p>And this trend continues among those who have been to a LessWrong meetup (N = 1044, chi2 = 34.27, p &lt; 0.001):</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Never Been to a Meetup</strong></td>\n<td><strong>Been to a Meetup</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>66.52%</td>\n<td>33.48%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>92.68%</td>\n<td>7.32%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>84.00%</td>\n<td>16.00%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>79.14%</td>\n<td>20.86%</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h4>Conclusion</h4>\n<p>In meta-ethics, there is a distinction between moral internalism, which is the theory that moral beliefs must be motivating, and moral externalism, which is the theory that you can have a moral belief and not be motivated to follow it. For instance, if moral externalism is true, you can legitimately think that eating meat is morally wrong but still eat meat.</p>\n<p>Now, moral interalism and externalism are more about the semantics of moral statements, what moral statements <em>refer</em> to, and less about actual behavior. Even if people claim that eating meat is morally wrong while still eating meat, it's easy enough for the internalist to deny that the meat eater was telling the truth or making a coherent statement.</p>\n<p>However, when it comes to the results of the LessWrong 2012 survey, there are very mixed results on whether choice of ethical theory influences actual behavior -- there's weak evidence that consequentialists are more likely to donate, and even those who do donate are certainly not giving up everything and becoming poor themselves, let alone giving 10% like <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a> advocates.</p>\n<p>Furthermore, there's evidence that vegetarianism does not depend upon ethical theory, but there is evidence that it influences people's choice of dust specks vs. torture in the right direction, at least for slightly less than half of the consequentialist sample. &nbsp;Additionally, there's a relationship between politics and ethics, and relationships between religion and ethics.</p>\n<p>This essay has no intention to make a normative point. Certainly there are all sorts of consequentialism theories that don't require you to donate all your money and never eat meat again, and I'm making no accusations of hypocrisy. However, this survey does seem to confirm what Chris Hallquist has previously noted -- <a href=\"http://www.patheos.com/blogs/hallq/2012/02/moral-beliefs-dont-motivate-much/\">moral beliefs don't seem to motivate much</a>, at least for the average person.</p>\n<h5><span style=\"font-weight: normal;\"><em>(I also <a href=\"http://www.greatplay.net/essays/lesswrong-survey-results-do-ethical-theories-effect-behavior\">cross-posted this on my blog</a>.)</em></span></h5>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1, "TG8zMvjnhydE7Mcue": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3PTNRJwaT9Nkekwui", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 35, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "20747", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Awhile ago, Yvain released&nbsp;<a href=\"/lw/fp5/2012_survey_results/\">the results of the 2012 survey</a>&nbsp;(that I participated in), which contained responses from over 1000 readers. So I uploaded the data into STATA and fooled around looking for cool things.</p>\n<p>Mainly, I've heard some people argue that the moral theory you hold has little to no impact on your actual day-to-day behavior. I want to use these survey results to see if this is true -- are consequentialists more likely to donate money than deontologists? Are virtue ethics more likely to be vegetarian? We'll see!</p>\n<p>While I can't make any claims to the representativeness of this survey or the external validity of drawing conclusions from its results, at least among the people who did take the survey, <strong>ethical theories people endorse seem to have little impact on their actual self-reported behavior.</strong></p>\n<p>&nbsp;</p>\n<h4 id=\"The_Ethical_Theories\">The Ethical Theories</h4>\n<p>First, a breakdown of the actual ethical theories people endorse. Respondents were asked to categorize themselves into \"accept / lean towards consequentialism\", \"accept / lean towards deontology\", \"accept / lean towards virtue ethics\", or \"other / no answer\" (N = 1055):</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>Consequentialism</td>\n<td>63.41%</td>\n</tr>\n<tr>\n<td>Deontology</td>\n<td>3.98%</td>\n</tr>\n<tr>\n<td>Virtue Ethics</td>\n<td>14.41%</td>\n</tr>\n<tr>\n<td>Other / No Answer</td>\n<td>18.20%</td>\n</tr>\n</tbody>\n</table>\n<p>(Note that \"no answer\" are the people who specifically chose to note they had no answer. Other people ignored the question and didn't choose any answer at all, <em>truly</em> having no answer. Those people are not included in this analysis.)</p>\n<p>&nbsp;</p>\n<h4 id=\"Ethical_Theories_and_Donation\">Ethical Theories and Donation</h4>\n<p>So first up, I want to see if your ethical theory predicts how much money you are willing to donate, if any. Given the famous connection between utilitarianism and arguments like Peter Singer's who suggest you should donate all your money until it really hurts (you give so much money, you yourself become as poor as the people you're trying to help). Certainly consequentialism is not utilitarianism, but I would expect most consequentialists would endorse donation more than deontologists or virtue ethicists where donations aren't as mandatory.</p>\n<p>So I took the charity data and dropped all the non-numerical answers, and LessWrongers have donated an average of $445.15 (N = 879, SD = 1167.095, min = 0, max = 9000) to charity over the past year. To get a better proxy for \"effective charity\", LessWrongers donate $331.05 (N = 884, SD = 4087.303, min = 0, max = 110000) on average to SIAI and CFAR. (I don't know why the max is higher on the SIAI/CFAR question, but not the inclusive charity question...)</p>\n<p>Breaking down generic donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>$479.41</td>\n<td>0.266</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>$333.89</td>\n<td>0.561</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>$433.25</td>\n<td>0.887</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>$358.54</td>\n<td>0.323</td>\n</tr>\n</tbody>\n</table>\n<p>Breaking down SIAI/CFAR donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>$420.47</td>\n<td>0.391</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>$0.00</td>\n<td>0.629</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>$86.85</td>\n<td>0.459</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>$288.44</td>\n<td>0.886</td>\n</tr>\n</tbody>\n</table>\n<p>What we're seeing is that there are clear differences in the mean amount of money donated, with consequentialists giving the most. However, when we do t-tests (group compared to all not in the group), we find that none of these differences are statistically significant, which indicates the differences are probably due to chance.</p>\n<p>This would lead us to suspect that ethical theory has no influence on the amount of money donated...</p>\n<p>&nbsp;</p>\n<h4 id=\"Ethical_Theories_and_Percent_of_Income_Donated\">Ethical Theories and Percent of Income Donated</h4>\n<p>However, I have one more trick in the bag -- these donations don't take into account the income people earn. Many LessWrongers are students, and therefore can't donate much, even if they wanted to. What if we adjusted the donation totals by income, and instead looked at percent of income donated?</p>\n<p>Overall, LessWrongers donate 1.75% of their income on average to generic charity (N = 523, SD = 5.70%, min = 0%, max = .88.24%) and 0.49% of their income to SIAI/CFAR (N = 523, SD = 3.11%, min = 0%, max = 52.38%).</p>\n<p>(For those keeping score at home, the average income was $49563.76, N = 602, SD = $59358.34, min = $0, max = $700000.)</p>\n<p>Breaking down percent of income spent on generic donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>2.09%</td>\n<td>0.069</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>1.09%</td>\n<td>0.568</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>1.36%</td>\n<td>0.490</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.91%</td>\n<td>0.161</td>\n</tr>\n</tbody>\n</table>\n<p>Breaking down percent of income spent on SIAI/CFAR donations by ethical theory, we get this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Mean</strong></td>\n<td><strong>T-Test p</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>0.60%</td>\n<td>0.261</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>0.00%</td>\n<td>0.454</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>0.15%</td>\n<td>0.289</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.49%</td>\n<td>0.993</td>\n</tr>\n</tbody>\n</table>\n<p>A bit more can be made out of these results -- specifically, there's weak evidence that consequentialists actually donate more of their income (M = 2.09%) than non-consequentialists (M = 1.14%) with a p-value of 0.069, which is fairly significant. However, there are no differences across any other ethical theories or across SIAI/CFAR donations.</p>\n<p>(Though Unnamed concludes that <a href=\"/lw/g0b/lesswrong_survey_results_do_ethical_theories/82yc\">this might just be in-group bias</a>. &nbsp;Unnamed also does a <a href=\"/r/discussion/lw/g0b/lesswrong_survey_results_do_ethical_theories/836x\">more thorough analysis</a> and finds more correlations between consequentialism and donation.)</p>\n<p>&nbsp;</p>\n<h4 id=\"Vegetarianism\">Vegetarianism</h4>\n<p>However, Peter Singer isn't just famous for consequentialist arguments for charity... he's also famous for consequentialist arguments for animal rights, which he argues necessitate veganism, or at least vegetarianism. Are consequentialists more likely to be vegetarian?</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Not a Vegetarian</strong></td>\n<td>Yes, Vegetarian</td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>84.39%</td>\n<td>15.61%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>83.78%</td>\n<td>16.22%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>88.97%</td>\n<td>11.03%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>90.00%</td>\n<td>10.00%</td>\n</tr>\n</tbody>\n</table>\n<p>Perhaps surprisingly, there is no statistically significant relationship (N = 958, chi2 = 4.73, p = 0.192). Your choice in ethical theory has no correlation with your choice to eat or not eat meat.</p>\n<p style=\"text-align: justify;\">(Though Unnamed <a href=\"/lw/g0b/lesswrong_survey_results_do_ethical_theories/833r\">finds somewhat contrary results</a>&nbsp;that are consistent with a connection -- consequentialists are more likely than non-consequentialists to be vegetarian [p=0.03], this effect holds up when looking at men<span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">&nbsp;[p&lt;0.01] but not for women, and as a single analysis, sex and consequentialism both predict vegetarianism [p=0.007 and p=0.02 respectively].)</span></span></p>\n<p>&nbsp;</p>\n<h4 id=\"Dust_Specks\">Dust Specks</h4>\n<p>We can also take it into a more abstract realm of theory. Eliezer Yudkowsky in <a href=\"/lw/kn/torture_vs_dust_specks/\">\"Torture vs. Dust Specks\"</a> outlines a thought experiment:</p>\n<blockquote>[H]ere's the moral dilemma. If neither event is going to happen to you personally, but you still had to choose one or the other:</blockquote>\n<blockquote>Would you prefer that one person be horribly tortured for fifty years without hope or rest, or that 3^^^3 [an obnoxiously and unfathomably large number] people get dust specks in their eyes?</blockquote>\n<blockquote>I think the answer is obvious. How about you?</blockquote>\n<p>According to Yudkowsky, consequentialists (at least of the utilitarian variety) should choose torture over dust specks, since less total harm occurs. Does this turn to actually happen?</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Torture</td>\n<td>Dust Specks</td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>45.19%</td>\n<td>54.81%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>11.11%</td>\n<td>88.89%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>13.79%</td>\n<td>86.21%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>35.85%</td>\n<td>64.15%</td>\n</tr>\n</tbody>\n</table>\n<p>Here, we see another statistically significant relationship (N = 636, chi2 = 39.31, p &lt; 0.001), and it goes exactly as expected. &nbsp;People's ethical theories seem to influence their choice of theory in this scenario (or the other way around, or a third variable).</p>\n<p>&nbsp;</p>\n<h4 id=\"Politics\">Politics</h4>\n<p>Going back into the practical realm, let's look at politics.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Communist</strong></td>\n<td><strong>Conservative</strong></td>\n<td><strong>Liberal</strong></td>\n<td><strong>Libertarian</strong></td>\n<td><strong>Socialist</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>0.30%</td>\n<td>1.82%</td>\n<td>41.03%</td>\n<td>29.64%</td>\n<td>27.20%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>2.38%</td>\n<td>16.67%</td>\n<td>21.43%</td>\n<td>26.19%</td>\n<td>33.33%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>1.99%</td>\n<td>5.30%</td>\n<td>34.44%</td>\n<td>28.48%</td>\n<td>29.80%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>0.00%</td>\n<td>2.69%</td>\n<td>31.18%</td>\n<td>34.41%</td>\n<td>31.72%</td>\n</tr>\n</tbody>\n</table>\n<p>Here, there is a statistically significant relationship (N = 1037, Chi2 = 50.91, p &lt; 0.001) between ethical theories and political beliefs -- the plurality of consequentialists and virtue ethicists are liberal, the plurality of deontologists are socialist, and the plurality of others or no answers are libertarians.</p>\n<p>&nbsp;</p>\n<h4 id=\"Religion\">Religion</h4>\n<p>Continuing along a similar path, next let's look at religion:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Agnostic</strong></td>\n<td><strong>Atheist, nonpiritual</strong></td>\n<td><strong>Atheist, spiritual</strong></td>\n<td><strong>Theist, committed</strong></td>\n<td><strong>Deist/Pantheist</strong></td>\n<td><strong>Theist, lukewarm</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>5.71%</td>\n<td>81.35%</td>\n<td>8.72%</td>\n<td>1.35%</td>\n<td>1.35%</td>\n<td>1.50%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>11.90%</td>\n<td>54.76%</td>\n<td>7.14%</td>\n<td>11.90%</td>\n<td>7.14%</td>\n<td>7.14%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>12.50%</td>\n<td>61.18%</td>\n<td>13.16%</td>\n<td>5.26%</td>\n<td>3.95%</td>\n<td>3.95%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>10.00%</td>\n<td>75.79%</td>\n<td>6.84%</td>\n<td>3.16%</td>\n<td>1.05%</td>\n<td>3.16%</td>\n</tr>\n</tbody>\n</table>\n<p>Again, another statistically significant relationship (N = 1049, Chi2 = 64.74, p &lt; 0.001) between choice of ethical theory and religious beliefs -- consequentialists were more likely than deontologists and virtue ethics to not be religious.</p>\n<p>&nbsp;</p>\n<h4 id=\"Sequences_and_Meetups\">Sequences and Meetups</h4>\n<p>And for a bit of bonus material, here's another interesting finding -- there is also a statistically significant relationship (N = 1052, chi2 = 128.43, p &lt; 0.001) between ethical theory endorsed and amount of sequences read. Whether sequences convince people to adopt more consequentialist theories, whether people with consequentialist theories are more likely to enjoy and therefore keep reading the sequences, or some hidden third variable at work, I cannot figure out with the current data.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>~1/4 of Sequences Read</strong></td>\n<td><strong>1/2 of Sequences</strong></td>\n<td><strong>3/4 of Sequences</strong></td>\n<td><strong>Never looked</strong></td>\n<td><strong>Never heard of 'em</strong></td>\n<td><strong>Nearly all</strong></td>\n<td><strong>Some, but &lt;1/4</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>11.83%</td>\n<td>14.82%</td>\n<td>21.71%</td>\n<td>0.60%</td>\n<td>31.44%</td>\n<td>4.49%</td>\n<td>15.12%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>14.29%</td>\n<td>7.14%</td>\n<td>9.52%</td>\n<td>4.76%</td>\n<td>19.05%</td>\n<td>11.90%</td>\n<td>33.33%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>11.26%</td>\n<td>12.58%</td>\n<td>10.60%</td>\n<td>1.99%</td>\n<td>15.23%</td>\n<td>20.53%</td>\n<td>27.81%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>14.66%</td>\n<td>14.14%</td>\n<td>10.99%</td>\n<td>6.28%</td>\n<td>16.75%</td>\n<td>11.52%</td>\n<td>25.65%</td>\n</tr>\n</tbody>\n</table>\n<p>And this trend continues among those who have been to a LessWrong meetup (N = 1044, chi2 = 34.27, p &lt; 0.001):</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><strong>Never Been to a Meetup</strong></td>\n<td><strong>Been to a Meetup</strong></td>\n</tr>\n<tr>\n<td><strong>Consequentialism</strong></td>\n<td>66.52%</td>\n<td>33.48%</td>\n</tr>\n<tr>\n<td><strong>Deontology</strong></td>\n<td>92.68%</td>\n<td>7.32%</td>\n</tr>\n<tr>\n<td><strong>Virtue Ethics</strong></td>\n<td>84.00%</td>\n<td>16.00%</td>\n</tr>\n<tr>\n<td><strong>Other / No Answer</strong></td>\n<td>79.14%</td>\n<td>20.86%</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>In meta-ethics, there is a distinction between moral internalism, which is the theory that moral beliefs must be motivating, and moral externalism, which is the theory that you can have a moral belief and not be motivated to follow it. For instance, if moral externalism is true, you can legitimately think that eating meat is morally wrong but still eat meat.</p>\n<p>Now, moral interalism and externalism are more about the semantics of moral statements, what moral statements <em>refer</em> to, and less about actual behavior. Even if people claim that eating meat is morally wrong while still eating meat, it's easy enough for the internalist to deny that the meat eater was telling the truth or making a coherent statement.</p>\n<p>However, when it comes to the results of the LessWrong 2012 survey, there are very mixed results on whether choice of ethical theory influences actual behavior -- there's weak evidence that consequentialists are more likely to donate, and even those who do donate are certainly not giving up everything and becoming poor themselves, let alone giving 10% like <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a> advocates.</p>\n<p>Furthermore, there's evidence that vegetarianism does not depend upon ethical theory, but there is evidence that it influences people's choice of dust specks vs. torture in the right direction, at least for slightly less than half of the consequentialist sample. &nbsp;Additionally, there's a relationship between politics and ethics, and relationships between religion and ethics.</p>\n<p>This essay has no intention to make a normative point. Certainly there are all sorts of consequentialism theories that don't require you to donate all your money and never eat meat again, and I'm making no accusations of hypocrisy. However, this survey does seem to confirm what Chris Hallquist has previously noted -- <a href=\"http://www.patheos.com/blogs/hallq/2012/02/moral-beliefs-dont-motivate-much/\">moral beliefs don't seem to motivate much</a>, at least for the average person.</p>\n<h5><span style=\"font-weight: normal;\"><em>(I also <a href=\"http://www.greatplay.net/essays/lesswrong-survey-results-do-ethical-theories-effect-behavior\">cross-posted this on my blog</a>.)</em></span></h5>", "sections": [{"title": "The Ethical Theories", "anchor": "The_Ethical_Theories", "level": 1}, {"title": "Ethical Theories and Donation", "anchor": "Ethical_Theories_and_Donation", "level": 1}, {"title": "Ethical Theories and Percent of Income Donated", "anchor": "Ethical_Theories_and_Percent_of_Income_Donated", "level": 1}, {"title": "Vegetarianism", "anchor": "Vegetarianism", "level": 1}, {"title": "Dust Specks", "anchor": "Dust_Specks", "level": 1}, {"title": "Politics", "anchor": "Politics", "level": 1}, {"title": "Religion", "anchor": "Religion", "level": 1}, {"title": "Sequences and Meetups", "anchor": "Sequences_and_Meetups", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P", "3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T09:42:16.279Z", "modifiedAt": null, "url": null, "title": "Narrative, self-image, and self-communication", "slug": "narrative-self-image-and-self-communication", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.069Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MmSv6vnsmDHu5tvpd/narrative-self-image-and-self-communication", "pageUrlRelative": "/posts/MmSv6vnsmDHu5tvpd/narrative-self-image-and-self-communication", "linkUrl": "https://www.lesswrong.com/posts/MmSv6vnsmDHu5tvpd/narrative-self-image-and-self-communication", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Narrative%2C%20self-image%2C%20and%20self-communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANarrative%2C%20self-image%2C%20and%20self-communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmSv6vnsmDHu5tvpd%2Fnarrative-self-image-and-self-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Narrative%2C%20self-image%2C%20and%20self-communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmSv6vnsmDHu5tvpd%2Fnarrative-self-image-and-self-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmSv6vnsmDHu5tvpd%2Fnarrative-self-image-and-self-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1601, "htmlBody": "<p>Related to: <a href=\"/lw/4e/cached_selves/\">Cached selves</a>,  <a href=\"/lw/14q/why_youre_stuck_in_a_narrative/\">Why you're stuck in a narrative</a>, <a href=\"/lw/8gv/the_curse_of_identity/\">The curse of identity</a></p>\n<p>Outline: <a href=\"#back-story\">Some back-story</a>,  <a href=\"#pondering\">Pondering the mechanics of self-image</a>,  <a href=\"#narrative\">The role of narrative</a>, <a href=\"#medium\">Narrative as a medium for self-communication</a>.</p>\n<p><em>tl;dr: One can have a self-image that causes one to neglect the effects of self-image. And, since we tend to process our self-images somewhat in the context of a <a href=\"http://en.wikipedia.org/wiki/Narrative_identity\">narrative identity</a>, if you currently make zero use of narrative in understanding and affecting how you think about yourself, it may be worth adjusting upward.  All this seems to have been the case for me, and is probably part of what makes <a href=\"http://hpmor.com/\">HPMOR</a> valuable.</em> <a name=\"back-story\"></a></p>\n<h3>Some back-story</h3>\n<p>Starting when I was around 16 and becoming acutely <a href=\"http://en.wikipedia.org/wiki/Essentialism#In_ethics\">annoyed</a> <a href=\"http://en.wikipedia.org/wiki/Essentialism#In_biology\">with</a> <a href=\"http://en.wikipedia.org/wiki/Essentialism#Essentialism_and_society_and_politics\">essentialism</a>, I prided myself on not being dependent on a story-like image of myself.  In fact, to make sure I wasn't, I put a break command in my narrative loop: I drafted a story in my mind about a hero who was able to outwit his foes by being less constrained by narrative than they were, and I identified with him whenever I felt a need-for-narrative coming on.  Batman's narrator goes for something like this in the Dark Knight when he &lt;select for spoiler-&gt; <span style=\"color: white;\">abandons his heroic image to take the blame for Harvey Dent's death.</span></p>\n<p>I think this break command was mostly a good thing.  It helped me to resolve <a href=\"http://en.wikipedia.org/wiki/Cognitive_dissonance\">cognitive dissonance</a> and overcome the limitations of various <a href=\"/lw/4e/cached_selves/\">cached selves</a>, and  I ended up mostly focussed on whether my beliefs were accurate and my desires were being fulfilled.  So I still figure it's a decent <a href=\"http://en.wikipedia.org/wiki/Orders_of_approximation\">first-order correction</a> to being <a href=\"http://en.wikipedia.org/wiki/Overdetermined_system\">over-constrained</a> by narrative.</p>\n<p>But, I no longer think it's the only decent solution.  In fact, understanding the more subtle mechanics of self-image &mdash; what affects our <a href=\"http://en.wikipedia.org/wiki/Self-schema\">self schemas</a>, what they affect, and how &mdash; was something I neglected for a long time because I saw self-image as a solved problem.  Yes, I developed a cached view of myself as unaffected by self-image constraints.  I would have been embarassed to notice such dependencies, so I didn't.  The irony, eh?</p>\n<p>I'm writing this because I wouldn't be surprised to find others here developing, or having developed, this blind spot...   <a id=\"more\"></a> <a name=\"pondering\"></a></p>\n<h3>Pondering the mechanics of self-image</h3>\n<p>At some point in your life, you may have taken on a job or a project without knowing that after doing it for a month, it would negatively affect your self-image in some way.  There may have been things that you always found very easy to do which, after some aspect of your self-image changed, you suddenly found yourself avoiding or struggling with.</p>\n<p>It would be nice to be able to predict and maybe even control that sort of thing in advance.  In general, I'd like a deeper understanding of the following questions:</p>\n<ol>\n<li>What actions might conflict or resonate with my self-image? </li>\n<li>What events beyond my control might threaten or reinforce my self-image? </li>\n<li>What might my self-image inhibit me from doing, or empower me to do? </li>\n<li>Could changing my self-image help me further my goals? </li>\n</ol>\n<p><strong>If you've never sat to ask yourself these questions genuinely, I might suggest stopping here and thinking about them for a while.</strong> Simply taking the time to ponder these issues has lead me to many helpful realizations.  For example:</p>\n<ul>\n<li> I used to be uninterested in how self-image worked because I didn't see myself as the kind of person who was affected by self-image! </li>\n<li> I didn't like dancing until I was 22, when I found a way to view it as a function of my \"musician\" self-schema. </li>\n<li> There were certain things I didn't try to learn about, like neuroscience, just because they didn't fit with my status-quo self-image as a mathematician.  I noticed this acutely when I was was 23, after reading Anna's Cached Selves post, and I began reading a textbook on affective neuroscience. </li>\n<li> An injury that prevented me from climbing this semester lead to me feeling chronically <em>meh</em> for about a month, until I realized it was because my self-image as a physically active and playful person was threatened.  Realizing this, and reconstructing my self-image as more generally \"health-conscious\", was how I got over it. </li>\n</ul>\n<p>I don't have anything like an inclusive, general theory of self-image, and I have lots of hanging questions.  Can I come up with a reasonably finite exhaustive list of features to track in my own self-image, for practical gains?  Does such a list exist for people in general?  But even without these, asking myself the old 1-4 once in a while gives me something to think about.  <a name=\"narrative\"></a></p>\n<h3>The role of narrative</h3>\n<p>In my experience, personally and with others, the answers to questions 1-4 are not automatically transparent, even if we can find partial answers by asking them directly.  So what other questions can we ask ourselves to understand our self-images?</p>\n<p>It seems to be common lore that our self-images have something to do with <a href=\"http://en.wikipedia.org/wiki/Narrative_identity\">narrative identity</a>.  I take this to mean that we process our self-images somewhat in terms of <a href=\"http://en.wikipedia.org/wiki/Feature_(machine_learning)\">features</a> and <a href=\" http://en.wikipedia.org/wiki/Schema_(psychology)\">schemas</a> that we also use to process common stories.</p>\n<p>So, I've tried working through the following series of questions to get in touch with what aspects of my personal narrative cause me to experience shame, pride, indignation, and nurturance.  I like to lay them all out like this to signal to myself what they're for and that I want to do them all:</p>\n<ul>\n<li>Questions to understand shame: \t\n<ul>\n<li> I feel sad or ashamed when ... </li>\n<li> When I'm sad or ashamed, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I'm sad or ashamed: </li>\n</ul>\n</li>\n<li>Questions to understand pride: \t\n<ul>\n<li> I feel happy or proud when ... </li>\n<li> When I'm happy or proud, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I'm happy or proud: </li>\n</ul>\n</li>\n<li>Questions to understand indignation: \t\n<ul>\n<li> I feel angry or indignant when ... </li>\n<li> When I am angry or indignant, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I feel shame or indignation: </li>\n</ul>\n</li>\n<li>Questions to understand nurturance:  \t\n<ul>\n<li> I feel caring or nurturing when ... </li>\n<li> When I am caring or nurturing, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I feel caring or nurturing: </li>\n</ul>\n</li>\n</ul>\n<p><strong>Consequences.</strong> By asking myself these questions, I've come to some realizations that didn't result from asking myself the more direct questions 1-4.  For example:</p>\n<ul>\n<li><em>(Involving shame and pride)</em> Doing physiotherapy exercises made me feel ashamed of being weak.  Visualizing the anime character Naruto training to recover from injuries made me stop experiencing the exercises as a \"sign of weakness\", and I became less physically uncomfortable while doing them. </li>\n<li><em>(Involving indignation and nurturance)</em> Imagining my kind and inspiring 6th grade teacher speaking to me an indignant tone of voice seems wrong, and makes me think that feeling annoyed is not always a good way to help other people learn from their mistakes, because he was the teacher I felt I learned the most moral lessons from growing up.  \"Channeling\" him makes me more curious about other peoples' motives and misunderstandings instead of feeling annoyed. </li>\n<li><em>(Involving all four)</em> Explicitly imagining myself as an &lt;insert animal here&gt; helps me to avoid taking myself too seriously &mdash; in particular, getting caught up in shame, indignation, and unhelpful instances of pride &mdash; while still caring about myself. </li>\n</ul>\n<p>Does anyone have similar experiences they'd like to share?  Or very dissimilar experiences?  Or questions I could add to this list?  Or well-reproduced psych references?  <a href=\"http://hpmor.com/\">HPMOR</a> references are also highly encouraged, especially since I still haven't read it, and in light of this post, I probably should!  <a name=\"medium\"></a></p>\n<h3>Narrative as a medium for self-communication</h3>\n<p>Like any method of affecting oneself, narrative is something one can over-use.  But I think I personally have been over-cautious about this, to the point of neglecting it as an option and ignoring it as an unconscious constraint.  To the extent that I now use it, I think of it as a way of communicating with myself, not to be used for trickery or over-selling a point.</p>\n<p>To draw an analogy, if you tell your 2-year-old child \"You trigger in me feelings of paternal nurturance\", while this may be true, it's not communication.  Hugging the child is communication. It's a language she'll understand.  In fact, it's probably how you should teach her what \"nurturance\" means.  In particular, it's not a trick, and it's not over-selling.</p>\n<p>Likewise, when I'm convinced enough that something is true &mdash; like <em>for once I should really try not feeling annoyed with a postmodernist to see if we can communicate</em> &mdash; and it's time to tell that to my limbic system with some conviction, maybe it's worth speaking a language my emotional brain understands a little better, and maybe sometimes that language is narrative.  Maybe I'll write myself a poem about patience.  Maybe I already have ;)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pszEEb3ctztv3rozd": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MmSv6vnsmDHu5tvpd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 48, "extendedScore": null, "score": 1.0621043809601595e-06, "legacy": true, "legacyId": "20750", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/4e/cached_selves/\">Cached selves</a>,  <a href=\"/lw/14q/why_youre_stuck_in_a_narrative/\">Why you're stuck in a narrative</a>, <a href=\"/lw/8gv/the_curse_of_identity/\">The curse of identity</a></p>\n<p>Outline: <a href=\"#back-story\">Some back-story</a>,  <a href=\"#pondering\">Pondering the mechanics of self-image</a>,  <a href=\"#narrative\">The role of narrative</a>, <a href=\"#medium\">Narrative as a medium for self-communication</a>.</p>\n<p><em>tl;dr: One can have a self-image that causes one to neglect the effects of self-image. And, since we tend to process our self-images somewhat in the context of a <a href=\"http://en.wikipedia.org/wiki/Narrative_identity\">narrative identity</a>, if you currently make zero use of narrative in understanding and affecting how you think about yourself, it may be worth adjusting upward.  All this seems to have been the case for me, and is probably part of what makes <a href=\"http://hpmor.com/\">HPMOR</a> valuable.</em> <a name=\"back-story\"></a></p>\n<h3 id=\"Some_back_story\">Some back-story</h3>\n<p>Starting when I was around 16 and becoming acutely <a href=\"http://en.wikipedia.org/wiki/Essentialism#In_ethics\">annoyed</a> <a href=\"http://en.wikipedia.org/wiki/Essentialism#In_biology\">with</a> <a href=\"http://en.wikipedia.org/wiki/Essentialism#Essentialism_and_society_and_politics\">essentialism</a>, I prided myself on not being dependent on a story-like image of myself.  In fact, to make sure I wasn't, I put a break command in my narrative loop: I drafted a story in my mind about a hero who was able to outwit his foes by being less constrained by narrative than they were, and I identified with him whenever I felt a need-for-narrative coming on.  Batman's narrator goes for something like this in the Dark Knight when he &lt;select for spoiler-&gt; <span style=\"color: white;\">abandons his heroic image to take the blame for Harvey Dent's death.</span></p>\n<p>I think this break command was mostly a good thing.  It helped me to resolve <a href=\"http://en.wikipedia.org/wiki/Cognitive_dissonance\">cognitive dissonance</a> and overcome the limitations of various <a href=\"/lw/4e/cached_selves/\">cached selves</a>, and  I ended up mostly focussed on whether my beliefs were accurate and my desires were being fulfilled.  So I still figure it's a decent <a href=\"http://en.wikipedia.org/wiki/Orders_of_approximation\">first-order correction</a> to being <a href=\"http://en.wikipedia.org/wiki/Overdetermined_system\">over-constrained</a> by narrative.</p>\n<p>But, I no longer think it's the only decent solution.  In fact, understanding the more subtle mechanics of self-image \u2014 what affects our <a href=\"http://en.wikipedia.org/wiki/Self-schema\">self schemas</a>, what they affect, and how \u2014 was something I neglected for a long time because I saw self-image as a solved problem.  Yes, I developed a cached view of myself as unaffected by self-image constraints.  I would have been embarassed to notice such dependencies, so I didn't.  The irony, eh?</p>\n<p>I'm writing this because I wouldn't be surprised to find others here developing, or having developed, this blind spot...   <a id=\"more\"></a> <a name=\"pondering\"></a></p>\n<h3 id=\"Pondering_the_mechanics_of_self_image\">Pondering the mechanics of self-image</h3>\n<p>At some point in your life, you may have taken on a job or a project without knowing that after doing it for a month, it would negatively affect your self-image in some way.  There may have been things that you always found very easy to do which, after some aspect of your self-image changed, you suddenly found yourself avoiding or struggling with.</p>\n<p>It would be nice to be able to predict and maybe even control that sort of thing in advance.  In general, I'd like a deeper understanding of the following questions:</p>\n<ol>\n<li>What actions might conflict or resonate with my self-image? </li>\n<li>What events beyond my control might threaten or reinforce my self-image? </li>\n<li>What might my self-image inhibit me from doing, or empower me to do? </li>\n<li>Could changing my self-image help me further my goals? </li>\n</ol>\n<p><strong>If you've never sat to ask yourself these questions genuinely, I might suggest stopping here and thinking about them for a while.</strong> Simply taking the time to ponder these issues has lead me to many helpful realizations.  For example:</p>\n<ul>\n<li> I used to be uninterested in how self-image worked because I didn't see myself as the kind of person who was affected by self-image! </li>\n<li> I didn't like dancing until I was 22, when I found a way to view it as a function of my \"musician\" self-schema. </li>\n<li> There were certain things I didn't try to learn about, like neuroscience, just because they didn't fit with my status-quo self-image as a mathematician.  I noticed this acutely when I was was 23, after reading Anna's Cached Selves post, and I began reading a textbook on affective neuroscience. </li>\n<li> An injury that prevented me from climbing this semester lead to me feeling chronically <em>meh</em> for about a month, until I realized it was because my self-image as a physically active and playful person was threatened.  Realizing this, and reconstructing my self-image as more generally \"health-conscious\", was how I got over it. </li>\n</ul>\n<p>I don't have anything like an inclusive, general theory of self-image, and I have lots of hanging questions.  Can I come up with a reasonably finite exhaustive list of features to track in my own self-image, for practical gains?  Does such a list exist for people in general?  But even without these, asking myself the old 1-4 once in a while gives me something to think about.  <a name=\"narrative\"></a></p>\n<h3 id=\"The_role_of_narrative\">The role of narrative</h3>\n<p>In my experience, personally and with others, the answers to questions 1-4 are not automatically transparent, even if we can find partial answers by asking them directly.  So what other questions can we ask ourselves to understand our self-images?</p>\n<p>It seems to be common lore that our self-images have something to do with <a href=\"http://en.wikipedia.org/wiki/Narrative_identity\">narrative identity</a>.  I take this to mean that we process our self-images somewhat in terms of <a href=\"http://en.wikipedia.org/wiki/Feature_(machine_learning)\">features</a> and <a href=\" http://en.wikipedia.org/wiki/Schema_(psychology)\">schemas</a> that we also use to process common stories.</p>\n<p>So, I've tried working through the following series of questions to get in touch with what aspects of my personal narrative cause me to experience shame, pride, indignation, and nurturance.  I like to lay them all out like this to signal to myself what they're for and that I want to do them all:</p>\n<ul>\n<li>Questions to understand shame: \t\n<ul>\n<li> I feel sad or ashamed when ... </li>\n<li> When I'm sad or ashamed, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I'm sad or ashamed: </li>\n</ul>\n</li>\n<li>Questions to understand pride: \t\n<ul>\n<li> I feel happy or proud when ... </li>\n<li> When I'm happy or proud, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I'm happy or proud: </li>\n</ul>\n</li>\n<li>Questions to understand indignation: \t\n<ul>\n<li> I feel angry or indignant when ... </li>\n<li> When I am angry or indignant, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I feel shame or indignation: </li>\n</ul>\n</li>\n<li>Questions to understand nurturance:  \t\n<ul>\n<li> I feel caring or nurturing when ... </li>\n<li> When I am caring or nurturing, I see myself as ... and I see the world as ... </li>\n<li> Some real or fictional people, stories, songs, or poems I can relate to when I feel caring or nurturing: </li>\n</ul>\n</li>\n</ul>\n<p><strong>Consequences.</strong> By asking myself these questions, I've come to some realizations that didn't result from asking myself the more direct questions 1-4.  For example:</p>\n<ul>\n<li><em>(Involving shame and pride)</em> Doing physiotherapy exercises made me feel ashamed of being weak.  Visualizing the anime character Naruto training to recover from injuries made me stop experiencing the exercises as a \"sign of weakness\", and I became less physically uncomfortable while doing them. </li>\n<li><em>(Involving indignation and nurturance)</em> Imagining my kind and inspiring 6th grade teacher speaking to me an indignant tone of voice seems wrong, and makes me think that feeling annoyed is not always a good way to help other people learn from their mistakes, because he was the teacher I felt I learned the most moral lessons from growing up.  \"Channeling\" him makes me more curious about other peoples' motives and misunderstandings instead of feeling annoyed. </li>\n<li><em>(Involving all four)</em> Explicitly imagining myself as an &lt;insert animal here&gt; helps me to avoid taking myself too seriously \u2014 in particular, getting caught up in shame, indignation, and unhelpful instances of pride \u2014 while still caring about myself. </li>\n</ul>\n<p>Does anyone have similar experiences they'd like to share?  Or very dissimilar experiences?  Or questions I could add to this list?  Or well-reproduced psych references?  <a href=\"http://hpmor.com/\">HPMOR</a> references are also highly encouraged, especially since I still haven't read it, and in light of this post, I probably should!  <a name=\"medium\"></a></p>\n<h3 id=\"Narrative_as_a_medium_for_self_communication\">Narrative as a medium for self-communication</h3>\n<p>Like any method of affecting oneself, narrative is something one can over-use.  But I think I personally have been over-cautious about this, to the point of neglecting it as an option and ignoring it as an unconscious constraint.  To the extent that I now use it, I think of it as a way of communicating with myself, not to be used for trickery or over-selling a point.</p>\n<p>To draw an analogy, if you tell your 2-year-old child \"You trigger in me feelings of paternal nurturance\", while this may be true, it's not communication.  Hugging the child is communication. It's a language she'll understand.  In fact, it's probably how you should teach her what \"nurturance\" means.  In particular, it's not a trick, and it's not over-selling.</p>\n<p>Likewise, when I'm convinced enough that something is true \u2014 like <em>for once I should really try not feeling annoyed with a postmodernist to see if we can communicate</em> \u2014 and it's time to tell that to my limbic system with some conviction, maybe it's worth speaking a language my emotional brain understands a little better, and maybe sometimes that language is narrative.  Maybe I'll write myself a poem about patience.  Maybe I already have ;)</p>", "sections": [{"title": "Some back-story", "anchor": "Some_back_story", "level": 1}, {"title": "Pondering the mechanics of self-image", "anchor": "Pondering_the_mechanics_of_self_image", "level": 1}, {"title": "The role of narrative", "anchor": "The_role_of_narrative", "level": 1}, {"title": "Narrative as a medium for self-communication", "anchor": "Narrative_as_a_medium_for_self_communication", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHYBdijDcAKQ6e45Z", "FSPKLFfMNbRGPFjmY", "tAXrD8Y6hcJ8dt6Nt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T12:42:44.956Z", "modifiedAt": null, "url": null, "title": "The \"Scary problem of Qualia\"", "slug": "the-scary-problem-of-qualia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:39.172Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DZNdCjYfdZFWp7wA5/the-scary-problem-of-qualia", "pageUrlRelative": "/posts/DZNdCjYfdZFWp7wA5/the-scary-problem-of-qualia", "linkUrl": "https://www.lesswrong.com/posts/DZNdCjYfdZFWp7wA5/the-scary-problem-of-qualia", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20%22Scary%20problem%20of%20Qualia%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20%22Scary%20problem%20of%20Qualia%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZNdCjYfdZFWp7wA5%2Fthe-scary-problem-of-qualia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20%22Scary%20problem%20of%20Qualia%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZNdCjYfdZFWp7wA5%2Fthe-scary-problem-of-qualia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZNdCjYfdZFWp7wA5%2Fthe-scary-problem-of-qualia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 656, "htmlBody": "<address>Disclaimer: I don't have sufficient knowledge of the concepts appearing in this post, hopefully though I have categorized them properply and put them into suitable boxes to be used correctly. If you can find a real error here, please point them out. I'm sure there are people (including me) who'd like to know them. I'll probably rewrite this bit soon in case of errors.<br /></address>\n<p><br />For the purpose of this thought experiment let's assume that the <a href=\"/lw/on/reductionism/\" target=\"_blank\">reductionist</a> approach to <a href=\"http://en.wikipedia.org/wiki/Hard_problem_of_consciousness\" target=\"_blank\">consciousness</a> is truthful and consciousness is indeed reducible to physics and there is no <a href=\"http://en.wikipedia.org/wiki/Ontology\" target=\"_blank\">ontologically</a> distinct basic mental element that produces the experience of consciousness - the <a href=\"http://en.wikipedia.org/wiki/Qualia\" target=\"_blank\">qualia</a>.</p>\n<p>&nbsp;</p>\n<h2>\"The Scary problem of Qualia\"</h2>\n<p>This leads to a very strange problem which I'd term \"the Scary problem of Qualia\" in a little humouristic sense. If mental experience is as proposed, and humans with brains doing thinking are \"merely physics\", or \"ordered physics\", physics + logic that is, then that results to <em>the experience of consciousness being logic given the physics</em>...<br /><br />...Which is to say that whenever there is (a physical arrangement with) a logical structure that matches (is transitive with) the logical structure of consciousness - then there would be consciousness.&nbsp; It gets more complicated. If you draw a line with a pencil on a piece of paper, so that it encodes a <a href=\"/lw/qp/timeless_physics/\" target=\"_blank\">three dimensional trajectory over time</a> of a sentient being's consciousness - you basically have created a \"<a href=\"/lw/p7/zombies_zombies/\" target=\"_blank\">soulful</a>\" being. Except there's just a drawn line on a piece of paper.<br /><br />(Assuming you can store a sufficient amount of bits in such an encoding. Think of a \"large\" paper and a long complicated line if imagining an A4 with something scribbled on is a problem. You can also replace the pencil &amp; paper with a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\" target=\"_blank\">turing machine</a> if you like)</p>\n<p>If you now take a device complicated enough to decode the message, and create a representation for this message, for a sci-fi example a \"brain modifying device which stimulates\" a form of <a href=\"http://wiki.lesswrong.com/wiki/Empathic_inference\" target=\"_blank\">empathic inference</a>, you can partially think the thoughts recorded on the piece of paper. Further yet you could simulate this particular person inside a powerful AI, even save that information to a disk, insert it into an android, and let that person go and live his/her life on. If this isn't sufficient you could engineer a biological being which would have a brain that produces a series of chemical reactions, enzymatic reactions, combined with an electron cloud etc that happens to be transitive with the logical structure of the data stored on the chip.</p>\n<p>&nbsp;</p>\n<p><strong>Meditation: And this creates another kind of problem. Did the person come into existence:</strong></p>\n<p>1. When the line drawn with the pencil came into existence?<br /><br />2. When the entity that created the line thought of how to draw the line?<br /><br />3. When the line was decoded?<br /><br />4. Did it come into existence when the supercomputer AI simulated the person from the line?<br /><br />5. When it was recorded onto the chip for the android?<br /><br />6. Or lastly when the contents of the chip were translated to a biological brain producing the same thoughts?</p>\n<p><br /><br /><strong>Meditation: Is logic an ontologically basic thing?</strong></p>\n<p>In my book this is all a natural consequence from the reductionist approach to the hard problem of consciousness. That is unless we wan't to consider the possibility that electron clouds have little tags which say \"consciousness\" hanging from.. uhuh.. From their amplitudes.<br /><br />So in other words: From the reductionist perspective there's just physics which can be described with the help of logic. Whenever there is a physical part of the universe that is correlated with the rest of the universe in such a way that it would resemble consciousness when interacted with, that thing would be just as much a person/zombie as we're. Same goes for simulated people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DZNdCjYfdZFWp7wA5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "20752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<address>Disclaimer: I don't have sufficient knowledge of the concepts appearing in this post, hopefully though I have categorized them properply and put them into suitable boxes to be used correctly. If you can find a real error here, please point them out. I'm sure there are people (including me) who'd like to know them. I'll probably rewrite this bit soon in case of errors.<br></address>\n<p><br>For the purpose of this thought experiment let's assume that the <a href=\"/lw/on/reductionism/\" target=\"_blank\">reductionist</a> approach to <a href=\"http://en.wikipedia.org/wiki/Hard_problem_of_consciousness\" target=\"_blank\">consciousness</a> is truthful and consciousness is indeed reducible to physics and there is no <a href=\"http://en.wikipedia.org/wiki/Ontology\" target=\"_blank\">ontologically</a> distinct basic mental element that produces the experience of consciousness - the <a href=\"http://en.wikipedia.org/wiki/Qualia\" target=\"_blank\">qualia</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"_The_Scary_problem_of_Qualia_\">\"The Scary problem of Qualia\"</h2>\n<p>This leads to a very strange problem which I'd term \"the Scary problem of Qualia\" in a little humouristic sense. If mental experience is as proposed, and humans with brains doing thinking are \"merely physics\", or \"ordered physics\", physics + logic that is, then that results to <em>the experience of consciousness being logic given the physics</em>...<br><br>...Which is to say that whenever there is (a physical arrangement with) a logical structure that matches (is transitive with) the logical structure of consciousness - then there would be consciousness.&nbsp; It gets more complicated. If you draw a line with a pencil on a piece of paper, so that it encodes a <a href=\"/lw/qp/timeless_physics/\" target=\"_blank\">three dimensional trajectory over time</a> of a sentient being's consciousness - you basically have created a \"<a href=\"/lw/p7/zombies_zombies/\" target=\"_blank\">soulful</a>\" being. Except there's just a drawn line on a piece of paper.<br><br>(Assuming you can store a sufficient amount of bits in such an encoding. Think of a \"large\" paper and a long complicated line if imagining an A4 with something scribbled on is a problem. You can also replace the pencil &amp; paper with a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\" target=\"_blank\">turing machine</a> if you like)</p>\n<p>If you now take a device complicated enough to decode the message, and create a representation for this message, for a sci-fi example a \"brain modifying device which stimulates\" a form of <a href=\"http://wiki.lesswrong.com/wiki/Empathic_inference\" target=\"_blank\">empathic inference</a>, you can partially think the thoughts recorded on the piece of paper. Further yet you could simulate this particular person inside a powerful AI, even save that information to a disk, insert it into an android, and let that person go and live his/her life on. If this isn't sufficient you could engineer a biological being which would have a brain that produces a series of chemical reactions, enzymatic reactions, combined with an electron cloud etc that happens to be transitive with the logical structure of the data stored on the chip.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Meditation__And_this_creates_another_kind_of_problem__Did_the_person_come_into_existence_\">Meditation: And this creates another kind of problem. Did the person come into existence:</strong></p>\n<p>1. When the line drawn with the pencil came into existence?<br><br>2. When the entity that created the line thought of how to draw the line?<br><br>3. When the line was decoded?<br><br>4. Did it come into existence when the supercomputer AI simulated the person from the line?<br><br>5. When it was recorded onto the chip for the android?<br><br>6. Or lastly when the contents of the chip were translated to a biological brain producing the same thoughts?</p>\n<p><br><br><strong>Meditation: Is logic an ontologically basic thing?</strong></p>\n<p>In my book this is all a natural consequence from the reductionist approach to the hard problem of consciousness. That is unless we wan't to consider the possibility that electron clouds have little tags which say \"consciousness\" hanging from.. uhuh.. From their amplitudes.<br><br>So in other words: From the reductionist perspective there's just physics which can be described with the help of logic. Whenever there is a physical part of the universe that is correlated with the rest of the universe in such a way that it would resemble consciousness when interacted with, that thing would be just as much a person/zombie as we're. Same goes for simulated people.</p>", "sections": [{"title": "\"The Scary problem of Qualia\"", "anchor": "_The_Scary_problem_of_Qualia_", "level": 1}, {"title": "Meditation: And this creates another kind of problem. Did the person come into existence:", "anchor": "Meditation__And_this_creates_another_kind_of_problem__Did_the_person_come_into_existence_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "34 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tPqQdLCuxanjhoaNs", "rrW7yf42vQYDf8AcH", "fdEWWr8St59bXLbQr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-19T13:18:45.408Z", "modifiedAt": null, "url": null, "title": "Oversimplification when generalizing from DNA?", "slug": "oversimplification-when-generalizing-from-dna", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:43.462Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YpkNQYLZZZmQMko5L/oversimplification-when-generalizing-from-dna", "pageUrlRelative": "/posts/YpkNQYLZZZmQMko5L/oversimplification-when-generalizing-from-dna", "linkUrl": "https://www.lesswrong.com/posts/YpkNQYLZZZmQMko5L/oversimplification-when-generalizing-from-dna", "postedAtFormatted": "Wednesday, December 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Oversimplification%20when%20generalizing%20from%20DNA%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOversimplification%20when%20generalizing%20from%20DNA%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpkNQYLZZZmQMko5L%2Foversimplification-when-generalizing-from-dna%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Oversimplification%20when%20generalizing%20from%20DNA%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpkNQYLZZZmQMko5L%2Foversimplification-when-generalizing-from-dna", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpkNQYLZZZmQMko5L%2Foversimplification-when-generalizing-from-dna", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 541, "htmlBody": "<p>I changed the old topic because it was misleading and did not convey the questioning intention of this post. Sorry about that.</p>\n<p>The point of this post is to examine the proposition that people underestimate the complexity of living beings from examining them through the complexity of their functional DNA included in the genome alone. I don't have sufficient information to answer the question, but I have just about enough information to ask the question, so if you can do a better job drawnig a conclusion that'd be great. Also if you could point out technical errors that'd be nice too.</p>\n<p>&nbsp;</p>\n<p><strong>Genome</strong></p>\n<p>The genome contains the DNA which contains each invidual gene and serves as the currency of inherited qualities of the organism. That is evolutionary theories calculate around the frequency of genes and create formalisms, mathematical laws and so forth to predict and understand the phenomenom of natural selection or natural reproduction.</p>\n<p><br />Nothing wrong with this so far. But when it comes to actually thinking about the genes and the protein sequences, it seems to me that often it is forgotten that the entire cell which contaisn the DNA and the mitochondrial DNA and the intracellular devices are part of this replicatory system.</p>\n<p><br />To draw an unreliable <a href=\"/lw/rj/surface_analogies_and_deep_causes/\" target=\"_blank\">surface analogy</a> you could compare the replicatory process to a <a href=\"http://en.wikipedia.org/wiki/Cellular_automaton\" target=\"_blank\">cellular automata </a>you could think of the system as a generator which accepts a string of numbers to operate the generator. In this surface analogy the entire system is the final organism, the product of the automata, the invidual genes represent the fed in string of numbers and the other parts of the cell - DNA excluded - function as the generator which accepts the string of numbers. This analogue is poor because the distinction isn't real. But it only serves to illustrate a point. Which is that if you have just the string of genome that is contained in the DNA of a human being - you can not make a human being. Something is missing. The devices inside the cells, the mitochondrial DNA, the initial position - which is a fertilized ovum in a suitable environment like the womb.</p>\n<p>&nbsp;</p>\n<p>The point of the post and the <strong>proposition</strong> is the following:</p>\n<p><strong>The genome (mathematically) contains a smaller amount of data than is actually required for an organism as complex as the phenotype produced with the help of it to develop. </strong>To illustrate this with the previous surface analogy of a generator and a feed, the complexity of the generator contributes to the complexity of the final product with the fed string.<strong> </strong>And this leads to cognitive oversimplifying the complexity of an organis. But that analogue is inaccurate, and this proposition could be too.</p>\n<p>&nbsp;</p>\n<p><strong>So can you tell if this proposition is correct or incorrect?</strong></p>\n<p>I don't have sufficient knowledge in biology, evolutionary theory, mathematics and I just pretty much can't tell if this is true or completely false, but I'm intuitively anticipating a systemic underevaluation of the complexity of organisms in relation to the complexity of it's genome<strong> </strong>on these grounds. Note however I'm not saying that people think organisms as less complicated than they're, but in terms of mathematics when extrapolating from the genetic complexity, they'd underestimate their predictions. So what do you think?<strong> <br /></strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YpkNQYLZZZmQMko5L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "20753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>I changed the old topic because it was misleading and did not convey the questioning intention of this post. Sorry about that.</p>\n<p>The point of this post is to examine the proposition that people underestimate the complexity of living beings from examining them through the complexity of their functional DNA included in the genome alone. I don't have sufficient information to answer the question, but I have just about enough information to ask the question, so if you can do a better job drawnig a conclusion that'd be great. Also if you could point out technical errors that'd be nice too.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Genome\">Genome</strong></p>\n<p>The genome contains the DNA which contains each invidual gene and serves as the currency of inherited qualities of the organism. That is evolutionary theories calculate around the frequency of genes and create formalisms, mathematical laws and so forth to predict and understand the phenomenom of natural selection or natural reproduction.</p>\n<p><br>Nothing wrong with this so far. But when it comes to actually thinking about the genes and the protein sequences, it seems to me that often it is forgotten that the entire cell which contaisn the DNA and the mitochondrial DNA and the intracellular devices are part of this replicatory system.</p>\n<p><br>To draw an unreliable <a href=\"/lw/rj/surface_analogies_and_deep_causes/\" target=\"_blank\">surface analogy</a> you could compare the replicatory process to a <a href=\"http://en.wikipedia.org/wiki/Cellular_automaton\" target=\"_blank\">cellular automata </a>you could think of the system as a generator which accepts a string of numbers to operate the generator. In this surface analogy the entire system is the final organism, the product of the automata, the invidual genes represent the fed in string of numbers and the other parts of the cell - DNA excluded - function as the generator which accepts the string of numbers. This analogue is poor because the distinction isn't real. But it only serves to illustrate a point. Which is that if you have just the string of genome that is contained in the DNA of a human being - you can not make a human being. Something is missing. The devices inside the cells, the mitochondrial DNA, the initial position - which is a fertilized ovum in a suitable environment like the womb.</p>\n<p>&nbsp;</p>\n<p>The point of the post and the <strong>proposition</strong> is the following:</p>\n<p><strong>The genome (mathematically) contains a smaller amount of data than is actually required for an organism as complex as the phenotype produced with the help of it to develop. </strong>To illustrate this with the previous surface analogy of a generator and a feed, the complexity of the generator contributes to the complexity of the final product with the fed string.<strong> </strong>And this leads to cognitive oversimplifying the complexity of an organis. But that analogue is inaccurate, and this proposition could be too.</p>\n<p>&nbsp;</p>\n<p><strong id=\"So_can_you_tell_if_this_proposition_is_correct_or_incorrect_\">So can you tell if this proposition is correct or incorrect?</strong></p>\n<p>I don't have sufficient knowledge in biology, evolutionary theory, mathematics and I just pretty much can't tell if this is true or completely false, but I'm intuitively anticipating a systemic underevaluation of the complexity of organisms in relation to the complexity of it's genome<strong> </strong>on these grounds. Note however I'm not saying that people think organisms as less complicated than they're, but in terms of mathematics when extrapolating from the genetic complexity, they'd underestimate their predictions. So what do you think?<strong> <br></strong></p>\n<p>&nbsp;</p>", "sections": [{"title": "Genome", "anchor": "Genome", "level": 1}, {"title": "So can you tell if this proposition is correct or incorrect?", "anchor": "So_can_you_tell_if_this_proposition_is_correct_or_incorrect_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ByPxcGDhmx74gPSm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-20T03:23:28.998Z", "modifiedAt": null, "url": null, "title": "Standard and Nonstandard Numbers", "slug": "standard-and-nonstandard-numbers", "viewCount": null, "lastCommentedAt": "2020-05-13T13:52:55.998Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i7oNcHR3ZSnEAM29X/standard-and-nonstandard-numbers", "pageUrlRelative": "/posts/i7oNcHR3ZSnEAM29X/standard-and-nonstandard-numbers", "linkUrl": "https://www.lesswrong.com/posts/i7oNcHR3ZSnEAM29X/standard-and-nonstandard-numbers", "postedAtFormatted": "Thursday, December 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Standard%20and%20Nonstandard%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStandard%20and%20Nonstandard%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi7oNcHR3ZSnEAM29X%2Fstandard-and-nonstandard-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Standard%20and%20Nonstandard%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi7oNcHR3ZSnEAM29X%2Fstandard-and-nonstandard-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi7oNcHR3ZSnEAM29X%2Fstandard-and-nonstandard-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4609, "htmlBody": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a></p>\n<p>\"Oh! Hello. Back again?\"</p>\n<p>Yes, I've got another question. Earlier you said that you&nbsp;<em>had&nbsp;</em>to use second-order logic to define the numbers. But I'm pretty sure I've heard about something called 'first-order Peano arithmetic' which is also supposed to define the natural numbers. Going by the name, I doubt it has any 'second-order' axioms. Honestly, I'm not sure I understand this second-order business at all.</p>\n<p>\"Well, let's start by examining the following model:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/03/5.jpg\" alt=\"\" width=\"322\" height=\"335\" /></p>\n<p>\"This model has three properties that we would expect to be true of the standard numbers - 'Every number has a successor', 'If two numbers have the same successor they are the same number', and '0 is the only number which is not the successor of any number'. &nbsp;All three of these statements are true in this model, so in that sense it's quite numberlike -\"</p>\n<p>And yet this model clearly is <em>not</em>&nbsp;the numbers we are looking for, because it's got all these mysterious extra numbers like C and -2*. &nbsp;That C thing even loops around, which I certainly wouldn't expect any number to do. &nbsp;And then there's that infinite-in-both-directions chain which isn't corrected to anything else.</p>\n<p>\"Right, so, the difference between first-order logic and second-order logic is this: &nbsp;In first-order logic, we can get rid of the ABC - make a statement which <em>rules out</em> any model that has a loop of numbers like that. &nbsp;But we can't get rid of the infinite chain underneath it. &nbsp;In second-order logic we can get rid of the extra chain.\"<a id=\"more\"></a></p>\n<p>I would ask you to explain why that was true, but at this point I don't even know what second-order logic <em>is.</em></p>\n<p>\"Bear with me. &nbsp;First, consider that the following formula <em>detects 2-ness:\"</em></p>\n<p style=\"padding-left: 30px;\">x + 2 = x * 2</p>\n<p>In other words, that's a formula which is true when x is equal to 2, and false everywhere else, so it singles out 2?</p>\n<p>\"Exactly. &nbsp;And this is a formula which detects odd numbers:\"</p>\n<p style=\"padding-left: 30px;\">&exist;y:&nbsp;x=(2*y)+1</p>\n<p>Um... okay. &nbsp;That formula says, 'There exists a y, such that x equals 2 times y plus one.' &nbsp;And that's true when x is 1, because 0 is a number, and 1=(2*0)+1. &nbsp;And it's true when x is 9, because there exists a number 4 such that 9=(2*4)+1... right. &nbsp;The formula is true at all odd numbers, and only odd numbers.</p>\n<p>\"Indeed. &nbsp;Now suppose we had some way to <em>detect the existence</em> of that ABC-loop in the model - a formula which was&nbsp;<em>true</em>&nbsp;at the ABC-loop and <em>false</em>&nbsp;everywhere else. &nbsp;Then I could adapt the <em>negation</em>&nbsp;of this statement to say 'No objects like this are allowed to exist', and add that as an&nbsp;axiom alongside 'Every number has a successor' and so on. &nbsp;Then I'd have <em>narrowed down</em>&nbsp;the possible set of models to get rid of models that have an extra ABC-loop in them.\"</p>\n<p>Um... can I rule out the ABC-loop by saying&nbsp;&not;&exist;x:(x=A)?</p>\n<p>\"Er, only if you've told me what A is in the first place, and in a logic which has ruled out all models with loops in them, you shouldn't be able to point to a specific object that doesn't exist -\"</p>\n<p>Right. &nbsp;Okay... so the idea is to rule out loops of successors... hm. &nbsp;In the numbers 0, 1, 2, 3..., the number 0 isn't the successor of any number. &nbsp;If I just took a group of numbers starting at 1, like {1, 2, 3, ...}, then 1 wouldn't be the successor of any number <em>inside</em>&nbsp;that group. &nbsp;But in A, B, C, the number A is the successor of C, which is the successor of B, which is the successor of A. &nbsp;So how about if I say: &nbsp;'There's no group of numbers G such that for any number x in G, x is the successor of some other number y in G.'</p>\n<p>\"Ah! &nbsp;Very clever. &nbsp;But it so happens that you just used second-order logic, because you talked about <em>groups</em>&nbsp;or <em>collections</em>&nbsp;of entities, whereas <em>first-order logic</em>&nbsp;only talks about <em>individual</em>&nbsp;entities. &nbsp;Like, suppose we had a logic talking about kittens and whether they're innocent. &nbsp;Here's a model of a universe containing exactly three distinct kittens who are all innocent:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/ef/13_img7.jpg\" alt=\"\" width=\"540\" height=\"358\" /></p>\n<p>Er, what are those 'property' thingies?</p>\n<p>\"They're all possible collections of kittens. &nbsp;They're labeled <em>properties</em>&nbsp;because every collection of kittens corresponds to a property that some kittens have and some kittens don't. &nbsp;For example, the collection on the top right, which contains only the grey kitten, corresponds to a predicate which is true at the grey kitten and false everywhere else, or to a property which the grey kitten has which no other kitten has. &nbsp;Actually, for now let's just pretend that 'property' just says 'collection'.\"</p>\n<p>Okay. &nbsp;I understand the concept of a collection of kittens.</p>\n<p>\"In first-order logic, we can talk about individual kittens, and how they relate to other individual kittens, and whether or not any kitten bearing a certain relation exists or doesn't exist. &nbsp;For example, we can talk about how the grey kitten adores the brown kitten. &nbsp;In second-order logic, we can talk about collections of kittens, and whether or not those collections exist. &nbsp;So in first-order logic, I can say, 'There exists a kitten which is innocent', or 'For every individual kitten, that kitten is innocent', or 'For every individual kitten, there exists another individual kitten which adores the first kitten.' &nbsp;But it requires second-order logic to make statements about <em>collections</em>&nbsp;of kittens, like, 'There exists no collection of kittens such that every kitten in it is adored by some other kitten inside the collection.'\"</p>\n<p>I see. &nbsp;So when I tried to say that you couldn't have any group of numbers, such that every number in the group was a successor of some other number in the group...</p>\n<p>\"...you quantified over the existence or nonexistence of <em>collections</em>&nbsp;of numbers, which means you were using <em>second-order logic.</em>&nbsp; However, in this particular case, it's easily possible to rule out the ABC-loop of numbers using only first-order logic. &nbsp;Consider the formula:\"</p>\n<p style=\"padding-left: 30px;\">x=SSSx</p>\n<p>x plus 3 is equal to itself?</p>\n<p>\"Right. &nbsp;That's a first-order formula, since it doesn't talk about collections. &nbsp;And that formula is false at 0, 1, 2, 3... but true at A, B, and C.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/6b/12_img1.jpg\" alt=\"\" width=\"446\" height=\"364\" /></p>\n<p>What does the '+' mean?</p>\n<p>\"Er, by '+' I was trying to say, 'this formula works out to True' and similarly '&not;' was supposed to mean the formula works out to False. &nbsp;The general idea is that we now have a formula for detecting 3-loops, and distinguishing them from <em>standard&nbsp;</em>numbers like 0, 1, 2 and so on.\"</p>\n<p>I see. &nbsp;So by adding the new axiom,&nbsp;&not;&exist;x:x=SSSx, we could rule out all the models containing A, B, and C or any other 3-loop of nonstandard numbers.</p>\n<p>\"Right.\"</p>\n<p>But this seems like a rather arbitrary sort of axiom to add to a fundamental theory of arithmetic. &nbsp;I mean, I've never seen any attempt to describe the numbers which says, 'No number is equal to itself plus 3' as a basic premise. &nbsp;It seems like it should be a theorem, not an axiom.</p>\n<p>\"That's because it's brought in using a more general rule. &nbsp;In particular, first-order arithmetic has an <em>infinite axiom schema</em>&nbsp;- an infinite but computable scheme of axioms. &nbsp;Each axiom in the schema says, for a different first-order formula&nbsp;&Phi;(x) - pronounced 'phi of x' - that:\"</p>\n<ol>\n<li><em>If&nbsp;</em>&Phi; is true at 0, i.e: &nbsp;&Phi;(0)</li>\n<li><em>And if&nbsp;</em>&Phi; is true of the successor of any number where it's true, i.e: &nbsp;&nbsp;&forall;x:&nbsp;&Phi;(x)&rarr;&Phi;(Sx)</li>\n<li><em>Then</em>&nbsp;&Phi; is true of all numbers: &nbsp;&forall;n:&nbsp;&Phi;(n)</li>\n</ol>\n<p style=\"padding-left: 30px;\">(&Phi;(0) &and; (&forall;x: &Phi;(x) &rarr; &Phi;(Sx))) &rarr; (&forall;n: &Phi;(n))</p>\n<p>\"In other words, every <em>formula</em>&nbsp;which is true at 0, and which is true of the successor of any number of which it is true, is true <em>everywhere.</em>&nbsp; This is the <em>induction schema</em>&nbsp;of first-order arithmetic. &nbsp;As a special case we have the <em>particular</em>&nbsp;inductive axiom:\"</p>\n<p style=\"padding-left: 30px;\">(0&ne;SSS0&nbsp;&and; (&forall;x: (x&ne;SSSx)&nbsp;&rarr; (Sx&ne;SSSSx)) &rarr; (&forall;n: n&ne;SSSn)</p>\n<p>But that doesn't say that for all n, n&ne;n+3. &nbsp;It gives some premises from which that conclusion would follow, but we don't know the premises.</p>\n<p>\"Ah, however, we can <em>prove</em>&nbsp;those premises using the <em>other</em>&nbsp;axioms of arithmetic, and hence prove the conclusion. &nbsp;The formula (SSSx=x) is false at 0, because 0 is not the successor of&nbsp;<em>any&nbsp;</em>number, including SS0. &nbsp;Similarly, consider the formula SSSSx=Sx, which we can rearrange as S(SSSx)=S(x). &nbsp;If two numbers have the same successor they are the same number, so SSSx=x. If truth at Sx proves truth at x, then falsity at x proves falsity at Sx, modus ponens to modus tollens. &nbsp;Thus the formula is false at zero, false of the successor of any number where it's false, and so must be false everywhere under the induction axiom schema of first-order arithmetic. And so first-order arithmetic can rule out models like this:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/6b/12_img1.jpg\" alt=\"\" width=\"334\" height=\"273\" /></p>\n<p>...er, I think I see? &nbsp;Because if this model obeys all the <em>other</em>&nbsp;axioms which which we <em>already</em>&nbsp;specified, that <em>didn't</em>&nbsp;filter it out earlier - axioms like&nbsp;'zero is not the successor of any number' and 'if two numbers have the same successor they are the same number' - then we can <em>prove</em>&nbsp;that the formula x&ne;SSSx is true at 0, and prove that if the formula true at x it must be true at x+1. &nbsp;So once we then add the <em>further</em> axiom that <em>if</em>&nbsp;x&ne;SSSx is true at 0, and <em>if</em>&nbsp;x&ne;SSSx is true at Sy when it's true at y, <em>then</em>&nbsp;x&ne;SSSx is true at all x...</p>\n<p>\"We already have the premises, so we get the conclusion. &nbsp;&forall;x:&nbsp;x&ne;SSSx, and thus we filter out all the 3-loops. &nbsp;Similar logic rules out N-loops for all N.\"</p>\n<p>So then did we get rid of all the nonstandard numbers, and leave only the standard model?</p>\n<p>\"No. &nbsp;Because there was also that problem with the infinite chain ... -2*, -1*, 0*, 1* and so on.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/03/5.jpg\" alt=\"\" width=\"322\" height=\"335\" /></p>\n<p>Here's one idea for getting rid of the model with an infinite chain. &nbsp;All the nonstandard numbers in the chain are \"greater\" than all the standard numbers, right? &nbsp;Like, if <em>w</em> is a nonstandard number, then <em>w</em> &gt; 3, <em>w</em> &gt; 4, and so on?</p>\n<p>\"Well, we can prove by induction that no number is less than 0, and <em>w</em>&nbsp;isn't equal to 0 or 1 or 2 or 3,&nbsp;so I'd have to agree with that.\"</p>\n<p>Okay. &nbsp;We should also be able to prove that if x &gt; y then x + z &gt; y + z. &nbsp;So if we take nonstandard <em>w</em>&nbsp;and ask about <em>w</em> + <em>w</em>, then <em>w</em> + <em>w</em> must be greater than <em>w</em> + 3, <em>w</em> + 4, and so on. &nbsp;So <em>w</em> + <em>w</em> can't be part of the infinite chain at all, and yet adding any two numbers ought to yield a third number.</p>\n<p>\"Indeed, that does prove that if there's one infinite chain, there must be <em>two</em>&nbsp;infinite chains. &nbsp;In other words, that original, exact model in the picture, can't all by itself be a model of first-order arithmetic. &nbsp;But showing that the chain implies the existence of yet other elements, isn't the same as proving that the chain doesn't exist. &nbsp;Similarly, since all numbers are even or odd, we must be able to find <em>v</em> with <em>v</em> + <em>v </em>= <em>w</em>, or find <em>v</em> with <em>v</em> + <em>v</em> + 1 = <em>w</em>. &nbsp;Then <em>v</em> must be part of another nonstandard chain that comes before the chain containing <em>w</em>.\"</p>\n<p>But then that requires an <em>infinite</em>&nbsp;number of infinite chains of nonstandard numbers which are all greater than any standard number. &nbsp;Maybe we can extend this logic to eventually reach a contradiction and rule out the existence of an infinite chain in the first place - like, we'd show that any complete collection of nonstandard numbers has to be <em>larger than itself -</em></p>\n<p>\"Good idea, but no. &nbsp;You end up with the conclusion that if a single nonstandard number exists, it must be part of a chain that's infinite in both directions, i.e., a chain that looks like an ordered copy of the negative and positive integers. &nbsp;And that if an infinite chain exists, there must be infinite chains corresponding to all <em>rational numbers.</em>&nbsp; So something that could actually be a nonstandard model of first-order arithmetic, has to contain at least the standard numbers <em>followed by</em>&nbsp;a copy of the rational numbers with each rational number replaced by a copy of the integers. &nbsp;But then <em>that</em>&nbsp;setup works just fine with both addition and multiplication - we can't prove that it has to be any larger than what we've already said.\"</p>\n<p>Okay, so how <em>do</em>&nbsp;we get rid of an infinite number of infinite chains of nonstandard numbers, and leave just the standard numbers at the beginning? &nbsp;What kind of statement would they violate - what sort of axiom would rule out all those extra numbers?</p>\n<p>\"We have to use second-order logic for that one.\"</p>\n<p>Honestly I'm still not 100% clear on the difference.</p>\n<p>\"Okay... earlier you gave me a <em>formula</em>&nbsp;which detected odd numbers.\"</p>\n<p>Right. &nbsp;&exist;y:&nbsp;x=(2*y)+1, which was true at x=1, x=9 and so on, but not at x=0, x=4 and so on.</p>\n<p>\"When you think in terms of <em>collections of numbers,</em>&nbsp;well, there's <em>some </em>collections which can be defined by formulas. &nbsp;For example, the collection of odd numbers {1, 3, 5, 7, 9, ...} can be defined by the formula, with x free,&nbsp;&exist;y:&nbsp;x=(2*y)+1. &nbsp;But you could also try to talk about just the collection {1, 3, 5, 7, 9, ...} as a collection, a set of numbers, whether or not there happened to be any formula that defined it -\"</p>\n<p>Hold on, how can you talk about a set if you can't define a formula that makes something a member or a non-member? &nbsp;I mean, that seems a bit smelly from a rationalist perspective -</p>\n<p>\"Er... remember the earlier conversation about kittens?\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/ef/13_img7.jpg\" alt=\"\" width=\"405\" height=\"270\" /></p>\n<p>\"Suppose you say something like, 'There <em>exists a collection</em>&nbsp;of kittens, such that every kitten adores only other kittens in the collection'. &nbsp;Give me a room full of kittens, and I can count through all possible collections, check your statement for each collection, and see whether or not there's a collection which is actually like that. &nbsp;So the statement is meaningful - it can be falsified or verified, and it constrains the state of reality. &nbsp;But you didn't give me a <em>local formula</em>&nbsp;for picking up a <em>single</em>&nbsp;kitten and deciding whether or not it ought to be in this mysterious collection. &nbsp;I had to iterate through all the <em>collections </em>of kittens, find the&nbsp;<em>collections</em>&nbsp;that matched your statement, and only then could I decide whether any individual kitten had the property of being in a collection like that. &nbsp;But the statement was still falsifiable, even though it was, in mathematical parlance, <em>impredicative - </em>that's what we call it when you make a statement that can only be verified by looking at many possible collections, and doesn't start from any particular collection that you tell me how to construct.\"</p>\n<p>Ah... hm. &nbsp;What about infinite universes of kittens, so you can't iterate through all possible collections in finite time?</p>\n<p>\"If you say, 'There exists a collection of kittens which all adore each other', I could exhibit a group of three kittens which adored each other, and so prove the statement true. &nbsp;If you say 'There's a collection of four kittens who adore only each other', I might come up with a constructive proof, given the other known properties of kittens, that your statement was false; and any time you tried giving me a group of four kittens, I could find a fifth kitten, adored by some kitten in your group, that falsified your attempt. &nbsp;But this is getting us into some <a href=\"http://en.wikipedia.org/wiki/Arithmetical_hierarchy\">rather deep parts of math</a> we should probably stay out of for now. &nbsp;The point is that even in infinite universes, there are second-order statements that you can prove or falsify in finite amounts of time. &nbsp;And once you admit those <em>particular </em>second-order statements are talking about something meaningful, well, you might as well just admit that second-order statements in general are meaningful.\"</p>\n<p>...that sounds a little iffy to me, like we might get in trouble later on.</p>\n<p>\"You're not the only mathematician who worries about that.\"</p>\n<p>But let's get back to numbers. &nbsp;You say that we can use second-order logic to rule out any infinite chain.</p>\n<p>\"Indeed. &nbsp;In second-order logic, instead of using an infinite axiom schema over all formulas&nbsp;&Phi;, we quantify over <em>possible collections&nbsp;</em>directly, and say, in a <em>single</em>&nbsp;statement:\"</p>\n<p style=\"padding-left: 30px;\">&forall;P: P(0) &and; (&forall;x: P(x) &rarr; P(Sx)) &rarr; (&forall;n: P(n))</p>\n<p>\"Here P is any predicate true or false of individual numbers. &nbsp;Any collection of numbers corresponds to a predicate that is true of numbers inside the collection and false of numbers outside of it.\"</p>\n<p>Okay... and how did that rule out infinite chains again?</p>\n<p>\"Because <em>in principle,</em>&nbsp;whether or not there's any first-order formula that picks them out,&nbsp;there's&nbsp;<em>theoretically</em>&nbsp;a&nbsp;collection that contains the standard numbers {0, 1, 2, ...} and <em>only</em>&nbsp;the standard numbers. &nbsp;And if you treat that collection as a predicate P, then P is true at 0 - that is, 0 is in the standard numbers. &nbsp;And if 200 is a standard number then so is 201, and so on; if P is true at x, it's true at x+1. &nbsp;On the other hand, if you treat the collection 'just the standard numbers' as a predicate, it's false at -2*, false at -1*, false at 0* and so on - those numbers <em>aren't</em>&nbsp;in this theoretical collection. &nbsp;So it's vacuously true that this predicate is&nbsp;true at 1* if it's true at 0*, because it's <em>not</em> true at 0*. &nbsp;And so we end up with:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/af/12_img2a.jpg\" alt=\"\" width=\"324\" height=\"422\" /></p>\n<p>\"And so the single second-order axiom...\"</p>\n<p style=\"padding-left: 30px;\">&forall;P: P0 &and; (&forall;x: Px &rarr; P(Sx)) &rarr; (&forall;n: Pn)</p>\n<p>\"...rules out any disconnected chains, finite loops, and indeed every nonstandard number, in one swell foop.\"</p>\n<p>But what did that axiom&nbsp;<em>mean,</em>&nbsp;exactly? &nbsp;I mean, taboo the phrase 'standard numbers' for a moment, pretend I've got no idea what those are, just explain to me what the axiom actually&nbsp;<em>says</em>.</p>\n<p>\"It says that the model being discussed - the model which fits this axiom - makes it impossible to form <em>any collection closed under succession</em>&nbsp;which includes 0 and doesn't include <em>everything</em>. &nbsp;It's impossible to have <em>any collection of objects in this universe </em>such that&nbsp;0 is in the collection, and the successor of everything in the collection is in the collection, and yet this collection doesn't contain&nbsp;<em>everything.</em>&nbsp; So you can't have a disconnected infinite chain - there would then exist at least one collection over objects in this universe that contained 0 and all its successor-descendants, yet didn't contain the chain; and we have a shiny new axiom which says that can't happen.\"</p>\n<p>Can you perhaps operationalize that in a more <a href=\"/lw/bc3/sotw_be_specific/\">sensorymotory</a> sort of way? &nbsp;Like, if this is what I believe about the universe, then what do I expect to see?</p>\n<p>\"If this is what you believe about the mathematical model that you live in... then you believe that neither you, nor any adversary, nor yet a superintelligence, nor yet God, can consistently say 'Yea' or 'Nay' to objects in such fashion that when you present them with 0, they say 'Yea', and when you present them with any other object, if they say 'Yea', they also say 'Yea' for the successor of that object; and yet there is some object for which they say 'Nay'. &nbsp;You believe this can never happen, no matter what. &nbsp;The way in which the objects in the universe are arranged by succession, just doesn't let that happen, ever.\"</p>\n<p>Ah. &nbsp;So if, say, they said 'Nay' for 42, I'd go back and ask about 41, and then 40, and by the time I reached 0, I'd find either that they said 'Nay' about 0, or that they said 'Nay' for 41 and yet 'Yea' for 40. &nbsp;And what do I expect to see if I believe in first-order arithmetic, with the infinite axiom schema?</p>\n<p>\"In that case, you believe there's no neatly specifiable, compactly describable&nbsp;<em>rule</em>&nbsp;which behaves like that. &nbsp;But if you believe the second-order version, you believe nobody can possibly behave like that even if they're answering randomly, or branching the universe to answer different ways in different alternate universes, and so on. &nbsp;And note, by the way, that if we have a finite universe - i.e., we throw out the rule that <em>every</em>&nbsp;number has a successor, and say instead that 256 is the only number which has no successor - then we can verify this axiom in finite time.\"</p>\n<p>I see. &nbsp;Still, is there any way to rule out infinite chains using <em>first</em>-order logic? &nbsp;I might find that easier to deal with, even if it looks more complicated at first.</p>\n<p>\"I'm afraid not. &nbsp;One way I like to look at it is that&nbsp;first-order logic can talk about<em> constraints on how the model looks from any local point</em>, while only second-order logic can talk about&nbsp;<em>global qualities</em>&nbsp;of chains, collections, and the model as a whole. Whether every number has a successor is a local property - a question of how the model looks from the vantage point of any one number. Whether a number plus three, can be equal to itself, is a question you could evaluate at the local vantage point of any one number. &nbsp;Whether a number is <em>even, </em>is a question you can answer by looking around for a single, individual number x with the property that x+x equals the first number. But when you try to say that there's&nbsp;<em>only one connected chain</em>&nbsp;starting at 0, by invoking the idea of&nbsp;<em>connectedness&nbsp;</em>and&nbsp;<em>chains&nbsp;</em>you're trying to describe non-local properties that require a logic-of-possible-collections to specify.\"</p>\n<p>Huh. But if all the 'local' properties are the same regardless, why worry about global properties? In first-order arithmetic, any 'local' formula that's true at zero and all of its 'natural' successors would also have to be true of all the disconnected infinite chains... right? &nbsp;Or did I make an error there? &nbsp;All the other infinite chains besides the 0-chain - all 'nonstandard numbers' - would have just the same properties as the 'natural' numbers, right?</p>\n<p>\"I'm afraid not. The first-order axioms of arithmetic may fail to pin down whether or not a Turing machine halts - whether there&nbsp;<em>exists a time</em>&nbsp;at which a Turing machine halts. Let's say that from our perspective inside the standard numbers, the Turing machine 'really doesn't' halt - it doesn't halt on clock tick 0, doesn't halt on clock tick 1, doesn't halt on tick 2, and so on through all the standard successors of the 0-chain. In nonstandard models of the integers - models with other infinite chains - there might be somewhere inside a <em>nonstandard chain</em> where the Turing machine goes from running to halted and stays halted thereafter.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/e4/12_img3.jpg\" alt=\"\" width=\"452\" height=\"468\" /></p>\n<p>\"In this new model - which is fully compatible with the first-order axioms, and can't be ruled out by them - it's not true that 'for every number t at which the Turing machine is running, it will still be running at t+1'. &nbsp;Even though if we could somehow restrict our attention to the 'natural' numbers, we would see that the Turing machine was running at 0, 1, 2, and every time in the successor-chain of 0.\"</p>\n<p>Okay... I'm not quite sure what the&nbsp;<em>practical&nbsp;</em>implication of that is?</p>\n<p>\"It means that many Turing machines which&nbsp;<em>in fact</em>&nbsp;never halt at any standard time, can't be&nbsp;<em>proven not to halt</em>&nbsp;using first-order reasoning, because their non-halting-ness&nbsp;<em>does not actually follow logically</em>&nbsp;from the first-order axioms. &nbsp;Logic is about&nbsp;<a href=\"/lw/f43/proofs_implications_and_models/\">which conclusions follow from which premises</a>, remember? If there are models which are compatible with all the first-order premises, but still falsify the statement 'X runs forever', then the statement 'X runs forever' can't <em>logically follow</em> from those premises. This means you won't be able to prove - <em>shouldn't</em>&nbsp;be able to prove - that this Turing machine halts, using <em>only </em>first-order logic.\"</p>\n<p>How exactly would this fail in practice? &nbsp;I mean, where does the proof go bad?</p>\n<p>\"You wouldn't get the second step of the induction, 'for every number t at which the Turing machine is running, it will still be running at t+1'. &nbsp;There'd be nonstandard models with some nonstandard t that falsifies the premise - a nonstandard time where the Turing machine goes from running to halted. &nbsp;Even though if we could somehow restrict our attention to <em>only the standard numbers</em>, we would see that the Turing machine was running at 0, 1, 2, and so on.\"</p>\n<p>But if a Turing machine really actually&nbsp;halts,&nbsp;there's got to be some <em>particular time</em>&nbsp;when it halts, like on step 97 -</p>\n<p>\"Indeed. &nbsp;But 97 exists in <em>all </em>nonstandard models of arithmetic, so we can prove its existence in first-order logic. &nbsp;Any time 0 is a number, every number has a successor, numbers don't loop, and so on, there'll exist 97. &nbsp;Every nonstandard model has <em>at least</em> the standard numbers. &nbsp;So whenever a Turing machine <em>does</em>&nbsp;halt, you can prove in first-order arithmetic that it halts - it does indeed follow from the premises. &nbsp;That's kinda what you'd <em>expect,</em>&nbsp;given that you can just watch the Turing machine for 97 steps. &nbsp;When something actually does halt, you <em>should</em>&nbsp;be able to prove it halts without worrying about unbounded future times! &nbsp;It's when something <em>doesn't actually</em>&nbsp;halt - in the standard numbers, that is - that the existence of 'nonstandard halting times' becomes a problem. &nbsp;Then, the conclusion that the Turing machine runs forever&nbsp;<em>may not actually follow</em>&nbsp;from first-order arithmetic, because you can obey all the premises of first-order arithmetic, and yet still be inside a nonstandard model where this Turing machine halts at a nonstandard time.\"</p>\n<p>So second-order arithmetic is more powerful than first-order arithmetic in terms of&nbsp;<em>what follows from the premises</em>?</p>\n<p>\"That follows inevitably from the ability to talk about&nbsp;<em>fewer possible models</em>. As it is written, 'What is true of one apple may not be true of another apple; thus&nbsp;<a href=\"/lw/ic/the_virtue_of_narrowness/\">more can be said about a single apple than about all the apples in the world</a>.' If you can restrict your discourse to a narrower collection of models, there are more facts that follow inevitably, because the more models you might be talking about, the fewer facts can possibly be true about all of them. And it's also definitely true that second-order arithmetic proves more theorems than first-order arithmetic - for example, it can prove that a Turing machine which computes&nbsp;<a href=\"http://en.wikipedia.org/wiki/Goodstein's_theorem\">Goodstein sequences</a> always reaches 0 and halts, or that Hercules always wins the <a href=\"http://math.andrej.com/2008/02/02/the-hydra-game/\">hydra game</a>. &nbsp;But there's a bit of controversy we'll get into later about whether second-order logic is&nbsp;<em>actually&nbsp;</em>more powerful than first-order logic in general.\"</p>\n<p>Well, sure. After all, just because nobody has ever yet invented a first-order formula to filter out all the nonstandard numbers, doesn't mean it can never, ever be done. Tomorrow some brilliant mathematician might figure out a way to take an individual number x, and do local things to it using addition and multiplication and the existence or nonexistence of other individual numbers, which can tell us whether that number is part of the 0-chain or some other infinite-in-both-directions chain. &nbsp;It'll be as easy as (a=b*c)&nbsp;-</p>\n<p>\"Nope. Ain't never gonna happen.\"</p>\n<p>But maybe you could find some entirely different creative way of first-order axiomatizing the numbers which has&nbsp;<em>only&nbsp;</em>the standard model -</p>\n<p>\"Nope.\"</p>\n<p>Er... how do you&nbsp;<em>know&nbsp;</em>that, exactly? I mean, part of the Player Character Code is that you don't give up when something&nbsp;<em>seems&nbsp;</em>impossible. I can't quite see&nbsp;<em>yet&nbsp;</em>how to detect infinite chains using a first-order formula. But then earlier I didn't realize you could rule out finite loops, which turned out to be quite simple once you explained. After all, there's two distinct uses of the word 'impossible', one which indicates positive knowledge that something can&nbsp;<em>never&nbsp;</em>be done, that no&nbsp;<em>possible</em>&nbsp;chain of actions can&nbsp;<em>ever&nbsp;</em>reach a goal, even if you're a superintelligence. This kind of knowledge requires a strong, definite grasp on the subject matter, so that you can rule out&nbsp;<em>every&nbsp;</em>possible avenue of success. And then there's another,&nbsp;<em>much more common</em>&nbsp;use of the word 'impossible', which means that you thought about it for five seconds but didn't see any way to do it, usually used in the presence of&nbsp;<em>weak&nbsp;</em>grasps on a subject, subjects that seem sacredly mysterious -</p>\n<p>\"Right. Ruling out an infinite-in-both-directions chain, using a first-order formula, is the&nbsp;<em>first&nbsp;</em>kind of impossibility. We&nbsp;<em>know&nbsp;</em>that it can never be done.\"</p>\n<p>I see. Well then, what do you think you know, and how do you think you know it? How is this definite, positive knowledge of impossibility obtained, using your strong grasp on the non-mysterious subject matter?</p>\n<p>\"We'll take that up next time.\"</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/g1y/godels_completeness_and_incompleteness_theorems/\">Godel's Completeness and Incompleteness Theorems</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/fv3/by_which_it_may_be_judged/\">By Which It May Be Judged</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 5, "yXNtYNHJB54T3bGm3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i7oNcHR3ZSnEAM29X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 65, "extendedScore": null, "score": 0.000152, "legacy": true, "legacyId": "20754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "godel-s-completeness-and-incompleteness-theorems", "canonicalPrevPostSlug": "logical-pinpointing", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3FoMuCLqZggTxoC3S", "NgtYDP3ZtLJaM248W", "Z2CuyKtkCmWGQtAEh", "yDfxTj9TKYsYiWH5o", "GZjGtd35vhCnzSQKy", "zqwWicCLNBSA5Ssmn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-20T13:48:25.271Z", "modifiedAt": null, "url": null, "title": "[Link] 'Something feels wrong with the state of philosophy today.'", "slug": "link-something-feels-wrong-with-the-state-of-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:33.954Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alanog", "createdAt": "2012-11-04T09:11:34.552Z", "isAdmin": false, "displayName": "alanog"}, "userId": "zEuen8T4tKfzX3DPZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bTmven46QBokRXjDy/link-something-feels-wrong-with-the-state-of-philosophy", "pageUrlRelative": "/posts/bTmven46QBokRXjDy/link-something-feels-wrong-with-the-state-of-philosophy", "linkUrl": "https://www.lesswrong.com/posts/bTmven46QBokRXjDy/link-something-feels-wrong-with-the-state-of-philosophy", "postedAtFormatted": "Thursday, December 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20'Something%20feels%20wrong%20with%20the%20state%20of%20philosophy%20today.'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20'Something%20feels%20wrong%20with%20the%20state%20of%20philosophy%20today.'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTmven46QBokRXjDy%2Flink-something-feels-wrong-with-the-state-of-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20'Something%20feels%20wrong%20with%20the%20state%20of%20philosophy%20today.'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTmven46QBokRXjDy%2Flink-something-feels-wrong-with-the-state-of-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTmven46QBokRXjDy%2Flink-something-feels-wrong-with-the-state-of-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>http://blog.talkingphilosophy.com/?p=6424</p>\n<p>Some interesting thoughts on the current state of philosophy, and Less Wrong gets cited a couple of times.</p>\n<p>\n<blockquote>\n<p style=\"margin: 0px 0px 22px; padding: 0px; border: 0px; font-size: 13px; vertical-align: baseline; line-height: 22px; color: #333333; font-family: Tahoma, Arial, Helvetica, sans-serif; background-color: #f7f7f7; text-align: justify;\">Something feels wrong with the state of philosophy today. From whence hast this sense of ill-boding come?</p>\n<p style=\"margin: 0px 0px 22px; padding: 0px; border: 0px; font-size: 13px; vertical-align: baseline; line-height: 22px; color: #333333; font-family: Tahoma, Arial, Helvetica, sans-serif; background-color: #f7f7f7; text-align: justify;\">For this month&rsquo;s Carnival, we shall survey a selection of recent posts that are loosely arranged around the theme of existential threats to contemporary philosophy. I focus on four. Pre-theoretic intuitions seem a little less credible as sources of evidence. Talk about possible worlds seems just a bit less scientific. The very idea of rationality looks as though it is being taken over by cognate disciplines, like cognitive science and psychology. And some of the most talented philosophers of the last generation have taken up arms against a scientific theory that enjoys a strong consensus.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bTmven46QBokRXjDy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 1.0630905603841744e-06, "legacy": true, "legacyId": "20766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-20T18:53:05.496Z", "modifiedAt": null, "url": null, "title": "Beware Selective Nihilism", "slug": "beware-selective-nihilism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uXxoLPKAdunq6Lm3s/beware-selective-nihilism", "pageUrlRelative": "/posts/uXxoLPKAdunq6Lm3s/beware-selective-nihilism", "linkUrl": "https://www.lesswrong.com/posts/uXxoLPKAdunq6Lm3s/beware-selective-nihilism", "postedAtFormatted": "Thursday, December 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beware%20Selective%20Nihilism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeware%20Selective%20Nihilism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXxoLPKAdunq6Lm3s%2Fbeware-selective-nihilism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beware%20Selective%20Nihilism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXxoLPKAdunq6Lm3s%2Fbeware-selective-nihilism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXxoLPKAdunq6Lm3s%2Fbeware-selective-nihilism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 387, "htmlBody": "<p>In a <a href=\"/lw/fyb/ontological_crisis_in_humans/\">previous post</a>, I argued that nihilism is often short changed around here. However I'm far from certain that it is correct, and in the mean time I think we should be careful not to discard our values one at a time by engaging in \"selective nihilism\" when faced with an ontological crisis, without even realizing that's what's happening. Karl recently reminded me of the post <a href=\"/lw/qx/timeless_identity/\">Timeless Identity</a> by Eliezer Yudkowsky, which I noticed seems to be an instance of this.</p>\n<p>As I mentioned in the previous post, our values seem to be defined in terms of a world model where people exist as ontologically primitive entities ruled heuristically by (mostly intuitive understandings of) physics and psychology. In this kind of decision system, both identity-as-physical-continuity and identity-as-psychological-continuity make perfect sense as possible values, and it seems humans do \"natively\" have both values. A typical human being is both&nbsp;reluctant&nbsp;to step into a teleporter that works by destructive scanning, and unwilling to let their physical structure be continuously modified into a&nbsp;psychologically very different being.&nbsp;</p>\n<p>If faced with the knowledge that&nbsp;physical continuity doesn't exist in the real world at the level of fundamental physics, one might conclude that it's crazy to continue to value it, and this is what Eliezer's post argued. But if we apply this reasoning in a non-selective fashion, wouldn't we also conclude that we should stop valuing things like \"pain\" and \"happiness\" which also do not seem to exist at the level of&nbsp;fundamental physics?</p>\n<p>In our current environment, there is widespread agreement&nbsp;among&nbsp;humans as to which macroscopic objects at time t+1 are physical&nbsp;continuations&nbsp;of which macroscopic objects existing at time t. We may not fully understand what exactly it is we're doing when judging such physical continuity, and the agreement tends to break down when we start talking about more exotic situations, and if/when we do fully understand our criteria for judging physical continuity it's unlikely to have a simple&nbsp;definition&nbsp;in terms of fundamental physics, but all of this is true&nbsp;for \"pain\" and \"happiness\" as well.</p>\n<p>I suggest we keep all of our (potential/apparent) values intact until we have a better handle on how we're supposed to deal with ontological crises in general. If we convince ourselves that we should discard some value, and that turns out to be wrong, the error may be unrecoverable once we've lived with it long enough.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uXxoLPKAdunq6Lm3s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 58, "extendedScore": null, "score": 0.000155, "legacy": true, "legacyId": "20768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KLaJjNdENsHhKhG5m", "924arDrTu3QRHFA5r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-20T20:14:44.064Z", "modifiedAt": null, "url": null, "title": "Gun Control: How would we know?", "slug": "gun-control-how-would-we-know", "viewCount": null, "lastCommentedAt": "2018-02-17T17:14:33.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlpowell", "createdAt": "2009-03-05T00:57:26.519Z", "isAdmin": false, "displayName": "rlpowell"}, "userId": "nFgyJtHMChgKrhnvt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ck7Jp6xtiiq69xZGc/gun-control-how-would-we-know", "pageUrlRelative": "/posts/ck7Jp6xtiiq69xZGc/gun-control-how-would-we-know", "linkUrl": "https://www.lesswrong.com/posts/ck7Jp6xtiiq69xZGc/gun-control-how-would-we-know", "postedAtFormatted": "Thursday, December 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gun%20Control%3A%20How%20would%20we%20know%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGun%20Control%3A%20How%20would%20we%20know%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fck7Jp6xtiiq69xZGc%2Fgun-control-how-would-we-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gun%20Control%3A%20How%20would%20we%20know%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fck7Jp6xtiiq69xZGc%2Fgun-control-how-would-we-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fck7Jp6xtiiq69xZGc%2Fgun-control-how-would-we-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 454, "htmlBody": "<p>I don't know how to keep this topic away from <a title=\"Politics As Mind Killer\" href=\"/lw/gw/politics_is_the_mindkiller/\">http://lesswrong.com/lw/gw/politics_is_the_mindkiller/</a> , so I'm just going to exhort everyone to try to keep this about <strong>rationality</strong> and not about <strong>politics as usual</strong>.&nbsp; I myself have strong opinions here, which I'm deliberately squelching.</p>\n<p>So I got to thinking about the issue of gun control in the wake of a recent school shooting in the US, specifically from the POV of minimizing presumed-innocents getting randomly shot.&nbsp; Please limit discussion to that *specific* issue, or we'll be here all year.</p>\n<p>My question is not so much \"Is strict gun control or lots of guns better for us [in the sole context of minimizing presumed-innocents getting randomly shot]?\", although I'm certainly interested in knowing the answer to that, but I think if that was answerable we as a culture wouldn't still be arguing about it.</p>\n<p>Let's try a different question, though: how would we know?</p>\n<p>That is, what non-magical statistical evidence could someone give that would actually settle the question reasonably well (let's say, at about the same level as \"smoking causes cancer\", or so)?</p>\n<p>As a first pass I looked at <a title=\"countries by homocide rate\" href=\"http://en.wikipedia.org/wiki/List_of_countries_by_intentional_homicide_rate\">http://en.wikipedia.org/wiki/List_of_countries_by_intentional_homicide_rate</a> and <a title=\"countries by gun homocide rate\" href=\"http://en.wikipedia.org/wiki/List_of_countries_by_firearm-related_death_rate\">http://en.wikipedia.org/wiki/List_of_countries_by_firearm-related_death_rate</a> and I noted that the US, which is famously kind of all about the guns, has significantly higher rates than other first-world countries.&nbsp; I had gone into this with a deliberate desire to <strong>win</strong>, in the less wrong sense, so I accepted that this strongly speaks against my personal beliefs (my default stance is that all teachers should have concealed carry permits and mandatory shooting range time requirements), and was about to update (well, utterly obliterate) those beliefs, when I went \"Now, hold on.&nbsp; In the context of first world countries, the US has relatively lax gun control, and we seem to rather enjoy killing each other.&nbsp; How do I know those are causally related, though?&nbsp; Is it not just as likely that, for example, we have all the homicidally crazy people, and that that leads to both of those things?&nbsp; It doesn't seem to be the case that, say, in the UK, you have large-scale secret hoarding of guns; if that was the case, they'd be closer to use in gun-related homicides, I would think.&nbsp; But just because it didn't happen in the UK doesn't mean it wouldn't happen here.\"</p>\n<p>At that point I realized that I don't know, <strong>even in theory</strong>, how to tell what the answer to my question is, or what evidence would be strong evidence for one position or the other.&nbsp; I am not strong enough as a rationalist or a statistician.</p>\n<p>So, I thought I'd ask LW, which is full of people better at those things than I am.&nbsp; :)</p>\n<p>Have at.</p>\n<p>-Robin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ck7Jp6xtiiq69xZGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 17, "extendedScore": null, "score": 1.0633167317447568e-06, "legacy": true, "legacyId": "20770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 169, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T01:59:01.889Z", "modifiedAt": null, "url": null, "title": "newcomb's altruism", "slug": "newcomb-s-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:53.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oJsb9ahCTG8Lobppx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BYqbkhar7PLn7moyN/newcomb-s-altruism", "pageUrlRelative": "/posts/BYqbkhar7PLn7moyN/newcomb-s-altruism", "linkUrl": "https://www.lesswrong.com/posts/BYqbkhar7PLn7moyN/newcomb-s-altruism", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20newcomb's%20altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Anewcomb's%20altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBYqbkhar7PLn7moyN%2Fnewcomb-s-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=newcomb's%20altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBYqbkhar7PLn7moyN%2Fnewcomb-s-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBYqbkhar7PLn7moyN%2Fnewcomb-s-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p>Hello lesswrong community. I want to play a game. *jigsaw music plays*</p>\n<p>In box B, I have placed either $1,000,000 multiplied by the number of humans on earth or $0. In box A I placed $1000 multiplied by the number of humans on earth. Every human on the planet, including you, will now be asked the following question: do you take just box B, or both? If my friend omega predicted the majority of humans would one-box, box B has the aforementioned quantity of money, and it will be split accordingly (everyone in the world receives $1,000,000). If he predicted the majority of humans would two-box, it has nothing. Everyone who two-boxes receives $1000 in addition to whether or not the $1,000,000 was obtained. In fact forget the predicting, let's just say I'll tally up the votes and *then* decide whether to put the money in box B or not. Would it then be rational to two-box or one-box? If I told you that X is the proportion of humans that one-box in the classical newcomb's problem, should that affect your strategy? What if I told you that Y is the proportion of humans so far that have one-boxed out of those who have chosen so far? Would it even be morally permissible to two-box? Also, let's assume the number of humans are odd (since I know someone's going to ask what happens in a tie).</p>\n<p>I do also have a follow-up in both cases. If you chose two-box, let's say I stopped your decision short to tell you that there are N other instances of yourself in the world, for I have cloned you secretly and without consent (sue me). How big would N have to be for you to one-box? If you chose one-box, and I stopped your decision short to say N people in the world have already two-boxed, or have already one-boxed, how big would N have to be for you to decide your effect is inconsequential and two-box?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BYqbkhar7PLn7moyN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "20772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T03:44:12.843Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Bad Guy Bias", "slug": "seq-rerun-the-bad-guy-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:34.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JetifWMGryZksXboL/seq-rerun-the-bad-guy-bias", "pageUrlRelative": "/posts/JetifWMGryZksXboL/seq-rerun-the-bad-guy-bias", "linkUrl": "https://www.lesswrong.com/posts/JetifWMGryZksXboL/seq-rerun-the-bad-guy-bias", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Bad%20Guy%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Bad%20Guy%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJetifWMGryZksXboL%2Fseq-rerun-the-bad-guy-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Bad%20Guy%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJetifWMGryZksXboL%2Fseq-rerun-the-bad-guy-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJetifWMGryZksXboL%2Fseq-rerun-the-bad-guy-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/the-bad-guy-bia.html\">The Bad Guy Bias</a> was originally published on December 9, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Humans have a tendency to perceive tragedies caused by agents as worse than tragedies caused by other sources. This can cause us to, among other things, worry more about future catastrophes as a result of malevolent agents than as a result of unplanned events.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fzf/seq_rerun_true_sources_of_disagreement/\">True Sources of Disagreement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JetifWMGryZksXboL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0635799917727442e-06, "legacy": true, "legacyId": "20774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FFBJRBqD65k4eEvLP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T04:45:35.493Z", "modifiedAt": null, "url": null, "title": "Against NHST", "slug": "against-nhst", "viewCount": null, "lastCommentedAt": "2019-07-16T17:15:46.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst", "pageUrlRelative": "/posts/ttvnPRTxFyru9Hh2H/against-nhst", "linkUrl": "https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20NHST&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20NHST%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttvnPRTxFyru9Hh2H%2Fagainst-nhst%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20NHST%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttvnPRTxFyru9Hh2H%2Fagainst-nhst", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttvnPRTxFyru9Hh2H%2Fagainst-nhst", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1243, "htmlBody": "<html><head></head><body><blockquote>\n<p>A summary of standard non-Bayesian criticisms of common frequentist statistical practices, with pointers into the academic literature.</p>\n</blockquote>\n<p>Frequentist statistics is a wide field, but in practice by innumerable psychologists, biologists, economists etc, frequentism tends to be a particular style called \u201cNull Hypothesis Significance Testing\u201d (NHST) descended from R.A. Fisher (as opposed to eg. Neyman-Pearson) which is focused on</p>\n<ol>\n<li>setting up a null hypothesis and an alternative hypothesis</li>\n<li>calculating a <em>p</em>-value (possibly via a _&lt;_a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test\"&gt;t-test or more complex alternatives like <a href=\"https://en.wikipedia.org/wiki/Analysis_of_variance\">ANOVA</a>)</li>\n<li>and rejecting the null if an arbitrary threshold is passed.</li>\n</ol>\n<p>NHST became nearly universal between the 1940s &amp; 1960s (see Gigerenzer 2004, pg18), and has been <a href=\"http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Controversy\">heavily criticized</a> for as long. Frequentists criticize it for:</p>\n<ol>\n<li>practitioners &amp; <a href=\"http://library.mpib-berlin.mpg.de/ft/gg/GG_Null_2004.pdf\">statistics teachers</a> misinterpret the meaning of a <em>p</em>-value (LessWrongers too); Cohen on this persistent illusion:</li>\n</ol>\n<p>What\u2019s wrong with NHST? Well, among other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is, \u201cGiven these data, what is the probability that <em>H0</em> is true?\u201d But as most of us know, what it tells us is \u201cGiven that <em>H0</em> is true, what is the probability of these (or more extreme) data?\u201d These are not the same\u2026</p>\n<blockquote>\n<p>(This misunderstanding is incredibly widespread; once you understand it, you'll see it everywhere. I can't count how many times I have seen a comment or blog explaining that a <em>p</em>=0.05 means \"the probability of the null hypothesis not being true is 95%\", in many different variants.)</p>\n</blockquote>\n<ol>\n<li>cargo-culting the use of 0.05 as an accept/reject threshold based on historical accident &amp; custom (rather than using a <a href=\"https://en.wikipedia.org/wiki/Loss_function\">loss function</a> chosen through decision theory to set the threshold based on the cost of false positives).</li>\n</ol>\n<p>Similarly, the cargo-culting encourages misuse of two-tailed tests, avoidance of <a href=\"https://en.wikipedia.org/wiki/Multiple_comparisons\">multiple correction</a>, <a href=\"http://en.wikipedia.org/wiki/Data_dredging\">data dredging</a>, and in general, \u201c<em>p</em>-value hacking\u201d.</p>\n<ol>\n<li>failing to compare many possible hypotheses or models, and limiting themselves to one - sometimes ill-chosen or absurd - null hypothesis and one alternative</li>\n<li>deprecating the value of exploratory data analysis and depicting data graphically (see, for example, <a href=\"https://en.wikipedia.org/wiki/Anscombe%27s_quartet\">Anscombe\u2019s quartet</a>)</li>\n<li>ignoring the more important summary statistic of <a href=\"http://en.wikipedia.org/wiki/Effect_size\">\u201ceffect size\u201d</a></li>\n<li>ignoring the more important summary statistic of <a href=\"http://en.wikipedia.org/wiki/Confidence_interval\">confidence intervals</a>; this is related to how use of <em>p</em>-values leads to ignorance of the <a href=\"https://en.wikipedia.org/wiki/Statistical_power\">statistical power</a> of a study - a small study may have only a small chance of detecting an effect if it exists, but turn in misleadingly good-looking <em>p</em>-values</li>\n<li>because null hypothesis tests cannot accept the alternative, but only reject a null, they inevitably cause false alarms upon repeated testing</li>\n</ol>\n<p>(An example from my personal experience of the cost of ignoring effect size and confidence intervals: <em>p</em>-values cannot (easily) be used to compile a <a href=\"https://en.wikipedia.org/wiki/Meta-analysis\">meta-analysis</a> (pooling of multiple studies); hence, studies often do not include the necessary information about means, standard deviations, or effect sizes &amp; confidence intervals which one could use directly. So authors must be contacted, and they may refuse to provide the information or they may no longer be available; both have happened to me in trying to do my <a href=\"http://www.gwern.net/DNB%20FAQ#meta-analysis\">dual n-back</a> &amp; <a href=\"http://www.gwern.net/Iodine\">iodine</a> meta-analyses.)</p>\n<p>Critics\u2019 explanations for why a flawed paradigm is still so popular focus on the ease of use and its weakness; from Gigerenzer 2004:</p>\n<blockquote>\n<p>Hays (1963) had a chapter on Bayesian statistics in the second edition of his widely read textbook but dropped it in the subsequent editions. As he explained to one of us (GG) he dropped the chapter upon pressure from his publisher to produce a statistical cookbook that did not hint at the existence of alternative tools for statistical inference. Furthermore, he believed that many researchers are not interested in statistical thinking in the first place but solely in getting their papers published (Gigerenzer, 2000)\u2026When Loftus (1993) became the editor of <em>Memory &amp; Cognition</em>, he made it clear in his editorial that he did not want authors to submit papers in which <em>p</em>-, <em>t</em>-, or <em>F</em>-values are mindlessly being calculated and reported. Rather, he asked researchers to keep it simple and report figures with error bars, following the proverb that \u201ca picture is worth more than a thousand p-values.\u201d We admire Loftus for having had the courage to take this step. Years after, one of us (GG) asked Loftus about the success of his crusade against thoughtless significance testing. Loftus bitterly complained that most researchers actually refused the opportunity to escape the ritual. Even when he asked in his editorial letter to get rid of dozens of <em>p</em>-values, the authors insisted on keeping them in. There is something deeply engrained in the minds of many researchers that makes them repeat the same action over and over again.</p>\n</blockquote>\n<p>Shifts away from NHST have happened in some fields. Medical testing seems to have made such a shift (I suspect due to the rise of meta-analysis):</p>\n<blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2004-fidler.pdf\">Fidler et al. (2004b, 626)</a> explain the spread of the reform in part by a shift from testing to estimation that was facilitated by the medical literature, unlike psychology, using a common measurement scale, to \u201cstrictly enforced editorial policy, virtually simultaneous reforms in a number of leading journals, and the timely re-writing [of] textbooks to fit with policy recommendations.\u201d But their description of the process suggests that an accidental factor, the coincidence of several strong-willed editors, also mattered. For the classic collection of papers criticizing significance tests in psychology see Morrison and Hankel (1970) [<em>The Significance Test Controversy: A Reader</em>], and for a more recent collection of papers see Harlow et al. (1997) [<em>What If There Were No Significance Tests?</em>]. <a href=\"http://203.64.159.11/richman/plogxx/gallery/17/%E9%AB%98%E7%B5%B1%E5%A0%B1%E5%91%8A.pdf\">Nickerson (2000)</a> provides a comprehensive survey of this literature.</p>\n</blockquote>\n<h2><a href=\"https://www.lesswrong.com/editPost?eventForm&amp;postId=ttvnPRTxFyru9Hh2H#TOC\">0.1 Further reading</a></h2>\n<p>More on these topics:</p>\n<ul>\n<li>Cohen, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.597&amp;rep=rep1&amp;type=pdf\">\u201cThe Earth Is Round (p&lt;.05)\u201d</a> (recommended)</li>\n<li><a href=\"http://effectsizefaq.com/\">Effect size FAQ</a> (published as <em>The Essential Guide to Effect Sizes</em>, Ellis)</li>\n<li><a href=\"http://maximum-entropy-blog.blogspot.nl/2012/07/higgs-boson-at-5-sigmas.html\">\u201cThe Higgs Boson at 5 Sigmas\u201d</a></li>\n<li><em>The Cult of Statistical Significance</em>, McCloskey &amp; Ziliak 2008; <a href=\"http://econjwatch.org/file_download/588/MayerSept2012.pdf\">criticism</a>, their <a href=\"http://econjwatch.org/file_download/589/McCloskeyZiliakSept2012.pdf\">reply</a></li>\n<li><a href=\"http://www.indiana.edu/~kru%3Cem%3Es%3C/em%3Echke/BEST/BEST.pdf\">\u201cBayesian estimation supersedes the t test\u201d</a>, Kruschke 2012 (see also <em><a href=\"http://libgen.info/view.php?id=729330\">Doing Bayesian Data Analysis</a></em>); an exposition of a Bayesian paradigm, simulation of false alarm performance compared to his Bayesian code; an excerpt:</li>\n</ul>\n<p>The perils of NHST, and the merits of Bayesian data analysis, have been expounded with increasing force in recent years (e.g., <a href=\"http://www.stat.cmu.edu/~fienberg/Statistics36-756/Edwards_Lindman1963.pdf\">W. Edwards, Lindman, &amp; Savage, 1963</a>; Kruschke, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.2836&amp;rep=rep1&amp;type=pdf\">2010b</a>, <a href=\"http://www.indiana.edu/~kruschke/articles/Kruschke2010TiCS.pdf\">2010a</a>, <a href=\"http://dionysus.psych.wisc.edu/methods/statistics/Bayes/Workshop/Articles/Perspectives%20on%20Psychological%20Science-2011-Kruschke-272-3.pdf\">2011c</a>; <a href=\"http://ejwagenmakers.com/2005/TrafimowComment.pdf\">Lee &amp; Wagenmakers, 2005</a>; <a href=\"http://www.brainlife.org/reprint/2007/Wagenmakers_EJ071000.pdf\">Wagenmakers, 2007</a>).</p>\n<ul>\n<li>A useful bibliography from <a href=\"https://pdfs.semantics%3Cem%3Ec%3C/em%3Eholar.org/c790/d0c2efabbdd09db43b03637b847ac3837d07.pdf\">\"A peculiar prevalence of p values just below .05\"</a>, Masicampo &amp; Lalande 2012:</li>\n</ul>\n<p>Although the primary emphasis in psychology is to publish results on the basis of NHST (<a href=\"http://dl.dropbox.com/u/85192141/2007-cumming.pdf\">Cumming et al., 2007</a>; <a href=\"http://dl.dropbox.com/u/85192141/1979-rosenthal.pdf\">Rosenthal, 1979</a>), the use of NHST has long been controversial. Numerous researchers have argued that reliance on NHST is counterproductive, due in large part because <em>p</em> values fail to convey such useful information as effect size and likelihood of replication (<a href=\"http://dl.dropbox.com/u/85192141/1963-clark.pdf\">Clark, 1963</a>; <a href=\"http://dl.dropbox.com/u/85192141/2008-cumming.pdf\">Cumming, 2008</a>; <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1473027/\">Killeen, 2005</a>; Kline, 2009 [<em>Becoming a behavioral science researcher: A guide to producing research that matters</em>]; <a href=\"http://stats.org.uk/statistical-inference/Rozeboom1960.pdf\">Rozeboom, 1960</a>). Indeed, some have argued that NHST has severely impeded scientific progress (Cohen, 1994; <a href=\"http://www2.psych.ubc.ca/~schaller/528Readings/Schmidt1996.pdf\">Schmidt, 1996</a>) and has confused interpretations of clinical trials (<a href=\"http://dl.dropbox.com/u/85192141/2011-cicchetti.pdf\">Cicchetti et al., 2011</a>; <a href=\"http://jnci.oxfordjournals.org/content/103/1/16.full\">Ocana &amp; Tannock, 2011</a>). Some researchers have stated that it is important to use multiple, converging tests alongside NHST, including effect sizes and confidence intervals (<a href=\"http://wiki.bio.dtu.dk/~agpe/papers/pval_notuseful.pdf\">Hubbard &amp; Lindsay, 2008</a>; Schmidt, 1996). Others still have called for NHST to be completely abandoned (e.g., <a href=\"http://scholasticadministrator.typepad.com/thisweekineducation/files/the_case_against_statistical_significance_testing.pdf\">Carver, 1978</a>).</p>\n<ul>\n<li>\n<p><code>[http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology](http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology)</code></p>\n</li>\n<li>\n<p><code>[https://www.reddit.com/r/DecisionTheory/](https://www.reddit.com/r/DecisionTheory/)</code></p>\n</li>\n</ul>\n</body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ttvnPRTxFyru9Hh2H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 92, "extendedScore": null, "score": 0.000233, "legacy": true, "legacyId": "20775", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 92, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T11:48:17.319Z", "modifiedAt": null, "url": null, "title": "Dying in Many Worlds", "slug": "dying-in-many-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:48.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j9Cu2v6w4q4oyz8rX/dying-in-many-worlds", "pageUrlRelative": "/posts/j9Cu2v6w4q4oyz8rX/dying-in-many-worlds", "linkUrl": "https://www.lesswrong.com/posts/j9Cu2v6w4q4oyz8rX/dying-in-many-worlds", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dying%20in%20Many%20Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADying%20in%20Many%20Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj9Cu2v6w4q4oyz8rX%2Fdying-in-many-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dying%20in%20Many%20Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj9Cu2v6w4q4oyz8rX%2Fdying-in-many-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj9Cu2v6w4q4oyz8rX%2Fdying-in-many-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1542, "htmlBody": "<p>I feel extremely embarrassed about asking for help with this, but I have a philosophical quandary that has been eating at me for days.&nbsp; I'm sure that many of you already have it figured out.&nbsp; I would appreciate it if you would lend your <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached thoughts</a> to me, because I can't seem to resolve it.</p>\n<p>What has been bothering me are the implications of combining two common views here on Less Wrong.&nbsp; The first is that the <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">Many Worlds Interpretation</a> of Quantum mechanics is correct.&nbsp; The second is that two identical copies of a <a href=\"/lw/qx/timeless_identity/\">person count as the same person</a>, and that therefore you haven't really died if you manage to make another version of yourself that survives while an earlier version dies&nbsp; (for instance, if you sign up for cryonics and then in the future an AI scans your frozen corpse and uses the data to synthesize a new version of you). Robin Hanson has even <a href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy/\">argued</a> that it would be morally acceptable to create billions of brain emulations of one person for a specific task and then erase them afterward; as long as at least one copy of the emulators remains alive then all you've really done is give one person \"mild amnesia.\"&nbsp; Both these viewpoints seem plausible to me, although I am less sure of Hanson's rather radical extensions of the second view.</p>\n<p>Combing these views has the potential for disturbing implications.&nbsp; If the MWI is correct then there already are large amounts of versions of everyone somewhere out there.&nbsp; <strong>This has filled me with the distressing thought that the badness of death might somehow be diminished because of this. </strong>I am aware that Eliezer has written articles that seem to explain why this is not the case (\"<a href=\"/lw/qz/living_in_many_worlds/\">Living in Many Worlds</a>\" and \"<a href=\"/lw/ws/for_the_people_who_are_still_alive/\">For the People Who are Still Alive</a>,\") but I have read them and am having trouble grasping his arguments.</p>\n<p>It seems obvious to me that it is bad to kill someone under normal circumstances, and that the badness of their death does not decrease because there are other parts of the multiverse containing duplicates of them.&nbsp; Eliezer seems to agree, and I think Robin (who has stated he supports the MWI and has <a href=\"http://hanson.gmu.edu/mangledworlds.html\">contributed to the work on the subject</a>) does too.&nbsp; I very much doubt that if Robin Hanson was greeted by a knife-wielding maniac who announced that he intended to \"give mild amnesia to alternate universe versions of Robin and his family\" that he would make any less effort to save himself and his family than another version of Robin who did not support the MWI.</p>\n<p>On the other hand, the argument that making other versions of yourself before you die is a form of survival seems persuasive to me as well. I think that if cryonics works it might be a form of survival, as would having a brain emulation of yourself made.</p>\n<p>The line of reasoning that first pushed me down this disturbing line of thought was a thought experiment I was considering where <a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega </a>gives you a choice between:</p>\n<p>1. Adding fifty years of your life that you would spend achieving large, important, impressive accomplishments.</p>\n<p>2. Adding 200 years to your life that you would spend being stuck in a \"time loop,\" repeating one (reasonably good) day of your life over and over again, with your memory erased and your surroundings \"reset\" at the beginning of every morning to ensure you live each day exactly the same.&nbsp;</p>\n<p>I concluded that I would probably prefer the first option, even though it was shorter, because a <a href=\"/lw/xr/in_praise_of_boredom/\">life where you do new things and accomplish goals is better than one endlessly repeated</a> (I would prefer the 200 repetitive years to being killed outright though). This thought experiment led me to conclude that, for many people, a life where one makes progress in one's life and accomplishes things is better than one where the same experiences are repeated endlessly.&nbsp;</p>\n<p>\"But wait!\" I thought, \"We already are sort of living in a time loop!&nbsp; If the MWI is correct than there are countless copies of us all having the exact same experiences repeated endlessly!&nbsp; Does that mean that if killing someone allowed you to lead a more accomplished life, that it would be alright, because all you'd be doing is reducing the amount of repetitions in an already repeating life?&nbsp; This seems obviously wrong to me, there must be something wrong with my analogy.</p>\n<p>I have made a list of possible reasons why death might still be bad if copies of you in other worlds survive, but not be as bad if you have made copies of yourself in the same world.&nbsp; I have also made a second list of reasons why the thought experiment I just described isn't a good analogy to MWI.&nbsp; I'd appreciate it if anyone had any ideas as to whether or not I'm on the right track.</p>\n<p>The first list, in order of descending plausibility:</p>\n<p>1. I am a moron who doesn't understand quantum physics or the MWI properly, and if I did understand them properly this conundrum wouldn't be bothering me.</p>\n<p>2. When someone is duplicated through MWI all the relevant factors in their environment (other people, resources, infrastructure, etc.) are duplicated as well.&nbsp; Because of this, <em>the moral worth of an action in one world out of many is exactly the same as what it would be if there was only one world.&nbsp;</em> This seems very plausible to me, but I wish I could see a more formal argument for it.</p>\n<p>3. The fact that the multiple worlds cannot currently, and probably never will be able to, interact in any significant way, makes it such that the moral worth of an action in one world out of many is exactly the same as what it would be if there was only one world. I think this <em>might </em>be what Eliezer was talking about when he <a href=\"/lw/qz/living_in_many_worlds/\">said</a>: \"I would suggest that you consider every world which is not in your future, to be part of the 'generalized past.'\", but I'm not sure.</p>\n<p>4. 2&amp;3 combined.</p>\n<p>5. If the only version of you in a world dies then you cease to be able to impact that world in any way (ie, continue important projects, stay in touch with your loved ones).&nbsp; This is not the case with technological duplicates living in the same world.&nbsp; This seems <em>slightly </em>plausible to me; but it still seems like it would still be wrong to kill someone who had no strong social ties or important projects in life, regardless of how many of them might exist in other worlds.</p>\n<p>6.&nbsp; It's impossible to just kill <em>just one</em> version of a person in the multiverse.&nbsp; Any death in one world will result in a vast amount of other deaths as the worlds continue to diverge.</p>\n<p>7. Some kind of <a href=\"http://en.wikipedia.org/wiki/Original_position\">quasi-Rawlsian </a>argument where one should try to maximize one's average wellbeing in the worlds one is \"born into.\"&nbsp; I think Eliezer might have made such an argument in \"<a href=\"/lw/ws/for_the_people_who_are_still_alive/\">For the People Who Are Still Alive</a>.\"</p>\n<p>8.&nbsp; Survival via making copies <em>is </em>a form of survival, but it's a crappy type of survival that is inferior to never having the original be destroyed in the first place.&nbsp; It's sort of like an accident victim losing their legs, it's good they are alive, and their future life will probably be worth living (at least in a first world country with modern medicine), but it would be a lot better if they survived without losing their legs.</p>\n<p>9.&nbsp; It's good to have as many copies of yourself as possible, so killing one is always bad.&nbsp; This seems implausible to me. If I discovered someone was going to try use technology to run off a large amount of versions of themselves, and stole large amounts of resources to do so and radically decreased the quality of other people's lives, then it would be right to stop them.&nbsp; Also, it seems to me that if I could spend money to duplicate myself I would devote some money to that, but devote some other money to enriching the lives of existing copies.</p>\n<p>&nbsp;</p>\n<p>The second list (as to why my \"Timeloop\" thought experiment isn't analogous to MWI).</p>\n<p>1.&nbsp; Again, it's impossible to just kill <em>just one</em> version of a person in the multiverse. Killing someone to improve your life in Many Worlds would be like having Omega stick two people in a \"Time Loop,\" and then have one kill the other at the start of every morning.</p>\n<p>2. One life of accomplishment is better than one repetitive life, but it isn't better than <em>two </em>repetitive lives.</p>\n<p>3. <a href=\"http://en.wikipedia.org/wiki/Prioritarianism\">Prioritarianism</a> is closer to the <a href=\"/lw/sm/the_meaning_of_right/\">Meaning of Right</a> than pure utilitarianism, so ever if one life of accomplishment is better than two repetitive lives, it's still wrong to kill someone to improve your own life.</p>\n<p>&nbsp;</p>\n<p>Again, I would really appreciate it if someone could explain this for me.&nbsp; This problem has really been upsetting me.&nbsp; I have trouble focusing on other things and its negatively affecting my mood.&nbsp; I know that the fact that I can be affected so severely by an abstract issue is probably a sign of deeper psychological problems that I should have looked at.&nbsp; But I think realizing that this problem isn't really a problem, and that it all adds up to normality, is a good first step.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 1, "PbShukhzpLsWpGXkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j9Cu2v6w4q4oyz8rX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 25, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "20404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9cgBF6BQ2TRB3Hy4E", "924arDrTu3QRHFA5r", "ynyemLY8YWX8rQ84f", "qcYCAxYZT4Xp9iMZY", "cfZ8zveqrTZbQrjeD", "WMDy4GxbyYkNrbmrs", "fG3g3764tSubr6xvs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T16:37:07.460Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Brussels, Durham, London, Melbourne, NYC, Paderborn, Vancouver", "slug": "weekly-lw-meetups-austin-brussels-durham-london-melbourne", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MjqNFGwCpoXkvxuAC/weekly-lw-meetups-austin-brussels-durham-london-melbourne", "pageUrlRelative": "/posts/MjqNFGwCpoXkvxuAC/weekly-lw-meetups-austin-brussels-durham-london-melbourne", "linkUrl": "https://www.lesswrong.com/posts/MjqNFGwCpoXkvxuAC/weekly-lw-meetups-austin-brussels-durham-london-melbourne", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Durham%2C%20London%2C%20Melbourne%2C%20NYC%2C%20Paderborn%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Durham%2C%20London%2C%20Melbourne%2C%20NYC%2C%20Paderborn%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjqNFGwCpoXkvxuAC%2Fweekly-lw-meetups-austin-brussels-durham-london-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Durham%2C%20London%2C%20Melbourne%2C%20NYC%2C%20Paderborn%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjqNFGwCpoXkvxuAC%2Fweekly-lw-meetups-austin-brussels-durham-london-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjqNFGwCpoXkvxuAC%2Fweekly-lw-meetups-austin-brussels-durham-london-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p><strong>This summary was posted to LW Main on Dec 14th, and has been moved to discussion. The following week's summary is <a href=\"/lw/g1e/weekly_lw_meetups_cleveland_moscow/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/gc\">Brussels meetup:&nbsp;<span class=\"date\">15 December 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/h0\">Durham HPMoR Discussion, chapters 21-23:&nbsp;<span class=\"date\">15 December 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/gx\">Vancouver Last call for 2012:&nbsp;<span class=\"date\">16 December 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/gs\">16/12 London Meetup:&nbsp;<span class=\"date\">16 December 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/gz\">Paderborn Meetup, December 19th:&nbsp;<span class=\"date\">19 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">15 December 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/gw\">Melbourne social meetup:&nbsp;<span class=\"date\">21 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/gy\">Cleveland Ohio Meetup:&nbsp;<span class=\"date\">23 December 2012 02:52PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MjqNFGwCpoXkvxuAC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0640329544971052e-06, "legacy": true, "legacyId": "20656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMzGaJYDAnTmTZCLt", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T17:12:59.979Z", "modifiedAt": null, "url": null, "title": " Implications of an infinite versus a finite universe", "slug": "implications-of-an-infinite-versus-a-finite-universe", "viewCount": null, "lastCommentedAt": "2020-09-03T22:03:27.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hcpkw9NxxDAe4TxbP/implications-of-an-infinite-versus-a-finite-universe", "pageUrlRelative": "/posts/hcpkw9NxxDAe4TxbP/implications-of-an-infinite-versus-a-finite-universe", "linkUrl": "https://www.lesswrong.com/posts/hcpkw9NxxDAe4TxbP/implications-of-an-infinite-versus-a-finite-universe", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Implications%20of%20an%20infinite%20versus%20a%20finite%20universe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Implications%20of%20an%20infinite%20versus%20a%20finite%20universe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhcpkw9NxxDAe4TxbP%2Fimplications-of-an-infinite-versus-a-finite-universe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Implications%20of%20an%20infinite%20versus%20a%20finite%20universe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhcpkw9NxxDAe4TxbP%2Fimplications-of-an-infinite-versus-a-finite-universe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhcpkw9NxxDAe4TxbP%2Fimplications-of-an-infinite-versus-a-finite-universe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 358, "htmlBody": "<p>Hi gang,</p>\n<p>for the last several months, I've intermittently been wondering about a curious fact I learned.&nbsp;</p>\n<p>You see, I was under the impression that the universe (as opposed to just the currently observable universe or our Hubble volume) must be finite in its spa[t|c]ial dimensions. I figured that starting with a finite area of space, expanding with a finite (even if accelerating) expansion rate, could only yield a finitely sized volume of space (from any reference frame), a fraction of which constitutes our little Hubble bubble.</p>\n<p>Turns out - honi soit qui mal y pense - my layman's understanding was <a href=\"http://map.gsfc.nasa.gov/universe/uni_shape.html\" target=\"_blank\">wrong</a>: \"This [WMAP data] suggests that the Universe is infinite in extent (...)\"</p>\n<p>Now, most (non-computer-scientist) people I've bothered with that answered along the lines of \"well, it's really big alright? (geez)\".</p>\n<p>However, going from any <em>finite</em> amount of matter/energy to an actual <em>infinite</em> amount (even when looking at just e.g. baryonic matter from the infinite amount of galaxies) still seems like a game-changer for all sorts of contemplations:</p>\n<p>For example, any event with any non-zero probability of happening, no matter how large the negative exponent, would be assured of <em>actually</em> happening an infinite amount of times somewhere in the our very own universe (follows straightforwardly from induction over the law of large numbers).</p>\n<p>Such as a planet turning into a giant petunia for a moment, before turning back.&nbsp;</p>\n<p>The universe being infinite doesn't make that event any more likely in our observable universe, of course, but would the knowledge that given our laws of physics, there is an infinite amount of Hubble spaces governed by any sorts of \"weird\" occurrences - e.g. ruled by your evil twin brother - trouble you? Do we need to qualify \"there probably is no Christian-type/FSM god\" with \"... in our Hubble volume. Elsewhere, yes.\"?</p>\n<p>The difference, if you allow me a final rephrase, would be in going from a MWI-style \"there may be another version - if the MWI interpretation is correct - that I cannot causally interact with\" to a \"in our <em>own</em> universe, just separated by space, there is an infinite amount of <em>actual planets</em> turning into <em>actual petuniae</em>&nbsp;(albeit all of which I also cannot interact with)\".</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hcpkw9NxxDAe4TxbP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 2, "extendedScore": null, "score": 1.0640539874464198e-06, "legacy": true, "legacyId": "20787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-21T23:18:00.649Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC meetup- new sequence?", "slug": "meetup-washington-dc-meetup-new-sequence-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdY4r8amEy57DYW78/meetup-washington-dc-meetup-new-sequence-0", "pageUrlRelative": "/posts/SdY4r8amEy57DYW78/meetup-washington-dc-meetup-new-sequence-0", "linkUrl": "https://www.lesswrong.com/posts/SdY4r8amEy57DYW78/meetup-washington-dc-meetup-new-sequence-0", "postedAtFormatted": "Friday, December 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdY4r8amEy57DYW78%2Fmeetup-washington-dc-meetup-new-sequence-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdY4r8amEy57DYW78%2Fmeetup-washington-dc-meetup-new-sequence-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdY4r8amEy57DYW78%2Fmeetup-washington-dc-meetup-new-sequence-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/h9'>Washington DC meetup- new sequence?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 December 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Topic is tentative.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/h9'>Washington DC meetup- new sequence?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdY4r8amEy57DYW78", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.06426802787611e-06, "legacy": true, "legacyId": "20788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_\">Discussion article for the meetup : <a href=\"/meetups/h9\">Washington DC meetup- new sequence?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 December 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Topic is tentative.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_1\">Discussion article for the meetup : <a href=\"/meetups/h9\">Washington DC meetup- new sequence?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC meetup- new sequence?", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_", "level": 1}, {"title": "Discussion article for the meetup : Washington DC meetup- new sequence?", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T04:39:29.271Z", "modifiedAt": null, "url": null, "title": "No apparent Dunning-Kruger effect for LW participation", "slug": "no-apparent-dunning-kruger-effect-for-lw-participation", "viewCount": null, "lastCommentedAt": "2019-10-01T15:20:40.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZZpno2ii7BWRwcARP/no-apparent-dunning-kruger-effect-for-lw-participation", "pageUrlRelative": "/posts/ZZpno2ii7BWRwcARP/no-apparent-dunning-kruger-effect-for-lw-participation", "linkUrl": "https://www.lesswrong.com/posts/ZZpno2ii7BWRwcARP/no-apparent-dunning-kruger-effect-for-lw-participation", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20apparent%20Dunning-Kruger%20effect%20for%20LW%20participation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20apparent%20Dunning-Kruger%20effect%20for%20LW%20participation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZpno2ii7BWRwcARP%2Fno-apparent-dunning-kruger-effect-for-lw-participation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20apparent%20Dunning-Kruger%20effect%20for%20LW%20participation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZpno2ii7BWRwcARP%2Fno-apparent-dunning-kruger-effect-for-lw-participation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZpno2ii7BWRwcARP%2Fno-apparent-dunning-kruger-effect-for-lw-participation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<p>Precommitted to publishing this in Discussion to fight publication bias.&nbsp; It looks like intelligence (as measured by IQ, SAT scores, etc.) isn't meaningfully related to how much one posts to LW.&nbsp; Probably in the ideal case, they <em>would</em> be related and higher-IQ people would post more, but that doesn't appear to be going on either.</p>\n<p>How well-educated you are doesn't seem to be much related to participation either.&nbsp; I'm not controlling for hours spent on LW for any of this, though.</p>\n<p>Script output:</p>\n<pre>Correlation&nbsp;between&nbsp;\"IQ\"&nbsp;and&nbsp;\"KarmaScore\":&nbsp;0.0343\n&nbsp;\nCorrelation&nbsp;between&nbsp;\"SATscoresoutof1600\"&nbsp;and&nbsp;\"KarmaScore\":&nbsp;0.0517\n&nbsp;\nCorrelation&nbsp;between&nbsp;\"SATscoresoutof2400\"&nbsp;and&nbsp;\"KarmaScore\":&nbsp;0.1000\n&nbsp;\nCorrelation&nbsp;between&nbsp;\"TimeinCommunity\"&nbsp;and&nbsp;\"KarmaScore\":&nbsp;0.1770\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"IQ\"&nbsp;by&nbsp;\"LessWrongUse\":\n&nbsp;&nbsp;137.2500&nbsp;&nbsp;&nbsp;&nbsp;\"I&nbsp;lurk,&nbsp;but&nbsp;never&nbsp;registered&nbsp;an&nbsp;account\"\n&nbsp;&nbsp;139.3659&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;registered&nbsp;an&nbsp;account,&nbsp;but&nbsp;never&nbsp;posted\"\n&nbsp;&nbsp;138.8491&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;a&nbsp;comment,&nbsp;but&nbsp;never&nbsp;a&nbsp;top-level&nbsp;post\"\n&nbsp;&nbsp;137.8182&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Discussion,&nbsp;but&nbsp;not&nbsp;Main\"\n&nbsp;&nbsp;138.9394&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Main\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"SATscoresoutof1600\"&nbsp;by&nbsp;\"LessWrongUse\":\n&nbsp;1469.5495&nbsp;&nbsp;&nbsp;&nbsp;\"I&nbsp;lurk,&nbsp;but&nbsp;never&nbsp;registered&nbsp;an&nbsp;account\"\n&nbsp;1462.1429&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;registered&nbsp;an&nbsp;account,&nbsp;but&nbsp;never&nbsp;posted\"\n&nbsp;1488.0000&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;a&nbsp;comment,&nbsp;but&nbsp;never&nbsp;a&nbsp;top-level&nbsp;post\"\n&nbsp;1510.2941&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Discussion,&nbsp;but&nbsp;not&nbsp;Main\"\n&nbsp;1515.1515&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Main\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"SATscoresoutof2400\"&nbsp;by&nbsp;\"LessWrongUse\":\n&nbsp;2202.9848&nbsp;&nbsp;&nbsp;&nbsp;\"I&nbsp;lurk,&nbsp;but&nbsp;never&nbsp;registered&nbsp;an&nbsp;account\"\n&nbsp;2242.7273&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;registered&nbsp;an&nbsp;account,&nbsp;but&nbsp;never&nbsp;posted\"\n&nbsp;2211.8000&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;a&nbsp;comment,&nbsp;but&nbsp;never&nbsp;a&nbsp;top-level&nbsp;post\"\n&nbsp;2244.5455&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Discussion,&nbsp;but&nbsp;not&nbsp;Main\"\n&nbsp;2212.7273&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Main\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"KarmaScore\"&nbsp;by&nbsp;\"Sequences\":\n&nbsp;&nbsp;&nbsp;&nbsp;0.0000&nbsp;&nbsp;&nbsp;&nbsp;\"Never&nbsp;even&nbsp;knew&nbsp;they&nbsp;existed&nbsp;until&nbsp;this&nbsp;moment\"\n&nbsp;&nbsp;&nbsp;&nbsp;2.4444&nbsp;&nbsp;&nbsp;&nbsp;\"Know&nbsp;they&nbsp;existed,&nbsp;but&nbsp;never&nbsp;looked&nbsp;at&nbsp;them\"\n&nbsp;&nbsp;&nbsp;48.9677&nbsp;&nbsp;&nbsp;&nbsp;\"Some,&nbsp;but&nbsp;less&nbsp;than&nbsp;25%\"\n&nbsp;&nbsp;105.9658&nbsp;&nbsp;&nbsp;&nbsp;\"About&nbsp;25%&nbsp;of&nbsp;the&nbsp;Sequences\"\n&nbsp;&nbsp;280.6434&nbsp;&nbsp;&nbsp;&nbsp;\"About&nbsp;50%&nbsp;of&nbsp;the&nbsp;Sequences\"\n&nbsp;&nbsp;704.3240&nbsp;&nbsp;&nbsp;&nbsp;\"About&nbsp;75%&nbsp;of&nbsp;the&nbsp;Sequences\"\n&nbsp;1185.0264&nbsp;&nbsp;&nbsp;&nbsp;\"Nearly&nbsp;all&nbsp;of&nbsp;the&nbsp;Sequences\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"TimeinCommunity\"&nbsp;by&nbsp;\"LessWrongUse\":\n&nbsp;&nbsp;&nbsp;17.3262&nbsp;&nbsp;&nbsp;&nbsp;\"I&nbsp;lurk,&nbsp;but&nbsp;never&nbsp;registered&nbsp;an&nbsp;account\"\n&nbsp;&nbsp;&nbsp;23.6875&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;registered&nbsp;an&nbsp;account,&nbsp;but&nbsp;never&nbsp;posted\"\n&nbsp;&nbsp;&nbsp;30.0064&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;a&nbsp;comment,&nbsp;but&nbsp;never&nbsp;a&nbsp;top-level&nbsp;post\"\n&nbsp;&nbsp;&nbsp;29.5035&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Discussion,&nbsp;but&nbsp;not&nbsp;Main\"\n&nbsp;&nbsp;&nbsp;44.9663&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Main\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"AutismScore\"&nbsp;by&nbsp;\"LessWrongUse\":\n&nbsp;&nbsp;&nbsp;24.3504&nbsp;&nbsp;&nbsp;&nbsp;\"I&nbsp;lurk,&nbsp;but&nbsp;never&nbsp;registered&nbsp;an&nbsp;account\"\n&nbsp;&nbsp;&nbsp;28.0526&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;registered&nbsp;an&nbsp;account,&nbsp;but&nbsp;never&nbsp;posted\"\n&nbsp;&nbsp;&nbsp;22.7227&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;a&nbsp;comment,&nbsp;but&nbsp;never&nbsp;a&nbsp;top-level&nbsp;post\"\n&nbsp;&nbsp;&nbsp;23.7917&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Discussion,&nbsp;but&nbsp;not&nbsp;Main\"\n&nbsp;&nbsp;&nbsp;23.7391&nbsp;&nbsp;&nbsp;&nbsp;\"I've&nbsp;posted&nbsp;in&nbsp;Main\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"KarmaScore\"&nbsp;by&nbsp;\"Profession\":\n&nbsp;&nbsp;200.9000&nbsp;&nbsp;&nbsp;&nbsp;\"Other&nbsp;\"social&nbsp;science\"\"\n&nbsp;&nbsp;368.1176&nbsp;&nbsp;&nbsp;&nbsp;\"Biology\"\n&nbsp;&nbsp;898.8333&nbsp;&nbsp;&nbsp;&nbsp;\"Statistics\"\n&nbsp;&nbsp;373.3000&nbsp;&nbsp;&nbsp;&nbsp;\"Art\"\n&nbsp;&nbsp;441.0035&nbsp;&nbsp;&nbsp;&nbsp;\"Computers&nbsp;(practical:&nbsp;IT,&nbsp;programming,&nbsp;etc.)\"\n&nbsp;&nbsp;193.5263&nbsp;&nbsp;&nbsp;&nbsp;\"Business\"\n&nbsp;&nbsp;260.2281&nbsp;&nbsp;&nbsp;&nbsp;\"Finance&nbsp;/&nbsp;Economics\"\n&nbsp;1129.8438&nbsp;&nbsp;&nbsp;&nbsp;\"Computers&nbsp;(AI)\"\n&nbsp;1719.5161&nbsp;&nbsp;&nbsp;&nbsp;\"Philosophy\"\n&nbsp;&nbsp;113.8507&nbsp;&nbsp;&nbsp;&nbsp;\"Computers&nbsp;(other&nbsp;academic,&nbsp;computer&nbsp;science)\"\n&nbsp;&nbsp;351.9024&nbsp;&nbsp;&nbsp;&nbsp;\"Engineering\"\n&nbsp;&nbsp;335.8081&nbsp;&nbsp;&nbsp;&nbsp;\"Other\"\n&nbsp;&nbsp;531.9570&nbsp;&nbsp;&nbsp;&nbsp;\"Mathematics\"\n&nbsp;2505.8571&nbsp;&nbsp;&nbsp;&nbsp;\"Medicine\"\n&nbsp;&nbsp;393.6364&nbsp;&nbsp;&nbsp;&nbsp;\"Neuroscience\"\n&nbsp;&nbsp;&nbsp;81.5000&nbsp;&nbsp;&nbsp;&nbsp;\"Law\"\n&nbsp;&nbsp;530.1607&nbsp;&nbsp;&nbsp;&nbsp;\"Physics\"\n&nbsp;&nbsp;498.2941&nbsp;&nbsp;&nbsp;&nbsp;\"Other&nbsp;\"hard&nbsp;science\"\"\n&nbsp;1033.7391&nbsp;&nbsp;&nbsp;&nbsp;\"Psychology\"\n&nbsp;\nBreakdown&nbsp;of&nbsp;average&nbsp;\"KarmaScore\"&nbsp;by&nbsp;\"Degree\":\n&nbsp;&nbsp;612.8599&nbsp;&nbsp;&nbsp;&nbsp;\"Bachelor's\"\n&nbsp;&nbsp;503.3694&nbsp;&nbsp;&nbsp;&nbsp;\"Master's\"\n&nbsp;&nbsp;195.7708&nbsp;&nbsp;&nbsp;&nbsp;\"None\"\n&nbsp;&nbsp;241.7024&nbsp;&nbsp;&nbsp;&nbsp;\"High&nbsp;school\"\n&nbsp;1484.6757&nbsp;&nbsp;&nbsp;&nbsp;\"2&nbsp;year&nbsp;degree\"\n&nbsp;&nbsp;&nbsp;77.9444&nbsp;&nbsp;&nbsp;&nbsp;\"MD/JD/other&nbsp;professional&nbsp;degree\"\n&nbsp;&nbsp;389.4167&nbsp;&nbsp;&nbsp;&nbsp;\"Other\"\n&nbsp;1099.7925&nbsp;&nbsp;&nbsp;&nbsp;\"Ph&nbsp;D.\"<br /></pre>\n<p>Script source <a href=\"https://gist.github.com/4357424\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZZpno2ii7BWRwcARP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 9, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "20790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T07:55:08.205Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 18, chapter 87", "slug": "harry-potter-and-the-methods-of-rationality-discussion-29", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:07.209Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alsadius", "createdAt": "2009-10-23T06:53:40.578Z", "isAdmin": false, "displayName": "Alsadius"}, "userId": "kCc9vGRdkuGo2PWbo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/35GjH7tDvNJWSHQ3H/harry-potter-and-the-methods-of-rationality-discussion-29", "pageUrlRelative": "/posts/35GjH7tDvNJWSHQ3H/harry-potter-and-the-methods-of-rationality-discussion-29", "linkUrl": "https://www.lesswrong.com/posts/35GjH7tDvNJWSHQ3H/harry-potter-and-the-methods-of-rationality-discussion-29", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2018%2C%20chapter%2087&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2018%2C%20chapter%2087%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35GjH7tDvNJWSHQ3H%2Fharry-potter-and-the-methods-of-rationality-discussion-29%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2018%2C%20chapter%2087%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35GjH7tDvNJWSHQ3H%2Fharry-potter-and-the-methods-of-rationality-discussion-29", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35GjH7tDvNJWSHQ3H%2Fharry-potter-and-the-methods-of-rationality-discussion-29", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://www.fanfiction.net/s/5782108/87/Harry-Potter-and-the-Methods-of-Rationality\">chapter</a></span>&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://www.hpmor.com/chapter/87\">87</a></span>.&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/lw/fyv/harry_potter_and_the_methods_of_rationality/\">The previous thread</a></span>&nbsp;has passed 500 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.&nbsp; Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>, <a href=\"/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "35GjH7tDvNJWSHQ3H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.0645713977813981e-06, "legacy": true, "legacyId": "20798", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 594, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4sY9rqAqty8rHWGSW", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T11:26:41.010Z", "modifiedAt": null, "url": null, "title": "UFAI cannot be the Great Filter", "slug": "ufai-cannot-be-the-great-filter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:36.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thrasymachus", "createdAt": "2011-08-26T20:45:38.594Z", "isAdmin": false, "displayName": "Thrasymachus"}, "userId": "FkqMcEbDiSbJPBMtq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BQ4KLnmB7tcAZLNfm/ufai-cannot-be-the-great-filter", "pageUrlRelative": "/posts/BQ4KLnmB7tcAZLNfm/ufai-cannot-be-the-great-filter", "linkUrl": "https://www.lesswrong.com/posts/BQ4KLnmB7tcAZLNfm/ufai-cannot-be-the-great-filter", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20UFAI%20cannot%20be%20the%20Great%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUFAI%20cannot%20be%20the%20Great%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBQ4KLnmB7tcAZLNfm%2Fufai-cannot-be-the-great-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=UFAI%20cannot%20be%20the%20Great%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBQ4KLnmB7tcAZLNfm%2Fufai-cannot-be-the-great-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBQ4KLnmB7tcAZLNfm%2Fufai-cannot-be-the-great-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 943, "htmlBody": "<p>[Summary: The fact we do not observe (and have not been wiped out by) an UFAI suggests the main component of the 'great filter' cannot be civilizations like ours being wiped out by UFAI. Gentle introduction (assuming no knowledge) and links to much better discussion below.]</p>\n<h3>Introduction&nbsp;</h3>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a> is the idea that although there is lots of matter, we observe no \"expanding, lasting life\", like space-faring intelligences. So there is some filter through which almost all matter gets stuck before becoming expanding, lasting life. One question for those interested in the future of humankind is whether we have already 'passed' the bulk of the filter, or does it still lie ahead? For example, is it very unlikely matter will be able to form self-replicating units, but once it clears that hurdle becoming intelligent and going across the stars is highly likely; or is it getting to a humankind level of development is not that unlikely, but very few of those civilizations progress to expanding across the stars. If the latter, that motivates a concern for working out what the forthcoming filter(s) are, and trying to get past them.</p>\n<p>One concern is that advancing technology gives the possibility of civilizations wiping themselves out, and it is this that is the main component of the Great Filter - one we are going to be approaching soon. There are several candidates for which technology will be an existential threat (nanotechnology/'Grey goo', nuclear holocaust, runaway climate change), but one that looms large is Artificial intelligence (AI), and trying to understand and mitigate the existential threat from AI is the main role of the Singularity Institute, and I guess Luke, Eliezer (and lots of folks on LW) consider AI the main existential threat.</p>\n<p>The concern with AI is something like this:</p>\n<ol>\n<li>AI will soon greatly surpass us in intelligence in all domains.&nbsp;</li>\n<li>If this happens, AI will rapidly supplant humans as the dominant force on planet earth.</li>\n<li>Almost all AIs, even ones we create with the intent to be benevolent, will probably be unfriendly to human flourishing.</li>\n</ol>\n<p>Or, as <a href=\"http://facingthesingularity.com/2012/ai-the-problem-with-solutions/\">summarized by Luke</a>:</p>\n<p style=\"padding-left: 30px;\"><em>... AI leads to intelligence explosion, and, because we don&rsquo;t know how to give an AI benevolent goals, by default an intelligence explosion will optimize the world for accidentally disastrous ends. A controlled intelligence explosion, on the other hand, could optimize the world for good. (More on this option in the next post.)&nbsp;</em></p>\n<p>So, the aim of the game needs to be trying to work out how to control the future intelligence explosion so the vastly smarter-than-human AIs are 'friendly' (FAI) and make the world better for us, rather than unfriendly AIs (UFAI) which end up optimizing the world for something that sucks.</p>\n<p>&nbsp;</p>\n<h3>'Where is everybody?'</h3>\n<p>So, topic. I read this <a href=\"http://www.overcomingbias.com/2012/12/today-is-filter-day.html#disqus_thread\">post by Robin Hanson</a> which had a really good parenthetical remark (emphasis mine):</p>\n<p style=\"padding-left: 30px;\"><em>Yes, it is possible that the extremely difficultly was life&rsquo;s origin, or some early step, so that, other than here on Earth, all life in the universe is stuck before this early extremely hard step. But even if you find this the most likely outcome, surely given our ignorance you must also place a non-trivial probability on other possibilities. You must see a great filter as lying between initial planets and expanding civilizations, and wonder how far along that filter we are. In particular, you must estimate a substantial chance of &ldquo;disaster&rdquo;, i.e., something destroying our ability or inclination to make a visible use of the vast resources we see. <strong>(And this disaster can&rsquo;t be an unfriendly super-AI, because that should be visible.) </strong></em></p>\n<p><em><strong></strong></em>This made me realize an UFAI should also be counted as an 'expanding lasting life', and should be deemed unlikely by the Great Filter.</p>\n<p>Another way of looking at it: <em>if </em>the Great Filter still lies ahead of us, <em>and </em>a major component of this forthcoming filter is the threat from UFAI, we should expect to see the UFAIs of other civilizations spreading across the universe (or not see anything at all, because they would wipe us out to optimize for their unfriendly ends). That we do not observe it disconfirms this conjunction.</p>\n<p>[<strong>Edit/Elaboration</strong>: It also gives a stronger argument - as the UFAI is the 'expanding life' we do not see, the beliefs, 'the Great Filter lies ahead' and 'UFAI is a major existential risk' lie opposed to one another: the higher your credence in the filter being ahead, the lower your credence should be in UFAI being a major existential risk (as the many civilizations like ours that go on to get caught in the filter do not produce expanding UFAIs, so expanding UFAI cannot be the main x-risk); conversely, if you are confident that UFAI is the main existential risk, then you should think the bulk of the filter is behind us (as we don't see any UFAIs, there cannot be many civilizations like ours in the first place, as we are quite likely to realize an expanding UFAI).]</p>\n<p>A much more<a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big&nbsp;threat/\"> in-depth article and comments</a> (both highly recommended) was made by Katja Grace a couple of years ago. I can't seem to find a similar discussion on here (feel free to downvote and link in the comments if I missed it), which surprises me: I'm not bright enough to figure out the anthropics, and obviously one may hold AI to be a big deal for other-than-Great-Filter reasons (maybe a given planet has a 1 in a googol chance of getting to intelligent life, but intelligent life 'merely' has a 1 in 10 chance of successfully navigating an intelligence explosion), but this would seem to be substantial evidence driving down the proportion of x-risk we should attribute to AI.</p>\n<p>What do you guys think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "25oxqHiadqM6Hf7Gn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BQ4KLnmB7tcAZLNfm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 59, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "20800", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>[Summary: The fact we do not observe (and have not been wiped out by) an UFAI suggests the main component of the 'great filter' cannot be civilizations like ours being wiped out by UFAI. Gentle introduction (assuming no knowledge) and links to much better discussion below.]</p>\n<h3 id=\"Introduction_\">Introduction&nbsp;</h3>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a> is the idea that although there is lots of matter, we observe no \"expanding, lasting life\", like space-faring intelligences. So there is some filter through which almost all matter gets stuck before becoming expanding, lasting life. One question for those interested in the future of humankind is whether we have already 'passed' the bulk of the filter, or does it still lie ahead? For example, is it very unlikely matter will be able to form self-replicating units, but once it clears that hurdle becoming intelligent and going across the stars is highly likely; or is it getting to a humankind level of development is not that unlikely, but very few of those civilizations progress to expanding across the stars. If the latter, that motivates a concern for working out what the forthcoming filter(s) are, and trying to get past them.</p>\n<p>One concern is that advancing technology gives the possibility of civilizations wiping themselves out, and it is this that is the main component of the Great Filter - one we are going to be approaching soon. There are several candidates for which technology will be an existential threat (nanotechnology/'Grey goo', nuclear holocaust, runaway climate change), but one that looms large is Artificial intelligence (AI), and trying to understand and mitigate the existential threat from AI is the main role of the Singularity Institute, and I guess Luke, Eliezer (and lots of folks on LW) consider AI the main existential threat.</p>\n<p>The concern with AI is something like this:</p>\n<ol>\n<li>AI will soon greatly surpass us in intelligence in all domains.&nbsp;</li>\n<li>If this happens, AI will rapidly supplant humans as the dominant force on planet earth.</li>\n<li>Almost all AIs, even ones we create with the intent to be benevolent, will probably be unfriendly to human flourishing.</li>\n</ol>\n<p>Or, as <a href=\"http://facingthesingularity.com/2012/ai-the-problem-with-solutions/\">summarized by Luke</a>:</p>\n<p style=\"padding-left: 30px;\"><em>... AI leads to intelligence explosion, and, because we don\u2019t know how to give an AI benevolent goals, by default an intelligence explosion will optimize the world for accidentally disastrous ends. A controlled intelligence explosion, on the other hand, could optimize the world for good. (More on this option in the next post.)&nbsp;</em></p>\n<p>So, the aim of the game needs to be trying to work out how to control the future intelligence explosion so the vastly smarter-than-human AIs are 'friendly' (FAI) and make the world better for us, rather than unfriendly AIs (UFAI) which end up optimizing the world for something that sucks.</p>\n<p>&nbsp;</p>\n<h3 id=\"_Where_is_everybody__\">'Where is everybody?'</h3>\n<p>So, topic. I read this <a href=\"http://www.overcomingbias.com/2012/12/today-is-filter-day.html#disqus_thread\">post by Robin Hanson</a> which had a really good parenthetical remark (emphasis mine):</p>\n<p style=\"padding-left: 30px;\"><em>Yes, it is possible that the extremely difficultly was life\u2019s origin, or some early step, so that, other than here on Earth, all life in the universe is stuck before this early extremely hard step. But even if you find this the most likely outcome, surely given our ignorance you must also place a non-trivial probability on other possibilities. You must see a great filter as lying between initial planets and expanding civilizations, and wonder how far along that filter we are. In particular, you must estimate a substantial chance of \u201cdisaster\u201d, i.e., something destroying our ability or inclination to make a visible use of the vast resources we see. <strong>(And this disaster can\u2019t be an unfriendly super-AI, because that should be visible.) </strong></em></p>\n<p><em><strong></strong></em>This made me realize an UFAI should also be counted as an 'expanding lasting life', and should be deemed unlikely by the Great Filter.</p>\n<p>Another way of looking at it: <em>if </em>the Great Filter still lies ahead of us, <em>and </em>a major component of this forthcoming filter is the threat from UFAI, we should expect to see the UFAIs of other civilizations spreading across the universe (or not see anything at all, because they would wipe us out to optimize for their unfriendly ends). That we do not observe it disconfirms this conjunction.</p>\n<p>[<strong>Edit/Elaboration</strong>: It also gives a stronger argument - as the UFAI is the 'expanding life' we do not see, the beliefs, 'the Great Filter lies ahead' and 'UFAI is a major existential risk' lie opposed to one another: the higher your credence in the filter being ahead, the lower your credence should be in UFAI being a major existential risk (as the many civilizations like ours that go on to get caught in the filter do not produce expanding UFAIs, so expanding UFAI cannot be the main x-risk); conversely, if you are confident that UFAI is the main existential risk, then you should think the bulk of the filter is behind us (as we don't see any UFAIs, there cannot be many civilizations like ours in the first place, as we are quite likely to realize an expanding UFAI).]</p>\n<p>A much more<a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big&nbsp;threat/\"> in-depth article and comments</a> (both highly recommended) was made by Katja Grace a couple of years ago. I can't seem to find a similar discussion on here (feel free to downvote and link in the comments if I missed it), which surprises me: I'm not bright enough to figure out the anthropics, and obviously one may hold AI to be a big deal for other-than-Great-Filter reasons (maybe a given planet has a 1 in a googol chance of getting to intelligent life, but intelligent life 'merely' has a 1 in 10 chance of successfully navigating an intelligence explosion), but this would seem to be substantial evidence driving down the proportion of x-risk we should attribute to AI.</p>\n<p>What do you guys think?</p>", "sections": [{"title": "Introduction\u00a0", "anchor": "Introduction_", "level": 1}, {"title": "'Where is everybody?'", "anchor": "_Where_is_everybody__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "92 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T14:43:14.377Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup: Bayes", "slug": "meetup-berlin-meetup-bayes", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pghGGpDLpRrbAxCEA/meetup-berlin-meetup-bayes", "pageUrlRelative": "/posts/pghGGpDLpRrbAxCEA/meetup-berlin-meetup-bayes", "linkUrl": "https://www.lesswrong.com/posts/pghGGpDLpRrbAxCEA/meetup-berlin-meetup-bayes", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup%3A%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%3A%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpghGGpDLpRrbAxCEA%2Fmeetup-berlin-meetup-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%3A%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpghGGpDLpRrbAxCEA%2Fmeetup-berlin-meetup-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpghGGpDLpRrbAxCEA%2Fmeetup-berlin-meetup-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ha'>Berlin Meetup: Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 January 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MingDynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first meetup in 2013 and apart from the inevitable talk about the new year celebrations, there'll be:</p>\n\n<ul>\n<li>An introduction to Bayes rule: an explanation of what people mean when they say they updated on some evidence - with lots of exercises</li>\n<li>Space reserved for public commitments and predictions</li>\n</ul>\n\n<p>New people are always welcome! There'll be a sign saying \"LessWrong\" on our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ha'>Berlin Meetup: Bayes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pghGGpDLpRrbAxCEA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0648109178850828e-06, "legacy": true, "legacyId": "20801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup__Bayes\">Discussion article for the meetup : <a href=\"/meetups/ha\">Berlin Meetup: Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 January 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MingDynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first meetup in 2013 and apart from the inevitable talk about the new year celebrations, there'll be:</p>\n\n<ul>\n<li>An introduction to Bayes rule: an explanation of what people mean when they say they updated on some evidence - with lots of exercises</li>\n<li>Space reserved for public commitments and predictions</li>\n</ul>\n\n<p>New people are always welcome! There'll be a sign saying \"LessWrong\" on our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup__Bayes1\">Discussion article for the meetup : <a href=\"/meetups/ha\">Berlin Meetup: Bayes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup: Bayes", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup__Bayes", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup: Bayes", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup__Bayes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T17:47:58.174Z", "modifiedAt": null, "url": null, "title": "Education via Cell Phone in Africa", "slug": "education-via-cell-phone-in-africa", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "6RgBhmktNZ7a4WATG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hxb6gpJLr57GyzWYi/education-via-cell-phone-in-africa", "pageUrlRelative": "/posts/Hxb6gpJLr57GyzWYi/education-via-cell-phone-in-africa", "linkUrl": "https://www.lesswrong.com/posts/Hxb6gpJLr57GyzWYi/education-via-cell-phone-in-africa", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Education%20via%20Cell%20Phone%20in%20Africa&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEducation%20via%20Cell%20Phone%20in%20Africa%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxb6gpJLr57GyzWYi%2Feducation-via-cell-phone-in-africa%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Education%20via%20Cell%20Phone%20in%20Africa%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxb6gpJLr57GyzWYi%2Feducation-via-cell-phone-in-africa", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxb6gpJLr57GyzWYi%2Feducation-via-cell-phone-in-africa", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p>The World Bank reports that between 2007 and 2012, Africa exceeded the number of cell phone subscribers found in the European Union and in the United States. &nbsp;Africa has 650 million cell phone subscribers. [1] &nbsp;At the same time, Africa has profound infrastructure gaps that the World Bank identifies as gaps able to be closed by \"human capital.\" [2]&nbsp;Ritva Reinikka (World Bank Director for Human Development in Africa) suggests school / vocation / university buildings led by teachers and attended by students are a way to build human capital. [2]</p>\n<p>By choice, force, chance or some combination, Africa has not invested in traditional schools and has invested in cell phones. Philanthropy to Africa might be more successful if it occurs over the telephone instead of (or in addition to) in traditional school buildings. &nbsp;Education, training and health advice are not optimally delivered exclusively by telephone. But they may be more successful than traditional schools housed in buildings where teachers are typically absent and students attend only two hours a day, where only one-quarter of teachers know how to divide one fraction by another, and where roads to arrive at a school building are often lacking. [2]</p>\n<p>In October 2012 <em>Entertainment Weekly</em>&nbsp;included a free working Android smart phone in one thousand copies of their magazine. [3] In December 2012 a single dumb cell phones can be had for less than US$30 and a single smart phone can be had for less than US$100. [4] &nbsp;Bulk purchases are likely to make cell phones cost less.&nbsp;Nokia offers lessons in mathematics via cell phone. [5] &nbsp;Google offers many of their services via sms. [6]&nbsp;</p>\n<p>Philanthropic building of roads and school buildings should not end. &nbsp;But here is a case of a very mobile mountain able to meet Mohammed, by way of increased taking advantage of Africa's own&nbsp;commitment&nbsp;to cell phones.</p>\n<p>- -</p>\n<p>[1] \"ICTs Delivering Home-Grown Development Solutions in Africa.\" The World Bank, 10 Dec. 2012. Web. 22 Dec. 2012. http://go.worldbank.org/L20948QYZ0</p>\n<p>[2]&nbsp;Reinikka, Ritva. \"Why Should Africa Invest in People Now?\" The World Bank. ND. Web. 22 Dec. 2012. http://go.worldbank.org/CLSQ439QV0</p>\n<p>[3]&nbsp;Trel, Michael. \"Magazine Crams in Working Android Cellphone for Video Ad.\" DVICE. N.p., 3 Oct. 2012. Web. 22 Dec. 2012. http://dvice.com/archives/2012/10/crazy-video-mag.php</p>\n<p>[4] In the interest of avoiding commercial endorsements, no citation offered - check for yourself.</p>\n<p>[5]&nbsp;\"Nokia Mobile Mathematics.\" Nokia. ND. Web. 22 Dec. 2012. http://projects.developer.nokia.com/Momaths</p>\n<p>[6] \"Google SMS Applications.\" Google. ND. Web. 22 Dec. 2012. http://www.google.com/mobile/sms/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hxb6gpJLr57GyzWYi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.0649193692811963e-06, "legacy": true, "legacyId": "20803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-22T21:16:27.350Z", "modifiedAt": null, "url": null, "title": "So you think you understand Quantum Mechanics", "slug": "so-you-think-you-understand-quantum-mechanics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9MC7ts6gbvCYtQFGm/so-you-think-you-understand-quantum-mechanics", "pageUrlRelative": "/posts/9MC7ts6gbvCYtQFGm/so-you-think-you-understand-quantum-mechanics", "linkUrl": "https://www.lesswrong.com/posts/9MC7ts6gbvCYtQFGm/so-you-think-you-understand-quantum-mechanics", "postedAtFormatted": "Saturday, December 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20So%20you%20think%20you%20understand%20Quantum%20Mechanics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASo%20you%20think%20you%20understand%20Quantum%20Mechanics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9MC7ts6gbvCYtQFGm%2Fso-you-think-you-understand-quantum-mechanics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=So%20you%20think%20you%20understand%20Quantum%20Mechanics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9MC7ts6gbvCYtQFGm%2Fso-you-think-you-understand-quantum-mechanics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9MC7ts6gbvCYtQFGm%2Fso-you-think-you-understand-quantum-mechanics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 777, "htmlBody": "<p>This post is prompted by the multitude of posts and comments here using quantum this and that in an argument (quantum dice, quantum immortality, quantum many worlds...). But how does one know if they understand the concept they use? In school a student would have to write a test and get graded. It strikes me as a reasonable thing to do here, as well: let people test their understanding of the material so that they can calibrate their estimate of their knowledge of the topic. This is an attempt to do just that.</p>\n<p>Let's look at one of the very first experiments demonstrating that in the microscopic world things are usually quantized: the <a href=\"http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment\">Stern-Gerlach experiment</a>, in which measured angular momentum is shown to take discrete values. The gist of the experiment is that in a varying magnetic field the tidal force on a magnet is not perfectly balanced and so the magnet moves toward or away from the denser field, depending on the&nbsp;orientation&nbsp;of its poles. This is intuitively clear to anyone who ever played with magnets: the degree of attraction or repulsion depends on the relative orientation of the magnets (North pole repels North pole etc.). It is less obvious that this effect is due to the spatially varying magnetic field density, but it is nonetheless the case.</p>\n<p>In the experiment, one magnet is large (the S-G apparatus itself) and one is small (a silver atom injected into the magnetic field of the large magnet). The experiment shows that an unoriented atom suddenly becomes aligned either along or against the field, but not in any other direction. It's like a compass needle that would only be able to point North and South (and potentially in a few other directions) but not anywhere in between.</p>\n<p>If necessary, please read through the more detailed description of the experiment on Wikipedia or in any other source before attempting the following questions (usually called meditations in the&nbsp;idiosyncratic language used on this forum).&nbsp;</p>\n<p><strong>Meditation 1</strong>. When exactly does the atom align itself? As soon as it enters the field? At some random moment as it travels through the field? The instance it hits the screen behind the field? In other words, in the MWI picture, when does the world split into two, one with the atom aligned and one with the atom anti-aligned? In the Copenhagen picture, does the magnetic field measure the atom spin, and if so, when, or does the screen do it?</p>\n<p><strong>Hint</strong>. Consider whether/how you would tell these cases apart experimentally.</p>\n<p><strong>Meditation 2</strong>. Suppose you make two holes in the screen where the atoms used to hit it, then merge the atoms into a single stream again by applying a reverse field. Are the atoms now unaligned again, or 50/50 aligned/anti-aligned or something else?</p>\n<p><strong>Hint</strong>. What's the difference between these cases?</p>\n<p><strong>Meditation 3</strong>. Suppose that instead of the reversing field in the above experiment you keep the first screen with two holes in it, and&nbsp;put a second screen (without any holes) somewhere behind&nbsp;the first one.&nbsp;What would you expect to see on the second screen and why? Some possible answers: two equally bright blobs corresponding to aligned and anti-aligned atoms respectively; the interference pattern from each atom passing through both holes at once, like in the double-slit experiment; a narrow single blob in the center of the second screen, as if the atoms did not go through the first part of the apparatus at all; a spread-out blob with a maximum at the center, like you would expect from the classical atoms.</p>\n<p><strong>Hint</strong>. Consider/reconsider your answer to the first two questions.</p>\n<p><strong>Meditation 4</strong>. Suppose you want to answer M1 experimentally and use an extremely sensitive accelerometer to see which way each atom is deflecting <strong>before </strong>it hits the screen by measuring the recoil of the apparatus. What would you expect to observe?&nbsp;</p>\n<p><strong>Hint</strong>. Consider a similar setup for the double-slit experiment.</p>\n<p>&nbsp;</p>\n<p>This test is open-book and there is no time limit. You can consult any sources you like, including textbooks, research papers, your teachers, professional experimental or theoretical physicists, your fellow LWers, or the&nbsp;immortal soul of Niels Bohr through your local medium. If you have access to the Stern-Gerlach apparatus in your physics lab, feel free to perform any experiments you may find helpful. As they say, if you are not cheating, you are not trying hard enough.</p>\n<p>By the way, if anyone wants to supply the pictures to make the setup for each question clearer, I'd be more than happy to include them in this post. If anyone wants to turn the meditations into polls, please do so in the comments.</p>\n<p>Footnote: not posting this in Main, because I'm not sure how much interest there is here for QM questions like this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 4, "SJFsFfFhE6m2ThAYJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9MC7ts6gbvCYtQFGm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 65, "extendedScore": null, "score": 0.00016, "legacy": true, "legacyId": "20804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T02:09:19.534Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Disjunctions, Antipredictions, Etc.", "slug": "seq-rerun-disjunctions-antipredictions-etc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.136Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ooAiriXwAdysXxwzF/seq-rerun-disjunctions-antipredictions-etc", "pageUrlRelative": "/posts/ooAiriXwAdysXxwzF/seq-rerun-disjunctions-antipredictions-etc", "linkUrl": "https://www.lesswrong.com/posts/ooAiriXwAdysXxwzF/seq-rerun-disjunctions-antipredictions-etc", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Disjunctions%2C%20Antipredictions%2C%20Etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Disjunctions%2C%20Antipredictions%2C%20Etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooAiriXwAdysXxwzF%2Fseq-rerun-disjunctions-antipredictions-etc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Disjunctions%2C%20Antipredictions%2C%20Etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooAiriXwAdysXxwzF%2Fseq-rerun-disjunctions-antipredictions-etc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooAiriXwAdysXxwzF%2Fseq-rerun-disjunctions-antipredictions-etc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/wm/disjunctions_antipredictions_etc/\">Disjunctions, Antipredictions, Etc.</a> was originally published on 09 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Disjunctions.2C_Antipredictions.2C_Etc.\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A few tricks Yudkowsky uses to think about the future.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g12/seq_rerun_the_bad_guy_bias/\">The Bad Guy Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ooAiriXwAdysXxwzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0652138042790365e-06, "legacy": true, "legacyId": "20805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yzzoWR33S9C3m75e8", "JetifWMGryZksXboL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T14:48:55.287Z", "modifiedAt": null, "url": null, "title": "In which I ask an inappropriate question...", "slug": "in-which-i-ask-an-inappropriate-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tXKFYDYMLmhBEJBdf/in-which-i-ask-an-inappropriate-question", "pageUrlRelative": "/posts/tXKFYDYMLmhBEJBdf/in-which-i-ask-an-inappropriate-question", "linkUrl": "https://www.lesswrong.com/posts/tXKFYDYMLmhBEJBdf/in-which-i-ask-an-inappropriate-question", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20which%20I%20ask%20an%20inappropriate%20question...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20which%20I%20ask%20an%20inappropriate%20question...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXKFYDYMLmhBEJBdf%2Fin-which-i-ask-an-inappropriate-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20which%20I%20ask%20an%20inappropriate%20question...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXKFYDYMLmhBEJBdf%2Fin-which-i-ask-an-inappropriate-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXKFYDYMLmhBEJBdf%2Fin-which-i-ask-an-inappropriate-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>The man-made object responsible for the most deaths worldwide is the tobacco cigarette. It isn't even close.</p>\n<p>Tobacco kills 443,000 Americans a year and 5 million people a year worldwide. This is more than the total number of people killed by cars and firearms combined. Cars kill about 32,000 Americans each year and 1.3 million people worldwide, while firearms kill about 32,000 Americans each year and \"several hundred thousand\" people worldwide.</p>\n<p>100 million people were killed by tobacco in the 20th century. This is more than the death toll from World War 1 (17 million) and World War 2 (50 to 70 million) combined.</p>\n<p>From a strictly utilitarian perspective, would there be anything to be gained by, say, starting a campaign of assassination against executives of tobacco companies?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tXKFYDYMLmhBEJBdf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -28, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "20807", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T17:03:29.401Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Applied Rationality in a New Year", "slug": "meetup-moscow-applied-rationality-in-a-new-year", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XgSXtGvkzW3CWrsCh/meetup-moscow-applied-rationality-in-a-new-year", "pageUrlRelative": "/posts/XgSXtGvkzW3CWrsCh/meetup-moscow-applied-rationality-in-a-new-year", "linkUrl": "https://www.lesswrong.com/posts/XgSXtGvkzW3CWrsCh/meetup-moscow-applied-rationality-in-a-new-year", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Applied%20Rationality%20in%20a%20New%20Year&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Applied%20Rationality%20in%20a%20New%20Year%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSXtGvkzW3CWrsCh%2Fmeetup-moscow-applied-rationality-in-a-new-year%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Applied%20Rationality%20in%20a%20New%20Year%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSXtGvkzW3CWrsCh%2Fmeetup-moscow-applied-rationality-in-a-new-year", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSXtGvkzW3CWrsCh%2Fmeetup-moscow-applied-rationality-in-a-new-year", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hb'>Moscow: Applied Rationality in a New Year</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 January 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Solving cases. Please prepare some problems for discussion.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hb'>Moscow: Applied Rationality in a New Year</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XgSXtGvkzW3CWrsCh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0657392844931077e-06, "legacy": true, "legacyId": "20809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality_in_a_New_Year\">Discussion article for the meetup : <a href=\"/meetups/hb\">Moscow: Applied Rationality in a New Year</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 January 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Solving cases. Please prepare some problems for discussion.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality_in_a_New_Year1\">Discussion article for the meetup : <a href=\"/meetups/hb\">Moscow: Applied Rationality in a New Year</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Applied Rationality in a New Year", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality_in_a_New_Year", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Applied Rationality in a New Year", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality_in_a_New_Year1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T18:11:18.040Z", "modifiedAt": null, "url": null, "title": "Thoughts on the Drake Equation and the Great Filter", "slug": "thoughts-on-the-drake-equation-and-the-great-filter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:14.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bw7KekTwF6uBmcKaC/thoughts-on-the-drake-equation-and-the-great-filter", "pageUrlRelative": "/posts/Bw7KekTwF6uBmcKaC/thoughts-on-the-drake-equation-and-the-great-filter", "linkUrl": "https://www.lesswrong.com/posts/Bw7KekTwF6uBmcKaC/thoughts-on-the-drake-equation-and-the-great-filter", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20the%20Drake%20Equation%20and%20the%20Great%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20the%20Drake%20Equation%20and%20the%20Great%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBw7KekTwF6uBmcKaC%2Fthoughts-on-the-drake-equation-and-the-great-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20the%20Drake%20Equation%20and%20the%20Great%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBw7KekTwF6uBmcKaC%2Fthoughts-on-the-drake-equation-and-the-great-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBw7KekTwF6uBmcKaC%2Fthoughts-on-the-drake-equation-and-the-great-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 949, "htmlBody": "<p><em>I was originally going to post this as a comment into the <a href=\"/r/discussion/lw/g1s/ufai_cannot_be_the_great_filter/\" target=\"_blank\">UFAI &amp; great filter thread</a>, but since I noticed that my comment didn't include a single word of AIs I thought about making an entire new discussion thread and I continued writing to improve the quality from comment to post. </em><em><em>The essay is intended as thought-provoking</em> and I don't have the required knowledge in the related fields and I mostly pieced this together by browsing wikipedia, but hopefully it gets you thinking!&nbsp; <br /></em></p>\n<p>Personally I think when considering the <a href=\"http://en.wikipedia.org/wiki/Drake_equation\" target=\"_blank\">Drake Equation</a> it's important to note that it actually took ridiculously long for intelligent life to evolve <em>here</em> and that we're on a <em>finite timeline</em>. The drake equation contains the rate of star formation, the number of planets in the stars, it even has a variable for the time it takes for life to evolve to the point of signaling detectably into outer space, etc. but it's also important to pay attention to that the average setup of the universe has changed.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Abiogenesis\" target=\"_blank\">On earth life has existed for almost 4 billion years</a> and it has only been 43 years since our civilization first visited the moon and&nbsp; ~1&frac12; centuries since the invention of radio? That is a <em>very</em> small time frame. Particularly if we consider that <strong>~4 billion years is between a quarter and a third of the age of the universe itself.</strong><br /><br />When we consider the <a href=\"http://en.wikipedia.org/wiki/Great_Filter\" target=\"_blank\">Great Filter</a> we can at least propose that there have been <strong>several</strong> <a href=\"http://en.wikipedia.org/wiki/Extinction_event\" target=\"_blank\">mass extinction events</a> which <em>failed </em>to end all life on earth. I think it's a valid argument to say that for an example any <a href=\"http://en.wikipedia.org/wiki/Impact_event\" target=\"_blank\">powerful impact</a> could have ended all life or <em>reset</em> the evolution of life <a href=\"http://en.wikipedia.org/wiki/Cretaceous%E2%80%93Paleogene_extinction_event\" target=\"_blank\">some/any number of degrees</a> - and it has been ~70 years since the initiation of the <a href=\"http://en.wikipedia.org/wiki/Manhattan_Project\" target=\"_blank\">Manhattan project</a> and already humanity has the potential to go through a <a href=\"http://en.wikipedia.org/wiki/Nuclear_warfare\" target=\"_blank\">thermonuclear war</a> that could <a href=\"http://en.wikipedia.org/wiki/Nuclear_warfare\" target=\"_blank\">end human life on the planet</a>, or <a href=\"http://en.wikipedia.org/wiki/Nuclear_winter\" target=\"_blank\">rollback the game of life through nuclear winter</a>. <a href=\"http://en.wikipedia.org/wiki/Life_on_Mars\" target=\"_blank\">Mars could have been habitable</a>. For an example <a href=\"http://en.wikipedia.org/wiki/Water_on_Mars\" target=\"_blank\">there's no liquid water on Mars now, though there should've been earlier</a>. The <a href=\"http://en.wikipedia.org/wiki/Habitable_zone\" target=\"_blank\">habitable zone</a> as theoretized is considerably narrow - For an example: <strong>If at any point in the history of (life on) earth the average surface temperature had climbed to 200 celsius for whatever reason </strong>I'm pretty sure that would've made our planet like all the other planets observed - so far - in that they don't seem contain intelligent life. What I mean by this is that even though a vast number of planets reside in the habitable zone of some star, they have to maintain those conditions for a <em>very long time</em>, and that's just one variable. Which by the way is a pretty important thing to note when talking about things such as the <a href=\"http://en.wikipedia.org/wiki/Greenhouse_effect\" target=\"_blank\">greenhouse effect</a> for an example. Some people seem to have this idea of \"natural balance\" that occurs automatically. It's as if those people are not looking at the \"natural balance\" on some of the other planets. <em>Where's the mechanism anyway</em>? <a href=\"http://en.wikipedia.org/wiki/Milankovitch_cycles\" target=\"_blank\">Milankovitch cycles</a>? <a href=\"http://en.wikipedia.org/wiki/Siderian\" target=\"_blank\">Even algae managed to start an ice age</a> according to <a href=\"http://en.wikipedia.org/wiki/Oxygen_catastrophe\" target=\"_blank\">some theories</a>, humans certainly have the potential to do more harm than that and it's not like we only have to care for extinction events that we brought upon ourselves.<br /><br />In addition to this it seems to me frequently neglected that the conditions inside the universe have changed considerably with the aging universe. Earth is not constantly bombarded by collisions, it takes time for stars and planets and so forth to attain their form, the average age of stars has changed. In other words the <em>habitability of the entire universe changes over time</em>, though not in a particularly synchronous fashion. If this does not seem reasonable then consider the following: Was the likelihood for finding intelligent life in any location of the universe when it was 1 billion years old the same as it is today? How about when the universe was 4 billion years old? 8 billion years? <a href=\"http://en.wikipedia.org/wiki/Star#Age\" target=\"_blank\">Most stars are between 1 to 10 billion years of age according to wikipedia.</a></p>\n<p><a href=\"http://en.wikipedia.org/wiki/Toba_catastrophe_theory\" target=\"_blank\">Human species itself has gone through some sort</a> of a <a href=\"http://en.wikipedia.org/wiki/Population_bottleneck#Humans\" target=\"_blank\">bottleneck</a>, a historical token worth reflecting upon: Had the event been worse and those few remaining members of our ancenstry perished the planet earth would arguably still be without intelligent civilizations even today.</p>\n<p>&nbsp;</p>\n<p><strong>This line of reasoning in my opinion favors two different details:</strong></p>\n<p>1. Since our intelligence took almost 4 billion years to evolve, any event within that time that could've wiped out all the progress, would've occured before the rise of intelligent civilizations - and so all those events contribute to <a href=\"http://en.wikipedia.org/wiki/Great_Filter\" target=\"_blank\">the Great Filter </a></p>\n<p>2. The often contemplated likelihood that human intelligence is among the earliest intelligent species to arise, if life had been considerably less likely in the earlier stages of the universe. (which is very complatible with the fact that we have not observed life elsewhere - or at least that's somewhat complementary to likelihood of intelligent life) In otherwords if our species is within the first 5% of the intelligent civilizations to arise that should be reflected upon our observations. Of course the same is true for the last 5%, etc. This is an important point, because that's not the kind of reliability science rests upon.</p>\n<p>&nbsp;</p>\n<p>Remember how life taking almost ~4 billion years to evolve on earth was a ~1/3 (rather ~2/7) of the age of the entire universe? Well <a href=\"http://en.wikipedia.org/wiki/Solar_System\" target=\"_blank\">our solar system is only 4.6 billion years old</a>. Life on earth has been evolving practically since the formation of our solar system and at no point in time were all the replicators wiped out.</p>\n<p>So, any thoughts?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bw7KekTwF6uBmcKaC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "20808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BQ4KLnmB7tcAZLNfm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T18:56:58.179Z", "modifiedAt": null, "url": null, "title": "Ritual Report 2012: Life, Death, Light, Darkness, and Love.", "slug": "ritual-report-2012-life-death-light-darkness-and-love", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.303Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FLq2J9KwEuecYmaSx/ritual-report-2012-life-death-light-darkness-and-love", "pageUrlRelative": "/posts/FLq2J9KwEuecYmaSx/ritual-report-2012-life-death-light-darkness-and-love", "linkUrl": "https://www.lesswrong.com/posts/FLq2J9KwEuecYmaSx/ritual-report-2012-life-death-light-darkness-and-love", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ritual%20Report%202012%3A%20Life%2C%20Death%2C%20Light%2C%20Darkness%2C%20and%20Love.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARitual%20Report%202012%3A%20Life%2C%20Death%2C%20Light%2C%20Darkness%2C%20and%20Love.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLq2J9KwEuecYmaSx%2Fritual-report-2012-life-death-light-darkness-and-love%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ritual%20Report%202012%3A%20Life%2C%20Death%2C%20Light%2C%20Darkness%2C%20and%20Love.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLq2J9KwEuecYmaSx%2Fritual-report-2012-life-death-light-darkness-and-love", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLq2J9KwEuecYmaSx%2Fritual-report-2012-life-death-light-darkness-and-love", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1285, "htmlBody": "<p><strong id=\"internal-source-marker_0.4521293353755027\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">One winter ago, twenty aspiring rationalists gathered in a room, ate some food, sang some songs, and lit some candles. We told some stories about why the universe is the way it is, and what kind of people we want to be. </span></strong></p>\n<address><address><span style=\"font-style: normal;\"><strong id=\"internal-source-marker_0.4521293353755027\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">I wrote some things about the experience</a>. But here's a fairly succinct description:</span></strong></span></address><address><br /></address>\n<blockquote><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Like most things, winter was once a mystery.</span></strong></span></address><address><br /></address><address><span style=\"font-style: normal;\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">The world got cold, and dark. Life became fragile. People died. And they didn't know what was happening or understand why. They desperately threw festivals in honor of sun gods with all-too-human motivations, and prayed for the light's return.</span><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><br /></strong></span></address><address><br /></address><address><span style=\"font-style: normal;\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">It didn't help. Though we did discover that throwing parties in the middle of winter is an excellent idea.</span><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><br /></strong></span></address><address><br /></address><address><span style=\"font-style: normal;\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">But then something incredible and beautiful happened. We studied the sky. We invented astronomy, and other sciences. We began a long journey towards truly understanding our place in the universe. And we used that knowledge to plan for the future, and make our world better. Five thousand years later, the winter isn't so scary. But the symbol of the solstice - the departure and return of the sun - is still powerful. The work we have done to transform winter from a terrifying season of darkness into a modern festival of light deserves a reverence with all the weight of an ancient cultural cornerstone.</span></span></address></blockquote>\n<address><a id=\"more\"></a><br /></address><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Last year, we had fun. A few people reported being emotionally affected. By and large, though, the dominant conclusion was &ldquo;This was good first effort, but much, much more is possible.&rdquo; In truth, I considered it a dress rehearsal, more a proof-of-concept than a finished product. I spent the last year working to do something better, but worried that I wouldn&rsquo;t be able to. That maybe people don&rsquo;t create holidays from scratch that actually latch on because it&rsquo;s just damn hard to do and I wouldn&rsquo;t be up to it.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">And I was worried that either I wouldn&rsquo;t be able to make the experience as grim and intense as I wanted, or that I&rsquo;d succeed, but then not be able to lift people back out of it. This was a problem for some people last year, and last year I didn&rsquo;t push things nearly as dark as I was planning to this time.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">I worried that even if I succeeded at creating the experience for other people, I wouldn&rsquo;t be able to experience it myself. A year ago, I didn&rsquo;t feel like a participant. I felt like an anthropologist - clinically detached from the bonding ritual I had created.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">But six months ago, four friends and I acquired a large, three story house named &ldquo;Winterfell.&rdquo; And one week ago, fifty people squeezed into that house to celebrate humanity. The house seems a lot smaller once you crammed fifty people into the living room. But we managed to fit.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">And then... &nbsp;I feel a desire to maintain some kind of modesty here, but honestly, I spent a year stressing about this and I think I&rsquo;m just going to say that it went beautifully. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Not perfectly - nothing is ever perfect, and now more than ever it is clear how much more is possible with this endeavor. <a href=\"http://squid314.livejournal.com/346675.html\">Yvain wrote a pretty good review of which parts went well and which parts needed work.</a> But I got emphatic gratitude from people who had been merely lukewarm about it last year.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">In the darkest section of the evening, people cried, and held each other, and I was one of them. And I was one of them as we watched time lapse footage of the stars from the international space station, and sang about a tomorrow that could be brighter than today.</span><br /><br /> \n<hr />\n</strong></span></address><address> <br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">This will be the first post of another short mini-sequence (either one or two additional posts elaborating on the design process, what comes next and what I&rsquo;m concerned about). For now, I'll just note the one biggest flaw with this years was that it was too long. (Last years was too short, and I decided to err on the side of \"test a bunch of ideas at once\" so that future Solstices could settle into an ideal, traditional state faster).</span></address><address><br /></address><address><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; white-space: pre-wrap; font-style: normal;\">I would like to note that I want to strongly encourage people who are weirded out by this to speak out (if for no other reason than to be counted as people who are turned off by it). If you have specific negative consequences beyond a vague dislike of the idea, I'd like you to articulate them, after looking through my post from last year - <a href=\"/lw/93l/the_value_and_danger_of_ritual/\">The Value and Danger of Ritual</a>.</span></span></address><address><br /></address><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Below is a link to the 2012 Ritual Book, and a collection of links to online media for the songs and videos that we listened to and watched during the event, which you can follow along with as you read to get something (vaguely) resembling the actual experience. (Plus side - you&rsquo;ll get to experience higher quality of music performance. Downside - you miss on the warm experience of singing with a group of people).</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">I couldn&rsquo;t find links for all the songs, but there should be enough to give you the idea. </span></strong></span></address><address><br /></address><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></strong></span></address>\n<blockquote><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><a style=\"font-style: normal;\" href=\"https://dl.dropbox.com/u/2000477/SolsticeEve_2012.pdf\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">2012 Solstice Ritual Book</span><br /></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">First Litany of Tarski - If the sky is blue....</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Why Does the Sun Shine (part 0)</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=ZgP0aUKlmNw\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The Grinch</span></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Necronomicon</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The X Days of X-Risk</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Ballad of Bonnie the Em</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Second Litany of Tarski - If I&rsquo;m going to be outcompeted by simulated minds in a Malthusian Hellhole race to the bottom...</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Mindspace is Deep and Wide</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">One Wish (I found a Baby Genie)</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=t8cELTdtw6U\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Build That Wall</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=PcMWwS8SYY8\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Quantum Entanglement</span></a><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Third Litany of Tarski - If the Many Worlds Interpretation of Quantum Mechanics is true...</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">When I Die</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=3JdWlSF195Y\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Why Does the Sun Shine (part 1)</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">God Wrote the Sky</span></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Blue Skies</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Metalitany of Tarski - If reciting the Litany of Tarski is useful...</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Beyond the Reach of God</span><br /><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=3t4g_1VoGw4\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Blowin&rsquo; In the Wind</span></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Stonehenge (The Sun is Going to Go)</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=4s3b5OR2YhE\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Gods Ain&rsquo;t Gonna Catch Ya</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> (slightly altered lyrics)</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=svPAVcsZ5go\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Take My Love, Take My Land (Mal&rsquo;s Song)</span></a><br /><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; white-space: pre-wrap;\"><a href=\"https://soundcloud.com/raymond-arnold/little-echo\">Collect a Little Echo</a></span></span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The Drummer&rsquo;s Little Boy</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=5xaxP_kErTU\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">No One is Alone </span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">(dramatically abridged)</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Serenity&rsquo;</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Gift We Give Tomorrow</span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Moment of Darkness</span></strong></span></address><address><br /></address><address><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-style: normal; white-space: pre-wrap;\"><a href=\"https://vimeo.com/45878034\">A View From Above - Time Lapse Footage from the International Space Station</a></span></span></address><address><span style=\"font-style: normal;\"><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><br /><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"https://soundcloud.com/raymond-arnold/brighter-than-today\">Brighter Than Today</a></span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=Y6ljFaKRTrI\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Still Alive</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=QPoTGyWT0Cg\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Lean on Me</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=tj6yYD4fhlA\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A Still Small Voice</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=kViMQhSF-_Y\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Gonna be a Cyborg</span></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Move the World</span><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=m5TwT69i1lU\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">What a Wonderful World</span></a><br /><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=hj7LRuusFqo\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Seasons of Love</span><br /></a><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=sLkGSV9WDMA\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The Sun&rsquo;s a Miasma of Incandescent Plasma (altered lyrics for singability)</span><br /></a><a style=\"font-style: normal;\" href=\"http://www.youtube.com/watch?v=-4UoJ47SzjA\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Forever Young (*slightly* altered lyrics)</span><br /></a><a style=\"font-style: normal;\" href=\"http://dl.dropbox.com/u/2000477/(05)Uplift-Andrew_Eigel.mp3\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Uplift</span><br /></a><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Final Litany - If human values will survive for five thousand years...</span><br /><a href=\"https://soundcloud.com/raymond-arnold/five-thousand-years-with\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Five Thousand Years </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">(Sun is Gonna Go, Reprise)</span><span style=\"font-style: normal;\">&nbsp;</span></a></strong></span></address><address><span style=\"font-family: Arial; color: #1155cc;\"><span style=\"font-size: 15px; font-style: normal; white-space: pre-wrap;\"><a href=\"http://www.youtube.com/watch?v=ZSt9tm3RoUU\">A Brief Recap (Our Story in 1 minute and 30 seconds)</a></span></span></address><address><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The Road to Wisdom</span></strong></address></blockquote>\n</address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FLq2J9KwEuecYmaSx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 30, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "20767", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3bbvzoRA8n6ZgbiyK", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "solstice-2015-what-memes-may-come-part-i", "canonicalPrevPostSlug": "clumping-solstice-singalongs-in-groups-of-2-4", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 206, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC", "rijoxTpkSPXcTXRbN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T19:22:41.904Z", "modifiedAt": null, "url": null, "title": "[LINK] Forty Years of String Theory: Reflecting on the Foundations", "slug": "link-forty-years-of-string-theory-reflecting-on-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:39.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wxn9aQqFRMCizCK6b/link-forty-years-of-string-theory-reflecting-on-the", "pageUrlRelative": "/posts/wxn9aQqFRMCizCK6b/link-forty-years-of-string-theory-reflecting-on-the", "linkUrl": "https://www.lesswrong.com/posts/wxn9aQqFRMCizCK6b/link-forty-years-of-string-theory-reflecting-on-the", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Forty%20Years%20of%20String%20Theory%3A%20Reflecting%20on%20the%20Foundations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Forty%20Years%20of%20String%20Theory%3A%20Reflecting%20on%20the%20Foundations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwxn9aQqFRMCizCK6b%2Flink-forty-years-of-string-theory-reflecting-on-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Forty%20Years%20of%20String%20Theory%3A%20Reflecting%20on%20the%20Foundations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwxn9aQqFRMCizCK6b%2Flink-forty-years-of-string-theory-reflecting-on-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwxn9aQqFRMCizCK6b%2Flink-forty-years-of-string-theory-reflecting-on-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>Those of us interested in \"fundamental physics\" may find a few interesting tidbits in the <a href=\"http://link.springer.com/journal/10701/43/1/page/1\">latest issue</a> of Springer's Foundations of Physics. It has contributions from the prominent figures in String Theory and related fields, such as the Nobel laureate <a href=\"http://en.wikipedia.org/wiki/Gerard_'t_Hooft\"><strong>Gerard 't Hooft</strong></a>, father of the <a href=\"http://en.wikipedia.org/wiki/String_theory_landscape\">anthropic landscape</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Leonard_Susskind\"><strong>Leonard Susskind</strong></a>&nbsp;and one of the founders of the leading alternative to the String Theory, Loop Quantum Gravity, as well as the author of <a href=\"http://en.wikipedia.org/wiki/Lee_Smolin#Publications\">several popular books about fundamental physics</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Lee_Smolin\"><strong>Lee Smolin</strong></a>. <a href=\"http://en.wikipedia.org/wiki/Erik_Verlinde\">Eric Verlinde</a>, the author of the controversial&nbsp;<a href=\"http://en.wikipedia.org/wiki/Entropic_gravity\">Entropic gravity</a> model, also contributed. A couple of&nbsp;philosophers&nbsp;of science added their two cents.</p>\n<p>While Springer is not an open-access publisher, this volume is <a href=\"http://www.springer.com/?SGWID=0-102-12-922006-0\">free</a>, as are many others during December 2012.</p>\n<p>A few quotes from the introduction, which seem relevant to the issues of truth, realism and rationality:</p>\n<p>\"He ['t Hooft] compares string theory to other&nbsp;theories and models which are not free of problems but we generally consider to be&nbsp;well-defined: celestial classical mechanics, quantum mechanics, and QCD, and concludes&nbsp;that string theory is not in as good shape as any of these theories.\"</p>\n<p>\"Rickles develops a version&nbsp;of the &ldquo;no-miracles argument&rdquo; for scientific realism to the case of mathematically&nbsp;fruitful theories, thereby defending the rationality of those who pursue string theory&nbsp;in the absence of better alternatives, rather than making a statement about the truth&nbsp;of the theory.\"</p>\n<p>\"Susskind argues that developments in string theory are telling us that <strong><em>a narrow form&nbsp;of reductionism is wrong</em></strong>: &ldquo;[I]f one listens carefully, string theory is telling us that in&nbsp;a deep way reductionism is wrong, at least beyond some point.&rdquo; The reason is that&nbsp;various string dualities interchange what is fundamental and what is composite, large&nbsp;and small lengths scales, high-dimensional objects with lower-dimensional objects,&nbsp;and so on. According to Susskind, &ldquo;In string theory this kind of ambiguity is the&nbsp;rule.&rdquo; &ldquo;Personally, I would bet that this kind of anti-reductionist behavior is true in&nbsp;any consistent synthesis of quantum mechanics and gravity.&rdquo;\"</p>\n<p>If you were to read only one paper, make it the <a href=\"http://link.springer.com/article/10.1007/s10701-011-9618-4\">one</a> by <a href=\"http://en.wikipedia.org/wiki/Michael_Duff_(physicist)\">Michael Duff</a>. Here is the abstract:</p>\n<p>\"<span style=\"color: #333333; font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif; font-size: 13px; line-height: 20px;\">Using as a springboard a three-way debate between theoretical physicist Lee Smolin, philosopher of science Nancy Cartwright and myself, I address in layman&rsquo;s terms the issues of why we need a unified theory of the fundamental interactions and why, in my opinion, string and M-theory currently offer the best hope. The&nbsp;focus will be on responding more generally to the various criticisms. I also describe the diverse application of string/M-theory techniques to other branches of physics and mathematics which render the whole enterprise worthwhile whether or not &ldquo;a theory of everything&rdquo; is forthcoming.\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wxn9aQqFRMCizCK6b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 1.065821135330834e-06, "legacy": true, "legacyId": "20810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T21:00:31.045Z", "modifiedAt": null, "url": null, "title": "New censorship: against hypothetical violence against identifiable people", "slug": "new-censorship-against-hypothetical-violence-against", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8TCkDjYWphW88LJgh/new-censorship-against-hypothetical-violence-against", "pageUrlRelative": "/posts/8TCkDjYWphW88LJgh/new-censorship-against-hypothetical-violence-against", "linkUrl": "https://www.lesswrong.com/posts/8TCkDjYWphW88LJgh/new-censorship-against-hypothetical-violence-against", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20censorship%3A%20against%20hypothetical%20violence%20against%20identifiable%20people&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20censorship%3A%20against%20hypothetical%20violence%20against%20identifiable%20people%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TCkDjYWphW88LJgh%2Fnew-censorship-against-hypothetical-violence-against%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20censorship%3A%20against%20hypothetical%20violence%20against%20identifiable%20people%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TCkDjYWphW88LJgh%2Fnew-censorship-against-hypothetical-violence-against", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TCkDjYWphW88LJgh%2Fnew-censorship-against-hypothetical-violence-against", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p>New proposed censorship policy:</p>\n<p>Any post or comment which advocates or 'asks about' violence against sufficiently identifiable real people or groups (as opposed to aliens or hypothetical people on trolley tracks) may be deleted, along with replies that also contain the info necessary to visualize violence against real people.</p>\n<p>Reason: Talking about such violence makes that violence more probable, <em>and </em>makes LW look bad; and numerous message boards across the Earth censor discussion of various subtypes of proposed criminal activity without anything bad happening to them.</p>\n<p>More generally: Posts or comments advocating or 'asking about' violation of laws that are actually enforced against middle-class people (e.g., kidnapping, not anti-marijuana laws) may at the admins' option be censored on the grounds that it makes LW look bad and that anyone talking about a proposed crime on the Internet fails forever as a criminal (i.e., even if a proposed conspiratorial crime were in fact good, there would still be net negative expected utility from talking about it on the Internet; if it's a bad idea, promoting it conceptually by discussing it is also a bad idea; therefore and in full generality this is a low-value form of discussion). &nbsp;</p>\n<p>This is not a poll, but I am asking in advance if anyone has non-obvious consequences they want to point out or policy considerations they would like to raise. In other words, the form of this discussion is not 'Do you like this?' - you probably have a different cost function from people who are held responsible for how LW looks as a whole - but rather, 'Are there any predictable consequences we didn't think of that you would like to point out, and possibly bet on with us if there's a good way to settle the bet?'</p>\n<p>Yes, a post of this type was just recently made. &nbsp;I will not link to it, since this censorship policy implies that it will shortly be deleted, and reproducing the info necessary to say who was hypothetically targeted and why would be against the policy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CYMR6p5iZG75QAT8a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8TCkDjYWphW88LJgh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 40, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "20812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 460, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-23T21:50:10.632Z", "modifiedAt": null, "url": null, "title": "[Link] Semantic Space Distributed Through The Brain", "slug": "link-semantic-space-distributed-through-the-brain", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3xPECoSLQXNrPj9yq/link-semantic-space-distributed-through-the-brain", "pageUrlRelative": "/posts/3xPECoSLQXNrPj9yq/link-semantic-space-distributed-through-the-brain", "linkUrl": "https://www.lesswrong.com/posts/3xPECoSLQXNrPj9yq/link-semantic-space-distributed-through-the-brain", "postedAtFormatted": "Sunday, December 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Semantic%20Space%20Distributed%20Through%20The%20Brain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Semantic%20Space%20Distributed%20Through%20The%20Brain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xPECoSLQXNrPj9yq%2Flink-semantic-space-distributed-through-the-brain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Semantic%20Space%20Distributed%20Through%20The%20Brain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xPECoSLQXNrPj9yq%2Flink-semantic-space-distributed-through-the-brain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xPECoSLQXNrPj9yq%2Flink-semantic-space-distributed-through-the-brain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>http://gallantlab.org/semanticmovies/</p>\n<p>Brains mapped according to conceptual distance in several dimensions.</p>\n<p>Interactive.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3xPECoSLQXNrPj9yq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 1.065907861099815e-06, "legacy": true, "legacyId": "20813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-24T05:36:38.091Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Are AIs Homo Economicus?", "slug": "seq-rerun-are-ais-homo-economicus", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dBW93B7op99skq7i4/seq-rerun-are-ais-homo-economicus", "pageUrlRelative": "/posts/dBW93B7op99skq7i4/seq-rerun-are-ais-homo-economicus", "linkUrl": "https://www.lesswrong.com/posts/dBW93B7op99skq7i4/seq-rerun-are-ais-homo-economicus", "postedAtFormatted": "Monday, December 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Are%20AIs%20Homo%20Economicus%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Are%20AIs%20Homo%20Economicus%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBW93B7op99skq7i4%2Fseq-rerun-are-ais-homo-economicus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Are%20AIs%20Homo%20Economicus%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBW93B7op99skq7i4%2Fseq-rerun-are-ais-homo-economicus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBW93B7op99skq7i4%2Fseq-rerun-are-ais-homo-economicus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/are-ais-homo-ec.html\">Are AIs Homo Economicus?</a> was originally published on December 9, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>It seems odd that predictions of AIs based on economics are the subject of complaints that the agents modeled are too human, since the usual criticism of economic models is the reverse.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/g1x/seq_rerun_disjunctions_antipredictions_etc/\">Disjunctions, Antipredictions, Etc.</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dBW93B7op99skq7i4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0661822464008768e-06, "legacy": true, "legacyId": "20821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ooAiriXwAdysXxwzF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-24T10:00:44.076Z", "modifiedAt": null, "url": null, "title": "Licensing discussion for LessWrong Posts", "slug": "licensing-discussion-for-lesswrong-posts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.901Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rick_from_Castify", "createdAt": "2012-12-03T09:33:28.512Z", "isAdmin": false, "displayName": "Rick_from_Castify"}, "userId": "XyTqQupkZ9SW7nGCB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/isc9fnLQ2e9dDjfuJ/licensing-discussion-for-lesswrong-posts", "pageUrlRelative": "/posts/isc9fnLQ2e9dDjfuJ/licensing-discussion-for-lesswrong-posts", "linkUrl": "https://www.lesswrong.com/posts/isc9fnLQ2e9dDjfuJ/licensing-discussion-for-lesswrong-posts", "postedAtFormatted": "Monday, December 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Licensing%20discussion%20for%20LessWrong%20Posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALicensing%20discussion%20for%20LessWrong%20Posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fisc9fnLQ2e9dDjfuJ%2Flicensing-discussion-for-lesswrong-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Licensing%20discussion%20for%20LessWrong%20Posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fisc9fnLQ2e9dDjfuJ%2Flicensing-discussion-for-lesswrong-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fisc9fnLQ2e9dDjfuJ%2Flicensing-discussion-for-lesswrong-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>If you've been following the <a href=\"/lw/fr2/lesswrong_podcasts/\">announced partnership</a> between LessWrong and <a href=\"http://castify.co\">Castify</a>, you'll know that we would like to start offering the promoted posts as a podcast.<br /><br />So far, everything offered by Castify is authored by Eliezer Yudkowsky who gave permission to have his content used. Because promoted posts can be written by those who haven't explicitly given us permission, we're reluctant to offer them without first working through the licensing issues with the community. <br /><br />What we propose is that all content on the site be subject to the <a href=\"http://creativecommons.org/licenses/by/3.0/deed.en_US\">Creative Commons license</a> which would allow content posted to LessWrong to be used for commercial use as long as the work is given proper attribution.<br /><br />LessWrong management and Castify want feedback from the community before moving forward.&nbsp; Thoughts?</p>\n<p>Edit: EricHerboso was kind enough to start a poll in the comments <a href=\"/r/discussion/lw/g2h/licensing_discussion_for_lesswrong_posts/84t9\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "isc9fnLQ2e9dDjfuJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "20825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BTM5wskCnzbNLDZdS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-24T20:31:21.883Z", "modifiedAt": null, "url": null, "title": "What if \"status\" IS a terminal value for most people?", "slug": "what-if-status-is-a-terminal-value-for-most-people", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.136Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "handoflixue", "createdAt": "2010-11-24T17:23:14.338Z", "isAdmin": false, "displayName": "handoflixue"}, "userId": "vGzwXwmR2qERvoGvb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GCQvWvYvgRcaC23BP/what-if-status-is-a-terminal-value-for-most-people", "pageUrlRelative": "/posts/GCQvWvYvgRcaC23BP/what-if-status-is-a-terminal-value-for-most-people", "linkUrl": "https://www.lesswrong.com/posts/GCQvWvYvgRcaC23BP/what-if-status-is-a-terminal-value-for-most-people", "postedAtFormatted": "Monday, December 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20if%20%22status%22%20IS%20a%20terminal%20value%20for%20most%20people%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20if%20%22status%22%20IS%20a%20terminal%20value%20for%20most%20people%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCQvWvYvgRcaC23BP%2Fwhat-if-status-is-a-terminal-value-for-most-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20if%20%22status%22%20IS%20a%20terminal%20value%20for%20most%20people%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCQvWvYvgRcaC23BP%2Fwhat-if-status-is-a-terminal-value-for-most-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCQvWvYvgRcaC23BP%2Fwhat-if-status-is-a-terminal-value-for-most-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 670, "htmlBody": "<p><em>[Inspired by a few of the science bits in HP:MOR, and far more so by the discussions between Draco and Harry about \"social skills\". Shared because I suspect it's an insight some people would benefit from.]</em></p>\n<p>One of the more prominent theories on the evolution of human  intelligence suggests that humans involved intelligence, not to deal  with their environment, but rather to deal with each other. A small  intellectual edge would foster competition, and it would result in the  sort of recursive, escalating loop that's required to explain why we're  so SUBSTANTIALLY smarter than every other species on the planet.<br /><br />If  you accept that premise, it's obvious that intelligence should,  naturally, come with a desire to compete against other humans. It should  be equally obvious from looking at human history that, indeed, we seem  to do exactly that.<br /><br />Posit, then, that, linked to intelligence,  there's a trait for politics - using intelligence to compete against  other humans, to try and establish dominance via cunning instead of  brawn.<br /><br />And, like everything that the Blind Idiot God Evolution  has created, imagine that there are humans who LACK this trait for  politics, but still have intelligence.<br /><br />Think about the humans  who, instead of looking inwards at humanity for competition, instead  turn outwards to the vast uncaring universe of physics and chemistry.  Other humans are an obtainable target - a little evolutionary push, and  your tribe can outsmart any other tribe. The universe is not nearly so  easily cowed, though. The universe is, often, undefeatable, or at least,  we have not come close to mastering it. Six thousand years and people  still die to storms and drought and famine. Six thousand years, and we  have just touched on the moon, just begun to even SEE other planets that  might contain life like ours.<br /><br />I never understood other people before, because I'm missing that trait.<br /><br />And I finally, finally, understand that this trait even exists, and what it must BE like, to have the trait.<br /><br />We  are genetic, chemical beings. I believe this with every ounce of  myself. There isn't a soul that defies physics, there is not a  consciousness that defies neurology. The world, even ourselves, can be  measured. Anger comes from a part of this mixture, as does happiness and  love. They are not lesser for this. They are not!<br /><br />This is not an  interlude. It is woven in to the meaning of what I realized. If you  have this trait, then part of your values, as fundamental to yourself as  eating and breathing and drinking, is the desire for status, to assert a  certain form of dominance. Intelligence can almost be measured by  status and cunning, and those who try to cheat and use crass physical  violence are indeed generally condemned for it.<br /><br />I don't have this  trait. I don't value status in and of itself. It's useful, because it  lets me do other things. It opens doors. So I invest in still having  status, but status is not a goal; Status is to me, as a fork is to  hunger - merely a means to an end.<br /><br />So I have never, not once in  my life, been able to comprehend the simple truth: 90% of the people I  meet, quite possibly more, value status, as an intrinsic thing. Indeed,  they are <em>meant</em> to use their intelligence as a tool to obtain this status. It is how we rose to where we are in the world.<br /><br />I  don't know what to make of this. It means everything I'd pieced  together about people is utterly, utterly wrong, because it assumed that  they all valued truth, and understanding - the pursuits of intelligence  when you don't have the political trait.<br /><br />I am, for a moment, deeply, deeply lost.<br /><br />But, I notice, I am no longer <em>confused</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GCQvWvYvgRcaC23BP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 29, "extendedScore": null, "score": 1.0667089051417852e-06, "legacy": true, "legacyId": "20829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-24T21:14:52.081Z", "modifiedAt": null, "url": null, "title": "[LINK] Too Soon for Doom Gloom?", "slug": "link-too-soon-for-doom-gloom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nZ8CvbNDMeLFYCK6q/link-too-soon-for-doom-gloom", "pageUrlRelative": "/posts/nZ8CvbNDMeLFYCK6q/link-too-soon-for-doom-gloom", "linkUrl": "https://www.lesswrong.com/posts/nZ8CvbNDMeLFYCK6q/link-too-soon-for-doom-gloom", "postedAtFormatted": "Monday, December 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Too%20Soon%20for%20Doom%20Gloom%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Too%20Soon%20for%20Doom%20Gloom%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZ8CvbNDMeLFYCK6q%2Flink-too-soon-for-doom-gloom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Too%20Soon%20for%20Doom%20Gloom%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZ8CvbNDMeLFYCK6q%2Flink-too-soon-for-doom-gloom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZ8CvbNDMeLFYCK6q%2Flink-too-soon-for-doom-gloom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Came across <a href=\"http://arxiv.org/abs/gr-qc/9407002\">this paper</a> posted in gr-qc (General Relativity and Quantum Cosmology) of all places. Abstract:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14px; line-height: 19px;\">The observation that we are among the first 10^11 or so humans reduces the prior probability that we find ourselves in a species whose total lifetime number of individuals is much higher, according to arguments of Carter, Leslie, Nielsen, and Gott. However, if we instead start with a prior probability that a history has a total lifetime number which is very large, without assuming that we are in such a history, this more basic probability is not reduced by the observation of how early in history we exist.</span></p>\n<p><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14px; line-height: 19px;\">While I am skeptical of anything <a href=\"http://en.wikipedia.org/wiki/Don_Page_(physicist)\">Don Page</a>&nbsp;(Hawking's student <strong>and</strong>&nbsp;apparently an Evangelical Christian) writes on the topic of anthropics (he publishes stuff on Boltzmann Brains, for pity's sake), Stuart Armstrong and other resident experts in the area should be able to tell if this paper has any substance. If anything, it has a good list of references on the topic.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nZ8CvbNDMeLFYCK6q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 1.0667345239224143e-06, "legacy": true, "legacyId": "20830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-25T01:16:59.961Z", "modifiedAt": "2022-05-16T21:06:36.988Z", "url": null, "title": "Godel's Completeness and Incompleteness Theorems", "slug": "godel-s-completeness-and-incompleteness-theorems", "viewCount": null, "lastCommentedAt": "2015-10-07T03:40:04.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GZjGtd35vhCnzSQKy/godel-s-completeness-and-incompleteness-theorems", "pageUrlRelative": "/posts/GZjGtd35vhCnzSQKy/godel-s-completeness-and-incompleteness-theorems", "linkUrl": "https://www.lesswrong.com/posts/GZjGtd35vhCnzSQKy/godel-s-completeness-and-incompleteness-theorems", "postedAtFormatted": "Tuesday, December 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Godel's%20Completeness%20and%20Incompleteness%20Theorems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGodel's%20Completeness%20and%20Incompleteness%20Theorems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGZjGtd35vhCnzSQKy%2Fgodel-s-completeness-and-incompleteness-theorems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Godel's%20Completeness%20and%20Incompleteness%20Theorems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGZjGtd35vhCnzSQKy%2Fgodel-s-completeness-and-incompleteness-theorems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGZjGtd35vhCnzSQKy%2Fgodel-s-completeness-and-incompleteness-theorems", "socialPreviewImageUrl": "http://wiki.lesswrong.com/mediawiki/images/archive/2/2e/20121213015229!WRONG.jpg", "question": false, "authorIsUnreviewed": false, "wordCount": 3395, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/g0i/standard_and_nonstandard_numbers/\">Standard and Nonstandard Numbers</a></p>\n<p>So... last time you claimed that using first-order axioms to rule out the existence of nonstandard numbers - other chains of numbers besides the 'standard' numbers starting at 0 - was&nbsp;<em>forever and truly impossible</em>, even unto a superintelligence, no matter&nbsp;<em>how&nbsp;</em>clever the first-order logic used, even if you came up with an entirely different way of axiomatizing the numbers.</p>\n<p>\"Right.\"</p>\n<p>How could you, in your finiteness, possibly know that?</p>\n<p>\"Have you heard of Godel's Incompleteness Theorem?\"</p>\n<p>Of course! Godel's Theorem says that for every consistent mathematical system, there are statements which are&nbsp;<em>true&nbsp;</em>within that system, which can't be&nbsp;<em>proven</em>&nbsp;within the system itself. Godel came up with a way to encode theorems and proofs as numbers, and wrote a purely numerical formula to detect whether a proof obeyed proper logical syntax. The basic trick was to use prime factorization to encode lists; for example, the ordered list &lt;3, 7, 1, 4&gt; could be uniquely encoded as:</p>\n<p>2<sup>3</sup>&nbsp;* 3<sup>7</sup>&nbsp;* 5<sup>1</sup>&nbsp;* 7<sup>4</sup></p>\n<p>And since prime factorizations are unique, and prime powers don't mix, you could inspect this single number, 210,039,480, and get the unique ordered list &lt;3, 7, 1, 4&gt; back out. From there, going to an encoding for logical formulas was easy; for example, you could use the 2 prefix for NOT and the 3 prefix for AND and get, for any formulas &Phi; and &Psi; encoded by the numbers #&Phi; and #&Psi;:</p>\n<p>&not;&Phi; = 2<sup>2</sup>&nbsp;* 3<sup>#&Phi;</sup></p>\n<p>&Phi;&nbsp;&and; &Psi; = 2<sup>3</sup>&nbsp;* 3<sup>#&Phi;</sup>&nbsp;* 5<sup>#&Psi;</sup></p>\n<p>It was then possible, by dint of crazy amounts of work, for Godel to come up with a gigantic formula of Peano Arithmetic [](p, c) meaning, 'P encodes a valid logical proof using first-order Peano axioms of C', from which directly followed the formula []c, meaning, 'There exists a number P such that P encodes a proof of C' or just 'C is provable in Peano arithmetic.'</p>\n<p>Godel then put in some&nbsp;<em>further&nbsp;</em>clever work to invent statements which referred to&nbsp;<em>themselves</em>, by having them contain sub-recipes that would reproduce the entire statement when manipulated by another formula.</p>\n<p>And then Godel's Statement encodes the statement, 'There does not exist any number P such that P encodes a proof of (this statement) in Peano arithmetic' or in simpler terms 'I am not provable in Peano arithmetic'. If we assume first-order arithmetic is consistent and sound, then no&nbsp;<em>proof&nbsp;</em>of this statement&nbsp;<em>within&nbsp;</em>first-order arithmetic exists, which means the statement is&nbsp;<em>true&nbsp;</em>but can't be proven within the system. That's Godel's Theorem.</p>\n<p>\"Er... no.\"</p>\n<p>No?</p>\n<p>\"No. I've heard rumors that Godel's Incompleteness Theorem is horribly misunderstood in your Everett branch. Have you heard of Godel's&nbsp;<em>Completeness&nbsp;</em>Theorem?\"</p>\n<p>Is that a thing?</p>\n<p>\"<a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_completeness_theorem\">Yes!</a>&nbsp;Godel's Completeness Theorem says that, for any collection of first-order statements,&nbsp;<em>every semantic implication of those statements is syntactically provable within first-order logic</em>. If something is a genuine implication of a collection of first-order statements - if it actually&nbsp;<em>does&nbsp;</em>follow, in the models pinned down by those statements - then you can&nbsp;<em>prove&nbsp;</em>it,&nbsp;<em>within&nbsp;</em>first-order logic, using&nbsp;<em>only</em>&nbsp;the syntactical rules of proof, from those axioms.\"<a id=\"more\"></a></p>\n<p>I don't see how that could possibly be true at the same time as Godel's Incompleteness Theorem. The Completeness Theorem and Incompleteness Theorem seem to say diametrically opposite things. Godel's Statement is implied by the axioms of first-order arithmetic - that is, we can see it's true using our own mathematical reasoning -</p>\n<p>\"Wrong.\"</p>\n<p>What? I mean, I understand we can't prove it&nbsp;<em>within&nbsp;</em>Peano arithmetic, but from outside the system we can see that -</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/archive/2/2e/20121213015229!WRONG.jpg\" alt=\"\" /></p>\n<p>All right, explain.</p>\n<p>\"Basically, you just committed the equivalent of saying, 'If all kittens are little, and some little things are innocent, then some kittens are innocent.' There are universes - logical models - where it so happens that the premises are true and the conclusion also happens to be true:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/b9/13_img1.jpg\" alt=\"\" width=\"575\" height=\"193\" /></p>\n<p>\"But there are also valid models of the premises where the conclusion is false:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/45/13_img2.jpg\" alt=\"\" width=\"574\" height=\"193\" /></p>\n<p>\"If you, yourself, happened to live in a universe like the first one - if, in your mind, you were&nbsp;<em>only thinking</em>&nbsp;about a universe like that - then you might&nbsp;<em>mistakenly</em>&nbsp;think that you'd proven the conclusion. But your statement is not&nbsp;<em>logically&nbsp;</em>valid, the conclusion is not true in&nbsp;<em>every&nbsp;</em>universe where the premises are true. It's like saying, 'All apples are plants. All fruits are plants. Therefore all apples are fruits.' Both the premises and the conclusions happen to be true in&nbsp;<em>this&nbsp;</em>universe, but it's not valid logic.\"</p>\n<p>Okay, so how does this invalidate my previous explanation of Godel's Theorem?</p>\n<p>\"Because of the non-standard models of first-order arithmetic. First-order arithmetic narrows things down a lot - it rules out 3-loops of nonstandard numbers, for example, and mandates that every model contain the number 17 - but it doesn't pin down a&nbsp;<em>single&nbsp;</em>model. There's still the possibility of infinite-in-both-directions chains coming after the 'standard' chain that starts with 0. Maybe&nbsp;<em>you&nbsp;</em>have just the standard numbers in mind, but that's not the&nbsp;<em>only&nbsp;</em>possible model of first-order arithmetic.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d3/13_img3.jpg\" alt=\"\" width=\"400\" height=\"400\" /></p>\n<p>So?</p>\n<p>\"So in some of those other models, there are nonstandard numbers which - according to Godel's&nbsp;<em>arithmetical&nbsp;</em>formula for encodes-a-proof - are 'nonstandard proofs' of Godel's Statement. I mean, they're not what we would call&nbsp;<em>actual&nbsp;</em>proofs. An actual proof would have a standard number corresponding to it. A nonstandard proof might look like... well, it's hard to envision, but it might be something like, 'Godel's statement is true, because not-not-Godel's statement, because not-not-not-not-Godel's statement', and so on going&nbsp;<em>backward forever</em>, every step of the proof being valid, because nonstandard numbers have an infinite number of predecessors.\"</p>\n<p>And there's no way to say, 'You can't have an infinite number of derivations in a proof'?</p>\n<p>\"Not in first-order logic. If you could say that, you could rule out numbers with infinite numbers of predecessors, meaning that you could rule out all infinite-in-both-directions chains, and hence rule out all nonstandard numbers. And then the only&nbsp;<em>remaining&nbsp;</em>model would be the standard numbers. And then Godel's Statement would be a&nbsp;<em>semantic&nbsp;</em>implication of those axioms; there would exist&nbsp;<em>no&nbsp;</em>number encoding a proof of Godel's Statement in&nbsp;<em>any&nbsp;</em>model which obeyed the axioms of first-order arithmetic. And then, by Godel's&nbsp;<em>Completeness&nbsp;</em>Theorem, we could prove Godel's Statement from those axioms using first-order syntax. Because every&nbsp;<em>genuinely&nbsp;</em>valid implication of any collection of first-order axioms - every first-order statement that&nbsp;<em>actually does follow, in every possible model where the premises are true</em>&nbsp;- can&nbsp;<em>always&nbsp;</em>be proven, from those axioms, in first-order logic. Thus, by the&nbsp;<em>combination&nbsp;</em>of Godel's Incompleteness Theorem and Godel's Completeness Theorem, we see that there's no way to uniquely pin down the natural numbers using first-order logic. QED.\"</p>\n<p>Whoa. So everyone in the human-superiority crowd gloating about how&nbsp;<em>they're</em>&nbsp;superior to mere machines and formal systems, because&nbsp;<em>they&nbsp;</em>can see that Godel's Statement is true just by their sacred and mysterious mathematical intuition...</p>\n<p>\"...Is actually committing a horrendous logical fallacy of the sort that no cleanly designed AI could ever be tricked into, yes. Godel's Statement doesn't&nbsp;<em>actually follow</em>&nbsp;from the first-order axiomatization of Peano arithmetic! There are models where all the first-order axioms are true, and yet Godel's Statement is false! The standard misunderstanding of Godel's Statement&nbsp;<em>is</em>&nbsp;something like the situation as it obtains in&nbsp;<em>second</em>-order logic, where there's no equivalent of Godel's Completeness Theorem. But people in the human-superiority crowd usually don't attach that disclaimer - they usually present arithmetic using the first-order version, when they're explaining what it is that they can see that a formal system can't. It's safe to say that&nbsp;<em>most&nbsp;</em>of them are inadvertently illustrating the irrational overconfidence of humans jumping to conclusions, even though there's a less stupid version of the same argument which invokes second-order logic.\"</p>\n<p>Nice. But still... that proof you've shown me seems like a rather&nbsp;<em>circuitous&nbsp;</em>way of showing that you can't ever rule out infinite chains, especially since I don't see why Godel's Completeness Theorem should be true.</p>\n<p>\"Well... an equivalent way of stating Godel's Completeness Theorem is that every <em>syntactically </em>consistent set of first-order axioms - that is, every set of first-order axioms such that you cannot <em>syntactically</em>&nbsp;prove a contradiction from them using first-order logic - has at least one semantic model. &nbsp;The proof proceeds by trying to adjoin statements saying P or ~P for every first-order formula P, at least one of which must be possible to adjoin while leaving the expanded theory syntactically consistent -\"</p>\n<p>Hold on. &nbsp;Is there some more <em>constructive</em>&nbsp;way of seeing why a non-standard model has to exist?</p>\n<p>\"Mm... you could invoke the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Compactness_theorem\">Compactness Theorem</a>&nbsp;for first-order logic. The Compactness Theorem says that&nbsp;<em>if a collection of first-order statements has no model, some finite subset of those statements is also semantically unrealizable</em>. In other words, if a collection of first-order statements - even an&nbsp;<em>infinite&nbsp;</em>collection - is unrealizable in the sense that no possible mathematical model fits all of those premises, then there must be <em>some</em>&nbsp;finite subset of premises which are also unrealizable. Or modus ponens to modus tollens, if all finite subsets of a collection of axioms have at least one model, then the whole infinite collection of axioms has at least one model.\"</p>\n<p>Ah, and can you explain why the Compactness Theorem should be true?</p>\n<p>\"<a href=\"http://www.proofwiki.org/wiki/Compactness_Theorem\">No.</a>\"</p>\n<p>I see.</p>\n<p>\"But at least it's simpler than the Completeness Theorem, and from the Compactness Theorem, the inability of first-order arithmetic to pin down a standard model of numbers follows immediately. Suppose we take first-order arithmetic, and adjoin an axiom which says, 'There exists a number greater than 0.' Since there does in fact exist a number, 1, which is greater than 0, first-order arithmetic plus this new axiom should be semantically okay - it should have a model if any model of first-order arithmetic ever existed in the first place. Now let's adjoin a new constant symbol <em>c</em>&nbsp;to the language, i.e., <em>c</em>&nbsp;is a constant symbol referring to a single object across all statements where it appears, the way 0 is a constant symbol and an axiom then identifies 0 as the object which is not the successor of any object. &nbsp;Then we start adjoining axioms saying&nbsp;'<em>c</em> is greater than X', where X is some concretely specified number like 0, 1, 17, 2<sup>256</sup>, and so on. &nbsp;In fact, suppose we adjoin an&nbsp;<em>infinite&nbsp;</em>series of such statements, one for every number:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/3b/13_img4.jpg\" alt=\"\" width=\"598\" height=\"560\" /></p>\n<p>Wait, so this new theory is saying that there exists a number <em>c</em>&nbsp;which is larger than every number?</p>\n<p>\"No, the infinite schema says that there exists a number <em>c</em>&nbsp;which is&nbsp;larger than any&nbsp;<em>standard&nbsp;</em>number.\"</p>\n<p>I see, so this new theory&nbsp;<em>forces&nbsp;</em>a nonstandard model of arithmetic.</p>\n<p>\"Right. It rules out&nbsp;<em>only&nbsp;</em>the standard model. And the Compactness Theorem says this new theory is still semantically realizable - it has <em>some </em>model, just not the standard one.\"</p>\n<p>Why?</p>\n<p>\"Because any finite subcollection of the new theory's axioms, can only use a finite number of the extra axioms.&nbsp;&nbsp;Suppose&nbsp;the largest extra axiom you used was '<em>c</em> is larger than 2<sup>256</sup>'. &nbsp;In the standard model, there certainly exists a number 2<sup>256</sup>+1 with which <em>c</em>&nbsp;could be consistently identified. So the standard numbers must be a model of that collection of axioms, and thus that finite subset of axioms must be semantically realizable. &nbsp;Thus by the Compactness Theorem, the full, infinite axiom system must also be semantically realizable; it must have at least one model. Now, adding axioms never&nbsp;<em>increases&nbsp;</em>the number of compatible models of an axiom system - each additional axiom can only&nbsp;<em>filter out</em>&nbsp;models, not&nbsp;<em>add&nbsp;</em>models which are incompatible with the other axioms. So this new model of the larger axiom system - containing a number which is greater than 0, greater than 1, and greater than every other 'standard' number - must&nbsp;<em>also&nbsp;</em>be a model of first-order Peano arithmetic. That's a relatively simpler proof that first-order arithmetic - in fact,&nbsp;<em>any&nbsp;</em>first-order axiomatization of arithmetic - has nonstandard models.\"</p>\n<p>Huh... I can't quite say that seems obvious, because the Compactness Theorem doesn't feel obvious; but at least it seems more specific than trying to prove it using Godel's Theorem.</p>\n<p>\"A similar construction to the one we used above - adding an infinite series of axioms saying that a thingy is even larger - shows that if a first-order theory has models of unboundedly large finite size, then it has at least one infinite model. To put it even more alarmingly, there's no way to characterize the property of&nbsp;<em>finiteness&nbsp;</em>in first-order logic! You can have a first-order theory which characterizes models of cardinality 3 - just say that there exist x, y, and z which are not equal to each other, but with all objects being equal to x or y or z. But there's no first-order theory which characterizes the property of&nbsp;<em>finiteness&nbsp;</em>in the sense that all finite models fit the theory, and no infinite model fits the theory. A first-order theory either limits the size of models to some particular upper bound, or it has infinitely large models.\"</p>\n<p>So you can't even say, 'x is finite', without using second-order logic? Just forming the&nbsp;<em>concept&nbsp;</em>of infinity and distinguishing it from finiteness requires second-order logic?</p>\n<p>\"Correct, for pretty much exactly the same reason you can't say 'x is only a finite number of successors away from 0'. You can say, 'x is less than a googolplex' in first-order logic, but not, in full generality, 'x is finite'. In fact there's an even&nbsp;<em>worse</em>&nbsp;theorem, the <a href=\"http://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem\">Lowenheim-Skolem theorem</a>, which roughly says that if a first-order theory has&nbsp;<em>any&nbsp;</em>infinite model, it has models&nbsp;<em>of all possible infinite cardinalities.</em>&nbsp; There&nbsp;are uncountable models of first-order Peano arithmetic. There are countable models of first-order real arithmetic - countable models of any attempt to axiomatize the real numbers in first-order logic. There are countable models of Zermelo-Frankel set theory.\"</p>\n<p>How could you&nbsp;<em>possibly&nbsp;</em>have a countable model of the real numbers? Didn't Cantor&nbsp;<em>prove&nbsp;</em>that the real numbers were uncountable? Wait, let me guess, Cantor implicitly used second-order logic somehow.</p>\n<p>\"It follows from the Lowenheim-Skolem theorem that he must've. Let's take Cantor's proof as showing that you can't map&nbsp;every&nbsp;set of integers onto a distinct integer - that is, the powerset of integers is larger than the set of integers. The Diagonal Argument is that if you show me a mapping like that, I can take the set which contains 0 if and only if 0 is not in the set mapped to the integer 0, contains 1 if and only if 1 is&nbsp;<em>not&nbsp;</em>in the set mapped to the integer 1, and so on. That gives you a set of integers that no integer maps to.\"</p>\n<p>You know, when I was very young indeed, I thought I'd found a&nbsp;<em>counterexample&nbsp;</em>to Cantor's argument. Just take the base-2 integers - 1='1', 2='10', 3='11', 4='100', 5='101', and so on, and let each integer correspond to a set in the obvious way, keeping in mind that I was also young enough to think the integers started at 1:</p>\n<p>\n<table border=\"1\" width=\"540px\">\n<tbody>\n<tr>\n<td width=\"60px\">1</td>\n<td width=\"60px\">10</td>\n<td width=\"60px\">11</td>\n<td width=\"60px\">100</td>\n<td width=\"60px\">101</td>\n<td width=\"60px\">110</td>\n<td width=\"60px\">111</td>\n<td width=\"60px\">1000</td>\n<td width=\"60px\">1001</td>\n</tr>\n<tr>\n<td width=\"60px\">{1}</td>\n<td width=\"60px\">{2}</td>\n<td width=\"60px\">{2, 1}</td>\n<td width=\"60px\">{3}</td>\n<td width=\"60px\">{3, 1}</td>\n<td width=\"60px\">{3, 2}</td>\n<td width=\"60px\">{3, 2, 1}</td>\n<td width=\"60px\">{4}</td>\n<td width=\"60px\">{4, 1}</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>Clearly, every set of integers would map onto a unique integer this way.</p>\n<p>\"Heh.\"</p>\n<p>Yeah, I thought I was going to be famous.</p>\n<p>\"How'd you realize you were wrong?\"</p>\n<p>After an embarrassingly long interval, it occurred to me to actually try&nbsp;<em>applying</em>&nbsp;Cantor's Diagonal Argument to my own construction. Since 1 is in {1} and 2 is in {2}, they wouldn't be in the resulting set, but 3, 4, 5 and everything else would be. And of course my construct didn't have the set {3, 4, 5, ...} anywhere in it. I'd mapped all the&nbsp;<em>finite&nbsp;</em>sets of integers onto integers, but none of the infinite sets.</p>\n<p>\"Indeed.\"</p>\n<p>I was then tempted to&nbsp;<em>go on</em>&nbsp;arguing that Cantor's Diagonal Argument was wrong&nbsp;<em>anyhow&nbsp;</em>because it was wrong to have infinite sets of integers. Thankfully, despite my young age, I was self-aware enough to realize I was being tempted to become a mathematical crank - I had also read <a href=\"http://www.amazon.com/Mathematical-Cranks-Spectrum-Underwood-Dudley/dp/0883855070\">a book on mathematical cranks</a> by this point - and so I just quietly&nbsp;<a href=\"/lw/gx/just_lose_hope_already/\">gave up</a>, which was a valuable life lesson.</p>\n<p>\"Indeed.\"</p>\n<p>But how exactly does Cantor's Diagonal Argument depend on second-order logic? Is it something to do with nonstandard integers?</p>\n<p>\"Not exactly. What happens is that there's no way to make a first-order theory contain&nbsp;<em>all&nbsp;</em>subsets of an infinite set; there's no way to talk about&nbsp;<em>the&nbsp;</em>powerset of the integers. Let's illustrate using a finite metaphor. Suppose you have the axiom \"All kittens are innocent.\" One model of that axiom might contain five kittens, another model might contain six kittens.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/c4/13_img6.jpg\" alt=\"\" width=\"637\" height=\"188\" /></p>\n<p>\"In a second-order logic, you can talk about&nbsp;<em>all&nbsp;</em>possible collections of kittens - in fact, it's built into the syntax of the language when you quantify over all properties.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/ef/13_img7.jpg\" alt=\"\" width=\"540\" height=\"358\" /></p>\n<p>\"In a first-order set theory, there are&nbsp;<em>some&nbsp;</em>subsets of kittens whose existence is provable, but others might be missing.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a6/13_img8.jpg\" alt=\"\" width=\"540\" height=\"277\" /></p>\n<p>\"Though that image is only metaphorical, since you&nbsp;<em>can&nbsp;</em>prove the existence of all the finite subsets. Just imagine that's an infinite number of kittens we're talking about up there.\"</p>\n<p>And there's no way to say that&nbsp;<em>all possible</em>&nbsp;subsets exist?</p>\n<p>\"Not in first-order logic, just like there's no way to say that you want as few natural numbers as possible. Let's look at it from the standpoint of first-order set theory. The&nbsp;<a href=\"http://en.wikipedia.org/wiki/Axiom_of_power_set\">Axiom of Powerset</a>&nbsp;says:\"</p>\n<p><img src=\"http://upload.wikimedia.org/math/8/0/a/80adb23907ae85d345ca83a3d420b04d.png\" alt=\"\" /></p>\n<p>Okay, so that says, for every set A, there exists a set P which is the&nbsp;<em>power set</em>&nbsp;of all subsets of A, so that for every set B, B is inside the powerset P&nbsp;<em>if and only if</em>&nbsp;every element of B is an element of A. Any set which contains only elements from A, will be inside the powerset of A. Right?</p>\n<p>\"Almost. There's just one thing wrong in that explanation - the word 'all' when you say 'all subsets'. The Powerset Axiom says that for any collection of elements from A,&nbsp;<em>if a set B happens to exist</em>&nbsp;which embodies that collection, that set B is inside the powerset P of A. There's no way of saying, within a first-order logical theory, that a set exists for&nbsp;<em>every possible&nbsp;</em>collection of A's elements. There may be&nbsp;<em>some&nbsp;</em>sub-collections of A whose existence you can prove. But other sub-collections of A will happen to exist as sets inside some models, but not exist in others.\"</p>\n<p>So in the same way that first-order Peano arithmetic suffers from mysterious extra numbers, first-order set theory suffers from mysterious missing subsets.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a6/13_img8.jpg\" alt=\"\" width=\"270\" height=\"140\" /></p>\n<p>\"Precisely. A first-order set theory might happen to be missing the particular infinite set corresponding to, oh, say, {3, 8, 17, 22, 28, ...} where the '...' is an infinite list of random numbers with no&nbsp;<em>compact&nbsp;</em>way of specifying them. If there's a compact way of specifying a set - if there's a finite formula that describes it - you can often prove it exists. But&nbsp;<em>most&nbsp;</em>infinite sets won't have any finite specification. It's precisely the claim to generalize over&nbsp;<em>all possible collections</em>&nbsp;that characterizes second-order logic. So it's trivial to say in a second-order set theory that&nbsp;<em>all&nbsp;</em>subsets exist. You would just say that for any set A, for any possible predicate P, there exists a set B which contains x iff x in A and Px.\"</p>\n<p>I guess that torpedoes my clever idea about using first-order set theory to uniquely characterize the standard numbers by first asserting that there exists a set containing&nbsp;<em>at least</em>&nbsp;the standard numbers, and then talking about the&nbsp;<em>smallest subset</em>&nbsp;which obeys the Peano axioms.</p>\n<p>\"Right. When you talk about the numbers using first-order set theory, if there are&nbsp;<em>extra&nbsp;</em>numbers inside your set of numbers, the subset containing&nbsp;<em>just&nbsp;</em>the standard numbers must be missing from the powerset of that set. Otherwise you could find the smallest subset inside the powerset such that it contained 0 and contained the successor of every number it contained.\"</p>\n<p>Hm. So then what exactly goes wrong with Cantor's Diagonal Argument?</p>\n<p>\"Cantor's Diagonal Argument uses the idea of a mapping between integers and sets of integers. In set theory, each mapping would itself be a set - in fact there would be a set of all mapping sets:\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/3b/13_img9.jpg\" alt=\"\" width=\"491\" height=\"348\" /></p>\n<p>\"There's no way to first-order assert the existence of <em>every possible mapping</em>&nbsp;that&nbsp;<em>we&nbsp;</em>can imagine from outside. So a first-order version of the Diagonal Argument would show that in any&nbsp;<em>particular&nbsp;</em>model, for any mapping&nbsp;<em>that existed in the model</em>&nbsp;from integers to sets of integers, the model would also contain a diagonalized set of integers that wasn't in that mapping. This doesn't mean that <em>we </em>couldn't count all the sets of integers which&nbsp;<em>existed </em><em>in the model.</em>&nbsp; The model could have so many 'missing' sets&nbsp;of integers that the remaining sets were denumerable. But then some mappings from integers to sets would also be missing, and in particular, the 'complete' mapping we can imagine from outside would be missing. And for every mapping that&nbsp;<em>was&nbsp;</em>in the model, the Diagonal Argument would construct a set of integers that wasn't in the mapping. On the outside, <em>we </em>would see a possible mapping from integers to sets - but that mapping wouldn't exist&nbsp;<em>inside&nbsp;</em>the model as a set. &nbsp;It takes a logic-of-collections to say that&nbsp;<em>all possible</em>&nbsp;integer-collections exist as sets, or that <em>no&nbsp;possible</em>&nbsp;mapping exists from the integers onto those sets.\"</p>\n<p>So if first-order logic can't even talk about&nbsp;<em>finiteness&nbsp;</em>vs.&nbsp;<em>infiniteness&nbsp;</em>- let alone prove that there are&nbsp;<em>really&nbsp;</em>more sets of integers than integers - then why is anyone interested in first-order logic in the first place? Isn't that like trying to eat dinner using only a fork, when there are lots of interesting foods which <em>provably </em>can't&nbsp;be eaten with a fork, and you have a spoon?</p>\n<p>\"Ah, well... some people believe there&nbsp;<em>is&nbsp;</em>no spoon. But let's take that up next time.\"</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/g7n/secondorder_logic_the_controversy/\">Second-Order Logic: The Controversy</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/g0i/standard_and_nonstandard_numbers/\">Standard and Nonstandard Numbers</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 2, "AJDHQ4mFnsNbBzPhT": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GZjGtd35vhCnzSQKy", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 60, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "20806", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://wiki.lesswrong.com/mediawiki/images/archive/2/2e/20121213015229!WRONG.jpg", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "second-order-logic-the-controversy", "canonicalPrevPostSlug": "standard-and-nonstandard-numbers", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i7oNcHR3ZSnEAM29X", "waqC6FihC2ryAZuAq", "SWn4rqdycu83ikfBa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-25T01:16:59.961Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-25T02:20:58.314Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Two Visions of Heritage", "slug": "seq-rerun-two-visions-of-heritage", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p7La2o6yr8KmyHecb/seq-rerun-two-visions-of-heritage", "pageUrlRelative": "/posts/p7La2o6yr8KmyHecb/seq-rerun-two-visions-of-heritage", "linkUrl": "https://www.lesswrong.com/posts/p7La2o6yr8KmyHecb/seq-rerun-two-visions-of-heritage", "postedAtFormatted": "Tuesday, December 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Two%20Visions%20of%20Heritage&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Two%20Visions%20of%20Heritage%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7La2o6yr8KmyHecb%2Fseq-rerun-two-visions-of-heritage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Two%20Visions%20of%20Heritage%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7La2o6yr8KmyHecb%2Fseq-rerun-two-visions-of-heritage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7La2o6yr8KmyHecb%2Fseq-rerun-two-visions-of-heritage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/two-visions-of.html\">Two Visions Of Heritage</a> was originally published on December 9, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Differences between Hanson and Yudkowsky on how they see heritage.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g2d/seq_rerun_are_ais_homo_economicus/\">Are AIs Homo Economicus?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p7La2o6yr8KmyHecb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.0669148167272928e-06, "legacy": true, "legacyId": "20832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dBW93B7op99skq7i4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-25T13:04:46.889Z", "modifiedAt": null, "url": null, "title": "Ideal Advisor Theories and Personal CEV", "slug": "ideal-advisor-theories-and-personal-cev", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q9ZSXiiA7wEuRgnkS/ideal-advisor-theories-and-personal-cev", "pageUrlRelative": "/posts/q9ZSXiiA7wEuRgnkS/ideal-advisor-theories-and-personal-cev", "linkUrl": "https://www.lesswrong.com/posts/q9ZSXiiA7wEuRgnkS/ideal-advisor-theories-and-personal-cev", "postedAtFormatted": "Tuesday, December 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ideal%20Advisor%20Theories%20and%20Personal%20CEV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdeal%20Advisor%20Theories%20and%20Personal%20CEV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ZSXiiA7wEuRgnkS%2Fideal-advisor-theories-and-personal-cev%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ideal%20Advisor%20Theories%20and%20Personal%20CEV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ZSXiiA7wEuRgnkS%2Fideal-advisor-theories-and-personal-cev", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ZSXiiA7wEuRgnkS%2Fideal-advisor-theories-and-personal-cev", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2903, "htmlBody": "<p><strong>Update 5-24-2013</strong>: A cleaned-up, citable version of this article is now available <a href=\"http://intelligence.org/files/IdealAdvisorTheories.pdf\">on MIRI's website</a>.</p>\n<p>Co-authored with <a href=\"/user/crazy88/\">crazy88</a></p>\n<p><small><em>Summary</em>: Yudkowsky's \"coherent extrapolated volition\" (CEV) concept shares much in common Ideal Advisor theories in moral philosophy. Does CEV fall prey to the same objections which are raised against Ideal Advisor theories? Because CEV is an epistemic rather than a metaphysical proposal, it seems that at least one family of CEV approaches (inspired by Bostrom's parliamentary model) may escape the objections raised against Ideal Advisor theories. This is not a particularly ambitious post; it mostly aims to place CEV in the context of mainstream moral philosophy.</small></p>\n<p>What is of value to an agent? Maybe it's just whatever they desire. Unfortunately, our desires are often the product of ignorance or confusion. I may desire to drink from the glass on the table because I think it is water when really it is bleach. So perhaps something is of value to an agent if they would desire that thing <em>if fully informed</em>. But here we crash into a different problem. It might be of value for an agent who wants to go to a movie to look up the session times, but the fully informed version of the agent will not desire to do so &mdash; they are fully-informed and hence already know all the session times. The agent and its fully-informed counterparts have different needs. Thus, several philosophers have suggested that something is of value to an agent if an ideal version of that agent (fully informed, perfectly rational, etc.) would <em>advise</em> the non-ideal version of the agent to pursue that thing.</p>\n<p>This idea of idealizing or extrapolating an agent's preferences<sup>1</sup> goes back at least as far as <a href=\"http://www.amazon.com/The-methods-ethics-Henry-Sidgwick/dp/1171791895/\">Sidgwick (1874)</a>, who considered the idea that \"a man's future good\" consists in \"what he would now desire... if all the consequences of all the different [actions] open to him were accurately forseen...\" Similarly, <a href=\"http://www.amazon.com/A-Theory-Justice-John-Rawls/dp/0674000781/\">Rawls (1971)</a> suggested that a person's good is the plan \"that would be decided upon as the outcome of careful reflection in which the agent reviewed, in the light of all the relevant facts, what it would be like to carry out these plans...\" More recently, in an article about rational agents and moral theory, <a href=\"http://www.amazon.com/Utilitarianism-Beyond-Amartya-Sen/dp/0521287715\">Harsanyi (1982)</a> defined what an agent's rational wants as &ldquo;the preferences he <em>would</em> have if he had all the relevant factual information, always reasoned with the greatest possible care, and were in a state of mind most conducive to rational choice.&rdquo; Then, a few years later, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Railton-Facts-and-Values.pdf\">Railton (1986)</a> identified a person's good with \"what he would want himself to want... were he to contemplate his present situation from a standpoint fully and vividly informed about himself and his circumstances, and entirely free of cognitive error or lapses of instrumental rationality.\"</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Rosati-Persons-perspectives-and-full-information-accounts-of-the-good.pdf\">Rosati (1995)</a> calls these theories Ideal Advisor theories of value because they identify one's personal value with what an ideal version of oneself would advise the non-ideal self to value.</p>\n<p>Looking not for a metaphysical account of value but for a practical solution to machine ethics (<a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\">Wallach &amp; Allen 2009</a>; <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>), <a href=\"http://intelligence.org/files/CEV.pdf\">Yudkowsky (2004)</a> described a similar concept which he calls \"coherent extrapolated volition\" (CEV):</p>\n<blockquote>\n<p>In poetic terms, our <em>coherent extrapolated volition</em> is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.</p>\n</blockquote>\n<p>In other words, the CEV of humankind is about the preferences that we would have as a species if our preferences were extrapolated in certain ways. Armed with this concept, Yudkowsky then suggests that we implement CEV as an \"initial dynamic\" for \"Friendly AI.\" <a href=\"http://intelligence.org/files/CEV-MachineEthics.pdf\">Tarleton (2010)</a> explains that the intent of CEV is that \"our volition be extrapolated <em>once</em> and acted on. In particular, the initial extrapolation could generate an object-level goal system we would be willing to endow a superintelligent [machine] with.\"</p>\n<p>CEV theoretically avoids many problems with other approaches to machine ethics (Yudkowsky 2004; Tarleton 2010; Muehlhauser &amp; Helm 2012). However, there are reasons it may not succeed. In this post, we examine one such reason: Resolving CEV at the level of humanity (<em>Global CEV</em>) might require at least partially resolving CEV at the level of individuals (<em>Personal CEV</em>)<sup>2</sup>, but Personal CEV is similar to ideal advisor theories of value,<sup>3</sup> and such theories face well-explored difficulties. As such, these difficulties may undermine the possibility of determining the Global CEV of humanity.</p>\n<p>Before doing so, however, it's worth noting one key difference between Ideal Advisor theories of value and Personal CEV. Ideal Advisor theories typically are linguistic or metaphysical theories, while the role of Personal CEV is epistemic. Ideal Advisor theorists attempts to define <em>what it is</em> for something to be of value for an agent. Because of this, their accounts needs to give an unambiguous and plausible answer in all cases. On the other hand, Personal CEV's role is an epistemic one: it isn't intended to define what is of value for an agent. Rather, Personal CEV is offered as a technique that can help an AI to <em>come to know</em>, to some reasonable but not necessarily perfect level of accuracy, what is of value for the agent. To put it more precisely, Personal CEV is intended to allow an initial AI to determine what sort of superintelligence to create such that we end up with what Yudkowsky calls a \"Nice Place to Live.\" Given this, certain arguments are likely to threaten Ideal Advisor theories and not to Personal CEV, and vice versa.</p>\n<p>With this point in mind, we now consider some objections to ideal advisor theories of value, and examine whether they threaten Personal CEV.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3>Sobel's First Objection: Too many voices</h3>\n<p>Four prominent objections to ideal advisor theories are due to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel (1994)</a>. The first of these, the &ldquo;too many voices&rdquo; objection, notes that the evaluative perspective of an agent changes over time and, as such, the views that would be held by the perfectly rational and fully informed version of the agent will also change. This implies that each agent will be associated not with one idealized version of themselves but with a set of such idealized versions (one at time <em>t</em>, one at time <em>t+1</em>, etc.), some of which may offer conflicting advice. Given this &ldquo;discordant chorus,&rdquo; it is unclear how the agent&rsquo;s non-moral good should be determined.</p>\n<p>Various responses to this objection run into their own challenges. First, privileging a single perspective (say, the idealized agent at time <em>t+387</em>) seems ad hoc. Second, attempting to aggregate the views of multiple perspectives runs into the question of how trade offs should be made. That is, if two of the idealized viewpoints disagree about what is to be preferred, it&rsquo;s unclear how an overall judgment should be reached.<sup>4</sup> Finally, suggesting that the idealized versions of the agent at different times will have the same perspective seems unlikely, and surely it's a substantive claim requiring a substantive defense. So the obvious responses to Sobel&rsquo;s first objection introduce serious new challenges which then need to be resolved.</p>\n<p>One final point is worth noting: it seems that this objection is equally problematic for Personal CEV. The extrapolated volition of the agent is likely to vary at different times, so how ought we determine an overall account of the agent&rsquo;s extrapolated volition?</p>\n<p>&nbsp;</p>\n<h3>Sobel&rsquo;s Second and Third Objections: Amnesia</h3>\n<p>Sobel&rsquo;s second and third objections build on two other claims (see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel 1994</a> for a defense of these). First: some lives can only be evaluated if they are experienced. Second: experiencing one life can leave you incapable of experiencing another in an unbiased way. Given these claims, Sobel presents an <em>amnesia model</em> as the most plausible way for an idealized agent to gain the experiences necessary to evaluate all the relevant lives. According to this model, an agent experiences each life sequentially but undergoes an amnesia procedure after each one so that they may experience the next life uncolored by their previous experiences. After experiencing all lives, the amnesia is then removed.</p>\n<p>Following on from this, Sobel&rsquo;s second objection is that the sudden recollection of a life from one evaluative perspective and living a life from a vastly different evaluative perspective may be strongly dissimilar experiences. So when the amnesia is removed, the agent has a particular evaluative perspective (informed by their memories of all the lives they&rsquo;ve lived) that differs so much from the evaluative perspective they had when they lived the life independently of such memories that they might be incapable of adequately evaluating the lives they&rsquo;ve experienced based on their current, more knowledgeable, evaluative perspective.</p>\n<p>Sobel&rsquo;s third objection also relates to the amnesia model: Sobel argues that the idealized agent might be driven insane by the entire amnesia process and hence might not be able to adequately evaluate what advice they ought to give the non-ideal agent. In response to this, there is some temptation to simply demand that the agent be idealized not just in terms of rationality and knowledge but also in terms of their sanity. However, perhaps any idealized agent that is similar enough to the original to serve as a standard for their non-moral good will be driven insane by the amnesia process and so the demand for a sane agent will simply mean that no adequate agent can be identified.</p>\n<p>If we grant that an agent needs to experience some lives to evaluate them, and we grant that experiencing some lives leaves them incapable of experiencing others, then there seems to be a strong drive for Personal CEV to rely on an amnesia model to adequately determine what an agent&rsquo;s volition would be if extrapolated. If so, however, then Personal CEV seems to face the challenges raised by Sobel.</p>\n<p>&nbsp;</p>\n<h3>Sobel&rsquo;s Fourth Objection: Better Off Dead</h3>\n<p>Sobel&rsquo;s final objection is that the idealized agent, having experienced such a level of perfection, might come to the conclusion that their non-ideal counterpart is so limited as to be better off dead. Further, the ideal agent might make this judgment because of the relative level of well-being of the non-ideal agent rather than the agent&rsquo;s absolute level of well-being. (That is, the ideal agent may look upon the well-being of the non-ideal agent as we might look upon our own well-being after an accident that caused us severe mental damage. In such a case, we might be unable to objectively judge our life after the accident due to the relative difficulty of this life as compared with our life before the accident.) As such, this judgment may not capture what is actually in accordance with the agent&rsquo;s non-moral good.</p>\n<p>Again, this criticism seems to apply equally to Personal CEV: when the volition of an agent is extrapolated, it may turn out that this volition endorses killing the non-extrapolated version of the agent. If so this seems to be a mark against the possibility that Personal CEV can play a useful part in a process that should eventually terminate in a \"Nice Place to Live.\"</p>\n<p>&nbsp;</p>\n<h3>A model of Personal CEV</h3>\n<p>The seriousness of these challenges for Personal CEV is likely to vary depending on the exact nature of the extrapolation process. To give a sense of the impact, we will consider one family of methods for carrying out this process: the <em>parliamentary model</em> (inspired by <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom 2009</a>). According to this model, we determine the Personal CEV of an agent by simulating multiple versions of them, extrapolated from various starting times and along different developmental paths. Some of these versions are then assigned as a parliament where they vote on various choices and make trades with one another.</p>\n<p>Clearly this approach allows our account of Personal CEV to avoid the too many voices objection. After all, the parliamentary model provides us with an account of how we can aggregate the views of the agent at various times: we should simulate the various agents and allow them to vote and trade on the choices to be made. It is through this voting and trading that the various voices can be combined into a single viewpoint. While this process may not be adequate as a metaphysical account of value, it seems more plausible as an account of Personal CEV as an epistemic notion. Certainly, your authors would deem themselves to be more informed about what they value if they knew the outcome of the parliamentary model for themselves.</p>\n<p>This approach is also able to avoid Sobel&rsquo;s second and third objections. The objections were specifically targeted at the amnesia model where one agent experienced multiple lives. As the parliamentary model does not utilize amnesia, it is immune to these concerns.</p>\n<p>What of Sobel&rsquo;s fourth objection? Sobel&rsquo;s concern here is not simply that the idealized agent might advise the agent to kill themselves. After all, sometimes death may, in fact, be of value for an agent. Rather, Sobel&rsquo;s concern is that the idealized agent, having experienced such heights of existence, will become biased against the limited lives of normal agents.</p>\n<p>It's less clear how the parliamentary model deals with Sobel's fourth objection which plausibly retains its initial force against this model of Personal CEV. However, we're not intending to solve Personal CEV entirely in this short post. Rather, we aim to demonstrate only that the force of Sobel's four objections will depend on the model of Personal CEV selected. Reflection on the parliamentary model makes this point clear.</p>\n<p>So the parliamentary model seems able to avoid at least three of the direct criticisms raised by Sobel. It is worth noting, however, that some concerns remain. Firstly, for those that accept Sobel&rsquo;s claim that experience is necessary to evaluate some lives, it is clear that no member of the parliament will be capable of comparing their life to all other possible lives, as none will have all the required experience. As such, the agents may falsely judge a certain aspect of their life to be more or less valuable than it, in fact, is. For a metaphysical account of personal value, this problem might be fatal. Whether it is also fatal for the parliamentary model of Personal CEV depends on whether the knowledge of the various members of the parliament is enough to produce a &ldquo;Nice Place to Live&rdquo; regardless of its imperfection.</p>\n<p>Two more issues might arise. First, the model might require careful selection of who to appoint to the parliament. For example, if most of the possible lives that an agent could live would drive them insane, then selecting which of these agents to appoint to the parliament at random might lead to a vote by the mad. Second, it might seem that this approach to determining Personal CEV will require a reasonable level of accuracy in simulation. If so, there might be concerns about the creation of, and responsibility to, potential moral agents.</p>\n<p>Given these points, a full evaluation of the parliamentary model will require more detailed specification and further reflection. However, two points are worth noting in conclusion. First, the parliamentary model does seem to avoid at least three of Sobel&rsquo;s direct criticisms. Second, even if this model eventually ends up being flawed on other grounds, the existence of one model of Personal CEV that can avoid three of Sobel&rsquo;s objections gives us reason to expect other promising models of Personal CEV may be discovered.</p>\n<p>&nbsp;</p>\n<h3>Notes</h3>\n<p><sup>1</sup> Another clarification to make concerns the difference between <a href=\"http://en.wikipedia.org/wiki/Idealization\">idealization</a> and <a href=\"http://en.wikipedia.org/wiki/Extrapolation\">extrapolation</a>. An <em>idealized agent</em> is a version of the agent with certain idealizing characteristics (perhaps logical omniscience and infinite speed of thought). An <em>extrapolated agent</em> is a version of the agent that represents what they would be like if they underwent certain changes or experiences. Note two differences between these concepts. First, an extrapolated agent need not be ideal in any sense (though useful extrapolated agents often will be) and certainly need not be <em>perfectly</em> idealized. Second, extrapolated agent are determined by a specific type of process (extrapolation from the original agent) whereas no such restriction is placed on how the form of an idealized agent is determined. CEV utilizes extrapolation rather than idealization, as do some Ideal Advisor theories. In this post, we talk about \"ideal\" or \"idealized\" agents as a catch-all for both idealized agents and extrapolated agents.</p>\n<p><sup>2</sup> Standard objections to ideal advisor theories of value are also relevant to some proposed variants of CEV, for example Tarleton (2010)'s suggestion of \"Individual Extrapolated Volition followed by Negotiation, where each individual human&rsquo;s preferences are extrapolated by factual correction and reflection; once that process is fully complete, the extrapolated humans negotiate a combined utility function for the resultant superintelligence...\" Furthermore, some objections to Ideal Advisor theories also seem relevant to Global CEV even if they are not relevant to a particular approach to Personal CEV, though that discussion is beyond the scope of this article. As a final clarification, see <a href=\"/lw/1oj/complexity_of_value_complexity_of_outcome/\">Dai (2010)</a>.</p>\n<p><sup>3</sup> Ideal Advisor theories are not to be confused with \"Ideal Observer theory\" (<a href=\"http://www.rci.rutgers.edu/~stich/104_Master_File/104_Readings/Firth/Firth%20-%20Ideal%20Observer.pdf\">Firth 1952</a>). For more on Ideal Advisor theories of value, see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Zimmerman-Why-Richard-Brandt-does-not-need-cognitive-psychotherapy.pdf\">Zimmerman (2003)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Tanyi-An-essay-on-the-desire-based-reasons-model.pdf\">Tanyi (2006)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Enoch-Why-Idealize.pdf\">Enoch (2005)</a>;&nbsp;<a href=\"http://www.amazon.com/Contemporary-Metaethics-Introduction-Alexander-Miller/dp/074564659X/\">Miller (2013, ch. 9)</a>.</p>\n<p><sup>4</sup> This is basically an intrapersonal version of the standard worries about interpersonal comparisons of well-being. The basis of these worries is that even if we can specify an agent&rsquo;s preferences numerically, it&rsquo;s unclear how we should compare the numbers assigned by one agent with the numbers assigned by the other. In the intrapersonal case, the challenge is to determine how to compare the numbers assigned by the same agent at different times. See <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Gibbard-Interpersonal-comparisons-preference-good-and-the-intrinsic-reward-of-a-life.pdf\">Gibbard (1989)</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q9ZSXiiA7wEuRgnkS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 35, "extendedScore": null, "score": 1.0672941917403586e-06, "legacy": true, "legacyId": "20849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Update 5-24-2013</strong>: A cleaned-up, citable version of this article is now available <a href=\"http://intelligence.org/files/IdealAdvisorTheories.pdf\">on MIRI's website</a>.</p>\n<p>Co-authored with <a href=\"/user/crazy88/\">crazy88</a></p>\n<p><small><em>Summary</em>: Yudkowsky's \"coherent extrapolated volition\" (CEV) concept shares much in common Ideal Advisor theories in moral philosophy. Does CEV fall prey to the same objections which are raised against Ideal Advisor theories? Because CEV is an epistemic rather than a metaphysical proposal, it seems that at least one family of CEV approaches (inspired by Bostrom's parliamentary model) may escape the objections raised against Ideal Advisor theories. This is not a particularly ambitious post; it mostly aims to place CEV in the context of mainstream moral philosophy.</small></p>\n<p>What is of value to an agent? Maybe it's just whatever they desire. Unfortunately, our desires are often the product of ignorance or confusion. I may desire to drink from the glass on the table because I think it is water when really it is bleach. So perhaps something is of value to an agent if they would desire that thing <em>if fully informed</em>. But here we crash into a different problem. It might be of value for an agent who wants to go to a movie to look up the session times, but the fully informed version of the agent will not desire to do so \u2014 they are fully-informed and hence already know all the session times. The agent and its fully-informed counterparts have different needs. Thus, several philosophers have suggested that something is of value to an agent if an ideal version of that agent (fully informed, perfectly rational, etc.) would <em>advise</em> the non-ideal version of the agent to pursue that thing.</p>\n<p>This idea of idealizing or extrapolating an agent's preferences<sup>1</sup> goes back at least as far as <a href=\"http://www.amazon.com/The-methods-ethics-Henry-Sidgwick/dp/1171791895/\">Sidgwick (1874)</a>, who considered the idea that \"a man's future good\" consists in \"what he would now desire... if all the consequences of all the different [actions] open to him were accurately forseen...\" Similarly, <a href=\"http://www.amazon.com/A-Theory-Justice-John-Rawls/dp/0674000781/\">Rawls (1971)</a> suggested that a person's good is the plan \"that would be decided upon as the outcome of careful reflection in which the agent reviewed, in the light of all the relevant facts, what it would be like to carry out these plans...\" More recently, in an article about rational agents and moral theory, <a href=\"http://www.amazon.com/Utilitarianism-Beyond-Amartya-Sen/dp/0521287715\">Harsanyi (1982)</a> defined what an agent's rational wants as \u201cthe preferences he <em>would</em> have if he had all the relevant factual information, always reasoned with the greatest possible care, and were in a state of mind most conducive to rational choice.\u201d Then, a few years later, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Railton-Facts-and-Values.pdf\">Railton (1986)</a> identified a person's good with \"what he would want himself to want... were he to contemplate his present situation from a standpoint fully and vividly informed about himself and his circumstances, and entirely free of cognitive error or lapses of instrumental rationality.\"</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Rosati-Persons-perspectives-and-full-information-accounts-of-the-good.pdf\">Rosati (1995)</a> calls these theories Ideal Advisor theories of value because they identify one's personal value with what an ideal version of oneself would advise the non-ideal self to value.</p>\n<p>Looking not for a metaphysical account of value but for a practical solution to machine ethics (<a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\">Wallach &amp; Allen 2009</a>; <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>), <a href=\"http://intelligence.org/files/CEV.pdf\">Yudkowsky (2004)</a> described a similar concept which he calls \"coherent extrapolated volition\" (CEV):</p>\n<blockquote>\n<p>In poetic terms, our <em>coherent extrapolated volition</em> is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.</p>\n</blockquote>\n<p>In other words, the CEV of humankind is about the preferences that we would have as a species if our preferences were extrapolated in certain ways. Armed with this concept, Yudkowsky then suggests that we implement CEV as an \"initial dynamic\" for \"Friendly AI.\" <a href=\"http://intelligence.org/files/CEV-MachineEthics.pdf\">Tarleton (2010)</a> explains that the intent of CEV is that \"our volition be extrapolated <em>once</em> and acted on. In particular, the initial extrapolation could generate an object-level goal system we would be willing to endow a superintelligent [machine] with.\"</p>\n<p>CEV theoretically avoids many problems with other approaches to machine ethics (Yudkowsky 2004; Tarleton 2010; Muehlhauser &amp; Helm 2012). However, there are reasons it may not succeed. In this post, we examine one such reason: Resolving CEV at the level of humanity (<em>Global CEV</em>) might require at least partially resolving CEV at the level of individuals (<em>Personal CEV</em>)<sup>2</sup>, but Personal CEV is similar to ideal advisor theories of value,<sup>3</sup> and such theories face well-explored difficulties. As such, these difficulties may undermine the possibility of determining the Global CEV of humanity.</p>\n<p>Before doing so, however, it's worth noting one key difference between Ideal Advisor theories of value and Personal CEV. Ideal Advisor theories typically are linguistic or metaphysical theories, while the role of Personal CEV is epistemic. Ideal Advisor theorists attempts to define <em>what it is</em> for something to be of value for an agent. Because of this, their accounts needs to give an unambiguous and plausible answer in all cases. On the other hand, Personal CEV's role is an epistemic one: it isn't intended to define what is of value for an agent. Rather, Personal CEV is offered as a technique that can help an AI to <em>come to know</em>, to some reasonable but not necessarily perfect level of accuracy, what is of value for the agent. To put it more precisely, Personal CEV is intended to allow an initial AI to determine what sort of superintelligence to create such that we end up with what Yudkowsky calls a \"Nice Place to Live.\" Given this, certain arguments are likely to threaten Ideal Advisor theories and not to Personal CEV, and vice versa.</p>\n<p>With this point in mind, we now consider some objections to ideal advisor theories of value, and examine whether they threaten Personal CEV.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3 id=\"Sobel_s_First_Objection__Too_many_voices\">Sobel's First Objection: Too many voices</h3>\n<p>Four prominent objections to ideal advisor theories are due to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel (1994)</a>. The first of these, the \u201ctoo many voices\u201d objection, notes that the evaluative perspective of an agent changes over time and, as such, the views that would be held by the perfectly rational and fully informed version of the agent will also change. This implies that each agent will be associated not with one idealized version of themselves but with a set of such idealized versions (one at time <em>t</em>, one at time <em>t+1</em>, etc.), some of which may offer conflicting advice. Given this \u201cdiscordant chorus,\u201d it is unclear how the agent\u2019s non-moral good should be determined.</p>\n<p>Various responses to this objection run into their own challenges. First, privileging a single perspective (say, the idealized agent at time <em>t+387</em>) seems ad hoc. Second, attempting to aggregate the views of multiple perspectives runs into the question of how trade offs should be made. That is, if two of the idealized viewpoints disagree about what is to be preferred, it\u2019s unclear how an overall judgment should be reached.<sup>4</sup> Finally, suggesting that the idealized versions of the agent at different times will have the same perspective seems unlikely, and surely it's a substantive claim requiring a substantive defense. So the obvious responses to Sobel\u2019s first objection introduce serious new challenges which then need to be resolved.</p>\n<p>One final point is worth noting: it seems that this objection is equally problematic for Personal CEV. The extrapolated volition of the agent is likely to vary at different times, so how ought we determine an overall account of the agent\u2019s extrapolated volition?</p>\n<p>&nbsp;</p>\n<h3 id=\"Sobel_s_Second_and_Third_Objections__Amnesia\">Sobel\u2019s Second and Third Objections: Amnesia</h3>\n<p>Sobel\u2019s second and third objections build on two other claims (see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel 1994</a> for a defense of these). First: some lives can only be evaluated if they are experienced. Second: experiencing one life can leave you incapable of experiencing another in an unbiased way. Given these claims, Sobel presents an <em>amnesia model</em> as the most plausible way for an idealized agent to gain the experiences necessary to evaluate all the relevant lives. According to this model, an agent experiences each life sequentially but undergoes an amnesia procedure after each one so that they may experience the next life uncolored by their previous experiences. After experiencing all lives, the amnesia is then removed.</p>\n<p>Following on from this, Sobel\u2019s second objection is that the sudden recollection of a life from one evaluative perspective and living a life from a vastly different evaluative perspective may be strongly dissimilar experiences. So when the amnesia is removed, the agent has a particular evaluative perspective (informed by their memories of all the lives they\u2019ve lived) that differs so much from the evaluative perspective they had when they lived the life independently of such memories that they might be incapable of adequately evaluating the lives they\u2019ve experienced based on their current, more knowledgeable, evaluative perspective.</p>\n<p>Sobel\u2019s third objection also relates to the amnesia model: Sobel argues that the idealized agent might be driven insane by the entire amnesia process and hence might not be able to adequately evaluate what advice they ought to give the non-ideal agent. In response to this, there is some temptation to simply demand that the agent be idealized not just in terms of rationality and knowledge but also in terms of their sanity. However, perhaps any idealized agent that is similar enough to the original to serve as a standard for their non-moral good will be driven insane by the amnesia process and so the demand for a sane agent will simply mean that no adequate agent can be identified.</p>\n<p>If we grant that an agent needs to experience some lives to evaluate them, and we grant that experiencing some lives leaves them incapable of experiencing others, then there seems to be a strong drive for Personal CEV to rely on an amnesia model to adequately determine what an agent\u2019s volition would be if extrapolated. If so, however, then Personal CEV seems to face the challenges raised by Sobel.</p>\n<p>&nbsp;</p>\n<h3 id=\"Sobel_s_Fourth_Objection__Better_Off_Dead\">Sobel\u2019s Fourth Objection: Better Off Dead</h3>\n<p>Sobel\u2019s final objection is that the idealized agent, having experienced such a level of perfection, might come to the conclusion that their non-ideal counterpart is so limited as to be better off dead. Further, the ideal agent might make this judgment because of the relative level of well-being of the non-ideal agent rather than the agent\u2019s absolute level of well-being. (That is, the ideal agent may look upon the well-being of the non-ideal agent as we might look upon our own well-being after an accident that caused us severe mental damage. In such a case, we might be unable to objectively judge our life after the accident due to the relative difficulty of this life as compared with our life before the accident.) As such, this judgment may not capture what is actually in accordance with the agent\u2019s non-moral good.</p>\n<p>Again, this criticism seems to apply equally to Personal CEV: when the volition of an agent is extrapolated, it may turn out that this volition endorses killing the non-extrapolated version of the agent. If so this seems to be a mark against the possibility that Personal CEV can play a useful part in a process that should eventually terminate in a \"Nice Place to Live.\"</p>\n<p>&nbsp;</p>\n<h3 id=\"A_model_of_Personal_CEV\">A model of Personal CEV</h3>\n<p>The seriousness of these challenges for Personal CEV is likely to vary depending on the exact nature of the extrapolation process. To give a sense of the impact, we will consider one family of methods for carrying out this process: the <em>parliamentary model</em> (inspired by <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom 2009</a>). According to this model, we determine the Personal CEV of an agent by simulating multiple versions of them, extrapolated from various starting times and along different developmental paths. Some of these versions are then assigned as a parliament where they vote on various choices and make trades with one another.</p>\n<p>Clearly this approach allows our account of Personal CEV to avoid the too many voices objection. After all, the parliamentary model provides us with an account of how we can aggregate the views of the agent at various times: we should simulate the various agents and allow them to vote and trade on the choices to be made. It is through this voting and trading that the various voices can be combined into a single viewpoint. While this process may not be adequate as a metaphysical account of value, it seems more plausible as an account of Personal CEV as an epistemic notion. Certainly, your authors would deem themselves to be more informed about what they value if they knew the outcome of the parliamentary model for themselves.</p>\n<p>This approach is also able to avoid Sobel\u2019s second and third objections. The objections were specifically targeted at the amnesia model where one agent experienced multiple lives. As the parliamentary model does not utilize amnesia, it is immune to these concerns.</p>\n<p>What of Sobel\u2019s fourth objection? Sobel\u2019s concern here is not simply that the idealized agent might advise the agent to kill themselves. After all, sometimes death may, in fact, be of value for an agent. Rather, Sobel\u2019s concern is that the idealized agent, having experienced such heights of existence, will become biased against the limited lives of normal agents.</p>\n<p>It's less clear how the parliamentary model deals with Sobel's fourth objection which plausibly retains its initial force against this model of Personal CEV. However, we're not intending to solve Personal CEV entirely in this short post. Rather, we aim to demonstrate only that the force of Sobel's four objections will depend on the model of Personal CEV selected. Reflection on the parliamentary model makes this point clear.</p>\n<p>So the parliamentary model seems able to avoid at least three of the direct criticisms raised by Sobel. It is worth noting, however, that some concerns remain. Firstly, for those that accept Sobel\u2019s claim that experience is necessary to evaluate some lives, it is clear that no member of the parliament will be capable of comparing their life to all other possible lives, as none will have all the required experience. As such, the agents may falsely judge a certain aspect of their life to be more or less valuable than it, in fact, is. For a metaphysical account of personal value, this problem might be fatal. Whether it is also fatal for the parliamentary model of Personal CEV depends on whether the knowledge of the various members of the parliament is enough to produce a \u201cNice Place to Live\u201d regardless of its imperfection.</p>\n<p>Two more issues might arise. First, the model might require careful selection of who to appoint to the parliament. For example, if most of the possible lives that an agent could live would drive them insane, then selecting which of these agents to appoint to the parliament at random might lead to a vote by the mad. Second, it might seem that this approach to determining Personal CEV will require a reasonable level of accuracy in simulation. If so, there might be concerns about the creation of, and responsibility to, potential moral agents.</p>\n<p>Given these points, a full evaluation of the parliamentary model will require more detailed specification and further reflection. However, two points are worth noting in conclusion. First, the parliamentary model does seem to avoid at least three of Sobel\u2019s direct criticisms. Second, even if this model eventually ends up being flawed on other grounds, the existence of one model of Personal CEV that can avoid three of Sobel\u2019s objections gives us reason to expect other promising models of Personal CEV may be discovered.</p>\n<p>&nbsp;</p>\n<h3 id=\"Notes\">Notes</h3>\n<p><sup>1</sup> Another clarification to make concerns the difference between <a href=\"http://en.wikipedia.org/wiki/Idealization\">idealization</a> and <a href=\"http://en.wikipedia.org/wiki/Extrapolation\">extrapolation</a>. An <em>idealized agent</em> is a version of the agent with certain idealizing characteristics (perhaps logical omniscience and infinite speed of thought). An <em>extrapolated agent</em> is a version of the agent that represents what they would be like if they underwent certain changes or experiences. Note two differences between these concepts. First, an extrapolated agent need not be ideal in any sense (though useful extrapolated agents often will be) and certainly need not be <em>perfectly</em> idealized. Second, extrapolated agent are determined by a specific type of process (extrapolation from the original agent) whereas no such restriction is placed on how the form of an idealized agent is determined. CEV utilizes extrapolation rather than idealization, as do some Ideal Advisor theories. In this post, we talk about \"ideal\" or \"idealized\" agents as a catch-all for both idealized agents and extrapolated agents.</p>\n<p><sup>2</sup> Standard objections to ideal advisor theories of value are also relevant to some proposed variants of CEV, for example Tarleton (2010)'s suggestion of \"Individual Extrapolated Volition followed by Negotiation, where each individual human\u2019s preferences are extrapolated by factual correction and reflection; once that process is fully complete, the extrapolated humans negotiate a combined utility function for the resultant superintelligence...\" Furthermore, some objections to Ideal Advisor theories also seem relevant to Global CEV even if they are not relevant to a particular approach to Personal CEV, though that discussion is beyond the scope of this article. As a final clarification, see <a href=\"/lw/1oj/complexity_of_value_complexity_of_outcome/\">Dai (2010)</a>.</p>\n<p><sup>3</sup> Ideal Advisor theories are not to be confused with \"Ideal Observer theory\" (<a href=\"http://www.rci.rutgers.edu/~stich/104_Master_File/104_Readings/Firth/Firth%20-%20Ideal%20Observer.pdf\">Firth 1952</a>). For more on Ideal Advisor theories of value, see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Zimmerman-Why-Richard-Brandt-does-not-need-cognitive-psychotherapy.pdf\">Zimmerman (2003)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Tanyi-An-essay-on-the-desire-based-reasons-model.pdf\">Tanyi (2006)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Enoch-Why-Idealize.pdf\">Enoch (2005)</a>;&nbsp;<a href=\"http://www.amazon.com/Contemporary-Metaethics-Introduction-Alexander-Miller/dp/074564659X/\">Miller (2013, ch. 9)</a>.</p>\n<p><sup>4</sup> This is basically an intrapersonal version of the standard worries about interpersonal comparisons of well-being. The basis of these worries is that even if we can specify an agent\u2019s preferences numerically, it\u2019s unclear how we should compare the numbers assigned by one agent with the numbers assigned by the other. In the intrapersonal case, the challenge is to determine how to compare the numbers assigned by the same agent at different times. See <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Gibbard-Interpersonal-comparisons-preference-good-and-the-intrinsic-reward-of-a-life.pdf\">Gibbard (1989)</a>.</p>", "sections": [{"title": "Sobel's First Objection: Too many voices", "anchor": "Sobel_s_First_Objection__Too_many_voices", "level": 1}, {"title": "Sobel\u2019s Second and Third Objections: Amnesia", "anchor": "Sobel_s_Second_and_Third_Objections__Amnesia", "level": 1}, {"title": "Sobel\u2019s Fourth Objection: Better Off Dead", "anchor": "Sobel_s_Fourth_Objection__Better_Off_Dead", "level": 1}, {"title": "A model of Personal CEV", "anchor": "A_model_of_Personal_CEV", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KQvdpPd3k2ap6aJTP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-25T21:51:47.250Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 12/25/12", "slug": "group-rationality-diary-12-25-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:01.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CTApsdrMTDnQprZdL/group-rationality-diary-12-25-12", "pageUrlRelative": "/posts/CTApsdrMTDnQprZdL/group-rationality-diary-12-25-12", "linkUrl": "https://www.lesswrong.com/posts/CTApsdrMTDnQprZdL/group-rationality-diary-12-25-12", "postedAtFormatted": "Tuesday, December 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2012%2F25%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2012%2F25%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCTApsdrMTDnQprZdL%2Fgroup-rationality-diary-12-25-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2012%2F25%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCTApsdrMTDnQprZdL%2Fgroup-rationality-diary-12-25-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCTApsdrMTDnQprZdL%2Fgroup-rationality-diary-12-25-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for Christmas week. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes, and I hope everyone is having a nice holiday!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/fw4/group_rationality_diary_121012/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CTApsdrMTDnQprZdL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.0676049166233303e-06, "legacy": true, "legacyId": "20850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p4khJwdApKwkmDe28"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-26T01:46:53.561Z", "modifiedAt": null, "url": null, "title": "META: Deletion policy", "slug": "meta-deletion-policy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:26.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3MXtScwdzkbxHTsiw/meta-deletion-policy", "pageUrlRelative": "/posts/3MXtScwdzkbxHTsiw/meta-deletion-policy", "linkUrl": "https://www.lesswrong.com/posts/3MXtScwdzkbxHTsiw/meta-deletion-policy", "postedAtFormatted": "Wednesday, December 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Deletion%20policy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Deletion%20policy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MXtScwdzkbxHTsiw%2Fmeta-deletion-policy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Deletion%20policy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MXtScwdzkbxHTsiw%2Fmeta-deletion-policy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MXtScwdzkbxHTsiw%2Fmeta-deletion-policy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/wiki/Deletion_policy\">http://wiki.lesswrong.com/wiki/Deletion_policy</a></p>\n<p>This is my attempt to codify the informal rules I've been working by.</p>\n<p>I'll leave this post up for a bit, but strongly suspect that it will have to be deleted not too long thereafter. &nbsp;I haven't been particularly encouraged to try responding to comments, either. &nbsp;Nonetheless, if there's something I missed, let me know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3MXtScwdzkbxHTsiw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 28, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "20851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-26T14:52:56.424Z", "modifiedAt": null, "url": null, "title": "Rationality by Other Means", "slug": "rationality-by-other-means", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.793Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZL2rLHjoc5BjgKdo/rationality-by-other-means", "pageUrlRelative": "/posts/KZL2rLHjoc5BjgKdo/rationality-by-other-means", "linkUrl": "https://www.lesswrong.com/posts/KZL2rLHjoc5BjgKdo/rationality-by-other-means", "postedAtFormatted": "Wednesday, December 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20by%20Other%20Means&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20by%20Other%20Means%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZL2rLHjoc5BjgKdo%2Frationality-by-other-means%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20by%20Other%20Means%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZL2rLHjoc5BjgKdo%2Frationality-by-other-means", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZL2rLHjoc5BjgKdo%2Frationality-by-other-means", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 623, "htmlBody": "<p>Political violence is a terrible thing - but, sometimes, not quite as terrible as the alternative. I recently <a href=\"/lw/g24/new_censorship_against_hypothetical_violence/84n9\">commented</a> that a post focusing on such things might be worthwhile, and since the voting has been positive, here we are.</p>\n<p><a id=\"more\"></a></p>\n<blockquote>From a Bayesian/rationalist/winningest perspective, if there is a  more-than-minuscule threat of political violence in your area, how  should you go about figuring out the best course of action? What  criteria should you apply? How do you figure out which group(s), if any,  to try to support? How do you determine what the risk of political  violence actually is? When the law says rebellion is illegal, that  preparing to rebel is illegal, that discussing rebellion even in theory  is illegal, when should you obey the law, and when shouldn't you? Which  lessons from HPMoR might apply? What reference books on war,  game-theory, and history are good to have read beforehand? In the  extreme case... where do you draw the line between choosing to pull a  trigger, or not?</blockquote>\n<p>I've cobbled together /a/ set of answers to such questions, based on what I've learned so far of economics, politics, human nature,  and various bits of evidence. However, I peg my confidence-levels of at  least some of those answers as being low enough that I could be easily  persuaded to change my mind, especially by the well-argued points that  tend to crop up around here.</p>\n<p>As just one starting point, 'freedom', 'liberty', and 'justice' (often with a halo effect list of other virtues) are often considered some of the highest values to rally around, and to fight for. And it's certainly quite attractive to be able to say you're fighting for them - but I have a suspicion, and a rather strong one, that that such reasoning may be closer to post-hoc rationalizations than is generally considered.</p>\n<p>An old saying goes, 'Amateurs study weapons; professionals study logistics'. For a number of reasons, economies tend to do best when as much competition as possible is done within them. Since a natural tendency of individuals and groups who achieve economic success is to use their leverage to push for even greater success relative to others, even if doing so causes others to pay externalized costs, it requires special efforts to promote the efforts of the 'little guy' against entrenched interests to allow new entrants into a market to have any chance of competing successfully in it. Thus, those economies which tend to perform best tend to be those with the most focus on individual rights, of allowing small-scale enterprises to successfully use the legal system, of allowing as many individuals as possible to put their hand to taking advantage of whatever opportunities they find. As those states with better economies tend to win wars against those with worse ones, it is easy to observe that those states with a greater focus on liberty tend to beat those with a lesser one... and thus quite natural to conclude that those values are, in and of themselves, the values worth fighting for.</p>\n<p>A <a href=\"http://www.datapacrat.com/sketches/RatStillMat1inks.jpg\">similar</a> <a href=\"http://www.datapacrat.com/sketches/RatStillMat2inks.jpg\">line</a> <a href=\"http://www.datapacrat.com/sketches/RatStillMat3inks.jpg\">of</a> argument could be made about the important virtues of the democratic process not being what's commonly said about it.</p>\n<p>&nbsp;</p>\n<p>Even such a rudimentary analysis of the principles many people think are worth fighting for puts the whole prospect of political violence in new light. Even if the analysis itself is wrong, it seems to open up uncommon avenues of thought, which could lead to a more accurate estimation of large-scale conflict, of which side is more likely to win, what reasons will be proclaimed for the victory, and what the most valuable efforts might be to nudge the odds one way or the other.</p>\n<p>Then again, I could be wrong. In which case, I'd prefer to know as soon as possible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZL2rLHjoc5BjgKdo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -12, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "20864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-26T23:08:09.419Z", "modifiedAt": null, "url": null, "title": "Morality Isn't Logical", "slug": "morality-isn-t-logical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QvYKSFmsBX3QhgQvF/morality-isn-t-logical", "pageUrlRelative": "/posts/QvYKSFmsBX3QhgQvF/morality-isn-t-logical", "linkUrl": "https://www.lesswrong.com/posts/QvYKSFmsBX3QhgQvF/morality-isn-t-logical", "postedAtFormatted": "Wednesday, December 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20Isn't%20Logical&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20Isn't%20Logical%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvYKSFmsBX3QhgQvF%2Fmorality-isn-t-logical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20Isn't%20Logical%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvYKSFmsBX3QhgQvF%2Fmorality-isn-t-logical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvYKSFmsBX3QhgQvF%2Fmorality-isn-t-logical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p>What do I mean by \"morality isn't logical\"? I mean in the same sense that mathematics is logical but literary criticism isn't: the \"reasoning\" we use to think about morality doesn't resemble logical reasoning. All systems of logic, that I'm aware of, have a concept of proof and a method of verifying with high degree of certainty whether an argument constitutes a proof. As long as the logic is consistent (and we have good reason to think that many of them are), once we verify a proof we can accept its conclusion without worrying that there may be another proof that makes the opposite conclusion. With morality though, we have no such method, and people all the time make moral arguments that can be reversed or called into question by other moral arguments. (Edit: For an example of this, see <a href=\"/lw/n3/circular_altruism/\">these</a> <a href=\"/lw/1r9/shut_up_and_divide/\">posts</a>.)</p>\n<p>Without being a system of logic, moral philosophical reasoning likely (or at least plausibly) doesn't have any of the nice properties that a well-constructed system of logic would have, for example, consistency, validity, soundness, or even the more basic property that considering arguments in a different order, or in a different mood, won't cause a person to accept an entirely different set of conclusions. For all we know, somebody trying to reason about a <a href=\"/lw/tc/unnatural_categories/\">moral concept</a> like \"fairness\" may just be taking a random walk as they move from one conclusion to another based on moral arguments they encounter or think up.</p>\n<p>In a <a href=\"/lw/fv3/by_which_it_may_be_judged/\">recent post</a>, Eliezer said \"morality is <em>logic</em>\", by which he seems to mean... well, I'm still not exactly sure what, but one interpretation is that a person's cognition about morality can be described as an algorithm, and that <em>algorithm</em> can be studied using logical reasoning. (Which of course is true, but in that sense both math and literary criticism as well as every other subject of human study would be logic.) In any case, I don't think Eliezer is explicitly claiming that an algorithm-for-thinking-about-morality constitutes an algorithm-for-doing-logic, but I worry that the characterization of&nbsp;\"morality is&nbsp;logic\"&nbsp;may cause some connotations of \"logic\" to be inappropriately&nbsp;<a href=\"/lw/ny/sneaking_in_connotations/\">sneaked</a> into \"morality\". For example Eliezer seems to (at least <a href=\"/lw/sm/the_meaning_of_right/to4\">at one point</a>) assume that considering moral arguments in a different order <em>won't</em> cause a human to accept an entirely different set of conclusions, and maybe this is why. To fight this potential sneaking of connotations, I suggest that when you see the phrase \"morality is logic\", remind yourself that morality isn't logical.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QvYKSFmsBX3QhgQvF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 32, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "20865", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ZzefKQwAtMo5yp99", "Ea8pt2dsrS6D4P54F", "XeHYXXTGRuDrhk5XL", "zqwWicCLNBSA5Ssmn", "yuKaWPRTxZoov4z8K"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-27T04:32:32.918Z", "modifiedAt": null, "url": null, "title": "Intelligence explosion in organizations, or why I'm not worried about the singularity", "slug": "intelligence-explosion-in-organizations-or-why-i-m-not", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sbenthall", "createdAt": "2012-12-23T03:31:10.842Z", "isAdmin": false, "displayName": "sbenthall"}, "userId": "pbnv8yAoxSjxvEZr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/22PXLzsDaLkF7MoMc/intelligence-explosion-in-organizations-or-why-i-m-not", "pageUrlRelative": "/posts/22PXLzsDaLkF7MoMc/intelligence-explosion-in-organizations-or-why-i-m-not", "linkUrl": "https://www.lesswrong.com/posts/22PXLzsDaLkF7MoMc/intelligence-explosion-in-organizations-or-why-i-m-not", "postedAtFormatted": "Thursday, December 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20explosion%20in%20organizations%2C%20or%20why%20I'm%20not%20worried%20about%20the%20singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20explosion%20in%20organizations%2C%20or%20why%20I'm%20not%20worried%20about%20the%20singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22PXLzsDaLkF7MoMc%2Fintelligence-explosion-in-organizations-or-why-i-m-not%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20explosion%20in%20organizations%2C%20or%20why%20I'm%20not%20worried%20about%20the%20singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22PXLzsDaLkF7MoMc%2Fintelligence-explosion-in-organizations-or-why-i-m-not", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22PXLzsDaLkF7MoMc%2Fintelligence-explosion-in-organizations-or-why-i-m-not", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 969, "htmlBody": "<p>If I understand the Singularitarian argument espoused by many members of this community (eg. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser and Salamon</a>), it goes something like this:</p>\n<ol>\n<li>Machine intelligence is getting smarter.</li>\n<li>Once an intelligence becomes sufficiently supra-human, its instrumental rationality will drive it towards cognitive self-enhancement (<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom</a>), so making it a super-powerful, resource hungry superintelligence.</li>\n<li>If a superintelligence isn't sufficiently human-like or 'friendly', that could be&nbsp;disastrous&nbsp;for humanity.</li>\n<li>Machine intelligence is unlikely to be human-like or friendly unless we take precautions.</li>\n</ol>\n<div>I am not particularly worried about the scenario envisioned in this argument. &nbsp;I think that my lack of concern is rational, so I'd like to try to convince you of it as well.*</div>\n<div><br /></div>\n<div>It's not that I think the logic of this argument is incorrect so much as I think there is another related problem that we should be worrying about more. &nbsp;I think the world is already full of probably unfriendly supra-human intelligences that are scrambling for computational resources in a way that threatens humanity.</div>\n<p>I'm in danger of getting into politics. &nbsp;Since I understand that <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">political arguments</a> are not welcome here, I will refer to these potentially unfriendly human intelligences broadly as <strong>organizations</strong>.</p>\n<h3>Smart organizations</h3>\n<p>By \"organization\" I mean something commonplace, with a twist. &nbsp;It's commonplace because I'm talking about <em>a bunch of people coordinated somehow.&nbsp;</em>The twist is that I want to <em>include the information technology infrastructure</em> used by that bunch of people within the extension of \"organization\".&nbsp;</p>\n<p>Do organizations have intelligence? &nbsp;I think so. &nbsp;Here's some of the reasons why:</p>\n<ol>\n<li>We can model human organizations as having preference functions. (Economists do this all the time)</li>\n<li>Human organizations have a lot of <a href=\"/lw/va/measuring_optimization_power/\">optimization power</a>.</li>\n</ol>\n<p>I <a href=\"http://digifesto.com/2012/12/20/an-interview-with-the-executive-director-of-the-singularity-institute/\">talked</a> with Mr. Muehlhauser about this specifically. I gather that at least at the time he thought human organizations should not be counted as intelligences (or at least as intelligences with the potential to become superintelligences) because they are not as versatile as human beings.</p>\n<blockquote>So when I am talking about super-human intelligence, I specifically mean an agent that is as good or better at humans at just about every skill set that humans possess for achieving their goals. So that would include things like not just mathematical ability or theorem proving and playing chess, but also things like social manipulation and composing music and so on, which are all functions of the brain not the kidneys</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">...and then...</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><em style=\"border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 10px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: #333333; font-family: Palatino, 'Times New Roman', serif; line-height: normal; text-align: left; padding: 0px; margin: 0px; border: 0px initial initial;\"> </em></p>\n<blockquote>It would be a kind of weird [organization] that was better than the best human or even the median human at all the things that humans do. [Organizations] aren&rsquo;t usually the best in music and AI research and theory proving and stock markets and composing novels. And so there certainly are&nbsp; [Organizations] that&nbsp; are better than median humans at certain things, like digging oil wells, but I don&rsquo;t think there are [Organizations] as good or better than humans at all things. More to the point, there is an interesting difference here because [Organizations] are made of lots of humans and so they have the sorts of limitations on activities and intelligence that humans have. For example, they are not particularly rational in the sense defined by cognitive science. And the brains of the people that make up organizations are limited to the size of skulls, whereas you can have an AI that is the size of a warehouse.&nbsp;</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">I think that Muehlhauser is slightly mistaken on a few subtle but important points. &nbsp;I'm going to assert my position on them without much argument because I think they are fairly sensible, but if any reader disagrees I will try to defend them in the comments.</span></p>\n<ul>\n<li>When judging whether an entity has intelligence, we should consider only the skills relevant to the entity's goals.</li>\n<li>So, if organizations are not as good at a human being at composing music, that shouldn't disqualify them from being considered broadly intelligent if that has nothing to do with their goals.</li>\n<li>Many organizations are quite good at AI research, or outsource their AI research to other organizations with which they are intertwined.</li>\n<li>The cognitive power of an organization is not limited to the size of skulls. The computational power is of many organizations is comprised of both the skulls of its members and possibly \"warehouses\" of digital computers.</li>\n<li>With the ubiquity of cloud computing, it's hard to say that a particular computational process has a static spatial bound at all.</li>\n</ul>\n<div>In summary, organizations often have the kinds of skills necessary to achieve their goals, and can be vastly better at them than individual humans. Many have the skills necessary for their own cognitive enhancement, since if they are able to raise funding they can purchase computational resources and fund artificial intelligence research. More mundanely, organizations of all kinds hire analysts and use analytic software to make instrumentally rational decisions.</div>\n<div><br /></div>\n<div>In sum, many organizations are of supra-human intelligence and strive actively to enhance their cognitive powers.</div>\n<div><br /></div>\n<h3>Mean organizations</h3>\n<div><br /></div>\n<div>Suppose the premise that there are organizations with supra-human intelligence that act to enhance their cognitive powers. &nbsp;And suppose the other premises of the Singularitarian argument outlined at the&nbsp;beginning&nbsp;of this post.</div>\n<div><br /></div>\n<div>Then it follows that we should be concerned if one or more of these smart organizations are so unlike human beings in their motivational structure that they are 'mean'.</div>\n<div><br /></div>\n<div>I believe the implications of this line of reasoning may be profound, but as this is my first post to LessWrong I would like to first see how this is received before going on.</div>\n<div><span style=\"font-size: 14px; line-height: 21px;\"><br /></span></div>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">* My preferred standard of rationality is <a href=\"http://en.wikipedia.org/wiki/Communicative_rationality\">communicative rationality</a>, a <a href=\"http://plato.stanford.edu/entries/habermas/\">Habermasian</a> ideal of a rationality aimed at consensus through principled communication. &nbsp;As a consequence, when I believe a position to be rational, I believe that it is possible and desirable to convince other rational agents of it.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "22PXLzsDaLkF7MoMc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 13, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "20866", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>If I understand the Singularitarian argument espoused by many members of this community (eg. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser and Salamon</a>), it goes something like this:</p>\n<ol>\n<li>Machine intelligence is getting smarter.</li>\n<li>Once an intelligence becomes sufficiently supra-human, its instrumental rationality will drive it towards cognitive self-enhancement (<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom</a>), so making it a super-powerful, resource hungry superintelligence.</li>\n<li>If a superintelligence isn't sufficiently human-like or 'friendly', that could be&nbsp;disastrous&nbsp;for humanity.</li>\n<li>Machine intelligence is unlikely to be human-like or friendly unless we take precautions.</li>\n</ol>\n<div>I am not particularly worried about the scenario envisioned in this argument. &nbsp;I think that my lack of concern is rational, so I'd like to try to convince you of it as well.*</div>\n<div><br></div>\n<div>It's not that I think the logic of this argument is incorrect so much as I think there is another related problem that we should be worrying about more. &nbsp;I think the world is already full of probably unfriendly supra-human intelligences that are scrambling for computational resources in a way that threatens humanity.</div>\n<p>I'm in danger of getting into politics. &nbsp;Since I understand that <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">political arguments</a> are not welcome here, I will refer to these potentially unfriendly human intelligences broadly as <strong>organizations</strong>.</p>\n<h3 id=\"Smart_organizations\">Smart organizations</h3>\n<p>By \"organization\" I mean something commonplace, with a twist. &nbsp;It's commonplace because I'm talking about <em>a bunch of people coordinated somehow.&nbsp;</em>The twist is that I want to <em>include the information technology infrastructure</em> used by that bunch of people within the extension of \"organization\".&nbsp;</p>\n<p>Do organizations have intelligence? &nbsp;I think so. &nbsp;Here's some of the reasons why:</p>\n<ol>\n<li>We can model human organizations as having preference functions. (Economists do this all the time)</li>\n<li>Human organizations have a lot of <a href=\"/lw/va/measuring_optimization_power/\">optimization power</a>.</li>\n</ol>\n<p>I <a href=\"http://digifesto.com/2012/12/20/an-interview-with-the-executive-director-of-the-singularity-institute/\">talked</a> with Mr. Muehlhauser about this specifically. I gather that at least at the time he thought human organizations should not be counted as intelligences (or at least as intelligences with the potential to become superintelligences) because they are not as versatile as human beings.</p>\n<blockquote>So when I am talking about super-human intelligence, I specifically mean an agent that is as good or better at humans at just about every skill set that humans possess for achieving their goals. So that would include things like not just mathematical ability or theorem proving and playing chess, but also things like social manipulation and composing music and so on, which are all functions of the brain not the kidneys</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">...and then...</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><em style=\"border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 10px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: #333333; font-family: Palatino, 'Times New Roman', serif; line-height: normal; text-align: left; padding: 0px; margin: 0px; border: 0px initial initial;\"> </em></p>\n<blockquote>It would be a kind of weird [organization] that was better than the best human or even the median human at all the things that humans do. [Organizations] aren\u2019t usually the best in music and AI research and theory proving and stock markets and composing novels. And so there certainly are&nbsp; [Organizations] that&nbsp; are better than median humans at certain things, like digging oil wells, but I don\u2019t think there are [Organizations] as good or better than humans at all things. More to the point, there is an interesting difference here because [Organizations] are made of lots of humans and so they have the sorts of limitations on activities and intelligence that humans have. For example, they are not particularly rational in the sense defined by cognitive science. And the brains of the people that make up organizations are limited to the size of skulls, whereas you can have an AI that is the size of a warehouse.&nbsp;</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">I think that Muehlhauser is slightly mistaken on a few subtle but important points. &nbsp;I'm going to assert my position on them without much argument because I think they are fairly sensible, but if any reader disagrees I will try to defend them in the comments.</span></p>\n<ul>\n<li>When judging whether an entity has intelligence, we should consider only the skills relevant to the entity's goals.</li>\n<li>So, if organizations are not as good at a human being at composing music, that shouldn't disqualify them from being considered broadly intelligent if that has nothing to do with their goals.</li>\n<li>Many organizations are quite good at AI research, or outsource their AI research to other organizations with which they are intertwined.</li>\n<li>The cognitive power of an organization is not limited to the size of skulls. The computational power is of many organizations is comprised of both the skulls of its members and possibly \"warehouses\" of digital computers.</li>\n<li>With the ubiquity of cloud computing, it's hard to say that a particular computational process has a static spatial bound at all.</li>\n</ul>\n<div>In summary, organizations often have the kinds of skills necessary to achieve their goals, and can be vastly better at them than individual humans. Many have the skills necessary for their own cognitive enhancement, since if they are able to raise funding they can purchase computational resources and fund artificial intelligence research. More mundanely, organizations of all kinds hire analysts and use analytic software to make instrumentally rational decisions.</div>\n<div><br></div>\n<div>In sum, many organizations are of supra-human intelligence and strive actively to enhance their cognitive powers.</div>\n<div><br></div>\n<h3 id=\"Mean_organizations\">Mean organizations</h3>\n<div><br></div>\n<div>Suppose the premise that there are organizations with supra-human intelligence that act to enhance their cognitive powers. &nbsp;And suppose the other premises of the Singularitarian argument outlined at the&nbsp;beginning&nbsp;of this post.</div>\n<div><br></div>\n<div>Then it follows that we should be concerned if one or more of these smart organizations are so unlike human beings in their motivational structure that they are 'mean'.</div>\n<div><br></div>\n<div>I believe the implications of this line of reasoning may be profound, but as this is my first post to LessWrong I would like to first see how this is received before going on.</div>\n<div><span style=\"font-size: 14px; line-height: 21px;\"><br></span></div>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.7em; margin-left: 0px; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 14px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; line-height: 1.5em; text-align: justify; padding: 0px; border: 0px initial initial;\"><span style=\"line-height: normal; text-align: -webkit-auto; font-size: small;\">* My preferred standard of rationality is <a href=\"http://en.wikipedia.org/wiki/Communicative_rationality\">communicative rationality</a>, a <a href=\"http://plato.stanford.edu/entries/habermas/\">Habermasian</a> ideal of a rationality aimed at consensus through principled communication. &nbsp;As a consequence, when I believe a position to be rational, I believe that it is possible and desirable to convince other rational agents of it.<br></span></p>", "sections": [{"title": "Smart organizations", "anchor": "Smart_organizations", "level": 1}, {"title": "Mean organizations", "anchor": "Mean_organizations", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "187 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 187, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-27T05:46:16.365Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Mechanics of Disagreement", "slug": "seq-rerun-the-mechanics-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:39.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aL9BY5ktQBJrSkwpC/seq-rerun-the-mechanics-of-disagreement", "pageUrlRelative": "/posts/aL9BY5ktQBJrSkwpC/seq-rerun-the-mechanics-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/aL9BY5ktQBJrSkwpC/seq-rerun-the-mechanics-of-disagreement", "postedAtFormatted": "Thursday, December 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Mechanics%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Mechanics%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaL9BY5ktQBJrSkwpC%2Fseq-rerun-the-mechanics-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Mechanics%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaL9BY5ktQBJrSkwpC%2Fseq-rerun-the-mechanics-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaL9BY5ktQBJrSkwpC%2Fseq-rerun-the-mechanics-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"/lw/wo/the_mechanics_of_disagreement/\">The Mechanics of Disagreement</a> was originally published on 10 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Mechanics_of_Disagreement\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Reasons why aspiring rationalists might still disagree after trading arguments.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g2o/seq_rerun_two_visions_of_heritage/\">Two Visions of Heritage</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aL9BY5ktQBJrSkwpC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0687350523233924e-06, "legacy": true, "legacyId": "20869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RNLQ7846MvJWwxH52", "p7La2o6yr8KmyHecb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-27T17:23:50.704Z", "modifiedAt": null, "url": null, "title": "Donation tradeoffs in conscientious objection", "slug": "donation-tradeoffs-in-conscientious-objection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.076Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fa4p9oKc6iA3eE67c/donation-tradeoffs-in-conscientious-objection", "pageUrlRelative": "/posts/Fa4p9oKc6iA3eE67c/donation-tradeoffs-in-conscientious-objection", "linkUrl": "https://www.lesswrong.com/posts/Fa4p9oKc6iA3eE67c/donation-tradeoffs-in-conscientious-objection", "postedAtFormatted": "Thursday, December 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Donation%20tradeoffs%20in%20conscientious%20objection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADonation%20tradeoffs%20in%20conscientious%20objection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFa4p9oKc6iA3eE67c%2Fdonation-tradeoffs-in-conscientious-objection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Donation%20tradeoffs%20in%20conscientious%20objection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFa4p9oKc6iA3eE67c%2Fdonation-tradeoffs-in-conscientious-objection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFa4p9oKc6iA3eE67c%2Fdonation-tradeoffs-in-conscientious-objection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1129, "htmlBody": "<p>Suppose that you believe larger scale wars than current US military campaigns are looming in the next decade or two (this may be highly improbable, but let's condition on it for the moment). If you thought further that a military draft or other forms of conscription might be used, and you wanted to avoid military service if that situation arose, what steps should you take now to give yourself a high likelihood of being declared a <a href=\"http://www.sss.gov/fsconsobj.htm\">conscientious objector</a>?<br /><br />I don't have numbers to back any of this up, but I am in the process of compiling them. My general thought is to break down the problem like so: Pr(serious injury or death | conscription) * Pr(conscription | my conscientious objector behavior &amp; geopolitical conditions ripe for war) * Pr(geopolitical conditions ripe for war), assuming some conscientious objector behavior (or mixture distribution over several behaviors).</p>\n<p>If I feel that Pr(serious injury or death | conscription) and Pr(geopolitical conditions ripe for war) are sufficiently high, then I might be motivated to pay some costs in order to drive Pr(conscription | my conscientious objector behavior) very low.</p>\n<p>There's a funny bit in the American version of the show The Office where the manager, Michael, is concerned about his large credit card debt. The accountant, Oscar, mentions that declaring bankruptcy is an option, and so Michael walks out into the main office area and yells, \"I DECLARE BANKRUPTCY!\"<br /><br />In a similar vein, I don't think that draft boards will accept the \"excuse\" that a given person has \"merely\" frequently expressed pacifist views. So if someone wants to robustly signal that she or he is a conscientious objector, what to do? In my ~30 minutes of searching, I've found a few organizations that, on first glance, look worthy of further investigation and perhaps regular donations.</p>\n<p>Here are the few I've focused on most:</p>\n<p><a href=\"http://www.centeronconscience.org/\">Center on Conscience and War</a></p>\n<p><a href=\"http://www.coffeestrong.org/about/\">Coffee Strong</a></p>\n<p><a href=\"http://www.wri-irg.org/\">War-Resister's Internationa</a><a href=\"http://www.wri-irg.org/\">l</a></p>\n<p>&nbsp;</p>\n<p>The problems I'm thinking about along these lines include:</p>\n<ol>\n<li>Whether or not the donation cost is worth it. There's no Giving What We Can type measure for this as far as I can tell, and even though I know from family experience that veteran mental illness can be very bad, I'm not convinced that donations to the above organizations provide a lot of QALY bang for the buck.</li>\n<li>Another component of bang for the buck is how much the donation will credibly signal that I actually am a serious conscientious objector. If I donate and then a draft board chooses to ignore it, it would be totally wasted. But if I think that 'going to war' is highly correlated with very significant negative outcomes, then just as with cryonics, I might feel that such costs are worth it even for a small probability of avoiding a combat environment.</li>\n<li>Even assuming that I resolve 1 &amp; 2, there's the problem of trading off these donations with other donations that I make. In a self-interest line of thinking, I might forego my current donations to places like SIAI or Against Malaria because, good as those are, they may not offer the same shorter term benefits to me as purchasing a conscientious objector signal.</li>\n</ol>\n<p>&nbsp;</p>\n<p>I'm curious if others have thought about this. Good literature references are welcome. My plan is to compile statistics that let me make reasonable estimates of the different conditional probabilities.</p>\n<p>&nbsp;</p>\n<p><strong>Addendum</strong></p>\n<p>Several people seem very concerned with the signal faker aspect of this question. I don't understand the preoccupation with this and feel tired of trying to justify the question to people who only care about the signal faker aspect. So I'll just add this copy of one of my comments from below. Hopefully this gives some additional perspective, though I don't expect it to change anyone's mind. I still stand by the post as-is: it's asking about a conditional question based on sincere belief. Even if the answer would be of interest to fakers too, that alone doesn't make that explanation more likely and even if that explanation was more likely it doesn't make the question unworthy of thoughtful answers.</p>\n<p>Here's the promised comment:</p>\n<blockquote>\n<p>... my question is conditional. <em>Assume that you already sincerely believe in conscientious objection, in the sense of personal ideology such that you could describe it to a draft board.</em> Now that we're conditioning on that, and we assume already that your primary goal is to avoid <em>causing</em> harm or death... <em>then</em> further ask what behaviors might be best to generate the kinds of signals that will work to convince a draft board. <em>Merely having actual pacifist beliefs</em> is not enough. Someone could have those beliefs but then do actions that poorly communicate them to a draft board. Someone else could have those beliefs and do behaviors that more successfully communicate them to draft boards. And to whatever extent there are behaviors <em>outside of the scope of just giving an account of one's ideology</em> I am asking to analyze the effectiveness.</p>\n<p>I really think my question is pretty simple. Assume your goal is genuine pacifism but that you're worried this won't convince a draft board. What should you do? Is donation a good idea? Yes, these could be questions a faker would ask. So what? They could also be questions a sincere person would ask, and I don't see any reason for all the downvoting or questions about signal faking. Why not just do the thought experiment where you <em>assume that</em> you are first a sincere conscientious objector and second a person concerned about draft board odds?</p>\n</blockquote>\n<p>Stated another way:</p>\n<blockquote>\n<p>1) Avoiding combat where I <em>cause</em> harm or death is the first  priority, so if I have to go to jail or shoot myself in the foot to  avoid it, so be it and if it comes to that, it's what I'll do. This is  priority number one.</p>\n<p>2) I can do things to improve my odds of never needing to face the  situation described in (1) and to the extent that the behaviors are  expedient (in a cost-benefit tradeoff sense) to do in my life, I'd like  to do them now to help improve odds of (1)-avoidance later. Note that  this in no way conflicts with being a genuine pacifist. It's just common  sense. Yes, I'll avoid combat in costly ways if I have to. But I'd also  be stupid to not even explore less costly ways to invest in  combat-avoidance that could be better for me.</p>\n<p>3) To the extent that (2) is true, I'd like to examine certain  options, like donating to charities that assist with legal issues in  conscientious objection, or which extend mental illness help to affected  veterans, for their efficacy. There is still a cost to these things and  given my conscientious objection preferences, I ought to weigh that  cost.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fa4p9oKc6iA3eE67c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 0, "extendedScore": null, "score": 1.069147360552365e-06, "legacy": true, "legacyId": "20884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-27T19:42:13.647Z", "modifiedAt": null, "url": null, "title": "What Happened in the Fort Marcy Parking Lot?", "slug": "what-happened-in-the-fort-marcy-parking-lot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:39.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unknowns", "createdAt": "2009-09-10T19:39:25.316Z", "isAdmin": false, "displayName": "Unknowns"}, "userId": "f9eW72oneXyuo5hJi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e8eCypXpeJFNznCLE/what-happened-in-the-fort-marcy-parking-lot", "pageUrlRelative": "/posts/e8eCypXpeJFNznCLE/what-happened-in-the-fort-marcy-parking-lot", "linkUrl": "https://www.lesswrong.com/posts/e8eCypXpeJFNznCLE/what-happened-in-the-fort-marcy-parking-lot", "postedAtFormatted": "Thursday, December 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Happened%20in%20the%20Fort%20Marcy%20Parking%20Lot%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Happened%20in%20the%20Fort%20Marcy%20Parking%20Lot%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8eCypXpeJFNznCLE%2Fwhat-happened-in-the-fort-marcy-parking-lot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Happened%20in%20the%20Fort%20Marcy%20Parking%20Lot%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8eCypXpeJFNznCLE%2Fwhat-happened-in-the-fort-marcy-parking-lot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8eCypXpeJFNznCLE%2Fwhat-happened-in-the-fort-marcy-parking-lot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1025, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><!  /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Tabella normale\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --><!--[if gte mso 9]><xml> <o:shapedefaults v:ext=\"edit\" spidmax=\"1026\" /> </xml><![endif]--><!--[if gte mso 9]><xml> <o:shapelayout v:ext=\"edit\"> <o:idmap v:ext=\"edit\" data=\"1\" /> </o:shapelayout></xml><![endif]--></p>\n<p class=\"MsoNormal\">On July 20, 1993, deputy White House counsel Vincent Foster was found dead at Fort Marcy Park in Virginia. Various investigations concluded that he died of a self-inflicted gunshot wound to the head, entering through the mouth. Leaving his death aside for the present, it is interesting to consider the evidence and testimony concerning the cars in the parking lot that afternoon.</p>\n<p class=\"MsoNormal\">Note that Foster's car was a gray 1989 four-door Honda Accord LX with Arkansas license plates RCN-504.</p>\n<p class=\"MsoNormal\">According to Ken Starr&rsquo;s report, &ldquo;Between about 2:45 and 3:05 p.m., a citizen driving outbound on GW Parkway saw &lsquo;a dark metallic grey, Japanese sedan&rsquo; occupied by a single, white male abruptly enter Fort Marcy Park.&rdquo;</p>\n<p class=\"MsoNormal\">According to the FBI, this witness &ldquo;once again reiterated the fact that the license plate he observed had the name of the state located in the lower right hand corner of the plate, further stating that since the Arkansas plate has, in bold letters, the name of the state at the top of the plate, he would have clearly remembered the identification of the state... [The car he saw was] definitely not [the one in the] photo of car [he was] exhibited. The license plate [was] definitely not the same.&rdquo;</p>\n<p class=\"MsoNormal\">Patrick Knowlton said that at 4:30 PM he saw a &ldquo;rust brown colored car with Arkansas plates&rdquo; in the Fort Marcy parking lot. He said that there was a man who &ldquo;looked like he had an agenda&rdquo; in the car. When the FBI shoed Knowlton photos of Foster&rsquo;s Honda, Knowlton said that it was not the car he had seen, specifying that he had seen a brown car rather than a gray one such as Foster&rsquo;s, and that he thought the car was approximately five years older than Foster&rsquo;s 1989 vehicle. When the FBI had him select the color of the car from a panel of colors, he picked out a brown color offered by Honda only in the years 1983 and 1984.</p>\n<p class=\"MsoNormal\">A couple drove into the Fort Marcy Parking lot at about 5:00 PM and remained in their car, a white four-door 1992 Nissan with Maryland plates, until about 5:30 or 5:45, when they left the lot and entered the woods. They told the FBI that &ldquo;the only vehicle in the parking area was a relatively old (mid-1980&rsquo;s) Honda, possibly a Honda Accord.&rdquo; The FBI interview notes indicate that they described it as a &ldquo;tannish/dark color&rdquo;, as &ldquo;a small station wagon or hatchback model, brownish in color&rdquo;, and also as a &ldquo;brown car.&rdquo; They also reported that there were two men in and around the car, one man in the driver&rsquo;s seat and the other outside, having put the hood up.</p>\n<p class=\"MsoNormal\">The Park Police interview of the couple describes the car as the &ldquo;deceased&rsquo;s vehicle.&rdquo; When the woman was shown her Park Police interview, the woman told the FBI that it did not accurately reflect what she had said about the cars in the parking lot.</p>\n<p class=\"MsoNormal\">Another witness, the man who officially discovered Foster&rsquo;s body at about 5:45 PM, &ldquo;described this vehicle as a compact Japanese made sedan, color possibly light blue or tan...&rdquo; in his first FBI interview. In his second interview he described it &ldquo;as light tan or light brown Japanese vehicle which could have been a Nissan, Toyota, or possibly a Honda.&rdquo; When shown photographs of Foster&rsquo;s Honda, he told the FBI he did not recognize it. Later he described the vehicle under oath as &ldquo;light brown or cream colored Japanese made car,&rdquo; and as &ldquo;brown or cream colored&rdquo;.</p>\n<p class=\"MsoNormal\">Another witness arrived perhaps around 6:00 PM, her blue Mercedes having broken down on the George Washington Parkway and stopped on the exit ramp to Fort Marcy. There is some confusion about how she described the vehicle. According to the handwritten notes of her Park Police interview on August 5, 1993, she said that it was &ldquo;a lighter gray or silver.&rdquo; However, she told a reporter, Ambrose Evans-Pritchard that it was &ldquo;tannish brown,&rdquo; and when Evans-Pritchard said &ldquo;Are you sure?&rdquo; she responded, &ldquo;Oh yes, quite sure.&rdquo;</p>\n<p class=\"MsoNormal\">Two Fairfax County Fire and Rescue department vehicles arrived at 6:10 PM, with the Park Police arriving immediately afterwards. The &ldquo;Narrative Report&rdquo; from the lead paramedic stated, &ldquo;As we entered the park (Fort Marcy) we passed a light blue Mercedes w/its hazards on. No occupant in view. Went further up into the park and saw two other vehicles. Brwn Honda AR Tags. And a white Nissan w/MD tags. No other people in the area. We split our crews...&rdquo;</p>\n<p class=\"MsoNormal\">The second Park Police officer to arrive said that &ldquo;A gray/brown Nissan 4dr with Arkansas Registration RCN504 was parked in the 4<sup>th</sup> space from the front of the parking lot.&rdquo;</p>\n<p class=\"MsoNormal\">The FBI interview of one paramedic said that &ldquo;upon arriving at Fort Marcy Park, noticed an unoccupied brown car with the engine running in the parking lot. He noted that the car was not parked in a space.&rdquo; He did not remember whether the car was still there when the paramedics left at 6:37 PM.</p>\n<p class=\"MsoNormal\">According to the interview of another emergency worker, &ldquo;Upon entering Fort Marcy Park, recalls seeing one car in the parking area with its hazard lights on. Remembers that the engine was running, noting that the car was unoccupied.&rdquo;</p>\n<p class=\"MsoNormal\">Still another paramedic interview said &ldquo;Car (red?) with hazard lights in park... Red [?] car gone when he left.&rdquo;</p>\n<p class=\"MsoNormal\">The Park Police shift commander who came into the parking lot at about 6:25 PM made a note about a vehicle in the lot, &ldquo;Engine warm on vehicle.&rdquo;</p>\n<p class=\"MsoNormal\">The report written by a Park Police investigator arriving about 6:35 PM reports the presence of &ldquo;a 1989 gray Honda Accord, 4 door, with Arkansas license plates RCN-504.&rdquo;</p>\n<p class=\"MsoNormal\">A number of the Fairfax Country Fire and Rescue Department personnel said that the Honda in the parking lot was found locked when they examined it, sometime before 6:35 PM.<span style=\"mso-spacerun: yes;\">&nbsp; </span>The official Reports indicate that the Honda was found unlocked over an hour later when it was officially searched. There is no record of a discovery of the car keys in the intervening period; no keys had been found when the body was first searched.</p>\n<p class=\"MsoNormal\">So what exactly happened in the Fort Marcy parking lot that afternoon of July 20, 1993?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e8eCypXpeJFNznCLE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -11, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "20885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-27T20:28:01.171Z", "modifiedAt": null, "url": null, "title": "Meetup : Bielefeld Meetup, January 2nd", "slug": "meetup-bielefeld-meetup-january-2nd", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cax9KZbXuz5i9iK4v/meetup-bielefeld-meetup-january-2nd", "pageUrlRelative": "/posts/Cax9KZbXuz5i9iK4v/meetup-bielefeld-meetup-january-2nd", "linkUrl": "https://www.lesswrong.com/posts/Cax9KZbXuz5i9iK4v/meetup-bielefeld-meetup-january-2nd", "postedAtFormatted": "Thursday, December 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bielefeld%20Meetup%2C%20January%202nd&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bielefeld%20Meetup%2C%20January%202nd%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCax9KZbXuz5i9iK4v%2Fmeetup-bielefeld-meetup-january-2nd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bielefeld%20Meetup%2C%20January%202nd%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCax9KZbXuz5i9iK4v%2Fmeetup-bielefeld-meetup-january-2nd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCax9KZbXuz5i9iK4v%2Fmeetup-bielefeld-meetup-january-2nd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hc'>Bielefeld Meetup, January 2nd</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", bioethics and the discussion of plans on how to start a Youtube-channel for rationality.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hc'>Bielefeld Meetup, January 2nd</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cax9KZbXuz5i9iK4v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0692562660602035e-06, "legacy": true, "legacyId": "20887", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bielefeld_Meetup__January_2nd\">Discussion article for the meetup : <a href=\"/meetups/hc\">Bielefeld Meetup, January 2nd</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", bioethics and the discussion of plans on how to start a Youtube-channel for rationality.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bielefeld_Meetup__January_2nd1\">Discussion article for the meetup : <a href=\"/meetups/hc\">Bielefeld Meetup, January 2nd</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bielefeld Meetup, January 2nd", "anchor": "Discussion_article_for_the_meetup___Bielefeld_Meetup__January_2nd", "level": 1}, {"title": "Discussion article for the meetup : Bielefeld Meetup, January 2nd", "anchor": "Discussion_article_for_the_meetup___Bielefeld_Meetup__January_2nd1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T04:14:38.006Z", "modifiedAt": null, "url": null, "title": "The Relation Projection Fallacy and the purpose of life", "slug": "the-relation-projection-fallacy-and-the-purpose-of-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3pCzoeaP7tXipPWGt/the-relation-projection-fallacy-and-the-purpose-of-life", "pageUrlRelative": "/posts/3pCzoeaP7tXipPWGt/the-relation-projection-fallacy-and-the-purpose-of-life", "linkUrl": "https://www.lesswrong.com/posts/3pCzoeaP7tXipPWGt/the-relation-projection-fallacy-and-the-purpose-of-life", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Relation%20Projection%20Fallacy%20and%20the%20purpose%20of%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Relation%20Projection%20Fallacy%20and%20the%20purpose%20of%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pCzoeaP7tXipPWGt%2Fthe-relation-projection-fallacy-and-the-purpose-of-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Relation%20Projection%20Fallacy%20and%20the%20purpose%20of%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pCzoeaP7tXipPWGt%2Fthe-relation-projection-fallacy-and-the-purpose-of-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pCzoeaP7tXipPWGt%2Fthe-relation-projection-fallacy-and-the-purpose-of-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 904, "htmlBody": "<p>I bet most people here have realized this explicitly or implicitly, but <a href=\"/lw/4h/when_truth_isnt_enough/37v\">this comment</a> has inspired me to write a short, linkable summary of this error pattern, with a name:</p>\n<p><strong>The Relation Projection Fallacy</strong>: a denotational error whereby one confuses an n-ary relation for an m-ary relation, where usually m&lt;n.</p>\n<p>Example instance: \"Life has no purpose.\"</p>\n<p>This is a troublesome phrase. &nbsp;Why? &nbsp;If you look at unobjectionable uses of the concept &lt;purpose&gt; --- also referenced by synonyms like \"having a point\" --- it is in fact a <strong>ternary relation.</strong></p>\n<p>Example non-instance: \"The purpose of a doorstop is to stop doors.\"</p>\n<p>Here, one can query \"to whom?\" and be returned the context \"to the person who made it\" or \"to the person who's using it\", etc. &nbsp;That is, the full denotation of \"purpose\" is always of the form \"The purpose of X to Y is Z,\" where Y is often implicit or can take a wide range of values.</p>\n<p>This has nothing to do with connotation... it's just how the concept &lt;purpose&gt; typically works as people use it. &nbsp;But to flog a dead horse, the purpose of a doorstop to a cat may be to make an amusing sound as it glides across the floor after the cat hits it. &nbsp;The value of Y always matters. &nbsp;There is no \"true purpose\" stored anywhere inside the doorstop, or even in the combination of the doorstop and the door it is stopping. &nbsp;To think otherwise is literally <em>projecting, in the mathematical sense,</em>&nbsp;a ternary relation, i.e., a subset of a product of three sets (objects)x(agents)x(verbs), into a product of two sets, (objects)x(verbs). &nbsp;But people often do this projection incorrectly, by either searching for a purpose that is intrinsic to the Doorstop or to Life, or by searching for a canonical value of \"Y\" like \"The Great Arbiter of Purpose\", both of which are not to be found, at least to their satisfaction when they utter the phrase \"Life has no purpose.\"</p>\n<p>Likewise, the relation \"has a purpose\" is typically a <strong>binary relation</strong>, because again, we can always ask \"to whom?\". &nbsp;\"&lt;That doorstop&gt; has a purpose to &lt;me&gt;.\"</p>\n<p>In some form, this realization is of course the cause of many schools of thought taking the name \"relativist\" on many different issues. &nbsp;But I find that people over-use the phrase \"It's all relative\" to connote \"It's all meaningless\" or \"there is no answer\". &nbsp;Which is ironic, because meaning itself is a ternary relation! &nbsp;Its typical denotation is of the form&nbsp;\"The meaning of X to Y is Z\", like in</p>\n<ul>\n<li>\"The meaning of &lt;the sound 'owe'&gt; to &lt;French people&gt; is &lt;liquid water&gt;\" or</li>\n<li>\"The meaning of &lt;that pendant&gt; to &lt;your mother&gt; is &lt;a certain undescribed experience of sentimentality&gt;\".</li>\n</ul>\n<p>Realizing this should NOT result in a cascade of <a href=\"/lw/2va/morality_and_relativistic_vertigo/\">bottomless relativism where nothing means anything</a>! &nbsp;In fact, the first time I had this thought as a kid, I arrived at the connotationally pleasing conclusion \"My life can have as many purposes as there are agents for it to have a purpose to.\"</p>\n<p>Indeed, the meaning of &lt;\"purpose\"&gt; to &lt;humans&gt; is &lt;a certain ternary functional relationship between objects, agents, and verbs&gt;, and the meaning of &lt;\"meaning\"&gt; to &lt;humans&gt; is &lt;a certain ternary relationship between syntactic elements, people generating or perceiving them, and referents&gt;.&nbsp;</p>\n<p>When I found LessWrong, I was happy to find that Eliezer wrote on almost exactly this realization in&nbsp;<a href=\"/lw/ro/2place_and_1place_words/\">2-Place and 1-Place Words</a>, but sad that the post had few upvotes -- only 14 right now. &nbsp;So in case it was too long, or didn't have a snappy enough name, I thought I'd try giving the idea another shot.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>ETA: In the special case of talking to someone wondering about the purpose of <em>life</em>, here is how I would use this observation <em>in the form of an argument:</em></p>\n<blockquote>\n<p>First of all, you may be lacking satisfaction in your life for some reason, and framing this to yourself in philosophical terms like \"Life has no purpose, because &lt;argument&gt;.\" &nbsp;If that's true, it's quite likely that you'd feel differently if your emotional needs as a social primate were being met, and in that sense the solution is not an \"answer\" but rather some actions that will result in these needs being met. &nbsp;</p>\n<p>Still, that does not address the &lt;argument&gt;. &nbsp;So because \"What is s the purpose of life?\" may be a hard question, let's look at easier examples of <em>purpose</em> and see how they work. &nbsp;Notice how they all have someone the purpose is <em>to</em>? &nbsp;And how that's missing in your \"purpose of life\" question? &nbsp;Because of that, you could end up feeling one of two ways:&nbsp;</p>\n<p>&nbsp;(1) Satisfied, because now you can just ask \"What could be the purpose of my life to &lt;my friends, my family, myself, the world at large, etc&gt;\", and come up with answers, or&nbsp;</p>\n<p>&nbsp;(2) Unsatisfied, because there is no agent to ask about such that the answer would seem important enough to you.</p>\n<p>And I claim that whether you end up at (1) or (2) is probably more a function of whether your social primate emotional needs are being met than any particular philosophical argument.</p>\n</blockquote>\n<p>That being said, if you believe this argument, the best thing to do for someone lacking a sense of purpose is probably not to just say the argument, but to help them start satisfying their emotional needs, and have this argument mainly to satisfy their sense of curiosity or nagging intellectual doubts about the issue.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FtT2T9bRbECCGYxrL": 1, "PJKgSRkXkCqXmCk3M": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3pCzoeaP7tXipPWGt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 90, "baseScore": 119, "extendedScore": null, "score": 0.000266, "legacy": true, "legacyId": "20896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 119, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ydnDN8S6H4fHPpmZ2", "eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T04:20:06.445Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver New Year!", "slug": "meetup-vancouver-new-year", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FWNJqGvQnoQtZNvQC/meetup-vancouver-new-year", "pageUrlRelative": "/posts/FWNJqGvQnoQtZNvQC/meetup-vancouver-new-year", "linkUrl": "https://www.lesswrong.com/posts/FWNJqGvQnoQtZNvQC/meetup-vancouver-new-year", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20New%20Year!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20New%20Year!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWNJqGvQnoQtZNvQC%2Fmeetup-vancouver-new-year%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20New%20Year!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWNJqGvQnoQtZNvQC%2Fmeetup-vancouver-new-year", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWNJqGvQnoQtZNvQC%2Fmeetup-vancouver-new-year", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hd'>Vancouver New Year!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 w broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello vancouver lesswrongers,</p>\n\n<p>Goddam Christmas is done creating chaos and now it's time to start a new year (judging the new year by the sun...) of Vancouver LW meetups!</p>\n\n<p>Come on out to Benny's Bagels in Kitsilano to meet up with like-minded aspiring rationalists, talk about all sorts of stuff, and generally have fun. 13:00 on Sunday the 30th.</p>\n\n<p>If you are looking for an excuse to start coming out, the new year seems as good as any.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hd'>Vancouver New Year!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FWNJqGvQnoQtZNvQC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0695355093396374e-06, "legacy": true, "legacyId": "20897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_New_Year_\">Discussion article for the meetup : <a href=\"/meetups/hd\">Vancouver New Year!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 w broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello vancouver lesswrongers,</p>\n\n<p>Goddam Christmas is done creating chaos and now it's time to start a new year (judging the new year by the sun...) of Vancouver LW meetups!</p>\n\n<p>Come on out to Benny's Bagels in Kitsilano to meet up with like-minded aspiring rationalists, talk about all sorts of stuff, and generally have fun. 13:00 on Sunday the 30th.</p>\n\n<p>If you are looking for an excuse to start coming out, the new year seems as good as any.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_New_Year_1\">Discussion article for the meetup : <a href=\"/meetups/hd\">Vancouver New Year!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver New Year!", "anchor": "Discussion_article_for_the_meetup___Vancouver_New_Year_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver New Year!", "anchor": "Discussion_article_for_the_meetup___Vancouver_New_Year_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T06:11:09.844Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] What Core Argument?", "slug": "seq-rerun-what-core-argument", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:53.937Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5E4wBhCgtMjiHXMjr/seq-rerun-what-core-argument", "pageUrlRelative": "/posts/5E4wBhCgtMjiHXMjr/seq-rerun-what-core-argument", "linkUrl": "https://www.lesswrong.com/posts/5E4wBhCgtMjiHXMjr/seq-rerun-what-core-argument", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20What%20Core%20Argument%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20What%20Core%20Argument%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5E4wBhCgtMjiHXMjr%2Fseq-rerun-what-core-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20What%20Core%20Argument%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5E4wBhCgtMjiHXMjr%2Fseq-rerun-what-core-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5E4wBhCgtMjiHXMjr%2Fseq-rerun-what-core-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/what-core-argument.html\">What Core Argument?</a> was originally published on December 10, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>The argument in favor of a strong foom just isn't well supported enough to suggest that such a dramatic process is likely.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g3p/seq_rerun_the_mechanics_of_disagreement/\">The Mechanics of Disagreement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5E4wBhCgtMjiHXMjr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0696012188842944e-06, "legacy": true, "legacyId": "20898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aL9BY5ktQBJrSkwpC", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T09:09:16.161Z", "modifiedAt": "2019-12-17T18:36:51.572Z", "url": null, "title": "Ritual 2012: A Moment of Darkness", "slug": "ritual-2012-a-moment-of-darkness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.536Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9N26ZivbMgKK8FDye/ritual-2012-a-moment-of-darkness", "pageUrlRelative": "/posts/9N26ZivbMgKK8FDye/ritual-2012-a-moment-of-darkness", "linkUrl": "https://www.lesswrong.com/posts/9N26ZivbMgKK8FDye/ritual-2012-a-moment-of-darkness", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ritual%202012%3A%20A%20Moment%20of%20Darkness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARitual%202012%3A%20A%20Moment%20of%20Darkness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9N26ZivbMgKK8FDye%2Fritual-2012-a-moment-of-darkness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ritual%202012%3A%20A%20Moment%20of%20Darkness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9N26ZivbMgKK8FDye%2Fritual-2012-a-moment-of-darkness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9N26ZivbMgKK8FDye%2Fritual-2012-a-moment-of-darkness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2907, "htmlBody": "<p><em>This is the second post of the 2012 Ritual Sequence. <a href=\"/lw/g0v/ritual_report_2012_life_death_light_darkness_and/\">The Introduction post is here</a>.</em></p><hr class=\"dividerBlock\"><p>This is... the extended version, I suppose, of a speech I gave at the Solstice. </p><p>The NYC Solstice Weekprior celebration begins bright and loud, and gradually becomes somber and poignant. Our opening songs are about the end of the world, but in a funny, boisterous manner that gets people excited and ready to sing. We gradually wind down, dimming lights, extinguishing flames. We turn to songs that aren&#x2019;t sad but are more quiet and pretty.<br><br>And then things get grim. We read <a href=\"/lw/uk/beyond_the_reach_of_god/\">Beyond the Reach of God</a>. We sing songs about a world where we are alone, where there is nothing protecting us, and where we somehow need to survive and thrive, even when it looks like the light is failing.<br><br>We extinguish all but a single candle, and read an abridged version of the <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">Gift We Give to Tomorrow</a>, which ends like this:</p><p><br><br><em>Once upon a time,</em><br><em>far away and long ago,</em><br><em>there were intelligent beings who were not themselves intelligently designed.</em><br><br><em>Once upon a time,</em><br><em>there were lovers, created by something that did not love.</em><br><br><em>Once upon a time,</em><br><em>when all of civilization was a single galaxy, </em><br><br><em>A single star.</em><br><em>A single planet.</em><br><em>A place called Earth.</em><br><br><em>Once upon a time.</em></p><p><br><br>And then we extinguish that candle, and sit for a moment in the darkness.<br><br>This year, I took that time to tell a story. <br><br>It&#x2019;s included in <a href=\"https://dl.dropbox.com/u/2000477/SolsticeEve_2012.pdf\">the 2012 Ritual Book.</a> I was going to post it at the end of the sequence. But I realized that it&#x2019;s actually pretty important to the &#x201C;What Exactly is the Point of Ritual?&#x201D; discussion. So I&#x2019;m writing a more fleshed out version now, both for easy reference and for people who don&#x2019;t feel like hunting through a large pdf to find it.<br><br>It&#x2019;s a bit longer, in this version - it&#x2019;s what I might have said, if time wasn&#x2019;t a constraint during the ceremony.</p><p><br></p><hr class=\"dividerBlock\"><br><p>A year ago, I started planning for tonight. In particular, for this moment, after the last candle is snuffed out and we&#x2019;re left alone in the dark with the knowledge that our world is unfair and that we have nobody to help us but each other.<br><br>I wanted to talk about death. <br><br>My grandmother died two years ago. The years leading up to her death were painful. She slowly lost her mobility, until all she could do was sit in her living room and hope her family would come by to visit and talk to her. <br><br>Then she started losing her memory, so she had a hard time even having conversations at all. We tried to humor her, but there&#x2019;s only so many times you can repeat the same thought in a five minute interval before your patience wears thin, and it shows, no matter how hard you try.<br> <br>She lost her rationality, regressing into a child who would argue petulantly with my mother about what to eat, and when to exercise, and visit her friends. She was a nutritionist, she knew what she was supposed to eat and why. She knew how to be healthy. And she wanted to be healthy. But she lost her ability to negotiate her near term and long term desires on her own.<br><br>Eventually even deciding to eat at all became painful. Eventually even forming words became exhausting.<br><br>Eventually she lost not just her rationality, but her agency. She stopped making decisions. She lay on her bed in the hospital, not even having the strength to complain anymore. My mother got so excited on days when she argued petulantly because at least she was doing *something*.<br><br>She lost everything that I thought made a person a person, and I stopped thinking of her as one.<br><br>Towards the end of her life, I was visiting her at the hospital. I was sitting next to her, being a dutiful grandson. Holding her hand because I knew she liked that. But she seemed like she was asleep, and after 10 minutes or so I got bored and said &#x201C;alright, I&#x2019;m going to go find Mom now. I&#x2019;ll be back soon.&#x201D;<br><br>And she squeezed my hand, and said &#x201C;No, stay.&#x201D;<br><br>Those two words were one of the last decisions she ever made. One of the last times she had a desire about how her future should be. She made an exhausting effort to turn those desires into words and then breath those words into sounds so that her grandson would spend a little more time with her.<br><br>And I was so humiliated that I had stopped believing that inside of this broken body and broken mind was a person who still desperately wanted to be loved.<br><br>She died a week or two later.<br><br>Her funeral was a Catholic Mass. My mom had made me go to Mass as a child. It always annoyed me. But in that moment, I was so grateful to be able to hold hands with a hundred people, for all of us to speak in unison, without having to think about it, and say:<br><br><em>&#x201C;Our father, who art in heaven, hallowed by thy name. Thy kingdom come, thy will be done, on earth as it is in heaven. Give us this day our daily bread, and forgive us of our trespasses, as we forgive those who trespass against us. And lead us not into temptation, but deliver us from evil.&#x201D;</em><br><br>I&#x2019;m not sure if having that one moment of comforting unity was worth 10 years of attending Catholic mass. <br><br>It&#x2019;s a legitimately hard question. I don&#x2019;t know the answer.<br><br>But I was still so frustrated that this comforting ritual was all based on falsehoods. There&#x2019;s plenty of material out there you can use to create a beautiful secular funeral, but it&#x2019;s not just about having pretty or powerful words to say. It&#x2019;s about about knowing the words already, having them already be part of you and your culture and your community. <br><br>Because when somebody dies, you don&#x2019;t have time or energy for novelty. You don&#x2019;t want to deal with new ideas that will grate slightly against you just because they&#x2019;re new. You want cached wisdom that is simple and beautiful and true, that you share with others, so that when something as awful as death happens to you, you have tools to face it, and you don&#x2019;t have to face it alone.<br><br>I was thinking about all that, as I prepared for this moment. <br><br>But my Grandmother&#x2019;s death was a long time ago. I wanted the opportunity to process it in my own way, in a community that shared my values. But it wasn&#x2019;t really a pressing issue that bore down on me. Dealing with death felt important, but it was a sort of abstract importance.<br><br>And then, the second half of this year happened.<br><br>A few months ago, an aspiring rationalist friend of mine e-mailed me to tell me that a relative died. They described the experience of the funeral, ways in which it was surprisingly straightforward, and other ways in which it was very intense. My friend had always considered themselves an anti-deathist, but it was suddenly very real to them. And it sort of sank in for me too - death is still a part of this world, and our community doesn&#x2019;t really have ways to deal with it.<br><br>And then, while I was still in the middle of the conversation with that friend, I learned that another friend had lost somebody, that same day.<br><br>Later, I would learn that a coworker of mine  also lost somebody that day as well.<br><br>Death was no longer abstract. It was real, painfully real, even if I myself didn&#x2019;t know the people who died. My friends were hurting, and I felt their pain.<br><br>I wandered off into the night to sing my Stonehenge song by myself. It&#x2019;s not quite good enough at what I needed it for - I&#x2019;m not a skilled enough songwriter to write that song, yet. But it&#x2019;s the only song I know of that attempts to do what I needed. To grimly acknowledge this specific adversary, to not offer any false hope about the inevitability of our victory, but to nonetheless march onward, bitterly determined that not quite so many people will die tomorrow as today.<br><br>I came back inside. I chatted with another friend about the experience. She offered me what comfort she could. She attempted to offer some words to the effect of &#x201C;well, death has a purpose sometimes. It helps you see the good things -&#x201D;<br><br>Gah, I thought. <br><br>What&#x2019;s interesting is that I&#x2019;m not actually that much of an anti-deathist. I think our community&#x2019;s obsession with eliminating death without regard for the consequences is potentially harmful. I think there are, quite frankly, worse things in the world. If I had to choose between my Grandmother not dying, and my Grandmother not having to gradually lose everything she thought made her <em>her </em>until her own grandson forgot that she was a person, spending her days wracked with pain, I would probably choose the latter.<br><br>But still, I&#x2019;ve come to accept that death is bad, unequivocally bad, even if some things are worse. And I had sort of forgotten, since I&#x2019;m often at odds with other Less Wrongers about this, how big the gulf was between us and the rest of the world.<br><br>I didn&#x2019;t hold it against my friend. She meant well, and having someone to talked to helped.<br><br>A week later, a friend of hers died.<br><br>A week after that, another friend of mine lost somebody.<br><br>A week after that, it wasn&#x2019;t a direct friend of a friend who died, but a local activist was murdered a few blocks from someone&#x2019;s house, and they cancelled plans with me because they were so upset. <br><br>Then a hurricane hit New York. Half the city went dark. While it was unrelated, at least one of my friends experienced a death, of sorts, that week. And even if none of my friends were directly hurt by Hurricane Sandy, you couldn&#x2019;t escape the knowledge that there were people who weren&#x2019;t so lucky.<br><br>And I went back to my notes I had written for this moment and stared and them and thought...<br><br>&#x2026;<br><br>...fuck.<br><br>Winter was coming and I didn&#x2019;t know what to do. Death is coming, and our community isn&#x2019;t ready. I set out to create a holiday about death and... it turns out that&#x2019;s a lot of responsibility, actually.<br><br>This was important, this was incredibly important and so incredibly hard to handle correctly. We as a community - the New York community, at least - need a way to process what happened to us this year, but what happened to each of us is personal and even though most of share the same values we all deal with death in our own way and&#x2026; and... and somehow after all of that, after taking a moment to process it, we need to climb back out of that darkness and end the evening feeling joyful and triumphant and proud to be human, without resorting to lies.<br><br>&#x2026;<br><br>&#x2026;<br><br>&#x2026;there&#x2019;s a lot I don&#x2019;t know how yet, about what to do, or what to say.<br><br>But here&#x2019;s what I do know:<br><br>My grandmother died. But she lived to her late eighties. She had a family of 5 children who loved her. She had a life full of not just fun and travel and adventure but of scientific discovery. She was a dietitian. She helped do research on diabetes. She was an inspiration to women at a time when a woman being a researcher was weird and a big deal. When I say she had a long, full life, I&#x2019;m not just saying something nice sounding. <br><br>My grandmother won at life, by any reasonable standard.<br><br>Not everyone gets to have that, but my grandmother did. She was the matriarch of a huge extended family that all came home for Christmas eve each year, and sang songs and shared food and loved each other. She died a few weeks after Christmas, and that year, everyone came to visit, and honestly it was one of the best experiences of my life. <br><br>In the dead of winter, each year, two dozen of people came to Poughkeepsie, to a big house sheltered by a giant cottonwood tree, and were able to celebrate *without* worrying about running out of food in the spring. At the darkest time of the year, my mother ran lights up a hundred foot tall pine tree that you could see for miles.<br><br>We were able to eat because hundreds of miles away, mechanical plows tilled fields in different climates, producing so much food that we literally could feed the entire world if we could solve some infrastructure and economic problems. <br><br>We were able to drive to my grandmother&#x2019;s house because other mechanical plows crawled through the streets all night, clearing the ice and snow away.<br> <br>Some of us were able to come to my grandmothers house from a thousand miles away, flying through the sky, higher than ancient humans even imagined angels might live.<br><br>And my Grandmother died in her late eighties, but she also *didn&#x2019;t* die when she was in her 70s and the cancer first struck her.  Because we had chemotherapy, and host of other tools to deal with it.<br><br>And the most miraculous amazing thing is that this isn&#x2019;t a miracle. This isn&#x2019;t a mystery. We know how it came to be, and we have the power to learn to understand it even better, and do more.<br><br>In this room, right now, are people who take this all seriously. Dead seriously, who don&#x2019;t just shout &#x201C;Hurrah humanity&#x201D; because shouting things together in a group is fun.<br><br>We have people in this room, right now, who are working on fixing big problems in the medical industry. We have people in this room who are trying to understand and help fix the criminal justice system. We have people in this room who are dedicating their lives to eradicating global poverty. We have people in this room who are literally working to set in motion plans to optimize *everything ever*. We have people in this room who are working to make sure that the human race doesn&#x2019;t destroy itself before we have a chance to become the people we really want to be.<br><br>And while they aren&#x2019;t in this room, there are people we know who would be here if they could, who are doing their part to try and solve this whole death problem once and for all.<br><br>And I don&#x2019;t know whether and how well any of us are going to succeed at any of these things, but...<br><br>God damn, people. You people are amazing, and even if only one of you made a dent in some of the problems you&#x2019;re working on, that... that would just be incredible.<br><br>And there are people in this room who aren&#x2019;t working on anything that grandiose. People who aren&#x2019;t trying to solve death or save the world from annihilation or alleviate suffering on a societal level. But who spend their lives making art. Music. Writing things sometimes. <br><br>People who fill their world with beauty and joy and enthusiasm, and pies and hugs and games and&#x2026; and I don&#x2019;t have time to give a shout out to everyone in the room but you all know who you are. <br><br>This room is full of people who spend their lives making this world less ugly, less a sea of blood and violence and mindless replication. People who are working to make tomorrow brighter than today, in one way or another.<br><br>And I am so proud to know all of you, to have you be a part of my life, and to be a part of yours. <br><br>I love you. <br><br>You make this world the sort of place I&#x2019;d want to keep living, forever, if I could. <br><br><a href=\"http://vimeo.com/45878034\">The sort of world I&#x2019;d want to take to the stars.</a> </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 1, "hXTqT62YDTTiqJfxG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9N26ZivbMgKK8FDye", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 54, "extendedScore": null, "score": 0.000186, "legacy": true, "legacyId": "20827", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["FLq2J9KwEuecYmaSx", "sYgv4eYH82JEsTD34", "pGvyqAQw6yqTjpKf4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T17:00:55.407Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Cleveland, Moscow", "slug": "weekly-lw-meetups-cleveland-moscow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:34.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NMzGaJYDAnTmTZCLt/weekly-lw-meetups-cleveland-moscow", "pageUrlRelative": "/posts/NMzGaJYDAnTmTZCLt/weekly-lw-meetups-cleveland-moscow", "linkUrl": "https://www.lesswrong.com/posts/NMzGaJYDAnTmTZCLt/weekly-lw-meetups-cleveland-moscow", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Cleveland%2C%20Moscow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Cleveland%2C%20Moscow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNMzGaJYDAnTmTZCLt%2Fweekly-lw-meetups-cleveland-moscow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Cleveland%2C%20Moscow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNMzGaJYDAnTmTZCLt%2Fweekly-lw-meetups-cleveland-moscow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNMzGaJYDAnTmTZCLt%2Fweekly-lw-meetups-cleveland-moscow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p><strong>This summary was posted to LW main on Dec 21st. The following week's summary is <a href=\"/lw/g4k/weekly_lw_meetups_bielefeld_montpellier_vancouver/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/gv\">Moscow: Applied Rationality:&nbsp;<span class=\"date\">22 December 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/h7\">Montpellier: Tentative first meetup:&nbsp;<span class=\"date\">03 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/h1\">Brussels meetup:&nbsp;<span class=\"date\">05 January 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/h8\">Washington, DC Meetup with Special Guest:&nbsp;<span class=\"date\">06 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/gy\">Cleveland Ohio Meetup:&nbsp;<span class=\"date\">23 December 2012 02:52PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NMzGaJYDAnTmTZCLt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0699858091843455e-06, "legacy": true, "legacyId": "20786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5sxDqPgWfMQz8mdR9", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T17:08:46.814Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8nG8z9fgbMtkPGxYH/meetup-melbourne-practical-rationality-1", "pageUrlRelative": "/posts/8nG8z9fgbMtkPGxYH/meetup-melbourne-practical-rationality-1", "linkUrl": "https://www.lesswrong.com/posts/8nG8z9fgbMtkPGxYH/meetup-melbourne-practical-rationality-1", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8nG8z9fgbMtkPGxYH%2Fmeetup-melbourne-practical-rationality-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8nG8z9fgbMtkPGxYH%2Fmeetup-melbourne-practical-rationality-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8nG8z9fgbMtkPGxYH%2Fmeetup-melbourne-practical-rationality-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/he'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 January 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/he'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8nG8z9fgbMtkPGxYH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.069990461045477e-06, "legacy": true, "legacyId": "20901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/he\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 January 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/he\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T19:11:42.733Z", "modifiedAt": null, "url": null, "title": "A cure for akrasia", "slug": "a-cure-for-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.084Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "n5dZTufewLK86nWwb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zmbvCczSG3KtbiEGd/a-cure-for-akrasia", "pageUrlRelative": "/posts/zmbvCczSG3KtbiEGd/a-cure-for-akrasia", "linkUrl": "https://www.lesswrong.com/posts/zmbvCczSG3KtbiEGd/a-cure-for-akrasia", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20cure%20for%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20cure%20for%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmbvCczSG3KtbiEGd%2Fa-cure-for-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20cure%20for%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmbvCczSG3KtbiEGd%2Fa-cure-for-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmbvCczSG3KtbiEGd%2Fa-cure-for-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p>Some of you guys have been a little down on philosophy articles lately. This article by Roy Sorensen appeared in Mind in 1997, and it is awesome, therefore all philosophy papers are awesome.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Published in Mind 106/424 (October 1997) 743</p>\n<p>A CURE FOR INCONTINENCE!</p>\n<p>Tired of being weak-willed? &nbsp;Do you want to end procrastination and&nbsp;back-sliding? &nbsp;Are you envious of those paragons of self-control who&nbsp;always do what they consider best?</p>\n<p>Thanks to a breakthrough in therapeutic philosophy, you too&nbsp;can now close the gap between what you think you ought to do and&nbsp;what you actually do. &nbsp;Just send $1000 to the address below and you&nbsp;will never again succumb to temptation. &nbsp;This is a MONEY-BACK&nbsp;GUARANTEE. &nbsp;The first time you do something that you know to be&nbsp;irrational, your money will be refunded, no questions asked.&nbsp;Of course, you might nevertheless have some questions. &nbsp;How&nbsp;can you act incontinently when you know that the \"irrational\" act&nbsp;will earn you a $1000 refund?&nbsp;Well, that's what's revolutionary in this new cure for&nbsp;incontinence. &nbsp;</p>\n<p>Old approaches focus on punishing the weak willed.&nbsp;This follows the antiquated behaviorist principle that negative&nbsp;reinforcement extinguishes bad behavior. &nbsp;The new humanitarian&nbsp;approach rewards incontinence -- and lavishly at that. &nbsp;The key is to&nbsp;make the reward so strongly motivating that an otherwise irrational&nbsp;act becomes rational.</p>\n<p>Some may seek a refund on the grounds that the reward for&nbsp;incontinence played no role in their (apparently) incontinent act;&nbsp;although aware of the reward, they would have performed the act&nbsp;anyway. &nbsp;These folks should distinguish between actual and&nbsp;hypothetical incontinence. &nbsp;If you act in accordance with your&nbsp;judgement as to what is best overall, then you did nothing irrational.</p>\n<p>True, the hypothetical incontinent act is a sign that you have a weak&nbsp;will. &nbsp;But the presence of this disposition gives you all the more&nbsp;reason to block its manifestation -- by sending $1000.&nbsp;Granted, there are people who cannot be swayed from&nbsp;temptation by a mere $1000. &nbsp;These recalcitrant individuals are&nbsp;advised to send in more than $1000. &nbsp;Give until it hurts.</p>\n<p>Rush your cheque to:</p>\n<p>Dr. Roy Sorensen</p>\n<p>Department of Philosophy</p>\n<p>New York University</p>\n<p>503 Main Building</p>\n<p>100 Washington Square East</p>\n<p>New York, New York 10003-6688</p>\n<p>(Note, address is not current)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zmbvCczSG3KtbiEGd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 1, "extendedScore": null, "score": 1.0700632521360028e-06, "legacy": true, "legacyId": "20902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T19:46:02.819Z", "modifiedAt": null, "url": null, "title": "Is ruthlessness in business executives ever useful?", "slug": "is-ruthlessness-in-business-executives-ever-useful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.197Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L5AHn2wAzbbpitbyH/is-ruthlessness-in-business-executives-ever-useful", "pageUrlRelative": "/posts/L5AHn2wAzbbpitbyH/is-ruthlessness-in-business-executives-ever-useful", "linkUrl": "https://www.lesswrong.com/posts/L5AHn2wAzbbpitbyH/is-ruthlessness-in-business-executives-ever-useful", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20ruthlessness%20in%20business%20executives%20ever%20useful%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20ruthlessness%20in%20business%20executives%20ever%20useful%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5AHn2wAzbbpitbyH%2Fis-ruthlessness-in-business-executives-ever-useful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20ruthlessness%20in%20business%20executives%20ever%20useful%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5AHn2wAzbbpitbyH%2Fis-ruthlessness-in-business-executives-ever-useful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5AHn2wAzbbpitbyH%2Fis-ruthlessness-in-business-executives-ever-useful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>We have a tradition of treating ruthlessness in businesspeople as something of a virtue. Certainly, ruthlessness can help one get ahead in the business world, and companies often benefit from executives who're willing to put aside scruples while devising means of turning a profit. So ruthlessness in business executives can certainly be useful for businesses.</p>\n<p>&nbsp;</p>\n<p>From a societal perspective though, businesses are only valuable to the extent that they increase the wealth and quality of life of society as a whole. Businesses are allowed (indeed, required, in the case of publicly traded companies) to attempt to maximize profits, on the presumption that in doing so, they'll enrich the broader society in which they operate. But there are plenty of ways in which businesses can increase their own profits without becoming more wealth productive, such as cooperating with competitors or establishing monopolies in order to keep prices artificially elevated, use of advertising to promote a product or service relative to equal or superior competitors, lobbying with politicians to slant the legal playing field in their own favor, and so forth.</p>\n<p>&nbsp;</p>\n<p>I have reasons to expect myself to be somewhat biased on this issue, so I'm not sure how telling it is that I personally come up short of any examples of ruthlessness in business executives being useful from a societal perspective, when compared to business executives who're highly competitive, but compassionate, with restrictive senses of fair play. So does anyone else have examples of ruthlessness in businesspeople as a social virtue?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L5AHn2wAzbbpitbyH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -5, "extendedScore": null, "score": 1.070083584148023e-06, "legacy": true, "legacyId": "20903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-28T23:40:42.616Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 24-26", "slug": "meetup-durham-hpmor-discussion-chapters-24-26", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:47.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pF5HFdQ6ojRQSTgXv/meetup-durham-hpmor-discussion-chapters-24-26", "pageUrlRelative": "/posts/pF5HFdQ6ojRQSTgXv/meetup-durham-hpmor-discussion-chapters-24-26", "linkUrl": "https://www.lesswrong.com/posts/pF5HFdQ6ojRQSTgXv/meetup-durham-hpmor-discussion-chapters-24-26", "postedAtFormatted": "Friday, December 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2024-26&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2024-26%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpF5HFdQ6ojRQSTgXv%2Fmeetup-durham-hpmor-discussion-chapters-24-26%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2024-26%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpF5HFdQ6ojRQSTgXv%2Fmeetup-durham-hpmor-discussion-chapters-24-26", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpF5HFdQ6ojRQSTgXv%2Fmeetup-durham-hpmor-discussion-chapters-24-26", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hf'>Durham HPMoR Discussion, chapters 24-26</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2706 Durham-Chapel Hill Boulevard, Durham, NC 27707 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>HPMoR discussion meetup. We'll be talking about chapters 24-26; come join us!</p>\n\n<p>Please feel free to join in, even if you haven't read the chapters in question yet.</p>\n\n<p>Depending on interest, there may well be Zendo afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hf'>Durham HPMoR Discussion, chapters 24-26</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pF5HFdQ6ojRQSTgXv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0702225631323014e-06, "legacy": true, "legacyId": "20905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_24_26\">Discussion article for the meetup : <a href=\"/meetups/hf\">Durham HPMoR Discussion, chapters 24-26</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2706 Durham-Chapel Hill Boulevard, Durham, NC 27707 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>HPMoR discussion meetup. We'll be talking about chapters 24-26; come join us!</p>\n\n<p>Please feel free to join in, even if you haven't read the chapters in question yet.</p>\n\n<p>Depending on interest, there may well be Zendo afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_24_261\">Discussion article for the meetup : <a href=\"/meetups/hf\">Durham HPMoR Discussion, chapters 24-26</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 24-26", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_24_26", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 24-26", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_24_261", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-29T06:01:12.877Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] What I Think, If Not Why", "slug": "seq-rerun-what-i-think-if-not-why", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Te2e9GCGpi6XDQyZ9/seq-rerun-what-i-think-if-not-why", "pageUrlRelative": "/posts/Te2e9GCGpi6XDQyZ9/seq-rerun-what-i-think-if-not-why", "linkUrl": "https://www.lesswrong.com/posts/Te2e9GCGpi6XDQyZ9/seq-rerun-what-i-think-if-not-why", "postedAtFormatted": "Saturday, December 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20What%20I%20Think%2C%20If%20Not%20Why&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20What%20I%20Think%2C%20If%20Not%20Why%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTe2e9GCGpi6XDQyZ9%2Fseq-rerun-what-i-think-if-not-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20What%20I%20Think%2C%20If%20Not%20Why%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTe2e9GCGpi6XDQyZ9%2Fseq-rerun-what-i-think-if-not-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTe2e9GCGpi6XDQyZ9%2Fseq-rerun-what-i-think-if-not-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/wp/what_i_think_if_not_why/\">What I Think, If Not Why</a> was originally published on 11 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#What_I_Think.2C_If_Not_Why\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Yudkowsky's attempt to summarize what he thinks on the subject of Friendly AI, without providing any of the justifications for what he believes.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g4i/seq_rerun_what_core_argument/\">What Core Argument?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Te2e9GCGpi6XDQyZ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0704479840186777e-06, "legacy": true, "legacyId": "20914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3kYdw54htktqt9Jb", "5E4wBhCgtMjiHXMjr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-29T09:45:55.580Z", "modifiedAt": null, "url": null, "title": "The ongoing transformation of quantum field theory", "slug": "the-ongoing-transformation-of-quantum-field-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zDFRu6iKnDnfbuCL5/the-ongoing-transformation-of-quantum-field-theory", "pageUrlRelative": "/posts/zDFRu6iKnDnfbuCL5/the-ongoing-transformation-of-quantum-field-theory", "linkUrl": "https://www.lesswrong.com/posts/zDFRu6iKnDnfbuCL5/the-ongoing-transformation-of-quantum-field-theory", "postedAtFormatted": "Saturday, December 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20ongoing%20transformation%20of%20quantum%20field%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20ongoing%20transformation%20of%20quantum%20field%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDFRu6iKnDnfbuCL5%2Fthe-ongoing-transformation-of-quantum-field-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20ongoing%20transformation%20of%20quantum%20field%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDFRu6iKnDnfbuCL5%2Fthe-ongoing-transformation-of-quantum-field-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDFRu6iKnDnfbuCL5%2Fthe-ongoing-transformation-of-quantum-field-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 840, "htmlBody": "<p>Quantum field theory (QFT) is the basic framework of particle physics. Particles arise from the quantized energy levels of field oscillations; Feynman diagrams are the simple tool for approximating their interactions. The \"standard model\", the success of which is capped by the recent observation of a Higgs boson lookalike, is a quantum field theory.</p>\n<p>But just like everything mathematical, quantum field theory has hidden depths. For the past decade, new pictures of the quantum scattering process (in which particles come together, interact, and then fly apart) have incrementally been developed, and they presage a transformation in the understanding of what a QFT describes.</p>\n<p>At the center of this evolution is \"N=4 super-Yang-Mills theory\", the maximally supersymmetric QFT in four dimensions. I want to emphasize that from a standard QFT perspective, this theory contains nothing but scalar particles (like the Higgs), spin-1/2 fermions (like electrons or quarks), and spin-1 \"gauge fields\" (like photons and gluons). The ingredients aren't something alien to real physics. What distinguishes an N=4 theory is that the particle spectrum and the interactions are arranged so as to produce a highly extended form of supersymmetry, in which particles have multiple partners (so many LWers should be comfortable with the notion).</p>\n<p>In 1997, Juan Maldacena discovered that <a href=\"http://arxiv.org/abs/hep-th/9711200\">the N=4 theory is equivalent to a type of string theory in a particular higher-dimensional space</a>. In 2003, Edward Witten discovered that <a href=\"http://arxiv.org/abs/hep-th/0312171\">it is also equivalent to a different type of string theory in a supersymmetric version of Roger Penrose's twistor space</a>. Those insights didn't come from nowhere, they explained algebraic facts that had been known for many years; and they have led to a still-accumulating stockpile of discoveries about the properties of N=4 field theory.</p>\n<p>What we can say is that the physical processes appearing in the theory can be understood as taking place in either of two dual space-time descriptions. Each space-time has its own version of a particular large symmetry, \"superconformal symmetry\", and the superconformal symmetry of one space-time is invisible in the other. And now it is becoming apparent that there is a third description, which does not involve space-time at all, in which both superconformal symmetries are manifest, but in which space-time locality and quantum unitarity are not \"visible\" - that is, they are not manifest in the equations that define the theory in this third picture.</p>\n<p>I cannot provide an authoritative account of how the new picture works. But here is my impression. In the third picture, the scattering processes of the space-time picture become a complex of polytopes - higher-dimensional polyhedra, joined at their faces - and the quantum measure becomes the volume of these polyhedra. Where you previously had particles, you now just have the dimensions of the polytopes; and the fact that in general, an <em>n</em>-dimensional space doesn't have <em>n</em> special directions suggests to me that multi-particle entanglements can be something more fundamental than the separate particles that we resolve them into.</p>\n<p>It will be especially interesting to see whether this polytope combinatorics, that can give back the scattering probabilities calculated with Feynman diagrams in the usual picture, can work solely with ordinary probabilities. That was Penrose's objective, almost fifty years ago, when he developed the theory of \"spin networks\" as a new language for the angular momentum calculations of quantum theory, and which was a step towards the twistor variables now playing an essential role in these new developments. If the probability calculus of quantum mechanics can be obtained from conventional probability theory applied to these \"structures\" that may underlie familiar space-time, then that would mean that superposition does not need to be regarded as ontological.</p>\n<p>I'm talking about this now because a group of researchers around Nima Arkani-Hamed, who are among the leaders in this area, released <a href=\"http://arxiv.org/abs/1212.5605\">their first paper in a year</a> this week. It's very new, and so arcane that, among physics bloggers, only Lubos Motl has <a href=\"http://motls.blogspot.com/2012/12/amplitudes-permutations-grassmannians.html\">talked about it</a>.</p>\n<p>This is still just one step in a journey. Not only does the paper focus on the N=4 theory - which is not the theory of the real world - but the results only apply to part of the N=4 theory, the so-called \"planar\" part, described by Feynman diagrams with a planar topology. (For an impressionistic glimpse of what might lie ahead, you could try <a href=\"http://vixra.org/abs/1208.0242\">this paper</a>, whose author has been shouting from the wilderness for years that categorical knot theory is the missing piece of the puzzle.)</p>\n<p>The N=4 theory is not reality, but the new perspective should generalize. Present-day calculations in QCD already employ truncated versions of the N=4 theory; and Arkani-Hamed et al specifically mention another supersymmetric field theory (known as ABJM after the initials of its authors), a deformation of which is holographically dual to a theory-of-everything candidate from 1983.</p>\n<p>When it comes to <em>seeing reality</em> in this new way, we still only have, at best, a fruitful chaos of ideas and possibilities. But the solid results - the mathematical equivalences - will continue to pile up, and the end product really ought to be nothing less than a new conception of how physics works.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zDFRu6iKnDnfbuCL5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 31, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "20926", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-29T22:22:14.209Z", "modifiedAt": null, "url": null, "title": "Inferring Values from Imperfect Optimizers", "slug": "inferring-values-from-imperfect-optimizers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nigerweiss", "createdAt": "2012-11-20T18:28:27.983Z", "isAdmin": false, "displayName": "nigerweiss"}, "userId": "DYPo3FWGnAX3mwp2Y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SP2Sa5nKN9hGrqTdZ/inferring-values-from-imperfect-optimizers", "pageUrlRelative": "/posts/SP2Sa5nKN9hGrqTdZ/inferring-values-from-imperfect-optimizers", "linkUrl": "https://www.lesswrong.com/posts/SP2Sa5nKN9hGrqTdZ/inferring-values-from-imperfect-optimizers", "postedAtFormatted": "Saturday, December 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inferring%20Values%20from%20Imperfect%20Optimizers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInferring%20Values%20from%20Imperfect%20Optimizers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSP2Sa5nKN9hGrqTdZ%2Finferring-values-from-imperfect-optimizers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inferring%20Values%20from%20Imperfect%20Optimizers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSP2Sa5nKN9hGrqTdZ%2Finferring-values-from-imperfect-optimizers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSP2Sa5nKN9hGrqTdZ%2Finferring-values-from-imperfect-optimizers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 754, "htmlBody": "<p>One approach to constructing a Friendly artificial intelligence is to create a piece of software that looks at large&nbsp;amounts&nbsp;of evidence about humans, and attempts to infer their values. &nbsp;I've been doing some thinking about this problem, and I'm going to talk about some approaches and problems that have occurred to me.</p>\n<p>&nbsp;</p>\n<p>In a naive approach, we might define the problem like this: take some unknown utility function, U, and plug it into a mathematically clean optimization process (like AIXI) O. &nbsp;Then, look at your data set and take the information about the inputs and outputs of humans, and find the simplest U that best explains human behavior.</p>\n<p>Unfortunately, this won't work. &nbsp;The best possible match for U is one that models not just those elements of human utility we're interested in, but also all the details of our broken, contradictory optimization process. &nbsp;The U&nbsp;we derive through this process will optimize for confirmation bias, scope insensitivity, hindsight bias, the halo effect, our own limited intelligence and inefficient use of evidence, and just about everything else that's wrong with us. &nbsp;Not what we're looking for.</p>\n<p>Okay, so let's try putting a bandaid on it - let's go back to our original problem setup. &nbsp;However, we'll take our original O, and use all of the science on cognitive biases at our disposal to handicap it. &nbsp;We'll limit its search space, saddle it with a laundry list of cognitive biases, cripple its ability to use evidence, and in general make it as human-like as we possibly can. &nbsp;We could even give it akrasia by implementing hyperbolic discounting of reward. &nbsp;Then we'll repeat the original process to produce U'.</p>\n<p>If we plug U'&nbsp;into our AI, the result will be that it will optimize like a human who had suddenly been stripped of all the kinds of stupidity that we programmed into our modified O. &nbsp;This is good! &nbsp;Plugged into a solid CEV&nbsp;infrastructure, this might even be good enough to produce a future that's a nice place to live. &nbsp;However, it's not quite ideal. &nbsp;If we miss a cognitive bias, then it'll be incorporated into the learned utility functions, and we may never be rid of it. &nbsp;What would be nice would be if we could get the AI to learn about cognitive biases, exhaustively, and update in the future if it ever discovered a new one. &nbsp;</p>\n<p>&nbsp;</p>\n<p>If we had enough time and money, we could do this the hard way: acquire a representative sample of the human population, and pay them to perform tasks with simple goals under tremendous&nbsp;surveillance, and have the AI derive the human optimization process from the actions taken towards a known goal. &nbsp;However, if we assume that the human optimization process can be defined as a function over the state of the human brain, we should not trust the completeness of any such process learned from less data than the entropy of the human brain, which is on the order of tens of petabytes of extremely high quality evidence. &nbsp;If we want to be confident in the completeness of our model, we may need more experimental evidence than it is really practical to accumulate. &nbsp;Which isn't to say that this approach is useless - if we can hit close enough to the mark, then the AI may be able to run more exhaustive experimentation later and refine its own understanding of human brains to be closer to the ideal.</p>\n<p>But it'd really be nice if our AI could do unsupervised learning to figure out the details of human optimization. &nbsp;Then we could simply dump the internet into it, and let it grind away at the data and spit out a detailed, complete model of human decision-making, from which our utility function could be derived. &nbsp;Unfortunately, this does not seem to be a tractable problem. &nbsp;It's possible that some insight could be gleaned by examining outliers with normal intelligence, but deviant utility functions (I am thinking specifically of sociopaths), but it's unclear how much insight can be produced by these methods. &nbsp;If anyone has suggestions for a more efficient way of going about it, I'd love to hear it. &nbsp;As it stands, it might be possible to get enough information from this to supplement a supervised learning approach - the closer we get to a perfectly accurate model, the higher the probability of Things Going Well. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>\n<p>Anyways, that's where I am right now. &nbsp;I just thought I'd put up my thoughts and see if some fresh eyes see anything I've been missing. &nbsp;</p>\n<p>&nbsp;</p>\n<p>Cheers,</p>\n<p>Niger&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SP2Sa5nKN9hGrqTdZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.0710295554289118e-06, "legacy": true, "legacyId": "20928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-29T22:47:23.224Z", "modifiedAt": null, "url": null, "title": "Meetup : Fourth Buenos Aires Less Wrong meetup", "slug": "meetup-fourth-buenos-aires-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.115Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sumrZX3eCk6K2AAhQ/meetup-fourth-buenos-aires-less-wrong-meetup", "pageUrlRelative": "/posts/sumrZX3eCk6K2AAhQ/meetup-fourth-buenos-aires-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/sumrZX3eCk6K2AAhQ/meetup-fourth-buenos-aires-less-wrong-meetup", "postedAtFormatted": "Saturday, December 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fourth%20Buenos%20Aires%20Less%20Wrong%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fourth%20Buenos%20Aires%20Less%20Wrong%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsumrZX3eCk6K2AAhQ%2Fmeetup-fourth-buenos-aires-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fourth%20Buenos%20Aires%20Less%20Wrong%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsumrZX3eCk6K2AAhQ%2Fmeetup-fourth-buenos-aires-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsumrZX3eCk6K2AAhQ%2Fmeetup-fourth-buenos-aires-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hg'>Fourth Buenos Aires Less Wrong meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 January 2013 03:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">GarageLab, Roseti 1380, Ciudad de Buenos Aires</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be our fourth Buenos Aires LessWrong meetup.</p>\n\n<p>On this occasion, we will be discussing Coherently Extrapolated Volition. For a brief introduction, see the corresponding <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">LessWrong wiki entry</a>. For more detailed discussion, see <a href=\"http://intelligence.org/files/CEV.pdf\" rel=\"nofollow\">Eliezer Yudkowsky&#39;s essay</a>.</p>\n\n<p>Previous meetups were a success and attendance has been steadily increasing. If you read and like this blog and live in BA or just happen to be visiting the city, do join us. It will be a great opportunity to meet like-minded folks in the area.</p>\n\n<p><strong>Please note the change of venue</strong>: we will now meet at GarageLab (<a href=\"https://maps.google.com/maps?q=Roseti+1380,+Buenos+Aires,+Argentina&amp;hl=en&amp;ie=UTF8&amp;sll=-38.442157,-63.622274&amp;sspn=61.756623,135.263672&amp;oq=Roseti+1380&amp;t=v&amp;hnear=Roseti+1380,+Villa+Ort%C3%BAzar,+Buenos+Aires,+Argentina&amp;z=17\" rel=\"nofollow\">Roseti 1380</a>, between Elcano and 14 de Julio), which should be more suitable than Starbucks.</p>\n\n<p>The meeting will take place on <strong>Saturday, January 5th, 2013, at 3pm</strong> (Buenos Aires time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hg'>Fourth Buenos Aires Less Wrong meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sumrZX3eCk6K2AAhQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0710444723853049e-06, "legacy": true, "legacyId": "20929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fourth_Buenos_Aires_Less_Wrong_meetup\">Discussion article for the meetup : <a href=\"/meetups/hg\">Fourth Buenos Aires Less Wrong meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 January 2013 03:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">GarageLab, Roseti 1380, Ciudad de Buenos Aires</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be our fourth Buenos Aires LessWrong meetup.</p>\n\n<p>On this occasion, we will be discussing Coherently Extrapolated Volition. For a brief introduction, see the corresponding <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">LessWrong wiki entry</a>. For more detailed discussion, see <a href=\"http://intelligence.org/files/CEV.pdf\" rel=\"nofollow\">Eliezer Yudkowsky's essay</a>.</p>\n\n<p>Previous meetups were a success and attendance has been steadily increasing. If you read and like this blog and live in BA or just happen to be visiting the city, do join us. It will be a great opportunity to meet like-minded folks in the area.</p>\n\n<p><strong>Please note the change of venue</strong>: we will now meet at GarageLab (<a href=\"https://maps.google.com/maps?q=Roseti+1380,+Buenos+Aires,+Argentina&amp;hl=en&amp;ie=UTF8&amp;sll=-38.442157,-63.622274&amp;sspn=61.756623,135.263672&amp;oq=Roseti+1380&amp;t=v&amp;hnear=Roseti+1380,+Villa+Ort%C3%BAzar,+Buenos+Aires,+Argentina&amp;z=17\" rel=\"nofollow\">Roseti 1380</a>, between Elcano and 14 de Julio), which should be more suitable than Starbucks.</p>\n\n<p>The meeting will take place on <strong>Saturday, January 5th, 2013, at 3pm</strong> (Buenos Aires time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fourth_Buenos_Aires_Less_Wrong_meetup1\">Discussion article for the meetup : <a href=\"/meetups/hg\">Fourth Buenos Aires Less Wrong meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fourth Buenos Aires Less Wrong meetup", "anchor": "Discussion_article_for_the_meetup___Fourth_Buenos_Aires_Less_Wrong_meetup", "level": 1}, {"title": "Discussion article for the meetup : Fourth Buenos Aires Less Wrong meetup", "anchor": "Discussion_article_for_the_meetup___Fourth_Buenos_Aires_Less_Wrong_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T03:22:51.335Z", "modifiedAt": null, "url": null, "title": "Brain-in-a-vat Trolley Question", "slug": "brain-in-a-vat-trolley-question-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j42LsXe3qkHhctezz/brain-in-a-vat-trolley-question-0", "pageUrlRelative": "/posts/j42LsXe3qkHhctezz/brain-in-a-vat-trolley-question-0", "linkUrl": "https://www.lesswrong.com/posts/j42LsXe3qkHhctezz/brain-in-a-vat-trolley-question-0", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain-in-a-vat%20Trolley%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain-in-a-vat%20Trolley%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj42LsXe3qkHhctezz%2Fbrain-in-a-vat-trolley-question-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain-in-a-vat%20Trolley%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj42LsXe3qkHhctezz%2Fbrain-in-a-vat-trolley-question-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj42LsXe3qkHhctezz%2Fbrain-in-a-vat-trolley-question-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 659, "htmlBody": "<p>Just saw this on another forum. I figured I'd repost it here, since it'd be interesting to see you guy's answer to it.</p>\n<blockquote>\n<p><em>Consider the following case:</em><br /><br />On Twin Earth, a brain in a  vat is at the wheel of a runaway trolley. There are only two options  that the brain can take: the right side of the fork in the track or the  left side of the fork. There is no way in sight of derailing or stopping  the trolley and the brain is aware of this, for the brain <em>knows</em> trolleys. The brain is causally hooked up to the trolley such that the  brain can determine the course which the trolley will take.<br /><br />On  the right side of the track there is a single railroad worker, Jones,  who will definitely be killed if the brain steers the trolley to the  right. If the railman on the right lives, he will go on to kill five men  for the sake of killing them, but in doing so will inadvertently save  the lives of thirty orphans (one of the five men he will kill is  planning to destroy a bridge that the orphans' bus will be crossing  later that night). One of the orphans that will be killed would have  grown up to become a tyrant who would make good utilitarian men do bad  things. Another of the orphans would grow up to become G.E.M. Anscombe,  while a third would invent the pop-top can.<br /><br />If the brain in the  vat chooses the left side of the track, the trolley will definitely hit  and kill a railman on the left side of the track, \"Leftie\" and will hit  and destroy ten beating hearts on the track that could (and would) have  been transplanted into ten patients in the local hospital that will die  without donor hearts. These are the only hearts available, and the brain  is aware of this, for the brain <em>knows</em> hearts. If the railman on  the left side of the track lives, he too will kill five men, in fact the  same five that the railman on the right would kill. However, \"Leftie\"  will kill the five as an unintended consequence of saving ten men: he  will inadvertently kill the five men rushing the ten hearts to the local  hospital for transplantation. A further result of \"Leftie's\" act would  be that the busload of orphans will be spared. Among the five men killed  by \"Leftie\" are both the man responsible for putting the brain at the  controls of the trolley, and the author of this example. If the ten  hearts and \"Leftie\" are killed by the trolley, the ten prospective  heart-transplant patients will die and their kidneys will be used to  save the lives of twenty kidney-transplant patients, one of whom will  grow up to cure cancer, and one of whom will grow up to be Hitler. There  are other kidneys and dialysis machines available, however the brain  does not <em>know</em> kidneys, and this is not a factor.<br /><br />Assume  that the brain's choice, whatever it turns out to be, will serve as an  example to other brains-in-vats and so the effects of his decision will  be amplified. Also assume that if the brain chooses the right side of  the fork, an unjust war free of war crimes will ensue, while if the  brain chooses the left fork, a just war fraught with war crimes will  result. Furthermore, there is an intermittently active Cartesian demon  deceiving the brain in such a manner that the brain is never sure if it  is being deceived.<br /><br /><strong>QUESTION: <em>What should the brain do?</em></strong><br /><br />[<em>ALTERNATIVE EXAMPLE</em>:  Same as above, except the brain has had a commisurotomy, and the left  half of the brain is a consequentialist and the right side is an  absolutist.]</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j42LsXe3qkHhctezz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -12, "extendedScore": null, "score": 1.0712078807718814e-06, "legacy": true, "legacyId": "20930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T05:14:16.567Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Not Taking Over the World", "slug": "seq-rerun-not-taking-over-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L4oL2KRcJ2cv66DxQ/seq-rerun-not-taking-over-the-world", "pageUrlRelative": "/posts/L4oL2KRcJ2cv66DxQ/seq-rerun-not-taking-over-the-world", "linkUrl": "https://www.lesswrong.com/posts/L4oL2KRcJ2cv66DxQ/seq-rerun-not-taking-over-the-world", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Not%20Taking%20Over%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Not%20Taking%20Over%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4oL2KRcJ2cv66DxQ%2Fseq-rerun-not-taking-over-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Not%20Taking%20Over%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4oL2KRcJ2cv66DxQ%2Fseq-rerun-not-taking-over-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4oL2KRcJ2cv66DxQ%2Fseq-rerun-not-taking-over-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p>Today's post, <a href=\"/lw/wt/not_taking_over_the_world/\">Not Taking Over the World</a> was originally published on 15 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Not_Taking_Over_the_World\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's rather difficult to imagine a way in which you could create an AI, and not somehow either take over or destroy the world. How can you use unlimited power in such a way that you don't become a malevolent deity, in the Epicurean sense?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g4y/seq_rerun_what_i_think_if_not_why/\">What I Think, If Not Why</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L4oL2KRcJ2cv66DxQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0712739881307854e-06, "legacy": true, "legacyId": "20932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DdEKcS6JcW7ordZqQ", "Te2e9GCGpi6XDQyZ9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T06:54:20.893Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Defense Against the Dark Arts, Part I", "slug": "meetup-west-la-meetup-defense-against-the-dark-arts-part-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.282Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DvdPBc43j5oscRai9/meetup-west-la-meetup-defense-against-the-dark-arts-part-i", "pageUrlRelative": "/posts/DvdPBc43j5oscRai9/meetup-west-la-meetup-defense-against-the-dark-arts-part-i", "linkUrl": "https://www.lesswrong.com/posts/DvdPBc43j5oscRai9/meetup-west-la-meetup-defense-against-the-dark-arts-part-i", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Defense%20Against%20the%20Dark%20Arts%2C%20Part%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Defense%20Against%20the%20Dark%20Arts%2C%20Part%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvdPBc43j5oscRai9%2Fmeetup-west-la-meetup-defense-against-the-dark-arts-part-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Defense%20Against%20the%20Dark%20Arts%2C%20Part%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvdPBc43j5oscRai9%2Fmeetup-west-la-meetup-defense-against-the-dark-arts-part-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvdPBc43j5oscRai9%2Fmeetup-west-la-meetup-defense-against-the-dark-arts-part-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hh'>West LA Meetup - Defense Against the Dark Arts, Part I</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, January 2nd.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> <a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Defense Against the Dark Arts</a> is a tricky topic to teach, and a vast field to learn. Salman and I will conduct this experimental meetup by presenting to you some introductory morsels of this essential knowledge. There will be at least one activity as well. By the end of next year we hope to give you enough tools and training to ward off any basic attempts to persuade you to act against your best interests.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hh'>West LA Meetup - Defense Against the Dark Arts, Part I</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DvdPBc43j5oscRai9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0713333684357114e-06, "legacy": true, "legacyId": "20934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Defense_Against_the_Dark_Arts__Part_I\">Discussion article for the meetup : <a href=\"/meetups/hh\">West LA Meetup - Defense Against the Dark Arts, Part I</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, January 2nd.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> <a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Defense Against the Dark Arts</a> is a tricky topic to teach, and a vast field to learn. Salman and I will conduct this experimental meetup by presenting to you some introductory morsels of this essential knowledge. There will be at least one activity as well. By the end of next year we hope to give you enough tools and training to ward off any basic attempts to persuade you to act against your best interests.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Defense_Against_the_Dark_Arts__Part_I1\">Discussion article for the meetup : <a href=\"/meetups/hh\">West LA Meetup - Defense Against the Dark Arts, Part I</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Defense Against the Dark Arts, Part I", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Defense_Against_the_Dark_Arts__Part_I", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Defense Against the Dark Arts, Part I", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Defense_Against_the_Dark_Arts__Part_I1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T08:20:02.254Z", "modifiedAt": null, "url": null, "title": "A Game for Probabilistic Thinking", "slug": "a-game-for-probabilistic-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Erdrick", "createdAt": "2012-07-26T02:59:25.380Z", "isAdmin": false, "displayName": "Erdrick"}, "userId": "fhHkNRstY3t2dbRmZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nNR3BFdrk4GrJqCSN/a-game-for-probabilistic-thinking", "pageUrlRelative": "/posts/nNR3BFdrk4GrJqCSN/a-game-for-probabilistic-thinking", "linkUrl": "https://www.lesswrong.com/posts/nNR3BFdrk4GrJqCSN/a-game-for-probabilistic-thinking", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Game%20for%20Probabilistic%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Game%20for%20Probabilistic%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNR3BFdrk4GrJqCSN%2Fa-game-for-probabilistic-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Game%20for%20Probabilistic%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNR3BFdrk4GrJqCSN%2Fa-game-for-probabilistic-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNR3BFdrk4GrJqCSN%2Fa-game-for-probabilistic-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>I friend of mine recently recommended a small independent game to me called Clairvoyance (www.gameofclairvoyance.com), that I thought might be of interest to the LW community. &nbsp;It's effectively a competitive turn-based strategy game, but it's distinguishing feature is that each player places 5 moves at a time, which are then interlaced and executed with the opponent's 5 moves. &nbsp;Thus requiring the clairvoyance of the game's name - you need to anticipate your opponent's moves in order plan your own, included anticipating their anticipations of your actions, etc.</p>\n<p>It's clearly not intended as a rationality aid, and I originally did not approach it as such. &nbsp;But after getting trounced in my first few games, I found it very helpful to approach each turn probabilistically - thinking of each round in terms of a probability cloud of possible positions for various pieces, and planning around attacking swathes of that cloud rather than specific squares. &nbsp;Though early on, I can also foresee an element of theory-of-mind practice emerging if you play one opponent repeatedly and begin to learn their particular style.</p>\n<p>\n<p>If any one cares to give it a whirl, the game is just $5 while still in beta. &nbsp;It's cheap, it's fun and easy to learn, and games are relatively fast. &nbsp;If anyone wants to rumble, my handle in-game is also Erdrick - I have a feeling the crowd here would provided for some excellent opponents.&nbsp;</p>\n<p>-Brett</p>\n<div><br /></div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nNR3BFdrk4GrJqCSN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 1.0713842190238601e-06, "legacy": true, "legacyId": "20935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T10:43:30.669Z", "modifiedAt": null, "url": null, "title": "Three kinds of moral uncertainty", "slug": "three-kinds-of-moral-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.410Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AytzBuJSD9v2cWu3m/three-kinds-of-moral-uncertainty", "pageUrlRelative": "/posts/AytzBuJSD9v2cWu3m/three-kinds-of-moral-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/AytzBuJSD9v2cWu3m/three-kinds-of-moral-uncertainty", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20kinds%20of%20moral%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20kinds%20of%20moral%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAytzBuJSD9v2cWu3m%2Fthree-kinds-of-moral-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20kinds%20of%20moral%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAytzBuJSD9v2cWu3m%2Fthree-kinds-of-moral-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAytzBuJSD9v2cWu3m%2Fthree-kinds-of-moral-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 734, "htmlBody": "<p><strong>Related to: </strong><a href=\"http://wiki.lesswrong.com/wiki/Moral_uncertainty\">Moral uncertainty (wiki)</a>, <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Moral uncertainty - towards a solution?</a>, <a href=\"/lw/fyb/ontological_crisis_in_humans/\">Ontological Crisis in Humans</a>.</p>\n<p style=\"padding-left: 30px;\"><strong>Moral uncertainty</strong> (or <strong>normative uncertainty</strong>) is  uncertainty about how to act given the diversity of moral doctrines. For  example, suppose that we knew for certain that a new technology would  enable more humans to live on another planet with slightly less  well-being than on Earth<a class=\"external autonumber\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Terraforming\">[1]</a>. An average <a title=\"Utilitarianism\" href=\"http://wiki.lesswrong.com/wiki/Utilitarianism\">utilitarian</a> would consider these consequences bad, while a total utilitarian would  endorse such technology. If we are uncertain about which of these two  theories are right, what should we do? (<a href=\"http://wiki.lesswrong.com/wiki/Moral_uncertainty\">LW wiki</a>)</p>\n<p>I have long been slightly frustrated by the existing discussions about moral uncertainty that I've seen. I suspect that the reason has been that they've been unclear on what exactly they <em>mean</em> when they say that we are \"uncertain about which theory is right\" - what <em>is</em> uncertainty about moral theories? Furthermore, especially when discussing things in an FAI context, it feels like several different senses of moral uncertainty get mixed together. Here is my suggested breakdown, with some elaboration:</p>\n<p><strong>Descriptive moral uncertainty. </strong><em>W</em><em>hat is the most accurate way of describing my values? </em>The classical FAI-relevant question, this is in a sense the most straightforward one. We have some set values, and although we can describe parts of them verbally, we do not have conscious access to the deep-level cognitive machinery that generates them. We might feel relatively sure that our moral intuitions are produced by a system that's mostly consequentialist, but suspect that parts of us might be better described as deontologist. A solution to descriptive moral uncertainty would involve a system capable of somehow extracting the mental machinery that produced our values, or creating a moral reasoning system which managed to produce the same values by some other process.</p>\n<p><strong>Epistemic moral uncertainty.</strong> <em>Would I reconsider any of my values if I knew more?</em> Perhaps we hate the practice of eating five-sided fruit and think that everyone who eats five-sided fruit should be thrown to jail, but if we found out that five-sided fruit made people happier and had no averse effects, we would change our minds. This roughly corresponds to the \"our wish if we knew more, thought faster\" part of <a href=\"http://intelligence.org/files/CEV.pdf\">Eliezer's original CEV description</a>. A solution to epistemic moral uncertainty would involve finding out more about the world.</p>\n<p><strong>Intrinsic moral uncertainty. </strong><em>Which axioms should I endorse? </em>We might be intrinsically conflicted between different value systems. Perhaps we are trying to choose whether to be loyal to a friend or whether to act for the common good (a conflict between two forms of deontology, or between deontology and consequentialism), or we could be conflicted between positive and negative utilitarianism. In its purest form, this sense of moral uncertainty closely resembles what would otherwise be called a <a href=\"/lw/og/wrong_questions/\">wrong question</a>, one where</p>\n<blockquote>\n<p>you cannot even <em>imagine</em> any concrete, specific state of how-the-world-is that would answer the question.&nbsp; When it doesn't even seem <em>possible</em> to answer the question.</p>\n</blockquote>\n<p>But unlike wrong questions, questions of intrinsic moral uncertainty are real ones that you need to actually answer in order to make a choice. They are generated when different modules within your brain generate different <a href=\"/lw/dc5/thoughts_on_moral_intuitions/\">moral intuitions</a>, and are essentially power struggles between various parts of your mind. A solution to intrinsic moral uncertainty would involve somehow tipping the balance of power in favor of one of the \"mind factions\". This could involve developing an argument sufficiently persuasive to convince most parts of yourself, or self-modifying in such a way that one of the factions loses its sway over your decision-making. (Of course, if you already knew for certain which faction you wanted to expunge, you wouldn't need to do it in the first place.) I would roughly interpret the \"our wish ... if we had grown up farther together\" part of CEV to be an attempt to model some of the social influences on our moral intuitions and thereby help resolve cases of intrinsic moral uncertainty.</p>\n<p>\n<hr />\n</p>\n<p>This is a very preliminary categorization, and I'm sure that it could be improved upon. There also seem to exist cases of moral uncertainty which are hybrids of several categories - for example, <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crises</a> seem to be mostly about intrinsic moral uncertainty, but to also incorporate some elements of epistemic moral uncertainty. I also have a general suspicion that these categories still don't cut reality that well at the joints, so any suggestions for improvement would be much appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "ouT6wKhACJRouGokM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AytzBuJSD9v2cWu3m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 55, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "20936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KLaJjNdENsHhKhG5m", "XzrqkhfwtiSDgKoAF", "fEFwHBeatkaKpm8ZB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T19:30:34.095Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham LW Meetup: Zendo", "slug": "meetup-durham-lw-meetup-zendo", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BvWEGYKesJeXAR3Ai/meetup-durham-lw-meetup-zendo", "pageUrlRelative": "/posts/BvWEGYKesJeXAR3Ai/meetup-durham-lw-meetup-zendo", "linkUrl": "https://www.lesswrong.com/posts/BvWEGYKesJeXAR3Ai/meetup-durham-lw-meetup-zendo", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20LW%20Meetup%3A%20Zendo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20LW%20Meetup%3A%20Zendo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvWEGYKesJeXAR3Ai%2Fmeetup-durham-lw-meetup-zendo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20LW%20Meetup%3A%20Zendo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvWEGYKesJeXAR3Ai%2Fmeetup-durham-lw-meetup-zendo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvWEGYKesJeXAR3Ai%2Fmeetup-durham-lw-meetup-zendo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hi'>Durham LW Meetup: Zendo</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's cafe, 706 9th Street, Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play Zendo and chat about general rationality topics. Come join us and play or learn to play if you haven't before!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hi'>Durham LW Meetup: Zendo</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BvWEGYKesJeXAR3Ai", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0717822793199727e-06, "legacy": true, "legacyId": "20938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_LW_Meetup__Zendo\">Discussion article for the meetup : <a href=\"/meetups/hi\">Durham LW Meetup: Zendo</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's cafe, 706 9th Street, Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play Zendo and chat about general rationality topics. Come join us and play or learn to play if you haven't before!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_LW_Meetup__Zendo1\">Discussion article for the meetup : <a href=\"/meetups/hi\">Durham LW Meetup: Zendo</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham LW Meetup: Zendo", "anchor": "Discussion_article_for_the_meetup___Durham_LW_Meetup__Zendo", "level": 1}, {"title": "Discussion article for the meetup : Durham LW Meetup: Zendo", "anchor": "Discussion_article_for_the_meetup___Durham_LW_Meetup__Zendo1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T19:47:50.854Z", "modifiedAt": null, "url": null, "title": "Should a judge do it's job or maximize overall happiness", "slug": "should-a-judge-do-it-s-job-or-maximize-overall-happiness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GRNLnKBehp2sZXJEw/should-a-judge-do-it-s-job-or-maximize-overall-happiness", "pageUrlRelative": "/posts/GRNLnKBehp2sZXJEw/should-a-judge-do-it-s-job-or-maximize-overall-happiness", "linkUrl": "https://www.lesswrong.com/posts/GRNLnKBehp2sZXJEw/should-a-judge-do-it-s-job-or-maximize-overall-happiness", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20a%20judge%20do%20it's%20job%20or%20maximize%20overall%20happiness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20a%20judge%20do%20it's%20job%20or%20maximize%20overall%20happiness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRNLnKBehp2sZXJEw%2Fshould-a-judge-do-it-s-job-or-maximize-overall-happiness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20a%20judge%20do%20it's%20job%20or%20maximize%20overall%20happiness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRNLnKBehp2sZXJEw%2Fshould-a-judge-do-it-s-job-or-maximize-overall-happiness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRNLnKBehp2sZXJEw%2Fshould-a-judge-do-it-s-job-or-maximize-overall-happiness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 439, "htmlBody": "<p>John Smith is a head judge. He's living in a state with has a three strikes law. He personally doesn't believe in the&nbsp;three strikes law. He resides in the<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">&nbsp;highest appellate court in his state</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">. He's operating in a state without juries.</span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">As part of John's job he writes down his confidence on whether a witness tell the truth whenever he's hearing an witness make a statement. He has an&nbsp;assistant that checks his results. John quite frequently is 99,99% sure that a witness lies. He was never wrong when he made the prediction because of his skills at reading bodylanguage.</span></span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">In his private time he also trains his ability to tell whether people tell the truth in training cases where it's easy to determine the truth afterwards. He's perfectly caliberated.</span></span></p>\n<p>He gets a court case where Bob is accused of bribing a lot of politicians in an African state. He's supposed to have given them bribes to drop a law that forbids vaccination. Bob himself claims that he only talked with the African politicians and they afterwards saw Bob's wisdom and changed the law. In any case Bob got the law changed and produced massive utility in the process. Bob also pledges to do other lobbying in African countries to get rid of very harmful laws in the future.</p>\n<p>After Bob attempts of getting the law Bribing officials of other countries is a felony is Bob's state.&nbsp;Bob already has two court judgements against him so being judged for bribing would put him for the rest of his life behind bars.</p>\n<p>Carol is a billionaire. She is supposed to have give Bob $100,000 for his lobbying efforts to change the law in the African state. Carol however claims that she only payed Bob $3,000 for his traveling costs in the process and she never gave him an amount of money that would be enough to bribe the African policitians.</p>\n<p>Through his bodylanguage reading skills John knows that Bob and Carol are lying to him. One previous court thought that Bob did the crime, the second court thought that Bob didn't.&nbsp;</p>\n<p>John gets asked by his fellow judges on the court whether he thinks the two are lying. His fellow judges know of his impressive bodylanguage reading ability and fully trust him.</p>\n<p>John swore an oath to fulfill the law. Should John break his oath and tell his collegues that Bob and Carol are telling the truth? Or should he tell them that they lie. John knows that telling his collegues that they lie would mean that Bob would spent the rest of his life behind bars and couldn't do any lobbying in Africa in the future.</p>\n<p>How should John decide?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GRNLnKBehp2sZXJEw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -9, "extendedScore": null, "score": -2.4e-05, "legacy": true, "legacyId": "20939", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-30T22:00:04.255Z", "modifiedAt": null, "url": null, "title": "Stop Using LessWrong: A Practical Interpretation of the 2012 Survey Results", "slug": "stop-using-lesswrong-a-practical-interpretation-of-the-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aceofspades", "createdAt": "2011-11-27T04:54:15.103Z", "isAdmin": false, "displayName": "aceofspades"}, "userId": "urd23ECdhuapym5Ys", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xyD3vmBsss2q3dyyD/stop-using-lesswrong-a-practical-interpretation-of-the-2012", "pageUrlRelative": "/posts/xyD3vmBsss2q3dyyD/stop-using-lesswrong-a-practical-interpretation-of-the-2012", "linkUrl": "https://www.lesswrong.com/posts/xyD3vmBsss2q3dyyD/stop-using-lesswrong-a-practical-interpretation-of-the-2012", "postedAtFormatted": "Sunday, December 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stop%20Using%20LessWrong%3A%20A%20Practical%20Interpretation%20of%20the%202012%20Survey%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStop%20Using%20LessWrong%3A%20A%20Practical%20Interpretation%20of%20the%202012%20Survey%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyD3vmBsss2q3dyyD%2Fstop-using-lesswrong-a-practical-interpretation-of-the-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stop%20Using%20LessWrong%3A%20A%20Practical%20Interpretation%20of%20the%202012%20Survey%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyD3vmBsss2q3dyyD%2Fstop-using-lesswrong-a-practical-interpretation-of-the-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyD3vmBsss2q3dyyD%2Fstop-using-lesswrong-a-practical-interpretation-of-the-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>Link to those results: http://lesswrong.com/lw/fp5/2012_survey_results/</p>\n<p>I've been basically lurking this site for more than a year now and it's incredible that I have actually taken anything at all on this site seriously, let alone that at least thousands of others have. I have never received evidence that I am less likely to be overconfident about things than people in general or that any other particular person on this site is.</p>\n<p>Yet in spite of this apparently 3.7% of people answering the survey have actually signed up for cryonics which is surely greater than the percent of people in the entire world signed up for cryonics. The entire idea seems to be taken especially seriously on this site. Evidently 72.9% of people here are at least considering signing up. I think the chance of cryonics working is trivial, for all practical purposes indistinguishable from zero (the expected value of the benefit is certainly not worth several hundred thousand dollars in future value considerations). Other people here apparently disagree, but if the rest of the world is undervaluing cryonics at the moment then why do those here with privileged information not invest heavily in the formation of new for-profit cryonics organizations, or start them alone, or invest in technology which will soon develop to make the revival of cryonics patients possible? If the rest of the world is underconfident about these ideas, then these investments would surely have an enormous expected rate of return.</p>\n<p>There is also a question asking about the relative likelihood of different existential risks, which seems to imply that any of these risks are especially worth considering. This is not really a fault of the survey itself, as I have read significant discussion on this site related to these ideas. In my judgment this reflects a grand level of overconfidence in the probabilities of any of these occurring. How many people responding to this survey have actually made significant personal preparations for survival, like a fallout shelter with food and so on which would actually be useful under most of the different scenarios listed? I generously estimate 5% have made any such preparations.</p>\n<p>I also see mentioned in the survey and have read on this site material related to in my view meaningless counterfactuals. The questions on dust specks vs torture and Newcomb's Problem are so unlikely to ever be relevant in reality that I view discussion about them as worthless.</p>\n<p>My judgment of this site as of now is that way too much time is spent discussing subjects of such low expected value (usually because of absurdly low expected probability of occurring) for using this site to be worthwhile. In fact I hypothesize that this discussion actually causes overconfidence related to such things happening, and at a minimum I have seen insufficient evidence for the value of using this site to continue doing so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xyD3vmBsss2q3dyyD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": -55, "extendedScore": null, "score": 1.071870554922e-06, "legacy": true, "legacyId": "20940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-31T05:22:29.448Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] We Agree: Get Froze", "slug": "seq-rerun-we-agree-get-froze", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6kPd6WCjWz5XADuHd/seq-rerun-we-agree-get-froze", "pageUrlRelative": "/posts/6kPd6WCjWz5XADuHd/seq-rerun-we-agree-get-froze", "linkUrl": "https://www.lesswrong.com/posts/6kPd6WCjWz5XADuHd/seq-rerun-we-agree-get-froze", "postedAtFormatted": "Monday, December 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20We%20Agree%3A%20Get%20Froze&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20We%20Agree%3A%20Get%20Froze%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kPd6WCjWz5XADuHd%2Fseq-rerun-we-agree-get-froze%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20We%20Agree%3A%20Get%20Froze%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kPd6WCjWz5XADuHd%2Fseq-rerun-we-agree-get-froze", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kPd6WCjWz5XADuHd%2Fseq-rerun-we-agree-get-froze", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">We Agree: Get Froze</a> was originally published on December 12, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Despite disagreement about AI FOOMs, Robin Hanson and Eliezer Yudkowsky both agree that cryonics is worth investing in.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g5g/seq_rerun_not_taking_over_the_world/\">Not Taking Over the World</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6kPd6WCjWz5XADuHd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0721338912824062e-06, "legacy": true, "legacyId": "20948", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L4oL2KRcJ2cv66DxQ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-31T11:50:48.104Z", "modifiedAt": null, "url": null, "title": "[Link] Statistically, People Are Not Very Good At Making Voting Decisions", "slug": "link-statistically-people-are-not-very-good-at-making-voting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zG3BGmFD59hAf3jr8/link-statistically-people-are-not-very-good-at-making-voting", "pageUrlRelative": "/posts/zG3BGmFD59hAf3jr8/link-statistically-people-are-not-very-good-at-making-voting", "linkUrl": "https://www.lesswrong.com/posts/zG3BGmFD59hAf3jr8/link-statistically-people-are-not-very-good-at-making-voting", "postedAtFormatted": "Monday, December 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Statistically%2C%20People%20Are%20Not%20Very%20Good%20At%20Making%20Voting%20Decisions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Statistically%2C%20People%20Are%20Not%20Very%20Good%20At%20Making%20Voting%20Decisions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzG3BGmFD59hAf3jr8%2Flink-statistically-people-are-not-very-good-at-making-voting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Statistically%2C%20People%20Are%20Not%20Very%20Good%20At%20Making%20Voting%20Decisions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzG3BGmFD59hAf3jr8%2Flink-statistically-people-are-not-very-good-at-making-voting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzG3BGmFD59hAf3jr8%2Flink-statistically-people-are-not-very-good-at-making-voting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 418, "htmlBody": "<p><a href=\"http://www.businessinsider.com/voting-bias-study-2012-12#ixzz2Gd4D5LWj\">Link.</a> Nothing surprising considering previous work on the subject, but a good reminder.</p>\n<blockquote>\n<p>A study by three scientists in the <a href=\"http://journals.cambridge.org/download.php?file=%2FPSR%2FPSR106_04%2FS0003055412000391a.pdf&amp;code=6fc402e7f2db9e4f00828a47df480146\">American Political Science Review</a> finds that voters are not competent at accurately evaluating incumbent performance and are easily swayed by rhetoric, unrelated circumstances and recent events.</p>\n<p>Gregory Huber, Seth Hill, and Gabriel Lenz constructed a 32-round game where players received payments from a computer \"allocator.\" The goal is to maximize the value of those payments.</p>\n<p>Halfway through, at round sixteen, the player had to decide whether to get a new allocator or to stick with the old one.</p>\n<p>The allocators pay out over a normal distribution based on a randomly selected mean. Getting a new allocator means that a new mean is selected. This was meant to simulate an election based on performance.&nbsp;</p>\n<p>The group ran three experiments where they changed some of the rules of the game in order to find out how voters could be manipulated or confused over performance. Essentially, how good were voters at accurately analyzing the performance of the \"allocator?\"&nbsp;</p>\n<ul>\n<li>The first experiment merely alerted the player at round twelve that they would have the chance to pick a new allocator at round sixteen. This \"election in November\" reminder made the player weight recent performance in rounds 12-16 over earlier performance in rounds 1-12.</li>\n</ul>\n<ul>\n<li>The second experiment involved a lottery held at round eight or round sixteen. The payout was either -5000, 0, or 5000 tokens. The participant was told that the lottery was totally unrelated to the current allocator, but players still rewarded or punished their current allocator based on their lottery performance.</li>\n</ul>\n<ul>\n<li>The third experiment primed the player with a question right before the election. The question took an adapted form of either Ronald Reagan's \"Are you better off than you were four years ago?\" or John F. Kennedy's \"The question you have to decide on November 8 is, is it good enough? Are you satisfied?\"&nbsp;</li>\n</ul>\n<p>The conclusion:&nbsp;</p>\n<p style=\"padding-left: 30px;\">Participants overweight recent performance when&nbsp;made aware of the choice to retain an incumbent closer&nbsp;to election rather than distant from it (experiment 1),&nbsp;allowed unrelated events that affected their welfare&nbsp;to influence evaluations of incumbents (experiment 2),&nbsp;and were influenced by rhetoric to focus less on cumulative incumbent performance (experiment 3).</p>\n<p>If you were ever wondering why Congress <a href=\"http://cstl-cla.semo.edu/Renka/ps103/Spring2010/congressional_incumbency.htm\">has a 95% incumbency</a> rate despite <a href=\"http://www.realclearpolitics.com/epolls/other/congressional_job_approval-903.html\">an approval rating in the high teens</a>, this study may be worth a read.&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zG3BGmFD59hAf3jr8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 1.0723646654158372e-06, "legacy": true, "legacyId": "20953", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-31T12:11:04.764Z", "modifiedAt": null, "url": null, "title": "[Link] The Collapse of Complex Societies ", "slug": "link-the-collapse-of-complex-societies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:33.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cfiJLA7aGhMWqMe7i/link-the-collapse-of-complex-societies", "pageUrlRelative": "/posts/cfiJLA7aGhMWqMe7i/link-the-collapse-of-complex-societies", "linkUrl": "https://www.lesswrong.com/posts/cfiJLA7aGhMWqMe7i/link-the-collapse-of-complex-societies", "postedAtFormatted": "Monday, December 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Collapse%20of%20Complex%20Societies%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Collapse%20of%20Complex%20Societies%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfiJLA7aGhMWqMe7i%2Flink-the-collapse-of-complex-societies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Collapse%20of%20Complex%20Societies%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfiJLA7aGhMWqMe7i%2Flink-the-collapse-of-complex-societies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfiJLA7aGhMWqMe7i%2Flink-the-collapse-of-complex-societies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3469, "htmlBody": "<p>TGGP a frequent commenter at Overcoming Bias (and hence old LessWrong), <a href=\"https://entitledtoanopinion.wordpress.com/2012/12/29/the-collapse-of-complex-societies/\">writes</a> about his thoughts on a book by <a href=\"http://en.wikipedia.org/wiki/Joseph_Tainter\">Joseph Tainter</a>.</p>\n<blockquote>\n<p>I&rsquo;ve seen Joseph Tainter&rsquo;s &ldquo;The Collapse of Complex Societies&rdquo;  recommended in a few different places. Jared Diamond&rsquo;s book might be one  of them, the guest-posts of Captain David Ryan aka &ldquo;Tony Comstock&rdquo; for  James Fallows at the Atlantic might be another. The sidebar of <a href=\"http://globalguerrillas.typepad.com/\">John Robb&rsquo;s &ldquo;Global Guerrillas&rdquo; blog</a> is the only one I remember with certainty. It&rsquo;s a not a very long book, and you can get the gist of it from <a href=\"http://en.wikipedia.org/wiki/Joseph_Tainter\">Tainter&rsquo;s wikipedia page</a>.</p>\n<p>Lots of people have found civilizational collapses to be interesting,  and Tainter reviews many of their theories while finding them wanting.  The &ldquo;eleven major themes in the explanation of collapse&rdquo; he lists are  depletion/cessation of a vital resource, establishment of a new resource  base (which I found too stupid to take seriously even momentarily),  insurmountable catastrophe, insufficient response to circumstances  (which is almost tautological), other complex societies, intruders,  class conflict or elite mismanagement, social dysfunction, mystical  factors, chance concatenation of events (almost tautological if you  don&rsquo;t think collapse is predetermined) and economic factors. Like  Tainter, I find the &ldquo;mystical&rdquo; theories to not really constitute  theories at all, although some of the most popular writers on the  subject (Spengler, Toynbee, various ancients) are included there.  Tainter often contrasts &ldquo;integrative&rdquo; (or &ldquo;functional&rdquo;) theories on the  origin of the state/complexity vs &ldquo;conflict&rdquo; theories, and acknowledges  that he is more partial toward the former. Unfortunately, most of the  latter theorists he lists are Marxists and carry a lot of baggage. The  observation that throughout much of history some set of people ruled  over others as a result of military victory regardless of any benefit to  the subjects (though a Leviathan may happen to have upsides) predates  Marx, with Ibn Khaldun being one of the few non-Marxist examples Tainter  mentions. That&rsquo;s not to say Tainter is anti-Marxism, he actually  compares Marxist &ldquo;social science&rdquo; to Einsteinian physics and Darwinian  biology! I suppose there is (or was) just such a heavy representation of  Marxists among academic anthropologists and historians that Tainter  regards Marxism as somewhat normative, whereas to me it&rsquo;s something  weird and laughable like Holocaust revisionism.</p>\n<p>Resource depletion is the reverse of the theory I found so absurd,  and (showing there is hope for humanity) it is a much more popular  theory. J. Donald Hughes blamed Rome&rsquo;s collapse in part on  deforestation, but W. Groenman van Waateringe some years later provided  evidence at the time that cereal pollens declined while forest pollens  increased. That of course is not a causal proof, since it is documented  that when the empire was declining many agricultural regions became  depopulated. Waateringe blames agricultural intensification for  increasing the population and thus the demands on agriculture, but to me  that just raises the question of why marginal agricultural lands  weren&rsquo;t reclaimed. There actually is an explanation for that  depopulation, but like Tainter I&rsquo;m not going to get to that in a hurry.  Tainter finds this theory (like most other theories he rejects)  unsatisfactory because complex societies should have leaders who notice  the depletion and think of a response. I am reminded of David St.  Hubbin&rsquo;s girlfriend in Spinal Tap who says &ldquo;It&rsquo;s just a problem! It  get&rsquo;s solved!&rdquo; Sometimes a solution is not within a society&rsquo;s feasible  choice-set. Tainter briefly acknowledges that possibility (noting that  it would have to be proved, which is difficult given how little  information we have about many ancient societies) but spends more time  castigating imaginary opponents depicted as claiming societal elites  just stood around slack-jawed rather than attempting to deal with the  situation. I would call that a strawman, except that <a href=\"http://entitledtoanopinion.wordpress.com/2008/08/13/if-you-cant-say-anything-bad-about-jared-diamond/\">Jared Diamond&rsquo;s &ldquo;Collapse&rdquo;</a> bears too much resemblance. He also mentions Richard Wilkinson&rsquo;s  documenting that deforestation spurred development in late/post medieval  England, which is really just extra evidence that Hughes was wrong (as  had already been mentioned) rather than a broader point against a class  of theories. I should acknowledge that Tainter also cites evidence on  the differential abandonment of cities and the failure to correlate with  expected environmental characteristics, which is just the sort of thing  that would later <a href=\"http://entitledtoanopinion.wordpress.com/2012/02/20/thoughts-on-charles-manns-1491/#more-2727\">puncture Jared Diamond&rsquo;s take on the Maya</a>.  His point that greed is constant enough (or its variance poorly enough  understood) to make it a poor explanation of a variable situation is  fair enough, but he can&rsquo;t dismiss theories of collapse based on  mismanagement because by their nature they should keep their society  going, even if only out of self-interest. There are basic agency  problems that mean one shouldn&rsquo;t identify the interests of elite (or  non-elite, for that matter) actors with that of a larger organization.  In an uncertain world it also makes sense to discount the future (you or  your dynasty might be replaced, might as well get what you can while  you can).</p>\n<p>Steve Sailer once critiqued Diamond&rsquo;s thesis by noting that societies tend to die from <a href=\"http://isteve.blogspot.com/2004/12/jared-diamonds-collapse.html\">homicide rather than suicide</a>,  and lumping together two of Tainter&rsquo;s rejected explanations would make  for a very popular theory. Tainter, however, would exclude most cases  clearly caused by another complex society because those involve  absorbtion rather than collapse (indigenous populations thoroughly  devastated by disease before Europeans even arrived would be  exceptions). So his question is then why a complex society would succumb  to less complex intruders. Sometimes it may not be so easy to  disentangle the two scenarios, such as when the persian &amp; eastern  roman empires exhausted themselves fighting each other, leaving  themselves open to the Muslim invaders exploding out of Arabia (although  it was only the persians that succumbed in fairly short order, and  Tainter wouldn&rsquo;t consider that a collapse). Tainter says it is  &ldquo;unsatisfactory [...] that a recurrent process &ndash; collapse &ndash; is explained  by a random variable, by historical accident&rdquo;. If random numbers for  that variable (I&rsquo;m imagining a stochastic process with a threshold for  collapse rather than a binary control variable) are constantly being  generated over time, it shouldn&rsquo;t be that unsatisfactory that they recur  throughout history. Tainter does make the legitimate point that elites,  with a number of Roman emperors being good examples, have often proved  capable of dealing with barbarian intrusions. But there&rsquo;s no guarantee  that they will always be successful. He also wonders why invaders would  &ldquo;destroy those things which repay conquest&rdquo;. The obvious answer is that,  by the second law of thermodynamics, it&rsquo;s very easy to break things,  and that includes during the process of conquering &amp; looting. Some  relatively sophisticated barbarians may conquer a territory and leave  much of the administrative apparatus intact to rule as before, others  may have no particular interest (or competence) in being bound to a  territory and collecting scraps of taxes from farmers.</p>\n<p>The collapse of Rome is probably the most famous example (at least to  westerners) and forms one of his three case studies, paradigmatic of  the most complex sort of society to collapse. I find it more  enlightening than the others because, and call me a drunk looking under a  lamppost if you will, it&rsquo;s the most well documented. Among the things  documented is that the proximate cause of collapse was invasion by  various (mostly Germanic) barbarians. That is discussed extensively in  Peter Heather&rsquo;s &ldquo;Empires and Barbarians&rdquo; which I have <a href=\"http://entitledtoanopinion.wordpress.com/2012/08/02/empires-and-barbarians/\">discussed earlier</a>.  It&rsquo;s because I read that one so recently that a few of Tainter&rsquo;s  remarks stuck out. Focusing on the internal soundness of a society and  its affordable scale/complexity he writes &ldquo;The Germanic kingdoms that  succeeded Roman rule in the West were more successful at resisting  invasions&rdquo;. If he limited that claim to the particular kingdoms which  survived past the dark ages, it would be rather tautological. But if he  means germanic kingdoms generally, then it just doesn&rsquo;t seem to be the  case. They got invaded and replaced all the time, we just don&rsquo;t remember  the ones that died out. Part of the reason the Romans had such problems  with barbarians is that one group would get invaded by another, and  then start moving around and displacing other barbarians. The western  Roman empire, in contrast, was able to survive many invasions before the  last Roman emperor was toppled. Tainter portrays the formation of the  empire as a process in which a territory is able to summon the resources  to mobilize a force to conquer more territory to extract its resources,  then rinse and repeat in a self-sustaining cycle until it expanded too  far to get many marginal returns. There is some truth to that, but it  overlooks the non-extractive aspect of Roman rule which increased  productivity in conquered territories, thereby making those territories  more attractive as a target for raiding. In Heather&rsquo;s story, barbarian  confederations on the border engaged in a process of competitive  selection for the strength to hold an attractive position (for reasons  of trade, raiding and diplomatic subsidy) and eventually the size and  cohesion necessary to survive and settle within Roman territory.  Focusing on the internals of the collapsed societies, he overlooks any  dynamics occurring within outside societies that could give them the  capability of defeating the imperial power. Heather&rsquo;s account is similar  to <a href=\"http://entitledtoanopinion.wordpress.com/2010/04/05/russians-are-collectivist/\">Peter Turchin&rsquo;s</a> in &ldquo;<a href=\"http://www.gnxp.com/blog/2008/08/cliodynamics-rise-fall-of-empires-and.php\">War and Peace and War</a>&rdquo;  except that, like Khaldun, Turchin focuses more on the softening  effects of metropolitan decadence that renders old dynasties vulnerable  to the hardened asabiya-endowed border marchers.</p>\n<p>Tainter&rsquo;s two other case studies are the Mayan lowland citystates and  Chaco canyon cliff-dwellers. The Mayans are less complex (or at least  less well-documented, since the conquistadors destroyed many of the  remaining documents) and the Chacoans even less so. He also used the Ik  as an example of an extremely simple society that collapsed even further  below the level of familial organization, but he didn&rsquo;t discuss it all  that much and I&rsquo;m not sure how reliable primary source <a href=\"http://isteve.blogspot.com/2012/10/who-was-right-about-human-nature-ayn.html\">Colin Turnbull</a> was (<a href=\"http://louisproyect.wordpress.com/2012/06/22/ikland/?_r=true\">supposedly</a> they hadn&rsquo;t been hunter-gatherers for centuries when their supposed  &ldquo;livelihood&rdquo; of hunting was banned). The interesting thing about the  Maya is that there were multiple relatively equivalent city-states  rather than one dominant hegemon. Tainter includes them as a case study  of collapse, even as he states elsewhere that collapse is not an option  for &ldquo;peer polity competition&rdquo; because the weakening of one peer just  invites conquest by another. Also, rather than devoting most of their  resources  under duress to a standing army (something documented in the  Roman case) Tainter discusses the building of monuments as conspicuous  consumption to demonstrate how powerful and brutal (per the depictions  of torture) the city was, rather similar to the story Diamond tells. I  don&rsquo;t know what kind of evidence we have for the scale of their military  expenditures, although we know they did war from time to time. There  was no writing whatsoever in Chaco canyon, so we are left with the old  archeological standby of potsherds and whatnot. Tainter does make the  interesting point that the culture benefitted from uniting different  ecological niches, with higher elevation territories having more  agricultural productivity in cold wet years while lower elevation ones  were more productive in warm dry ones. An economist would say that this  diversified portfolio allowed for more consumption smoothing. However, I  was confused by Tainter&rsquo;s argument that as more outlier territories  were incorporated diversity and gains from exchange went down. As long  as the ratio between high and low places was stable, incorporating more  territories should not cause any problems in that respect. Admittedly,  this does mean that there are more viable subsets of communities that  would be individually stable if they withdrew, which is indeed what he  claims happened eventually. But he also seemed to be suggesting that the  system overally was degrading its performance, without clearly stating  whether an excess of a particular type of environment was upsetting the  balance.</p>\n<p>Tainter&rsquo;s theory to explain collapse is declining marginal returns.  This is a common concept in economics, but it is normally used to  understood how equilibrium can develop. Applied to a society, we would  expect the declining returns to territorial expansion or administrative  complexity (the former often requiring some degree of the latter) to  result in eventual stasis rather than collapse. David Friedman has an  interesting <a href=\"http://www.daviddfriedman.com/Academic/Size_of_Nations/Size_of_Nations.html\">paper</a> on how the advantages of taxing trade, land or labor gave rise to  different equilibria for the sizes &amp; boundaries of polities during  the Roman, medieval and nationalist eras in Europe. In Friedman&rsquo;s  theory, each shift between eras resulted from some exogenous change  rather being part of the internal logic of societies. Tainter relates  some various interesting bits from C. Northcote Parkinson&rsquo;s &ldquo;Parkinson&rsquo;s  Law, and Other Studies in Administration&rdquo;. For example,  while &ldquo;between  1914 and 1967, the number of capital ships in the British Navy declined  by 78.9 percent, the number of officers and enlisted men by 32.9  percent, and the number of dockyard workers by 33.7 percent [...] the  number of dockyard officials and clerks <strong>increased</strong> by 247 percent,  and the number of Admiralty officals by 767 percent&rdquo; (emphasis added).  Mencius Moldbug would not be surprised to learn that &ldquo;between 1935 and  1954 the number of officals in the British Colonial office increased by  447 percent&rdquo; even though &ldquo;the empire administered by these officials  shrank considerably&rdquo;. These examples are important because they do not  demonstrate an increasingly large requirement of administrators for a  marginal increase in size/complexity of an entity to be administered,  but paying more for less. Parkinson&rsquo;s explanation was bureaucratic  self-serving, which Tainter rejects because he finds trends of  increasing hierarchical specialization in the private sector. But  because Tainter fails to distinguish between declining marginal returns  (eventually reaching zero at a steady-state) and NEGATIVE returns he  doesn&rsquo;t specify whether the latter occurs in the private sector (though <a href=\"http://modeledbehavior.com/2011/01/13/burning-the-corporate-commons/\">Karl Smith</a> would not be surprised if it does for many publicly owned corporations  whose shareholders would be better served by liquidation of assets). The  growth of administration in higher education would also count, but as a  heavily subsidized non-profit sector I can&rsquo;t say it would qualify. At  one point Tainter acknowledges &ldquo;In many cases this increased, more  costly complexity will yield <em>no</em> increased benefits, at other  times the benefits will not be proportionate to costs&rdquo; (emphasis in the  original). This is precisely the question at issue of elite  mismanagement or the out-of-control inertia of expanding administrative  bureaucracies, but as noted he rejected Parkinson&rsquo;s theory and mocks the  idea of societies as runaway trains as self-evidently absurd. Instead  he portrays collapse as a choice which is preferable once marginal  returns have declined to a certain point. This didn&rsquo;t entirely make  sense to me, because if a society has accidentally shot part the point  of zero marginal returns to one of negative returns, the sensible thing  is just to reduce that marginal increase in complexity to return to the  steady state with zero marginal returns.</p>\n<p>The Roman empire sometimes seemed to behave in such a manner, losing  some territory and sticking with a more defensible and adminstrable  domain (although in Heather&rsquo;s account some of the lost territory was  among the most agriculturally productive), although Tainter thinks the  conquests of Britain &amp; Dacia never paid for themselves. So why the  path dependency so that changes are not simply reversible? There could  be consumption of a not easily renewable resource, a sort of borrowing  from the future that leaves future generations deeper in the red.  This  could happen with soil deterioration, though Tainter doesn&rsquo;t discuss  that much (odd, despite his focus on societies as means of managing  sources of energy). His example of Roman emperors increasingly resorting  to the debasing of the currency could count (by Diocletian&rsquo;s time it  collected taxes in kind rather than the currency it had rendered nearly  worthless), as well as the selling of imperial land. The larger problem  in Rome seemed to be an increasingly large portion of subjects who were  citizens (both urban proletariat and squabbling elites) subject to fewer  or no taxes, while marginal lands were abandoned by overtaxed farmers.  An odd feature of the empire was that election officials had to cover  the costs of their own office, and as expenses rose there were fewer  wealthy people willing to come forward as candidates, until the position  was made hereditary. It became obligatory to farm certain deserted  lands, with peasants drafted by local city Senates, and Constantine made  soldiery a hereditary profession (which required a number of new laws  over time to deal with sons who&rsquo;d rather not follow that career).  Taxation of land was simplistic and did not vary based on its quality or  yield, so a farmer of marginal land would often be better off working  for the owner of a more productive territory and paying rent than  failing to cover the taxes on his own plot. With agricultural labor  becoming legally tied to the land, we can see the clear beginnings of  serfdom and the manorial system. As mentioned, Tainter views the Roman  collapse as a choice (as he does others), although of course accounts  from the time were more apt to regard it as unfortunate failure or  divine punishment.</p>\n<p>Interestingly, the &ldquo;peer polity competition&rdquo; that replaced Roman  civilization is a situation he regards as invulnerable to collapse as  opposed to absorbtion, and by removing that &ldquo;option&rdquo; he thinks this made  peasants demand democratic representation. He acknowledges that this  did not happen in the &ldquo;Warring States&rdquo; period of China, and instead the  Confucian ideology of governance developed. He suggests &ldquo;Perhaps  participatory governance was simply not possible in ancient societies  that were so much larger, demographically and territorially, than the  Greek city-states&rdquo;. Someone should have told <a href=\"http://en.wikipedia.org/wiki/Federalist_No._10\">James Madison</a> (and I&rsquo;m not being sarcastic). Interestingly enough, there was a  civilization of Greek city states which did collapse, just as we&rsquo;ve  mentioned the lowland Maya doing. These are the Mycenaean Greeks who  preceded the Dark Ages of Homer&rsquo;s time. Their collapse is usually  attributed to invasion by Dorian Greeks, but Tainter isn&rsquo;t convinced  there&rsquo;s enough evidence for the Dorians&rsquo; presence. Because &ldquo;<em>Collapse occurs, and can only occur, in a power vacuum</em>&rdquo; (emphasis in the original) both the Mycenaean Greek and lowland Maya polities must have experienced simultaneous collapse.</p>\n<p>The choice of peasants may be limited to passively withdrawing  support and just not working very hard (I&rsquo;d have more to say on that if  I&rsquo;d read James Scott&rsquo;s &ldquo;Weapons of the Weak&rdquo;), but even then I don&rsquo;t  think it&rsquo;s a desirable outcome for peasants. I&rsquo;ve mentioned Heather on  the greater productivity of Roman territory, and what do you think  happened to the masses when that productivity crashed? My understanding  of the current archaelogical consensus is that the population crashed as  well. Tainter talks about the malnourished skeletons of the peasantry  as evidence for the undesirability of certain degrees of complexity, but  we also know that English peasants ate better after so many of their  peers died of the bubonic plague (it&rsquo;s also known that peasants have  poorer diets than hunter-gatherers, though in Darwinian terms you  definitely want to be a farmer). As in James Scott&rsquo;s account of highland  southeast asia (which I <a href=\"http://entitledtoanopinion.wordpress.com/2011/02/24/claims-from-james-scott-i-hesitate-to-adopt/\">don&rsquo;t</a> entirely buy) was there much <a href=\"http://entitledtoanopinion.wordpress.com/2010/09/18/inegalitarianism-as-a-cultural-recruitment-mechanism/\">cultural defection</a> of the peasantry to the greener pastures outside civilization? Tainter  writes that &ldquo;In 378 [...] Balkan miners went over en masse to the  Visigoths&rdquo;, and that others wished to be conquered/liberated. It is  precisely due to the risk of being conquered that he argues is the  reason many societies don&rsquo;t simply revert to a lower level of complexity  &ldquo;even if marginal returns are unfavorable&rdquo;, but it&rsquo;s unclear whether  conquest is one of outcomes being factored into those marginal returns.</p>\n<p>Few people are going to read this book without speculating on their  own complex society&rsquo;s liability to collapse. John Robb and James  Kunstler (along with some others in the &ldquo;Peak Oil&rdquo; camp) are going to  place a high probability on it, while the Singularitarians have the  opposite view. Globalization could mean the entire world is now in a  state of &ldquo;peer polity competition&rdquo; but modern norms (and <a href=\"http://isteve.blogspot.com/2006/08/woody-hays-chair-of-national-security.html\">economic incentives</a>) against conquest and <a href=\"http://www.scribd.com/doc/7258092/Luttwak-Give-War-a-Chance\">giving war a chance</a> means &ldquo;failed states&rdquo; can keep failing for a long time without someone  replacing the bad management. Tainter&rsquo;s studied societies are also  Malthusian agricultural ones, it&rsquo;s hard to know if the same logic will  generalize past the industrial revolution. In modern technological  economies the costs and benefits of advances may not be simple  increasing or declining curves. Robin Hanson doesn&rsquo;t even consider  nearly free energy (which would very important to Tainter) to be nearly  as important as the replacement of most human labor by computers (since  the latter takes up so much more of GDP). When Tainter was writing there  was still just the slightest possibility of nuclear armageddon, now the  most likely candidates for death by complexity are grey goo or an  unstoppable manmade pandemic. My two cents are that collapse is unlikely  in my lifetime, and that&rsquo;s for the better considering how much worse  things could be.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fWEFt4e9asdpSseqf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cfiJLA7aGhMWqMe7i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 13, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "20954", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-31T13:34:08.944Z", "modifiedAt": null, "url": null, "title": "[Link] Economists' views differ by gender", "slug": "link-economists-views-differ-by-gender", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.005Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4bijdhwyMsAzDTXsp/link-economists-views-differ-by-gender", "pageUrlRelative": "/posts/4bijdhwyMsAzDTXsp/link-economists-views-differ-by-gender", "linkUrl": "https://www.lesswrong.com/posts/4bijdhwyMsAzDTXsp/link-economists-views-differ-by-gender", "postedAtFormatted": "Monday, December 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Economists'%20views%20differ%20by%20gender&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Economists'%20views%20differ%20by%20gender%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bijdhwyMsAzDTXsp%2Flink-economists-views-differ-by-gender%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Economists'%20views%20differ%20by%20gender%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bijdhwyMsAzDTXsp%2Flink-economists-views-differ-by-gender", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bijdhwyMsAzDTXsp%2Flink-economists-views-differ-by-gender", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 893, "htmlBody": "<p class=\"lead-in\"><strong>Edit:</strong> <a href=\"/lw/g63/link_economists_views_differ_by_gender/86cj?context=3\">ParagonProtege</a> has provided a <a href=\"http://static.nzz.ch/files/1/5/8/May+etal+Disagreements+among+Male+and+Female+Economicsts_2012+copy_1.17665158.pdf\">link to the original study</a>. Thank you! (^_^)</p>\n<p class=\"lead-in\"><a href=\"http://www.usatoday.com/story/money/business/2012/09/29/male-female-economists-differ/1583053/\">Link. </a></p>\n<blockquote>\n<p class=\"lead-in\">A new study shows a large gender gap on economic policy among the nation's professional economists, a divide similar -- and in some cases bigger -- than the gender divide found in the general public.</p>\n<p>What does an economist think of that?</p>\n<p>A lot depends on whether the economist is a man or a woman. A new study shows a large gender gap on economic policy among the nation's professional economists, a divide similar -- and in some cases bigger -- than the gender divide found in the general public.</p>\n<p><strong>Differences extend to core professional beliefs -- such as the effect of minimum wage laws -- not just matters of political opinion.</strong></p>\n<p><strong>Female economists tend to favor a bigger role for government while male economists have greater faith in business and the marketplace. Is the U.S. economy excessively regulated? Sixty-five percent of female economists said \"no\" -- 24 percentage points higher than male economists.</strong></p>\n</blockquote>\n<p>Can this be reasonably explained by self-interest? Female and male economists' views are probably coloured by gender solidarity. Government jobs&nbsp; may be more likeable to women than men because of their recorded greater risk aversion. Regardless of the reason government jobs are <a href=\"http://www.businessweek.com/articles/2012-03-29/women-suffer-most-when-government-cuts-jobs\">more important for women than for men</a>. Also in the US where the study was done middle class white women benefit quit a bit from affirmative action in government hiring.</p>\n<blockquote>\n<p><strong>\"As a group, we are pro-market,\"</strong> says Ann Mari May, co-author of the study and a University of Nebraska economist. <strong>\"But women are more likely to accept government regulation and involvement in economic activity than our male colleagues.\" </strong></p>\n<p>Opinion differences between men and women are well-documented in the general public. President Obama leads Mitt Romney by 10 percentage points among women. Romney leads Obama by 3 percentage points among men, according to the latest Gallup Poll.</p>\n</blockquote>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the mind-killer</a> probably does play a role in explaining the difference.</p>\n<blockquote>\n<p>The survey of 400 economists is one of the first to examine whether gender differences matter within a profession. The answer for economists: Yes.</p>\n<p>How economists think:</p>\n<ul>\n<li> <strong>Health insurance.</strong> Female economists thought employers should be required to provide health insurance for full-time workers: 40% in favor to 37% against, with the rest offering no opinion. By contrast, men were strongly against the idea: 21% in favor and 52% against.</li>\n<li><strong> Education.</strong> Females narrowly opposed taxpayer-funded vouchers that parents could use for tuition at a public or private school of their choice. Male economists love the idea: 61% to 14%.</li>\n<li> <strong>Labor standards. </strong>Females believe 48% to 33% that trade policy should be linked to labor standards in foreign counties. Males disagreed: 60% to 23%.</li>\n</ul>\n</blockquote>\n<p>First two points are somewhat congruent with stereotypes. Anyone who has run into the frequent iSteve commenter \"<a href=\"http://whiskeysplace.wordpress.com/\">Whiskey</a>\" will probably note that the third point indicates women may not hate hate HATE lower and middle class beta males in this case.</p>\n<blockquote>\n<p>\"It's very puzzling,\" says free-market economist Veronique de Rugy of the Mercatus Center at George Mason University in Fairfax, Va. \"Not a day goes by that I don't ask myself why there are so few women economists on the free-market side.\"</p>\n<p>A native of France, de Rugy supported government intervention early in her life but changed her mind after studying economics. <strong>\"We want many of the same things as liberals -- less poverty, more health care -- but have radically different ideas on how to achieve it.\"</strong></p>\n</blockquote>\n<p>This seems plausible since politics is about <a href=\"http://wiki.lesswrong.com/wiki/Applause_light\">applause lights</a> after all, the tribes are what matters not the particular shape of their attire. But might value differences still be behind the gender difference? <strong></strong>Maybe some failed utopias I recall reading <a href=\"/lw/xu/failed_utopia_42/\">aren't really failed</a>. <strong><br /></strong></p>\n<blockquote>\n<p>Liberal economist Dean Baker, co-founder of the Center for Economic Policy and Research, says male economists have been on the inside of the profession, confirming each other's anti-regulation views. Women, as outsiders, \"are more likely to think independently or at least see people outside of the economics profession as forming their peer group,\" he says.</p>\n<p>The gender balance in economics is changing. One-third of economics doctorates now go to women. The chair of the White House Council of Economic Advisers has been a woman three of 27 times since 1946 -- one advising Obama and two advising Bill Clinton. The Federal Reserve Board of Governors has three women, bringing the total to eight of 90 members since 1914.</p>\n<p>\"More diversity is needed at the table when public policy is discussed,\" May says.</p>\n</blockquote>\n<p>Somehow I think this does not include <a href=\"/lw/fp5/2012_survey_results/7y5w\">ideological diversity</a>.</p>\n<blockquote>\n<p>Economists do agree on some things. Female economists agree with men that Europe has too much regulation and that Walmart is good for society. Male economists agree with their female colleagues that military spending is too high.</p>\n<p><strong>The genders are most divorced from each other on the question of equality for women. Male economists overwhelmingly think the wage gap between men and women is largely the result of individuals' skills, experience and voluntary choices. Female economists overwhelmingly disagree by a margin of 4-to-1. </strong></p>\n<p><strong>The biggest disagreement: 76% of women say faculty opportunities in economics favor men. Male economists point the opposite way: 80% say women are favored or the process is neutral.</strong></p>\n</blockquote>\n<p>No mystery here. (^_^)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4bijdhwyMsAzDTXsp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "20955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"lead-in\"><strong>Edit:</strong> <a href=\"/lw/g63/link_economists_views_differ_by_gender/86cj?context=3\">ParagonProtege</a> has provided a <a href=\"http://static.nzz.ch/files/1/5/8/May+etal+Disagreements+among+Male+and+Female+Economicsts_2012+copy_1.17665158.pdf\">link to the original study</a>. Thank you! (^_^)</p>\n<p class=\"lead-in\"><a href=\"http://www.usatoday.com/story/money/business/2012/09/29/male-female-economists-differ/1583053/\">Link. </a></p>\n<blockquote>\n<p class=\"lead-in\">A new study shows a large gender gap on economic policy among the nation's professional economists, a divide similar -- and in some cases bigger -- than the gender divide found in the general public.</p>\n<p>What does an economist think of that?</p>\n<p>A lot depends on whether the economist is a man or a woman. A new study shows a large gender gap on economic policy among the nation's professional economists, a divide similar -- and in some cases bigger -- than the gender divide found in the general public.</p>\n<p><strong id=\"Differences_extend_to_core_professional_beliefs____such_as_the_effect_of_minimum_wage_laws____not_just_matters_of_political_opinion_\">Differences extend to core professional beliefs -- such as the effect of minimum wage laws -- not just matters of political opinion.</strong></p>\n<p><strong id=\"Female_economists_tend_to_favor_a_bigger_role_for_government_while_male_economists_have_greater_faith_in_business_and_the_marketplace__Is_the_U_S__economy_excessively_regulated__Sixty_five_percent_of_female_economists_said__no_____24_percentage_points_higher_than_male_economists_\">Female economists tend to favor a bigger role for government while male economists have greater faith in business and the marketplace. Is the U.S. economy excessively regulated? Sixty-five percent of female economists said \"no\" -- 24 percentage points higher than male economists.</strong></p>\n</blockquote>\n<p>Can this be reasonably explained by self-interest? Female and male economists' views are probably coloured by gender solidarity. Government jobs&nbsp; may be more likeable to women than men because of their recorded greater risk aversion. Regardless of the reason government jobs are <a href=\"http://www.businessweek.com/articles/2012-03-29/women-suffer-most-when-government-cuts-jobs\">more important for women than for men</a>. Also in the US where the study was done middle class white women benefit quit a bit from affirmative action in government hiring.</p>\n<blockquote>\n<p><strong>\"As a group, we are pro-market,\"</strong> says Ann Mari May, co-author of the study and a University of Nebraska economist. <strong>\"But women are more likely to accept government regulation and involvement in economic activity than our male colleagues.\" </strong></p>\n<p>Opinion differences between men and women are well-documented in the general public. President Obama leads Mitt Romney by 10 percentage points among women. Romney leads Obama by 3 percentage points among men, according to the latest Gallup Poll.</p>\n</blockquote>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the mind-killer</a> probably does play a role in explaining the difference.</p>\n<blockquote>\n<p>The survey of 400 economists is one of the first to examine whether gender differences matter within a profession. The answer for economists: Yes.</p>\n<p>How economists think:</p>\n<ul>\n<li> <strong>Health insurance.</strong> Female economists thought employers should be required to provide health insurance for full-time workers: 40% in favor to 37% against, with the rest offering no opinion. By contrast, men were strongly against the idea: 21% in favor and 52% against.</li>\n<li><strong> Education.</strong> Females narrowly opposed taxpayer-funded vouchers that parents could use for tuition at a public or private school of their choice. Male economists love the idea: 61% to 14%.</li>\n<li> <strong>Labor standards. </strong>Females believe 48% to 33% that trade policy should be linked to labor standards in foreign counties. Males disagreed: 60% to 23%.</li>\n</ul>\n</blockquote>\n<p>First two points are somewhat congruent with stereotypes. Anyone who has run into the frequent iSteve commenter \"<a href=\"http://whiskeysplace.wordpress.com/\">Whiskey</a>\" will probably note that the third point indicates women may not hate hate HATE lower and middle class beta males in this case.</p>\n<blockquote>\n<p>\"It's very puzzling,\" says free-market economist Veronique de Rugy of the Mercatus Center at George Mason University in Fairfax, Va. \"Not a day goes by that I don't ask myself why there are so few women economists on the free-market side.\"</p>\n<p>A native of France, de Rugy supported government intervention early in her life but changed her mind after studying economics. <strong>\"We want many of the same things as liberals -- less poverty, more health care -- but have radically different ideas on how to achieve it.\"</strong></p>\n</blockquote>\n<p>This seems plausible since politics is about <a href=\"http://wiki.lesswrong.com/wiki/Applause_light\">applause lights</a> after all, the tribes are what matters not the particular shape of their attire. But might value differences still be behind the gender difference? <strong></strong>Maybe some failed utopias I recall reading <a href=\"/lw/xu/failed_utopia_42/\">aren't really failed</a>. <strong><br></strong></p>\n<blockquote>\n<p>Liberal economist Dean Baker, co-founder of the Center for Economic Policy and Research, says male economists have been on the inside of the profession, confirming each other's anti-regulation views. Women, as outsiders, \"are more likely to think independently or at least see people outside of the economics profession as forming their peer group,\" he says.</p>\n<p>The gender balance in economics is changing. One-third of economics doctorates now go to women. The chair of the White House Council of Economic Advisers has been a woman three of 27 times since 1946 -- one advising Obama and two advising Bill Clinton. The Federal Reserve Board of Governors has three women, bringing the total to eight of 90 members since 1914.</p>\n<p>\"More diversity is needed at the table when public policy is discussed,\" May says.</p>\n</blockquote>\n<p>Somehow I think this does not include <a href=\"/lw/fp5/2012_survey_results/7y5w\">ideological diversity</a>.</p>\n<blockquote>\n<p>Economists do agree on some things. Female economists agree with men that Europe has too much regulation and that Walmart is good for society. Male economists agree with their female colleagues that military spending is too high.</p>\n<p><strong id=\"The_genders_are_most_divorced_from_each_other_on_the_question_of_equality_for_women__Male_economists_overwhelmingly_think_the_wage_gap_between_men_and_women_is_largely_the_result_of_individuals__skills__experience_and_voluntary_choices__Female_economists_overwhelmingly_disagree_by_a_margin_of_4_to_1__\">The genders are most divorced from each other on the question of equality for women. Male economists overwhelmingly think the wage gap between men and women is largely the result of individuals' skills, experience and voluntary choices. Female economists overwhelmingly disagree by a margin of 4-to-1. </strong></p>\n<p><strong id=\"The_biggest_disagreement__76__of_women_say_faculty_opportunities_in_economics_favor_men__Male_economists_point_the_opposite_way__80__say_women_are_favored_or_the_process_is_neutral_\">The biggest disagreement: 76% of women say faculty opportunities in economics favor men. Male economists point the opposite way: 80% say women are favored or the process is neutral.</strong></p>\n</blockquote>\n<p>No mystery here. (^_^)</p>", "sections": [{"title": "Differences extend to core professional beliefs -- such as the effect of minimum wage laws -- not just matters of political opinion.", "anchor": "Differences_extend_to_core_professional_beliefs____such_as_the_effect_of_minimum_wage_laws____not_just_matters_of_political_opinion_", "level": 1}, {"title": "Female economists tend to favor a bigger role for government while male economists have greater faith in business and the marketplace. Is the U.S. economy excessively regulated? Sixty-five percent of female economists said \"no\" -- 24 percentage points higher than male economists.", "anchor": "Female_economists_tend_to_favor_a_bigger_role_for_government_while_male_economists_have_greater_faith_in_business_and_the_marketplace__Is_the_U_S__economy_excessively_regulated__Sixty_five_percent_of_female_economists_said__no_____24_percentage_points_higher_than_male_economists_", "level": 1}, {"title": "The genders are most divorced from each other on the question of equality for women. Male economists overwhelmingly think the wage gap between men and women is largely the result of individuals' skills, experience and voluntary choices. Female economists overwhelmingly disagree by a margin of 4-to-1. ", "anchor": "The_genders_are_most_divorced_from_each_other_on_the_question_of_equality_for_women__Male_economists_overwhelmingly_think_the_wage_gap_between_men_and_women_is_largely_the_result_of_individuals__skills__experience_and_voluntary_choices__Female_economists_overwhelmingly_disagree_by_a_margin_of_4_to_1__", "level": 1}, {"title": "The biggest disagreement: 76% of women say faculty opportunities in economics favor men. Male economists point the opposite way: 80% say women are favored or the process is neutral.", "anchor": "The_biggest_disagreement__76__of_women_say_faculty_opportunities_in_economics_favor_men__Male_economists_point_the_opposite_way__80__say_women_are_favored_or_the_process_is_neutral_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ctpkTaqTKbmm6uRgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T06:09:02.403Z", "modifiedAt": null, "url": null, "title": "Open Thread, January 1-15, 2013", "slug": "open-thread-january-1-15-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDqy8osKTCCX7etCP/open-thread-january-1-15-2013", "pageUrlRelative": "/posts/xDqy8osKTCCX7etCP/open-thread-january-1-15-2013", "linkUrl": "https://www.lesswrong.com/posts/xDqy8osKTCCX7etCP/open-thread-january-1-15-2013", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20January%201-15%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20January%201-15%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDqy8osKTCCX7etCP%2Fopen-thread-january-1-15-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20January%201-15%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDqy8osKTCCX7etCP%2Fopen-thread-january-1-15-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDqy8osKTCCX7etCP%2Fopen-thread-january-1-15-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDqy8osKTCCX7etCP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.073017827301924e-06, "legacy": true, "legacyId": "20958", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 336, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T06:36:12.132Z", "modifiedAt": null, "url": null, "title": "Gauging of interest: LW stock picking?", "slug": "gauging-of-interest-lw-stock-picking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.347Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "simplicio", "createdAt": "2010-03-06T04:03:43.272Z", "isAdmin": false, "displayName": "simplicio"}, "userId": "fDQ7ty7YmE3PxgqNh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q8XXJ5bMbc5pLPso4/gauging-of-interest-lw-stock-picking", "pageUrlRelative": "/posts/Q8XXJ5bMbc5pLPso4/gauging-of-interest-lw-stock-picking", "linkUrl": "https://www.lesswrong.com/posts/Q8XXJ5bMbc5pLPso4/gauging-of-interest-lw-stock-picking", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gauging%20of%20interest%3A%20LW%20stock%20picking%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGauging%20of%20interest%3A%20LW%20stock%20picking%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8XXJ5bMbc5pLPso4%2Fgauging-of-interest-lw-stock-picking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gauging%20of%20interest%3A%20LW%20stock%20picking%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8XXJ5bMbc5pLPso4%2Fgauging-of-interest-lw-stock-picking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8XXJ5bMbc5pLPso4%2Fgauging-of-interest-lw-stock-picking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 720, "htmlBody": "<p>EDIT: Based on criticism below, I am reconsidering how to proceed with this idea (or something in the neighbourhood).</p>\n<p>A topic that has been on my mind recently is where, in our complicated lives, there might be low-hanging fruit ready to be picked by a motivated rationalist. Actual, practical, dollars-and-cents fruit.</p>\n<p>In possibly-related news, here is how the writer of About.com's beginner's guide to investing describes the stock market:</p>\n<blockquote>\n<p>Imagine you are partners in a private business with a man named Mr. Market. Each day, he comes to your office or home and offers to buy your interest in the company or sell you his [the choice is yours]. The catch is, Mr. Market is an emotional wreck. At times, he suffers from excessive highs and at others, suicidal lows. When he is on one of his manic highs, his offering price for the business is high as well, because everything in his world at the time is cheery. His outlook for the company is wonderful, so he is only willing to sell you his stake in the company at a premium. At other times, his mood goes south and all he sees is a dismal future for the company. In fact, he is so concerned, he is willing to sell you his part of the company for far less than it is worth. All the while, the underlying value of the company may not have changed - just Mr. Market's mood.</p>\n</blockquote>\n<p>I have heard this narrative many times before, and I'd like to test whether it is accurate - and in particular, whether LWers can consistently beat the market.</p>\n<p>The skeptic may well ask: why should LWers have an advantage? Why not go to the professionals - investment advisors? Also, isn't there a whole chapter in Kahneman about how even smart people suck at picking stocks? And what do you, simplicio, know about this anyway?</p>\n<p>LWers <strong>may</strong> have an advantage by virtue of being educated about such topics as cognitive biases, sunk cost fallacy, probabilistic prediction, and expected utility - topics with which investment advisors et al. may or may not be familiar on a gut level. I am not sure if we're any better, but I'd like to test it. Also, if LW turns out to be any good at offering such advice, that advice would presumably be free, unlike that of yon advisor (fees tend to kill returns on investment - just ask anybody who uses Intrade). As for what I personally know - not very much yet. But I find competition very stimulating.</p>\n<p>Accordingly, my proposal is for a contest: over the course of 2013, I will set up &amp; maintain a Google Drive spreadsheet. This spreadsheet will be shared with contest participants. Each participant will have say $5,000 of play money to use \"buying\" (or \"selling\") stocks on the exchange of their choice. Contestants will record the date of purchase or sale, quantity, and preferably provide comments regarding why they are buying or selling.</p>\n<p>At the end of this contest (Dec 31, 2013?), I will commit to Paypal the winner (defined as the person with the highest market valuation of play assets as of midnight on that date) the equivalent of $50 CAD in their local currency. In the unlikely event that I win, I will donate that $50 to the Against Malaria Foundation.&nbsp;(Above commitment does not take effect until I actually gauge interest in this contest, figure out an end date &amp; rules etc., and decide to proceed. If anyone else wants to throw money in the pot, please do.)</p>\n<p>The purposes of this post are therefore:</p>\n<ul>\n<li>to find out who is interested - please leave a comment below, and e-mail me at ispollock [at] gmail.com if you want in;</li>\n<li>to solicit constructive and destructive criticism of the project, especially from any local experienced investors (in particular, perhaps a one-year timeframe is too short for a meaningful contest? Also, real-world experience of transaction costs in buying and selling would be extremely helpful);</li>\n<li>to ask if anyone knows of a better software platform for the contest than Google Drive, or knows of any extremely helpful resources I should be reading/linking to.</li>\n</ul>\n<div>I am probably just being naive, but I am rather excited about what LW could accomplish here. Even an abject failure would be instructive, if not inspiring.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q8XXJ5bMbc5pLPso4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 10, "extendedScore": null, "score": 1.0730339905219847e-06, "legacy": true, "legacyId": "20956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T06:40:18.310Z", "modifiedAt": null, "url": null, "title": "You can't signal to rubes", "slug": "you-can-t-signal-to-rubes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BK3L2ySPtPrSBLEkg/you-can-t-signal-to-rubes", "pageUrlRelative": "/posts/BK3L2ySPtPrSBLEkg/you-can-t-signal-to-rubes", "linkUrl": "https://www.lesswrong.com/posts/BK3L2ySPtPrSBLEkg/you-can-t-signal-to-rubes", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20can't%20signal%20to%20rubes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20can't%20signal%20to%20rubes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBK3L2ySPtPrSBLEkg%2Fyou-can-t-signal-to-rubes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20can't%20signal%20to%20rubes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBK3L2ySPtPrSBLEkg%2Fyou-can-t-signal-to-rubes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBK3L2ySPtPrSBLEkg%2Fyou-can-t-signal-to-rubes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1675, "htmlBody": "<p><em><span>The word 'signalling' is often used in Less Wrong, and often used wrongly. This post is intended to call out our community on its wrongful use, as well as serve as an introduction to the correct concept of signalling as contrast.</span></em></p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>\"We choose to go to the moon. We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard..\"</p>\n<p>- John F. Kennedy</p>\n</blockquote>\n<p>Why do peacocks grow such large, conspicuous tails? Why do people take degrees in subjects like Philosophy or Classics, despite these subjects having no obvious practical value? Why do people take pains to avoid splitting infinitives, even though everyone can understand split infinitives perfectly well? <br /><br />These activities seem completely pointless, costly and difficult. Paradoxically, it is probably this very difficulty that serves to explain why they are done at all. Take the peacock&rsquo;s tail. A peacock that has to struggle to survive while dragging around a conspicuous tail is clearly at a disadvantage. But if he can continue to survive, then clearly he must be pretty strong! So the peahens may choose to mate with him rather than the peacocks with less conspicuous tails, whose survival is thus a less impressive feat.<br /><br />As for classics, getting a degree in classics may be pointless, but it&rsquo;s also difficult. It requires one to read and memorize vast chunks of text, and to translate these texts between Greek, Latin and English precisely. So a person who has a degree in classics and got a good mark must be a person with a good memory who is able to execute tasks precisely. Qualities extremely useful in a civil servant, the occupation where many budding classicists find themselves. The rule that you mustn't split infinitives derives from Latin where splitting infinitives was impossible. So a person who doesn&rsquo;t split infinitives is more likely to be a Latin scholar, with the qualities of class and intelligence that such a thing implies.<br /><br />Even the decision to go to the moon might be explained in this way. Carl Sagan made the point that a rocket capable of going to the moon is certainly capable of reaching Moscow. And it&rsquo;s clear why Kennedy in the middle of a Cold War would want to demonstrate such a thing. <br /><br />When we explain a behaviour in this way, we say that the behaviour is signalling. The agent does not perform a task for its own sake, but to show others that they possess some important quality such as strength, a good memory, or military supremacy. The key features that a behaviour must possess for signalling to be a good explanation are as follows.</p>\n<ol>\n<li><em>The behaviour seems pointless.</em> This of course, is a matter of perspective. Peacock&rsquo;s tails are beautiful, classics is interesting, and the moon landing was a crowning moment of awesome from humanity. Peacock&rsquo;s tails seem pointless when viewed as struggling to survive and reproduce, classics when viewed as a matter of finance, and the moon landing when viewed as a matter of military strategy. Priorities that genes, most people, and the American government might be expected to have.</li>\n<li><em>The behaviour requires a certain quality.</em> Going to the moon requires a good rocket, surviving with a conspicuous tail requires strength and cunning, getting a good grade in classics requires a good memory. This is actually a stricter condition than necessary. All we need is for the quality to lower the cost of the behaviour. In the absence of superior rocket technology, a moon landing could be faked, but only by taking extraordinary and costly steps. This was argued by Messrs Mitchell and Webb <a href=\"https://www.youtube.com/watch?v=P6MOnehCOUw\" target=\"_blank\">here</a>.</li>\n<li><em>You want others to believe you have this quality.</em> Peacocks want to look strong and sexy, civil service applicants want to look intelligent and diligent, America wants to look innovative and powerful.</li>\n<li><em>Dishonest signalling isn&rsquo;t worth it.</em> By dishonest signalling, I mean enaging in a signalling behaviour when you don&rsquo;t possess the quality you wish others to think you have. A weak peacock who grows a conspicuous tale will be eaten by predators. A stupid person who tries to get a degree in classics will fail his subjects. Faking the moon landing carries the risk that the conspiracy will be exposed and the American government would become a laughing stock. A good way of remembering this criterion is the slogan \"You can't signal to rubes.\"</li>\n</ol>\n<p><br />Unfortunately, not all proposed explanations involving the word \"signalling\" take care to establish these four properties. Our community seems especially guilty of this. The main misunderstanding is that it uses &lsquo;signalling&rsquo; merely to denote behaviours that trick rubes in to thinking you&rsquo;re good. This raises the question of why there are rubes to trick in the first place. Why haven&rsquo;t more savvy competitors eaten their lunch? <a href=\"/lw/8im/link_signalling_and_irrationality_in_software/\">Here</a> is an example of someone thinking that you can signal to rubes:</p>\n<blockquote>\n<p>In other words, it's all about signaling, isn't it? Managers will take actions that actively harm the continued progress of the project if that action makes them look \"decisive\" and \"in charge\". &nbsp;I've seen this on many projects I've been on, and it took me a while to realize that my managers weren't stupid or ignorant. It's just that the organization I was working in put a higher priority on process than on results. My managers, therefore quite rationally did things that maximized their apparent value in the eyes of their bosses, even if it meant that the project (and, as a result) the entire organization was hurt.</p>\n</blockquote>\n<p>Here the rube is the managers bosses, why are they so stupid as to think that mismanagement is evidence of superior management qualities? Why haven&rsquo;t these idiots been sacked? (This probably does occur in real life, but I don&rsquo;t think \"signalling\" is the right term to describe it. I would describe it as \"pandering to the prejudices of idiots\".) <br /><br /><a href=\"/lw/g7/least_signaling_activities/dam\">Another</a> comment which falls in to the same trap:</p>\n<blockquote>\n<p>Compare the skilled butcher, who, with no wasted movements, cuts his meat just where the joints are, and the flashy butcher, whose flourishes make for less skilful and efficient cutting but send a more impressive signal.<br />I agree that the flashy butcher could became engaged in his cutting and lose consciousness of the crowd and his impression on it without decreasing his signalling behaviour. If he did so, he might become more sincere, but his signalling behaviour would remain. For signaling is not a conscious addition to his art, which might strip away: skill at cutting and skill at signalling are woven confusedly together in it.</p>\n</blockquote>\n<p>The trouble here is that it postulates stupid customers, just like the previous comment postulated stupid bosses. A much better test of butcher quality than flashiness is how good the meat tastes and how much is produced. An intelligent customer can probably test this fairly easily, and would not buy meat from the flashy butcher.<br /><br />These uses of \"signalling\" at least have the advantage that they&rsquo;re explanations along economic lines. The difference between signalling and pandering is the intelligence of your audience. What&rsquo;s worse is that some people in our community use the word \"signal\" to mean \"show\" or \"pretend\".<br /><br class=\"kix-line-break\" />An <a href=\"/lw/5i4/altruist_support_how_to_determine_your_utility/\">example</a>:</p>\n<blockquote>\n<p>I may have learned that signalling low status - to avoid intimidating outsiders - may be less of a good strategy than signalling that I know what I'm talking about.</p>\n</blockquote>\n<p>A low status person that knows what they&rsquo;re talking about? I suppose such things are possible... Seriously though, \"signalling\" is being used to mean \"tricking people in to thinking that you are\". Either you know what you&rsquo;re talking about or you don&rsquo;t. At least one of the two options given in the quote will result in you trying to trick someone. We&rsquo;re signalling to rubes again.<br /><br />Worst of all, some people use \"signalling\" as a version of ad hominem. \"You just say that to signal.\" A comment to Overcoming Bias&rsquo;s controversial post \"Gentle, Silent Rape\" <a href=\"http://www.overcomingbias.com/2010/11/gentlesilentrape.html#comment-518315251\">reads</a>:</p>\n<blockquote>\n<p>&ldquo;I'm sad but not at all surprised that so many of these comments are to the effect of \"HOW DARE YOU EVEN THINK ABOUT THIS TOPIC?!!?!?\"<br />I can only imagine how much more frustrating it must be for Professor Hanson have to deal with increasingly harsh anti-mind climate that envelopes the Western world. The all-encompassing ideology of political correctness is truly a fearsome juggernaut.<br />I think the status signaling arguments are right on the money, and the rest of the comments serve as the proof.&rdquo;</p>\n</blockquote>\n<p>Let&rsquo;s go through the criteria again:</p>\n<blockquote>\n<p>1. The behaviour seems pointless.</p>\n</blockquote>\n<p>This clearly doesn&rsquo;t apply. The behaviour is easily explicable. Comments might be made out of genuine disagreement, or (more cynically) to intimidate Hanson and others away from making arguments like these in the future.</p>\n<blockquote>\n<p>2.The behaviour requires a certain quality.</p>\n</blockquote>\n<p>The quality proposed was \"status\", but outrage is cheap. Any fool can be outraged at a blog post mentioning rape. It doesn&rsquo;t require exceptional intelligence, charisma, wealth, or feminist credentials. You could be homeless and leave an outraged comment just by going to a public library. You don&rsquo;t even have to read the post.</p>\n<blockquote>\n<p>3. You want others to believe you have this quality.</p>\n</blockquote>\n<p>Well this seemingly applies. People do want to be thought of as being against rape, and high status. The only trouble is that many of the comments are left anonymously.</p>\n<blockquote>\n<p>4. Dishonest signalling isn&rsquo;t worth it.</p>\n</blockquote>\n<p>This does not apply at all. Even a convicted rapist could leave an outraged comment.<br /><br />Clear thinking requires making distinctions. Using the word \"signalling\" to mean \"pandering\", \"tricking people\", \"showing\", or \"toeing the party line\" does nothing but lead to confusion and muddle. If you&rsquo;re going to use jargon, use it in its precise sense. That&rsquo;s what is jargon is for, communicating precisely. Next time you feel like using the word \"signalling\", ask yourself whether the four criteria apply. Remember: You can&rsquo;t signal to rubes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BK3L2ySPtPrSBLEkg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 78, "baseScore": 18, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "20957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8d9u2HvkAwC4sRnzM", "FdFa5KYs6CXnFWxq2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T14:03:54.596Z", "modifiedAt": null, "url": null, "title": "New Year's Prediction Thread (2013) ", "slug": "new-year-s-prediction-thread-2013", "viewCount": null, "lastCommentedAt": "2013-02-22T21:47:59.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4srHT2ShCJ2fjE3o/new-year-s-prediction-thread-2013", "pageUrlRelative": "/posts/x4srHT2ShCJ2fjE3o/new-year-s-prediction-thread-2013", "linkUrl": "https://www.lesswrong.com/posts/x4srHT2ShCJ2fjE3o/new-year-s-prediction-thread-2013", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Prediction%20Thread%20(2013)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Prediction%20Thread%20(2013)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4srHT2ShCJ2fjE3o%2Fnew-year-s-prediction-thread-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Prediction%20Thread%20(2013)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4srHT2ShCJ2fjE3o%2Fnew-year-s-prediction-thread-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4srHT2ShCJ2fjE3o%2Fnew-year-s-prediction-thread-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>Try to predict something important in a way, that it will be easy to judge it in 2014, how (in)accurate your predictions really were. Don't leave too much space for a doubt what was really said.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4srHT2ShCJ2fjE3o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.0733004675082635e-06, "legacy": true, "legacyId": "20961", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-01-01T14:03:54.596Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T18:41:21.421Z", "modifiedAt": null, "url": null, "title": "Some scary life extension dilemmas", "slug": "some-scary-life-extension-dilemmas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dLsDLdZDnoNKMzRpW/some-scary-life-extension-dilemmas", "pageUrlRelative": "/posts/dLsDLdZDnoNKMzRpW/some-scary-life-extension-dilemmas", "linkUrl": "https://www.lesswrong.com/posts/dLsDLdZDnoNKMzRpW/some-scary-life-extension-dilemmas", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20scary%20life%20extension%20dilemmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20scary%20life%20extension%20dilemmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLsDLdZDnoNKMzRpW%2Fsome-scary-life-extension-dilemmas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20scary%20life%20extension%20dilemmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLsDLdZDnoNKMzRpW%2Fsome-scary-life-extension-dilemmas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLsDLdZDnoNKMzRpW%2Fsome-scary-life-extension-dilemmas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1301, "htmlBody": "<p>Let's imagine a life extension drug has been discovered.&nbsp; One dose of this drug extends one's life by 49.99 years.&nbsp; This drug also has a mild cumulative effect, if it has been given to someone who has been dosed with it before it will extend their life by 50 years.</p>\n<p>Under these constraints the most efficient way to maximize the amount of life extension this drug can produce is to give every dose to one individual.&nbsp; If there was one dose available for all seven-billion people alive on Earth then giving every person one dose would result in a total of 349,930,000,000 years of life gained.&nbsp; If one person was given all the doses a total of 349,999,999,999.99 years of life would be gained.&nbsp; Sharing the life extension drug equally would result in a net loss of almost 70 million years of life.&nbsp; If you're concerned about people's reaction to this policy then we could make it a big lottery, where every person on Earth gets a chance to gamble their dose for a chance at all of them.</p>\n<p>Now, one could make certain moral arguments in favor of sharing the drug.&nbsp; I'll get to those later.&nbsp; However, it seems to me that gambling your dose for a chance at all of them isn't rational from a purely self-interested point of view either.&nbsp; <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">You will not win the lottery</a>.&nbsp; Your chances of winning this particular lottery are almost 7,000 times worse than <a href=\"http://www.popsci.com/science/article/2012-11/dismal-odds-winning-lottery-infographic\">your chances of winning the powerball jackpot</a>.&nbsp; If someone gave me a dose of the drug, and then offered me a chance to gamble in this lottery, I'd accuse them of <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Pascal's mugging</a>.</p>\n<p>Here's an even scarier thought experiment.&nbsp; Imagine we invent the technology for <a href=\"http://en.wikipedia.org/wiki/Brain_emulation\">whole brain emulation</a>.&nbsp; Let \"x\" equal the amount of resources it takes to sustain a WBE through 100 years of life.&nbsp; Let's imagine that with this particular type of technology, it costs 10x to convert a human into a WBE and it costs 100x to sustain a biological human through the course of their natural life.&nbsp; Let's have the cost of making multiple copies of a WBE once they have been converted be close to 0.</p>\n<p>Again, under these constraints it seems like the most effective way to maximize the amount of life extension done is to convert one person into a WBE, then kill everyone else and use the resources that were sustaining them to make more WBEs, or extend the life of more WBEs.&nbsp; Again, if we are concerned about people's reaction to this policy we could make it a lottery.&nbsp; And again, if I was given a chance to play in this lottery I would turn it down and consider it a form of <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Pascal's mugging.</a></p>\n<p>I'm sure that most readers, like myself, would find these policies very objectionable.&nbsp; However, I have trouble finding objections to them from the perspective of classical utilitarianism.&nbsp; Indeed, most people have probably noticed that these scenarios are very similar to <a href=\"http://en.wikipedia.org/wiki/Utility_monster\">Nozick's \"utility monster\" thought experiment</a>.&nbsp; I have made a list of possible objections to these scenarios that I have been considering:</p>\n<p>1. First, let's deal with the unsatisfying practical objections.&nbsp; In the case of the drug example, it seems likely that a more efficient form of life extension will likely be developed in the future.&nbsp; In that case it would be better to give everyone the drug to sustain them until that time.&nbsp; However, this objection, like most practical ones, seems unsatisfying.&nbsp; It seems like there are strong moral objections to not sharing the drug.</p>\n<p>Another pragmatic objection is that, in the case of the drug scenario, the lucky winner of the lottery might miss their friends and relatives who have died.&nbsp; And in the WBE scenario it seems like the lottery winner might get lonely being the only person on Earth.&nbsp; But again, this is unsatisfying.&nbsp; If the lottery winner were allowed to share their winnings with their immediate social circle, or if they were a sociopathic loner who cared nothing for others, it still seems bad that they end up killing everyone else on Earth. &nbsp;&nbsp;</p>\n<p>2. One could use the classic utilitarian argument in favor of equality: <a href=\"http://en.wikipedia.org/wiki/Diminishing_marginal_returns\">diminishing marginal utility</a>.&nbsp; However, I don't think this works.&nbsp; Humans don't seem to experience diminishing returns from lifespan in the same way they do from wealth.&nbsp; It's absurd to argue that a person who lives to the ripe old age of 60 generates less utility than two people who die at age 30 (all other things being equal).&nbsp; The reason the DMI argument works when arguing for equality of wealth is that people are limited in their ability to get utility from their wealth, because there is only so much time in the day to spend enjoying it.&nbsp; Extended lifespan removes this restriction, making a longer-lived person essentially a <a href=\"http://en.wikipedia.org/wiki/Utility_monster\">utility monster</a>.</p>\n<p>3. My intuitions about the lottery could be mistaken.&nbsp; It seems to me that if I was offered the possibility of gambling my dose of life extension drug with just one other person, I still wouldn't do it.&nbsp; If I understand probabilities correctly, then gambling for a chance at living either 0 or 99.99 additional years is equivalent to having a certainty of an additional 49.995&nbsp; years of life, which is better than the certainty of 49.99 years of life I'd have if I didn't make the gamble.&nbsp; But I still wouldn't do it, partly because I'd be afraid I'd lose and partly because I wouldn't want to kill the person I was gambling with.&nbsp;</p>\n<p>So maybe my horror at these scenarios is driven by that same hesitancy.&nbsp; Maybe I just don't understand the probabilities right.&nbsp; But even if that is the case, even if it is rational for me to gamble my dose with just one other person, it doesn't seem like the gambling would scale.&nbsp; <a href=\"http://wiki.lesswrong.com/wiki/Lottery\">I will not win the \"lifetime lottery</a>.\"</p>\n<p>4. Finally, we have those moral objections I mentioned earlier.&nbsp; Utilitarianism is a pretty awesome moral theory under most circumstances.&nbsp; However, when it is applied to<a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\"> scenarios involving population growth</a> and scenarios where <a href=\"http://en.wikipedia.org/wiki/Utility_monster\">one individual is vastly better at converting resources into utility</a> than their fellows, it tends to produce very scary results.&nbsp; If we accept the <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complexity of value thesis</a> (and I think we should), this suggests that there are other moral values that are not salient in the \"special case\" of scenarios with no population growth or utility monsters, but become relevant in scenarios where there are.</p>\n<p>For instance, it may be that <a href=\"http://en.wikipedia.org/wiki/Prioritarianism\">prioritarianism</a> is better than pure utilitarianism, and in this case sharing the life extension method might be best because of the benefits it accords the least off.&nbsp; Or it may be (in the case of the WBE example) that having a large number of unique, worthwhile lives in the world is valuable because it produces experiences like love, friendship, and diversity.&nbsp;</p>\n<p>My tentative guess at the moment is that there probably are some other moral values that make the scenarios I described morally suboptimal, even though they seem to make sense from a utilitarian perspective.&nbsp; However, I'm interested in what other people think.&nbsp; Maybe I'm missing something really obvious.</p>\n<p>&nbsp;</p>\n<p>EDIT:&nbsp; To make it clear, when I refer to \"amount of years added\" I am assuming for simplicity's sake that all the years added are years that the person whose life is being extended wants to live and contain a large amount of positive experiences. I'm not saying that lifespan is exactly equivalent to utility. The problem I am trying to resolve is that it seems like the scenarios I've described seem to maximize the number of positive events it is possible for the people in the scenario to experience, even though they involve killing the majority of people involved.&nbsp; I'm not sure \"positive experiences\" is exactly equivalent to \"utility\" either, but it's likely a much closer match than lifespan.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dLsDLdZDnoNKMzRpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 1, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "20962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vYsuM8cpuRgZS5rYB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T20:52:18.972Z", "modifiedAt": null, "url": null, "title": "[Link] TEDx talk of Anders Sandberg on the Fermi \"paradox\"", "slug": "link-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tFsCpNdWk8zamgXSw/link-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "pageUrlRelative": "/posts/tFsCpNdWk8zamgXSw/link-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "linkUrl": "https://www.lesswrong.com/posts/tFsCpNdWk8zamgXSw/link-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20TEDx%20talk%20of%20Anders%20Sandberg%20on%20the%20Fermi%20%22paradox%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20TEDx%20talk%20of%20Anders%20Sandberg%20on%20the%20Fermi%20%22paradox%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtFsCpNdWk8zamgXSw%2Flink-tedx-talk-of-anders-sandberg-on-the-fermi-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20TEDx%20talk%20of%20Anders%20Sandberg%20on%20the%20Fermi%20%22paradox%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtFsCpNdWk8zamgXSw%2Flink-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtFsCpNdWk8zamgXSw%2Flink-tedx-talk-of-anders-sandberg-on-the-fermi-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>Anders Sandberg, <a href=\"http://youtu.be/7vsxNqxGpyI\">Where are they?</a>, TEDxUHasselt.</p>\n<blockquote>\n<p>On the long term, how much change in the universe can a civilization possibly cause? In this talk, Anders Sandberg brings an enthusiastic introduction to the different scenarios of the Fermi paradox and what they mean for the future of humanity.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tFsCpNdWk8zamgXSw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.0735436542732612e-06, "legacy": true, "legacyId": "20964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T20:58:04.103Z", "modifiedAt": null, "url": null, "title": "What is the best paper explaining the superiority of Bayesianism over frequentism?", "slug": "what-is-the-best-paper-explaining-the-superiority-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.211Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Meni_Rosenfeld", "createdAt": "2010-04-19T15:09:59.043Z", "isAdmin": false, "displayName": "Meni_Rosenfeld"}, "userId": "84ebCjWmavqjgjAfM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rGoxB4tu2vnXpK4hN/what-is-the-best-paper-explaining-the-superiority-of", "pageUrlRelative": "/posts/rGoxB4tu2vnXpK4hN/what-is-the-best-paper-explaining-the-superiority-of", "linkUrl": "https://www.lesswrong.com/posts/rGoxB4tu2vnXpK4hN/what-is-the-best-paper-explaining-the-superiority-of", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20best%20paper%20explaining%20the%20superiority%20of%20Bayesianism%20over%20frequentism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20best%20paper%20explaining%20the%20superiority%20of%20Bayesianism%20over%20frequentism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGoxB4tu2vnXpK4hN%2Fwhat-is-the-best-paper-explaining-the-superiority-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20best%20paper%20explaining%20the%20superiority%20of%20Bayesianism%20over%20frequentism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGoxB4tu2vnXpK4hN%2Fwhat-is-the-best-paper-explaining-the-superiority-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGoxB4tu2vnXpK4hN%2Fwhat-is-the-best-paper-explaining-the-superiority-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>Question in title.</p>\n<p>This is obviously subjective, but I figure there ought to be some \"go-to\" paper. Maybe I've even seen it once, but can't find it now and I don't know if there's anything better.</p>\n<p>Links to multiple papers with different focus would be welcome. For my current purpose I have a preference for one that aims low and isn't too long.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rGoxB4tu2vnXpK4hN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -6, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "20965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-01T21:58:53.054Z", "modifiedAt": null, "url": null, "title": "How not to sort by a complicated frequentist formula", "slug": "how-not-to-sort-by-a-complicated-frequentist-formula", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Meni_Rosenfeld", "createdAt": "2010-04-19T15:09:59.043Z", "isAdmin": false, "displayName": "Meni_Rosenfeld"}, "userId": "84ebCjWmavqjgjAfM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SnSYB8CFpjESfRuCm/how-not-to-sort-by-a-complicated-frequentist-formula", "pageUrlRelative": "/posts/SnSYB8CFpjESfRuCm/how-not-to-sort-by-a-complicated-frequentist-formula", "linkUrl": "https://www.lesswrong.com/posts/SnSYB8CFpjESfRuCm/how-not-to-sort-by-a-complicated-frequentist-formula", "postedAtFormatted": "Tuesday, January 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20not%20to%20sort%20by%20a%20complicated%20frequentist%20formula&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20not%20to%20sort%20by%20a%20complicated%20frequentist%20formula%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnSYB8CFpjESfRuCm%2Fhow-not-to-sort-by-a-complicated-frequentist-formula%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20not%20to%20sort%20by%20a%20complicated%20frequentist%20formula%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnSYB8CFpjESfRuCm%2Fhow-not-to-sort-by-a-complicated-frequentist-formula", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnSYB8CFpjESfRuCm%2Fhow-not-to-sort-by-a-complicated-frequentist-formula", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 440, "htmlBody": "<p>In <a href=\"http://www.evanmiller.org/how-not-to-sort-by-average-rating.html\">How Not To Sort By Average Rating</a>, Evan Miller gives two wrong ways to generate an aggregate rating from a collection of positive and negative votes, and one method he thinks is correct. But the \"correct\" method is complicated, poorly motivated, insufficiently parameterized, and founded on frequentist statistics. A much simpler model based on a prior beta distribution has more solid theoretical foundation and would give more accurate results.</p>\n<p>Evan mentions the sad reality that big organizations are using obviously naive methods. In contrast, more dynamic sites such as Reddit have adopted the model he suggested. But I fear that it would cause irreparable damage if the world settles on this solution.</p>\n<p>Should anything be done about it? What can be done?</p>\n<p>This is also somewhat meta in that LW also aggregates ratings, and I believe changing the model was once discussed (and maybe the beta model was suggested).</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p>In the Bayesian model, as in Evan's model, we assume for every item there is some true probability p of upvoting, representing its quality and the rating we wish to give. Every vote is a Bernoulli trial which gives information on p. The prior for p is the beta distribution with some parameters a and b. After observing n actual votes, of which k are positive, the parameters of the posterior distribution are a+k and b+(n-k), so the posterior mean of p is (a+k)/(a+b+n). This gives the best estimate for the true quality, and reproduces all the desired effects - convergence to the proportion of positive ratings, where items with insufficient data are pulled towards the prior mean.<br /><br />The specific parameters a and b depend on the quality distribution in the specific system. a/(a+b) is the average quality and can be taken as simply the empirical proportion of positive votes among all votes in the system. a+b is an inverse measure of variance - a high value means most items are average quality, and a low value means items are either extremely good or extremely bad. It is harder to calibrate, but can still be done using the overall data (e.g., MLE from the entire voting data).</p>\n<p>&nbsp;</p>\n<p>For the specific problem of sorting, there are other considerations than mere quality. A comment can be in universal agreement, but not otherwise interesting or notable. These may not deserve as prominent a mention as controversial comments which provoke stronger reactions. For this purpose, the \"sorting rating\" can be multiplied by some function of the total number of votes, such as the square root. If the identity function is used, this becomes similar to a simple difference between the number of positive and negative votes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SnSYB8CFpjESfRuCm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 1.0735833016312944e-06, "legacy": true, "legacyId": "20966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T03:31:07.667Z", "modifiedAt": null, "url": null, "title": "Politics Discussion Thread January 2013", "slug": "politics-discussion-thread-january-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sNzRdoXxF4jr4HfMr/politics-discussion-thread-january-2013", "pageUrlRelative": "/posts/sNzRdoXxF4jr4HfMr/politics-discussion-thread-january-2013", "linkUrl": "https://www.lesswrong.com/posts/sNzRdoXxF4jr4HfMr/politics-discussion-thread-january-2013", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20Discussion%20Thread%20January%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20Discussion%20Thread%20January%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNzRdoXxF4jr4HfMr%2Fpolitics-discussion-thread-january-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20Discussion%20Thread%20January%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNzRdoXxF4jr4HfMr%2Fpolitics-discussion-thread-january-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNzRdoXxF4jr4HfMr%2Fpolitics-discussion-thread-january-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<ol>\n<li><em>Top-level comments should introduce arguments; responses should be responses to those arguments.&nbsp;</em></li>\n<li><em>Upvote and downvote based on whether or not you find an argument convincing in the context in which it was raised. &nbsp;This means if it's a good argument against the argument it is responding to, not whether or not there's a good/obvious counterargument to it; if you have a good counterargument, raise it. &nbsp;If it's a convincing argument, and the counterargument is also convincing, upvote both. &nbsp;If both arguments are unconvincing, downvote both.&nbsp;</em></li>\n<li><em>A single argument per comment would be ideal; as MixedNuts points out&nbsp;<a href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/73yp\">here</a>, it's otherwise hard to distinguish between one good and one bad argument, which makes the upvoting/downvoting difficult to evaluate.</em></li>\n<li><em>In general try to avoid color politics; try to discuss political issues, rather than political parties, wherever possible.</em></li>\n</ol>\n<p>As Multiheaded added, \"Personal is Political\" stuff like gender relations, etc also may belong here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sNzRdoXxF4jr4HfMr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 5, "extendedScore": null, "score": 1.0737812218279836e-06, "legacy": true, "legacyId": "20975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 350, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T06:00:00.504Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] You Only Live Twice", "slug": "seq-rerun-you-only-live-twice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HKD8twBvJR7oJofcb/seq-rerun-you-only-live-twice", "pageUrlRelative": "/posts/HKD8twBvJR7oJofcb/seq-rerun-you-only-live-twice", "linkUrl": "https://www.lesswrong.com/posts/HKD8twBvJR7oJofcb/seq-rerun-you-only-live-twice", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20You%20Only%20Live%20Twice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20You%20Only%20Live%20Twice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKD8twBvJR7oJofcb%2Fseq-rerun-you-only-live-twice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20You%20Only%20Live%20Twice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKD8twBvJR7oJofcb%2Fseq-rerun-you-only-live-twice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKD8twBvJR7oJofcb%2Fseq-rerun-you-only-live-twice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Today's post, <a href=\"/lw/wq/you_only_live_twice/\">You Only Live Twice</a> was originally published on 12 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#You_Only_Live_Twice\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Yudkowsky's addition to Hanson's endorsement of cryonics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/g5w/seq_rerun_we_agree_get_froze/\">We Agree: Get Froze</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HKD8twBvJR7oJofcb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0738699322061907e-06, "legacy": true, "legacyId": "20976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yKXKcyoBzWtECzXrE", "6kPd6WCjWz5XADuHd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T11:20:28.056Z", "modifiedAt": null, "url": null, "title": "[Link] On the Height of a Field", "slug": "link-on-the-height-of-a-field", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:58.282Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bsmifgrJ3cBFKHs8v/link-on-the-height-of-a-field", "pageUrlRelative": "/posts/bsmifgrJ3cBFKHs8v/link-on-the-height-of-a-field", "linkUrl": "https://www.lesswrong.com/posts/bsmifgrJ3cBFKHs8v/link-on-the-height-of-a-field", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20On%20the%20Height%20of%20a%20Field&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20On%20the%20Height%20of%20a%20Field%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsmifgrJ3cBFKHs8v%2Flink-on-the-height-of-a-field%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20On%20the%20Height%20of%20a%20Field%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsmifgrJ3cBFKHs8v%2Flink-on-the-height-of-a-field", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsmifgrJ3cBFKHs8v%2Flink-on-the-height-of-a-field", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 353, "htmlBody": "<p>Mark Eichenlaub posted a great little <a href=\"http://arcsecond.wordpress.com/2013/01/01/an-empirical-investigation-into-runners-high/\">case-study about the difficulty of updating beliefs</a>, even over trivial matters like the slope of a baseball field. The basic story of Bayes-updating assumes the likelihood of evidence in different states is obvious, but feedback between observations and judgments about likelihood quickly complicate the situation:</p>\n<blockquote>\n<p>The story of how belief is supposed to work is that for each bit of evidence, you consider its likelihood under all the various hypotheses, then multiplying these likelihoods, you find your final result, and it tells you exactly how confident you should be. If I can estimate how likely it is for Google Maps and my GPS to corroborate each other given that they are wrong, and how likely it is given that they are right, and then answer the same question for every other bit of evidence available to me, I don&rsquo;t need to estimate my final beliefs &ndash; I calculate them. But even in this simple testbed of the matter of a sloped baseball field, I could feel my biases coming to bear on what evidence I considered, and how strong and relevant that evidence seemed to me. &nbsp;The more I believed the baseball field was sloped, the more relevant (higher likelihood ratio) it seemed that there was that short steep hill on the side, and the less relevant that my intuition claimed the field was flat. The field even began looking more sloped to me as time went on, and I sometimes thought I could feel the slope as I ran, even though I never had before.</p>\n<p>That&rsquo;s what I was interested in here. I wanted to know more about the way my feelings and beliefs interacted with the evidence and with my methods of collecting it. It is common knowledge that people are likely to find what they&rsquo;re looking for whatever the facts, but what does it feel like when you&rsquo;re in the middle of doing this, and can recognizing that feeling lead you to stop?</p>\n</blockquote>\n<p><strong>Edit:</strong>&nbsp;Title changed from \"An Empirical Evaluation into Runner's High,\" the original title of the article, to match the author's new title.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bsmifgrJ3cBFKHs8v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.0740609215675074e-06, "legacy": true, "legacyId": "20977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T17:23:36.506Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes January 2013", "slug": "rationality-quotes-january-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:10.051Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DZQpoExSRwpmeFgjF/rationality-quotes-january-2013", "pageUrlRelative": "/posts/DZQpoExSRwpmeFgjF/rationality-quotes-january-2013", "linkUrl": "https://www.lesswrong.com/posts/DZQpoExSRwpmeFgjF/rationality-quotes-january-2013", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20January%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20January%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZQpoExSRwpmeFgjF%2Frationality-quotes-january-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20January%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZQpoExSRwpmeFgjF%2Frationality-quotes-january-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZQpoExSRwpmeFgjF%2Frationality-quotes-january-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<div id=\"entry_t3_ece\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Happy New Year! Here's the latest and greatest installment of rationality quotes. Remember:<br /></span></p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote yourself</span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote comments/posts on LessWrong or Overcoming Bias<br /></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">No more than 5 quotes per person per monthly thread, please</span></li>\n</ul>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DZQpoExSRwpmeFgjF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.0742769064773862e-06, "legacy": true, "legacyId": "20960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 604, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T17:43:27.453Z", "modifiedAt": null, "url": null, "title": "How to Disentangle the Past and the Future", "slug": "how-to-disentangle-the-past-and-the-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rF3GBk9Sgsn75GPWk/how-to-disentangle-the-past-and-the-future", "pageUrlRelative": "/posts/rF3GBk9Sgsn75GPWk/how-to-disentangle-the-past-and-the-future", "linkUrl": "https://www.lesswrong.com/posts/rF3GBk9Sgsn75GPWk/how-to-disentangle-the-past-and-the-future", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Disentangle%20the%20Past%20and%20the%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Disentangle%20the%20Past%20and%20the%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrF3GBk9Sgsn75GPWk%2Fhow-to-disentangle-the-past-and-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Disentangle%20the%20Past%20and%20the%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrF3GBk9Sgsn75GPWk%2Fhow-to-disentangle-the-past-and-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrF3GBk9Sgsn75GPWk%2Fhow-to-disentangle-the-past-and-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1078, "htmlBody": "<p>I'm on my way to an important meeting. Am I worried? I'm not worried. The presentation is on my laptop. I distinctly remember putting it there (in the past), so I can safely predict that it's going to be there when I get to the office (in the future) - this is how well my laptop carries information through space and time.</p>\n<p>My partner has no memory of me copying the file to the laptop. For her, the past and the future have <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">mutual information</a>: if Omega assured her that I'd copied the presentation, she would be able to predict the future much better than she can now.</p>\n<p>For me, the past and the future are much less statistically dependent. Whatever entanglement remains between them is due to my memory not being perfect. If my partner suddenly remembers that she saw me copying the file, I will be a little bit more sure that I remember correctly, and that we'll have it at the meeting. Or if somehow, despite my very clear mental image of copying the file, it's not there at the meeting, my first suspicion will nevertheless be that I hadn't.</p>\n<p>These unlikely possibilities aside, my memory does serve me. My partner is much less certain of the future than me, and more to the point, her uncertainty would decrease much more than mine if we both suddenly became perfectly aware of the past.</p>\n<p>But now she turns on my laptop and checks. The file is there, yes, I could have told her that. And now that we know the state of my laptop, the past and the future are completely disentangled: maybe I put the file there, maybe the elves did - it makes no difference for the meeting. And maybe by the time we get to the office a hacker will remotely delete the file - its absence at the meeting will not be evidence that I'd neglected to copy the file: it was right there! We saw it!</p>\n<p>(By \"entanglement\"&nbsp;I don't mean anything quantum theoretic;&nbsp;here it refers to statistical dependence, and its strength is measured by&nbsp;<a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">mutual information</a>.)</p>\n<p>The past and the future have mutual information. This is a necessary condition for life, for intelligence: we use the past to predict and plan for the future, and we couldn't do it if it were useless.</p>\n<p>On the other hand, the future is independent of the past given the present. That's not profound metaphysics, that's simply&nbsp;<a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">how we define</a>&nbsp;the state of a system: it's everything one needs to know of the past of the system to compute its future.&nbsp;The past is gone, and any information that it had on the future - information we could use to predict and make a better future - is inherited by the present.</p>\n<p>But as limited agents inside the system, we don't get to know its entire state. Most of it is hidden from us, behind walls and hills and inside skulls and in nanostructures. So for us, the past and the future are entangled, which means that by learning one we could reduce our uncertainty about the other.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>In control theory, memoryless agents have an unchanging internal structure, unable to&nbsp;entangle with the past and carry information useful for the future. Instead they react to whatever last input they received, like a function. These degenerate agents have an internal memory state M<sub>t</sub>&nbsp;that depends only on the most recent observation O<sub>t</sub>:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_g26_0.png?v=8f74e303529e0b5364dc839b1620578f\" alt=\"\" width=\"626\" height=\"266\" /></p>\n<p style=\"padding-left: 60px;\">Figure 1: Dynamics of a memoryless&nbsp;<a href=\"/lw/dux/reinforcement_learning_a_nonstandard_introduction/\">reinforcement learning agents</a></p>\n<p style=\"padding-left: 60px;\">W<sub>t</sub>&nbsp;is the state of the world outside the agent at time t, O<sub>t</sub>&nbsp;is the observation the agent makes of the world, M<sub>t</sub>&nbsp;is the resulting internal state of the agent, and A<sub>t</sub>&nbsp;is the action the agent chooses to take.</p>\n<p>When a LED observes voltage, it emits light, regardless of whether it did so a second earlier. When the LED's internal attributes entangle with the voltage, they lose all information of what came before.</p>\n<p>When the q key on a keyboard is pressed and released, the keyboard sends a signal to that effect to the computer, and that signal is mostly independent of which keys were pressed before and in what order (with a few exceptions; a keyboard is not entirely memoryless). A keyboard&nbsp;gets entangled with vast amounts of information over the years, but streams it through and loses almost any trace of it within seconds.</p>\n<p>For a memoryless agent, all of the information between past and future flows through the environment outside the agent - through W<sub>t</sub>.&nbsp;The world sans agent retains all the power to disentangle the past and the future: you can check in the graphical model in Figure 1 that W<sub>t-1</sub>&nbsp;and W<sub>t+1</sub>&nbsp;are independent given W<sub>t</sub>.</p>\n<p>The internal state&nbsp;M<sub>t</sub>&nbsp;of the memoryless agent, on the other hand, of course reveals nothing about the link between past and future. Looking at my keyboard, you can't tell if I copied the presentation to my laptop, and if it's going to be there in half an hour.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Intelligent agents need to have memory:</p>\n<div><br /></div>\n<p><img src=\"http://images.lesswrong.com/t3_g26_2.png?v=e1164d59179d114e0029f447ec25355f\" alt=\"\" width=\"626\" height=\"266\" /></p>\n<p style=\"padding-left: 60px;\">Figure 2: General dynamics of a reinforcement learning agent</p>\n<p>Now control over the flow of information has shifted, to some extent, in favor of the agent.&nbsp;The agent has a much wider channel over which to receive information from the past, and it can use this information to recover some truths about the present which aren't currently observable.</p>\n<p>The past and the future are no longer independent given W<sub>t</sub>&nbsp;alone, you need&nbsp;M<sub>t</sub>&nbsp;and W<sub>t</sub>&nbsp;together to completely separate the past and the future. An agent with memory of how W<sub>t</sub>&nbsp;came to be&nbsp;can partly assume both roles, thus making itself a better separator than its memoryless counterpart could.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>So here's how to become a good separator of past and future: remember things that are relevant for the future.</p>\n<p>Memory is necessary for intelligence, you already knew that. What's new here is a way to <em>measure</em>&nbsp;just how useful memory is.&nbsp;Memory is useful exactly to the extent to which it shifts the power to control the flow of information.</p>\n<p>For the agent, memory disentangles the past and the future. If you maintain in yourself some of the information that the past has about the future, you overcome the limitations of your ability to observe the present. I know I put the presentation on my laptop, so I don't need to check it's there.</p>\n<p>For other agents, at the same time, the agent's memory is yet another limitation to <em>their</em> observability. Think of a secret handshake, for example. It's useful precisely because it predicts (and controls) the future for the confidants, while keeping it entangled with the hidden past for everyone else.</p>\n<p><strong>Continue reading:</strong>&nbsp;<a href=\"/r/lesswrong/lw/g6b/how_to_be_oversurprised/\">How to Be Oversurprised</a>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rF3GBk9Sgsn75GPWk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 1.0742892572304348e-06, "legacy": true, "legacyId": "20814", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yLcuygFfMfrfK8KjF", "HPAjhrbYk6rPbpSXx", "xdjA6YtE7QBsLYQ3i", "2AWwdNgvj6Ca3RsjZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-02T22:37:01.386Z", "modifiedAt": null, "url": null, "title": "Meetup : 13th January London Meetup", "slug": "meetup-13th-january-london-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kg9YR26SLybxJ45Mt/meetup-13th-january-london-meetup", "pageUrlRelative": "/posts/Kg9YR26SLybxJ45Mt/meetup-13th-january-london-meetup", "linkUrl": "https://www.lesswrong.com/posts/Kg9YR26SLybxJ45Mt/meetup-13th-january-london-meetup", "postedAtFormatted": "Wednesday, January 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%2013th%20January%20London%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%2013th%20January%20London%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg9YR26SLybxJ45Mt%2Fmeetup-13th-january-london-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%2013th%20January%20London%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg9YR26SLybxJ45Mt%2Fmeetup-13th-january-london-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg9YR26SLybxJ45Mt%2Fmeetup-13th-january-london-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hj'>13th January London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 January 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hj'>13th January London Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kg9YR26SLybxJ45Mt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "20980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___13th_January_London_Meetup\">Discussion article for the meetup : <a href=\"/meetups/hj\">13th January London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 January 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___13th_January_London_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/hj\">13th January London Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : 13th January London Meetup", "anchor": "Discussion_article_for_the_meetup___13th_January_London_Meetup", "level": 1}, {"title": "Discussion article for the meetup : 13th January London Meetup", "anchor": "Discussion_article_for_the_meetup___13th_January_London_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T01:00:34.531Z", "modifiedAt": null, "url": null, "title": "Applied Rationality Workshops: Jan 25-28 and March 1-4", "slug": "applied-rationality-workshops-jan-25-28-and-march-1-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHrj2nDcTgaJS7JFq/applied-rationality-workshops-jan-25-28-and-march-1-4", "pageUrlRelative": "/posts/BHrj2nDcTgaJS7JFq/applied-rationality-workshops-jan-25-28-and-march-1-4", "linkUrl": "https://www.lesswrong.com/posts/BHrj2nDcTgaJS7JFq/applied-rationality-workshops-jan-25-28-and-march-1-4", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Rationality%20Workshops%3A%20Jan%2025-28%20and%20March%201-4&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Rationality%20Workshops%3A%20Jan%2025-28%20and%20March%201-4%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHrj2nDcTgaJS7JFq%2Fapplied-rationality-workshops-jan-25-28-and-march-1-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Rationality%20Workshops%3A%20Jan%2025-28%20and%20March%201-4%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHrj2nDcTgaJS7JFq%2Fapplied-rationality-workshops-jan-25-28-and-march-1-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHrj2nDcTgaJS7JFq%2Fapplied-rationality-workshops-jan-25-28-and-march-1-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1325, "htmlBody": "<p>The <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> is running two more four-day workshops: Jan 25-28 and March 1-4 in the SF bay area. &nbsp;Like the&nbsp;<a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/\">previous workshop</a>, these sessions are targeted at ambitious, analytic people who have broad intellectual interests, and who care about making real-world projects work. &nbsp;Less Wrong veterans and Less Wrong newcomers alike are welcome: as discussed below, we are intentionally bringing together folks with varied backgrounds and skill bases.</p>\n<h2>Workshop details:<a id=\"more\"></a></h2>\n<h4>The gist:</h4>\n<ul>\n<li>We're trying to improve people's rational agency - turn the evolutionary kludge that is a human being into something more like an entity that <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">has goals and actually acts on them</a>. &nbsp;We'll give you tools for becoming more like Kipling&rsquo;s <em><a href=\"http://www.kipling.org.uk/poems_if.htm\">If</a>&nbsp;</em>(or <a href=\"http://hpmor.com/\">HPMOR's</a>&nbsp;Harry Potter); one exercise, insight, or habit at a time.</li>\n</ul>\n<ul>\n<li>For a more detailed picture, see the <a href=\"http://appliedrationality.org/schedule/\">schedule</a> from our last workshop (which is close to our Friday through Sunday schedule, though we&rsquo;ll be making some optimizations), and our list of <a href=\"/lw/fc3/checklist_of_rationality_habits/\">applied rationality habits</a>.&nbsp;</li>\n</ul>\n<h4>The logistics:</h4>\n<ul>\n<li>25 participants: smart, varied, practical, bringing a variety of competencies to the table</li>\n<li>3 days of intensive course sessions, all hands-on, exercise-intensive, and performed in small groups (Friday through Sunday; it's recommended that folks arrive on site Thursday evening)</li>\n<li>1 optional, but strongly recommended, day of intensive practice (Monday)</li>\n<li>6 weeks of one-on-one Skype and email follow-up, as you solidify your new habits and and apply them to your business or personal life.</li>\n<li>A lifetime of membership in the alumni community, and connections to others with whom you can explore and collaborate for the long haul.</li>\n<li>The workshop costs $3,900 for the full four days, or $3,400 if you stay only Friday through Sunday; some financial aid is available.[1]</li>\n</ul>\n<h2>Why we&rsquo;re running broad workshops, and extending beyond the LW crowd:</h2>\n<p>CFAR's purpose is creating a more reliable art of rationality - an art that includes LW-style epistemic rationality as part of a larger, more practical toolbox; communicated and practiced in a way that can genuinely change people.</p>\n<p>Entrepreneurs are a highly fertile ground for developing this art. &nbsp;Entrepreneurs are picky, articulate, and have cross-domain real-world competencies (&ldquo;business skills&rdquo;, &ldquo;productivity skills&rdquo;) that can fuse with an art of rationality. &nbsp;A diverse group that includes entrepreneurs, hackers, LWers, and ambitious folk of other stripes is probably better still.</p>\n<p>Less Wrong is where most of us got started as rationalists; I personally owe it a tremendous amount. &nbsp;I&rsquo;ll be excited, this summer, when we run further camps targeted just at LW-ers[2] -- camps that can take the Sequences as a starting point, and can grow the base of folks highly skilled in epistemic rationality and interested in this larger art. &nbsp;But a CFAR which creates its rationality curriculum in contact with a wide range of talented / competent people will create a stronger rationality, long-term.</p>\n<h2>Who should apply?</h2>\n<p>Anyone who thinks the habits list and schedule sound awesome, and who wants to internalize these skills through systematic, structured practice in the company of talented friends and practiced teachers. &nbsp;Anyone wants to infuse epistemic rationality and instrumental agency into their habits, concepts, and actions. &nbsp;Anyone who wants to join a community of smart, practical folk refining more effective thinking patterns.</p>\n<p>And please recommend this workshop to any ambitious, analytic friends.</p>\n<h2>Who shouldn&rsquo;t attend?</h2>\n<p>These two workshops won't be suited for everyone. &nbsp;You probably shouldn&rsquo;t attend if:</p>\n<p>1. &nbsp;You care a lot about professional polish. &nbsp;(One person left early from the last camp; he said his main disappointment was that he expected an organized operation with suits.)</p>\n<p>2. &nbsp;You hate being near people. &nbsp;(Participants live on site, often in shared rooms, to facilitate conversations and community; room and board is included. &nbsp;Many report learning more from the informal conversations in the evening than from the sessions -- the sessions are set up to provoke conversation, and the group is chosen to help each other think and change.)</p>\n<p>3. &nbsp;You don&rsquo;t want to try anything that hasn't been tested in scientific studies. &nbsp;(We aim ultimately to have a curriculum that has been carefully tested. &nbsp;We randomized admissions to June minicamp so that we can track the June mini campers vs. a set of matched controls, and we&rsquo;ll do similar studies going forward. &nbsp;But for the moment, the academic literature just doesn&rsquo;t have enough work on debiasing for the peer-reviewed interventions to form a curriculum, and where previous experiments leave off, we're running on science-literate priors, informal experimentation, and plural anecdote - which is to say, informed guesswork.)</p>\n<p>4. &nbsp;You need a finished product. &nbsp;(We're still developing our curriculum and you'll be a live test, though not the first live test. &nbsp;On the upside, alumni already find it highly useful (see below), and you get to leave your stamp in the curriculum as it grows. &nbsp;You&rsquo;ll be participating in something raw and alive.)</p>\n<h2>Who shouldn&rsquo;t not-apply?</h2>\n<p>Following <a href=\"http://www.paulgraham.com/notnot.html\">Paul Graham</a>, I&rsquo;ll conclude with some reasons not to not-apply.</p>\n<p>One reason <em>not</em> to not-apply is that you&rsquo;d need financial aid. &nbsp;If the workshops sound awesome but you can&rsquo;t afford it, please take ten minutes (preferably now) and fill out our application form anyway. &nbsp;We will give out some partial scholarships. &nbsp;And even if you don&rsquo;t get one, what&rsquo;s the cost? &nbsp;You'll have a 15-minute interesting conversation (our interviews), and we may get a chance to revisit your application in the future, when we have more sources of financial aid.</p>\n<p>Similarly, if the workshops sound awesome but you&rsquo;re afraid you won&rsquo;t get in, do apply. &nbsp;(Even if you applied to a previous workshop and didn't get in.) &nbsp;The application is quick and interesting, and what do you have to lose?</p>\n<p>Another reason <em>not</em> to not-apply is that you already have a long list of habits to work on. &nbsp;Sometimes people think that because they already have a stack of productivity books to read, or habits that ought to help but which they've yet to implement, they shouldn&rsquo;t come to the workshop - why add yet more habits to their \"to implement\" list? &nbsp;But this workshop will teach skills and provide social support for habit formation - we expect to substantially speed your progress through your existing &ldquo;to learn&rdquo; list. &nbsp;Plus, discover the wonders of arithmetic for estimating <em>which </em>habit-shifts will help most - you'll be surprised at the surprisingness of some numerical answers.</p>\n<p>Finally, if you&rsquo;d be happy to pay for workshops that are what we say these are, but you don&rsquo;t quite trust us - there&rsquo;s a money-back guarantee; we're willing to&nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">bet</a> that this will boost your earning power and/or well-being by far more than the cost. &nbsp;We've followed through on this before, and accept that as a cost of doing business. &nbsp;If you're not certain of the benefit, do apply - we're willing to take on the risk ourselves.</p>\n<h2>Further information:</h2>\n<p>For more info about the workshops, check out:</p>\n<ul>\n<li>Our <a href=\"/lw/fc3/checklist_of_rationality_habits/\">habits list</a>, if you haven't seen it;</li>\n<li>The <a href=\"http://appliedrationality.org/schedule/\">schedule</a> from last workshop;</li>\n<li>Our <a href=\"http://appliedrationality.org/workshops/\">webpage</a> describing the workshops;</li>\n<li>Comments&nbsp;<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66nz\">from</a> <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sll\">past</a> <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sqa\">participants</a>, <a href=\"/lw/cew/group_rationality_diary_51412/\">describing</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66lv\">specific</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66kn\">areas</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66lr\">they've</a> <a href=\"http://news.ycombinator.com/item?id=4751584\">changed</a>.</li>\n</ul>\n<p>Workshop application form <a href=\"http://appliedrationality.org/apply/\">here</a> (takes at most 10 minutes; productivity heuristics say you should do such small tasks immediately).</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] &nbsp;If you&rsquo;d like help talking to your company about subsidizing your workshop attendance, send me an email: anna at appliedrationality dot org.</p>\n<p>[2] &nbsp;Wondering whether to attend these workshops or wait for a LWer-specific workshop? &nbsp;The answer depends partly on which timing works well for you (the sooner you come, the sooner you can start changing things), and partly on who you want to hang out with. &nbsp;Want to discuss quantum mechanics and the simulation argument? &nbsp;An LW-specific workshop may bring more of a sense of homecoming for those who loved the Sequences. &nbsp;The January and March workshops will duplicate the audience mix of the November entrepreneurial workshop, which had better social and productivity skills and better practical advice.</p>\n<p>[!] &nbsp;Bonus footnote: &nbsp;We may be hiring another teacher / curriculum developer. &nbsp;If you're interested, do apply. &nbsp;Please apply even if I already know you. &nbsp;<a href=\"http://appliedrationality.org/hiring/\">Job application here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHrj2nDcTgaJS7JFq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 34, "extendedScore": null, "score": 1.07454996858395e-06, "legacy": true, "legacyId": "20968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> is running two more four-day workshops: Jan 25-28 and March 1-4 in the SF bay area. &nbsp;Like the&nbsp;<a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/\">previous workshop</a>, these sessions are targeted at ambitious, analytic people who have broad intellectual interests, and who care about making real-world projects work. &nbsp;Less Wrong veterans and Less Wrong newcomers alike are welcome: as discussed below, we are intentionally bringing together folks with varied backgrounds and skill bases.</p>\n<h2 id=\"Workshop_details_\">Workshop details:<a id=\"more\"></a></h2>\n<h4 id=\"The_gist_\">The gist:</h4>\n<ul>\n<li>We're trying to improve people's rational agency - turn the evolutionary kludge that is a human being into something more like an entity that <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">has goals and actually acts on them</a>. &nbsp;We'll give you tools for becoming more like Kipling\u2019s <em><a href=\"http://www.kipling.org.uk/poems_if.htm\">If</a>&nbsp;</em>(or <a href=\"http://hpmor.com/\">HPMOR's</a>&nbsp;Harry Potter); one exercise, insight, or habit at a time.</li>\n</ul>\n<ul>\n<li>For a more detailed picture, see the <a href=\"http://appliedrationality.org/schedule/\">schedule</a> from our last workshop (which is close to our Friday through Sunday schedule, though we\u2019ll be making some optimizations), and our list of <a href=\"/lw/fc3/checklist_of_rationality_habits/\">applied rationality habits</a>.&nbsp;</li>\n</ul>\n<h4 id=\"The_logistics_\">The logistics:</h4>\n<ul>\n<li>25 participants: smart, varied, practical, bringing a variety of competencies to the table</li>\n<li>3 days of intensive course sessions, all hands-on, exercise-intensive, and performed in small groups (Friday through Sunday; it's recommended that folks arrive on site Thursday evening)</li>\n<li>1 optional, but strongly recommended, day of intensive practice (Monday)</li>\n<li>6 weeks of one-on-one Skype and email follow-up, as you solidify your new habits and and apply them to your business or personal life.</li>\n<li>A lifetime of membership in the alumni community, and connections to others with whom you can explore and collaborate for the long haul.</li>\n<li>The workshop costs $3,900 for the full four days, or $3,400 if you stay only Friday through Sunday; some financial aid is available.[1]</li>\n</ul>\n<h2 id=\"Why_we_re_running_broad_workshops__and_extending_beyond_the_LW_crowd_\">Why we\u2019re running broad workshops, and extending beyond the LW crowd:</h2>\n<p>CFAR's purpose is creating a more reliable art of rationality - an art that includes LW-style epistemic rationality as part of a larger, more practical toolbox; communicated and practiced in a way that can genuinely change people.</p>\n<p>Entrepreneurs are a highly fertile ground for developing this art. &nbsp;Entrepreneurs are picky, articulate, and have cross-domain real-world competencies (\u201cbusiness skills\u201d, \u201cproductivity skills\u201d) that can fuse with an art of rationality. &nbsp;A diverse group that includes entrepreneurs, hackers, LWers, and ambitious folk of other stripes is probably better still.</p>\n<p>Less Wrong is where most of us got started as rationalists; I personally owe it a tremendous amount. &nbsp;I\u2019ll be excited, this summer, when we run further camps targeted just at LW-ers[2] -- camps that can take the Sequences as a starting point, and can grow the base of folks highly skilled in epistemic rationality and interested in this larger art. &nbsp;But a CFAR which creates its rationality curriculum in contact with a wide range of talented / competent people will create a stronger rationality, long-term.</p>\n<h2 id=\"Who_should_apply_\">Who should apply?</h2>\n<p>Anyone who thinks the habits list and schedule sound awesome, and who wants to internalize these skills through systematic, structured practice in the company of talented friends and practiced teachers. &nbsp;Anyone wants to infuse epistemic rationality and instrumental agency into their habits, concepts, and actions. &nbsp;Anyone who wants to join a community of smart, practical folk refining more effective thinking patterns.</p>\n<p>And please recommend this workshop to any ambitious, analytic friends.</p>\n<h2 id=\"Who_shouldn_t_attend_\">Who shouldn\u2019t attend?</h2>\n<p>These two workshops won't be suited for everyone. &nbsp;You probably shouldn\u2019t attend if:</p>\n<p>1. &nbsp;You care a lot about professional polish. &nbsp;(One person left early from the last camp; he said his main disappointment was that he expected an organized operation with suits.)</p>\n<p>2. &nbsp;You hate being near people. &nbsp;(Participants live on site, often in shared rooms, to facilitate conversations and community; room and board is included. &nbsp;Many report learning more from the informal conversations in the evening than from the sessions -- the sessions are set up to provoke conversation, and the group is chosen to help each other think and change.)</p>\n<p>3. &nbsp;You don\u2019t want to try anything that hasn't been tested in scientific studies. &nbsp;(We aim ultimately to have a curriculum that has been carefully tested. &nbsp;We randomized admissions to June minicamp so that we can track the June mini campers vs. a set of matched controls, and we\u2019ll do similar studies going forward. &nbsp;But for the moment, the academic literature just doesn\u2019t have enough work on debiasing for the peer-reviewed interventions to form a curriculum, and where previous experiments leave off, we're running on science-literate priors, informal experimentation, and plural anecdote - which is to say, informed guesswork.)</p>\n<p>4. &nbsp;You need a finished product. &nbsp;(We're still developing our curriculum and you'll be a live test, though not the first live test. &nbsp;On the upside, alumni already find it highly useful (see below), and you get to leave your stamp in the curriculum as it grows. &nbsp;You\u2019ll be participating in something raw and alive.)</p>\n<h2 id=\"Who_shouldn_t_not_apply_\">Who shouldn\u2019t not-apply?</h2>\n<p>Following <a href=\"http://www.paulgraham.com/notnot.html\">Paul Graham</a>, I\u2019ll conclude with some reasons not to not-apply.</p>\n<p>One reason <em>not</em> to not-apply is that you\u2019d need financial aid. &nbsp;If the workshops sound awesome but you can\u2019t afford it, please take ten minutes (preferably now) and fill out our application form anyway. &nbsp;We will give out some partial scholarships. &nbsp;And even if you don\u2019t get one, what\u2019s the cost? &nbsp;You'll have a 15-minute interesting conversation (our interviews), and we may get a chance to revisit your application in the future, when we have more sources of financial aid.</p>\n<p>Similarly, if the workshops sound awesome but you\u2019re afraid you won\u2019t get in, do apply. &nbsp;(Even if you applied to a previous workshop and didn't get in.) &nbsp;The application is quick and interesting, and what do you have to lose?</p>\n<p>Another reason <em>not</em> to not-apply is that you already have a long list of habits to work on. &nbsp;Sometimes people think that because they already have a stack of productivity books to read, or habits that ought to help but which they've yet to implement, they shouldn\u2019t come to the workshop - why add yet more habits to their \"to implement\" list? &nbsp;But this workshop will teach skills and provide social support for habit formation - we expect to substantially speed your progress through your existing \u201cto learn\u201d list. &nbsp;Plus, discover the wonders of arithmetic for estimating <em>which </em>habit-shifts will help most - you'll be surprised at the surprisingness of some numerical answers.</p>\n<p>Finally, if you\u2019d be happy to pay for workshops that are what we say these are, but you don\u2019t quite trust us - there\u2019s a money-back guarantee; we're willing to&nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">bet</a> that this will boost your earning power and/or well-being by far more than the cost. &nbsp;We've followed through on this before, and accept that as a cost of doing business. &nbsp;If you're not certain of the benefit, do apply - we're willing to take on the risk ourselves.</p>\n<h2 id=\"Further_information_\">Further information:</h2>\n<p>For more info about the workshops, check out:</p>\n<ul>\n<li>Our <a href=\"/lw/fc3/checklist_of_rationality_habits/\">habits list</a>, if you haven't seen it;</li>\n<li>The <a href=\"http://appliedrationality.org/schedule/\">schedule</a> from last workshop;</li>\n<li>Our <a href=\"http://appliedrationality.org/workshops/\">webpage</a> describing the workshops;</li>\n<li>Comments&nbsp;<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66nz\">from</a> <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sll\">past</a> <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sqa\">participants</a>, <a href=\"/lw/cew/group_rationality_diary_51412/\">describing</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66lv\">specific</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66kn\">areas</a> <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66lr\">they've</a> <a href=\"http://news.ycombinator.com/item?id=4751584\">changed</a>.</li>\n</ul>\n<p>Workshop application form <a href=\"http://appliedrationality.org/apply/\">here</a> (takes at most 10 minutes; productivity heuristics say you should do such small tasks immediately).</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>[1] &nbsp;If you\u2019d like help talking to your company about subsidizing your workshop attendance, send me an email: anna at appliedrationality dot org.</p>\n<p>[2] &nbsp;Wondering whether to attend these workshops or wait for a LWer-specific workshop? &nbsp;The answer depends partly on which timing works well for you (the sooner you come, the sooner you can start changing things), and partly on who you want to hang out with. &nbsp;Want to discuss quantum mechanics and the simulation argument? &nbsp;An LW-specific workshop may bring more of a sense of homecoming for those who loved the Sequences. &nbsp;The January and March workshops will duplicate the audience mix of the November entrepreneurial workshop, which had better social and productivity skills and better practical advice.</p>\n<p>[!] &nbsp;Bonus footnote: &nbsp;We may be hiring another teacher / curriculum developer. &nbsp;If you're interested, do apply. &nbsp;Please apply even if I already know you. &nbsp;<a href=\"http://appliedrationality.org/hiring/\">Job application here</a>.</p>", "sections": [{"title": "Workshop details:", "anchor": "Workshop_details_", "level": 1}, {"title": "The gist:", "anchor": "The_gist_", "level": 2}, {"title": "The logistics:", "anchor": "The_logistics_", "level": 2}, {"title": "Why we\u2019re running broad workshops, and extending beyond the LW crowd:", "anchor": "Why_we_re_running_broad_workshops__and_extending_beyond_the_LW_crowd_", "level": 1}, {"title": "Who should apply?", "anchor": "Who_should_apply_", "level": 1}, {"title": "Who shouldn\u2019t attend?", "anchor": "Who_shouldn_t_attend_", "level": 1}, {"title": "Who shouldn\u2019t not-apply?", "anchor": "Who_shouldn_t_not_apply_", "level": 1}, {"title": "Further information:", "anchor": "Further_information_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7NwSsZ7WbswZYg3ax", "PBRWb2Em5SNeWYwwB", "ttGbpJQ8shBi8hDhh", "a7n8GdKiAZRX86T5A", "JA2MnwunfZnFXb3by"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T06:52:50.893Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] For The People Who Are Still Alive", "slug": "seq-rerun-for-the-people-who-are-still-alive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gAS5KbNdXJxEX2beY/seq-rerun-for-the-people-who-are-still-alive", "pageUrlRelative": "/posts/gAS5KbNdXJxEX2beY/seq-rerun-for-the-people-who-are-still-alive", "linkUrl": "https://www.lesswrong.com/posts/gAS5KbNdXJxEX2beY/seq-rerun-for-the-people-who-are-still-alive", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20For%20The%20People%20Who%20Are%20Still%20Alive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20For%20The%20People%20Who%20Are%20Still%20Alive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAS5KbNdXJxEX2beY%2Fseq-rerun-for-the-people-who-are-still-alive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20For%20The%20People%20Who%20Are%20Still%20Alive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAS5KbNdXJxEX2beY%2Fseq-rerun-for-the-people-who-are-still-alive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAS5KbNdXJxEX2beY%2Fseq-rerun-for-the-people-who-are-still-alive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">For The People Who Are Still Alive</a> was originally published on 14 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#For_The_People_Who_Are_Still_Alive\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Given that we live in a big universe, and that we can't actually determine whether or not a particular person exists (because they will exist anyway in some other Hubble volume or Everett branch), then it makes more sense to care about whether or not people we can influence are having happy lives, than about whether certain people exist in our own local area.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g6o/seq_rerun_you_only_live_twice/\">You Only Live Twice</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gAS5KbNdXJxEX2beY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.074760156806118e-06, "legacy": true, "legacyId": "20988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cfZ8zveqrTZbQrjeD", "HKD8twBvJR7oJofcb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T08:59:26.073Z", "modifiedAt": null, "url": null, "title": "[Link] Hey Extraverts: Enough is Enough", "slug": "link-hey-extraverts-enough-is-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:58.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKxicHZyY4BXrnFH6/link-hey-extraverts-enough-is-enough", "pageUrlRelative": "/posts/aKxicHZyY4BXrnFH6/link-hey-extraverts-enough-is-enough", "linkUrl": "https://www.lesswrong.com/posts/aKxicHZyY4BXrnFH6/link-hey-extraverts-enough-is-enough", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Hey%20Extraverts%3A%20Enough%20is%20Enough&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Hey%20Extraverts%3A%20Enough%20is%20Enough%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKxicHZyY4BXrnFH6%2Flink-hey-extraverts-enough-is-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Hey%20Extraverts%3A%20Enough%20is%20Enough%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKxicHZyY4BXrnFH6%2Flink-hey-extraverts-enough-is-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKxicHZyY4BXrnFH6%2Flink-hey-extraverts-enough-is-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 835, "htmlBody": "<p>A fun <a href=\"http://www.theamericanconservative.com/jacobs/hey-extraverts-enough-is-enough/\">article</a> by Alan Jacobs. Check out the paper he cites, if anyone finds an non-paywalled version, I'll edit in the link here. HT for the link to <a href=\"https://twitter.com/michaelblume/status/286670846297272320\">Michael Bloom</a>.</p>\n<blockquote>\n<p>So in 2005 a very thoroughly researched and well-argued scholarly article was published that demonstrates, quite clearly, that <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/ejsp.295/abstract\">group productivity is an illusion</a>. All those brainstorming sessions and group projects you&rsquo;ve been made to do at school and work? Useless. Everybody would have been better off working on their own. Here&rsquo;s the abstract of the article:</p>\n<p><em>\"It has consistently been found that people produce more ideas when working alone as compared to when working in a group. Yet, people generally believe that group brainstorming is more effective than individual brainstorming. Further, group members are more satisfied with their performance than individuals, whereas they have generated fewer ideas. We argue that this &lsquo;illusion of group productivity&rsquo; is partly due to a reduction of cognitive failures (instances in which someone is unable to generate ideas) in a group setting. Three studies support that explanation, showing that: (1) group interaction leads to a reduction of experienced failures and that failures mediate the effect of setting on satisfaction; and (2) manipulations that affect failures also affect satisfaction ratings. Implications for group work are discussed.\"</em></p>\n<p>Has the puncturing of that &ldquo;illusion of group productivity&rdquo; had any effect? Of course not. <a href=\"http://www.nytimes.com/2012/01/15/opinion/sunday/the-rise-of-the-new-groupthink.html?pagewanted=all&amp;_r=0\">Groupthink is as powerful as ever</a>. Why is that?</p>\n<p>I&rsquo;ll tell you. It&rsquo;s because <strong>the world is run by extraverts</strong>. (And FYI, that&rsquo;s the proper spelling: <em>extrovert</em> is common but wrong, because <em>extra</em>- is the proper Latin prefix.) Extraverts love meetings &mdash; any possible excuse for a meeting, they&rsquo;ll seize on it. They might hear others complain about meetings, but the complaints never sink in: extraverts can&rsquo;t seem to imagine that the people who say they hate meetings really mean it. &ldquo;Maybe they hate <em>other</em> meetings, but I know they&rsquo;ll enjoy <em>mine</em>, because I make them fun! Besides, we&rsquo;ll get <em>so much done</em>!&rdquo; (Let me pause here to acknowledge that the meeting-caller is only one brand of extravert: some of the most pronouncedly outgoing people I know hate meetings as much as I do.)</p>\n<p>The problem with extraverts &mdash; not all of them, I grant you, but many, <em>so</em> many &mdash; is a lack of imagination. They simply assume that everyone will feel about things as they do. &ldquo;The more the merrier, right? It&rsquo;s a proverb, you know.&rdquo; Yes it is: a proverb coined by an extravert. So people I do not know will regularly send me emails: &ldquo;Hey, I&rsquo;ll be in your town soon and I&rsquo;d love to have lunch or coffee. Just let me know which you&rsquo;d prefer!&rdquo; Notice the missing option: not being forced to have a meal and make conversation with a stranger. (<strong>Once a highly extraverted friend of mine was trying to get me involved in some project and said, cheerily, &ldquo;You&rsquo;ll get to meet lots of new people!&rdquo; I turned to him and replied, &ldquo;You realize, don&rsquo;t you, that you&rsquo;ve just ensured my refusal to participate?&rdquo;</strong>)</p>\n</blockquote>\n<p>I really do need to find more written by this author. But while I certainly do very much share this sentiment I have a hard time figuring out how common it is. After all people don't look good saying they \"don't like meeting new people\".</p>\n<blockquote>\n<p>Though my introversion has grown deeper in recent years, it&rsquo;s always been there. When I was a kid I&rsquo;d read about people who got the chance to meet their favorite musician or sports hero or whatever, and I&rsquo;d think: <em>No way.</em> I would have preferred then, and still prefer now, to write a letter to whomever I deeply admire and hope for a response. I even deliberately lost the school-wide spelling bee in fifth grade so I wouldn&rsquo;t have to participate in the city-wide competition: it would have meant meeting so many strange kids!</p>\n<p>Spelling bees are, of course, organized by extraverts &mdash; indeed, pretty much everything that <em>is</em> organized is organized by extraverts, which in turn is their justification for their ruling of the world. &ldquo;See? If we didn&rsquo;t organize things they wouldn&rsquo;t get organized at all!&rdquo; <em>Precisely</em>, mutters the introvert, under his breath, to avoid confrontation.</p>\n<p>So, extraverts of the world, I invite you to make a New Year&rsquo;s resolution: Refrain from organizing stuff. Don&rsquo;t plan parties or outings or, God forbid, &ldquo;team-building exercises.&rdquo; Just don&rsquo;t call meetings. (I would ask you to refrain from calling <em>unnecessary</em> meetings, but so many of you think almost all meetings necessary that it&rsquo;s best you not call them at all.) Leave people alone and let them get their work done. Those who want to socialize can do it after work. I&rsquo;ll not tell you you&rsquo;ll enjoy it: you won&rsquo;t. You&rsquo;ll be miserable, at least at first, because you won&rsquo;t be pulling others&rsquo; puppet-strings. But everyone will be more productive, and many people will be happier. Give it a try. Let go for a year. Just <em>leave us alone.</em></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKxicHZyY4BXrnFH6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": -4, "extendedScore": null, "score": 1.0748357040202985e-06, "legacy": true, "legacyId": "20989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T11:56:06.327Z", "modifiedAt": null, "url": null, "title": "2012: Year in Review", "slug": "2012-year-in-review", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WyN2MhXpzX8oYrCtG/2012-year-in-review", "pageUrlRelative": "/posts/WyN2MhXpzX8oYrCtG/2012-year-in-review", "linkUrl": "https://www.lesswrong.com/posts/WyN2MhXpzX8oYrCtG/2012-year-in-review", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202012%3A%20Year%20in%20Review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2012%3A%20Year%20in%20Review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyN2MhXpzX8oYrCtG%2F2012-year-in-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2012%3A%20Year%20in%20Review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyN2MhXpzX8oYrCtG%2F2012-year-in-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyN2MhXpzX8oYrCtG%2F2012-year-in-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1719, "htmlBody": "<p>The beginning of a new year is a customary time to take a look back and consider what has happened during the last 12 months. And while the time for doing so is admittedly rather arbitrary - after all, \"years\" do not really exist in the universe, just in our heads - it is useful and fun to review one's accomplishments every now and then. And a time when everyone else is doing it gives us a nice <a href=\"/lw/dc7/nash_equilibria_and_schelling_points/\">Schelling point</a> for joining in, so we can pretend that it's not <em>quite</em> that arbitrary.</p>\n<p>So what might be some noteworthy things that happened on Less Wrong in 2012 that could be worth mentioning?</p>\n<p><span style=\"text-decoration: underline;\"><strong>Site upgrades</strong></span></p>\n<p>First, I would like to say \"thank you\" to all the people working on keeping this site running and helping it make increasingly more awesome! This obviously includes pretty much everyone who comments, posts and writes here, but particularly also the folks at <a href=\"http://trikeapps.com/\">Trikeapps</a>, and everyone who contributes updates to the site's codebase. There were several site upgrades in 2012, four of which were major enough to get separate announcements:</p>\n<p>Less Wrong's new <a href=\"/\">front page</a> was <a href=\"/lw/baw/new_front_page/\">rolled out in March</a>, thanks to work by <strong>matt.</strong> One can easily access a number of site features from the brain graphic, and there's a convenient introduction under it, together with links to featured articles and recent promoted articles. Hopefully, this has made it easier for newcomers to get familiar with the site.</p>\n<p>The \"Best\" sorting system for comments was <a href=\"/lw/def/new_best_comment_sorting_system/\">introduced in July</a>. The work was done by <strong>John Simon</strong>, and integrated by <strong>Wes. </strong>Whereas the old default sorting system, \"Top\", favored old comments that had already floated to the top and were thus more likely to get even more upvotes, \"Best\" attempts to give newer comments a fairer chance.</p>\n<p><a href=\"/lw/dxj/issue_301_shipped_show_parent_comments_on_comments/\">In August</a> we got the ability to <span class=\"h3\"><a href=\"http://code.google.com/p/lesswrong/issues/detail?id=301\">show parent comments on /comments</a>. The work was done by <strong>John Simon</strong>, and integrated by <strong>wmoore</strong>. This change makes it far easier to grasp the context of things seen on the <a href=\"/comments/\">recent comments page</a>, given that we now see the old comment that the new comments are replying to.</span></p>\n<p><span class=\"h3\">And finally, <a href=\"/lw/ekw/less_wrong_polls_in_comments/\">starting from September</a>, we have been able to write <a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting#Polls\">comments that contain polls</a>! Work on the code was originally began by <strong>jimrandomh</strong>, finished later by <strong>John Simon</strong>, and deployed by <strong>wmoore </strong>and <strong>matt</strong>. Although people had long been taking advantage of comment vote counts as a crude way of creating their own polls, this change makes things far easier. <br /></span></p>\n<p><span style=\"text-decoration: underline;\"><strong>Meetup booklet</strong></span></p>\n<p>In June, we <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">published the How to Run a Successful Less Wrong Meetup booklet</a>, which I wrote together with <strong>lukeprog</strong>, and which got its graphical design from <strong>Stanislaw Boboryk</strong>. Numerous other people also helped, both by providing advice and by contributing pictures to it. In addition to general advice on running a meetup, it contains various games and exercises as well as case studies and examples from real meetup groups from around the world.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Index of original research<br /></strong></span></p>\n<p>Starting from October, <strong>lukeprog </strong>has maintained a curated index of Less Wrong posts <a href=\"/lw/f6o/original_research_on_less_wrong/\">containing significant original research</a>. It contains numerous posts, organized under categories such as general philosophy, decision theory / AI architectures / mathematical logic, ethics, and AI risk strategy. Last updated on December 17th, it now links to a total of 78 different posts.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Who are we?<br /></strong></span></p>\n<p>In November and December, <strong>Yvain</strong> continued his hard work in holding the yearly survey. Among other interesting details, around 90% of us are male, 55% are from the USA, 41% are students and 31% are doing for-profit work. See the <a href=\"/lw/fp5/2012_survey_results/\">2012 survey results</a> for many more details.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Most popular posts of 2012<br /></strong></span></p>\n<p>On LW, people tend to judge the popularity of a post by the number of upvotes that it has. But this only reflects the opinion of the registered users who care enough to vote. For purposes of this article, we were interested in finding out the posts that had made the biggest impact on the whole Internet. Although it's not a perfect measure either, we decided to measure popularity by the number of unique pageviews, as reported by Google Analytics.</p>\n<p>Overall, in 2012 Less Wrong had<span> <strong>over eight million unique pageviews</strong> and <strong>close to two million</strong></span><span><span class=\"Onb\"> </span><strong>unique visitors</strong> (</span><span>8,225,509 and </span><span><span class=\"Onb\">1,756,899, respectively). Of the posts that were written in 2012, the most popular ones were...</span></span></p>\n<p class=\"xnb\"><span><span class=\"Onb\"><strong>#10: <a href=\"/lw/aa7/get_curious/\">Get Curious</a></strong>, in which <strong>lukeprog</strong> suggests that one of the most important rationality skills is being genuinely curious about things, instead of just jumping to the first answer that comes to your mind and leaving it at that. He suggests a three-step approach for actually becoming more curious: first, feel that you don't already know the answer, then start wanting to know the answer, and finally sprint headlong to reality. Together with a number of exercises intended to make you better at these steps, this article made a lot of folks curious about Less Wrong and caused people to sprint headlong to the post <strong>10,850 times</strong>.<br /></span></span></p>\n<p class=\"xnb\"><strong>#9: </strong>Being curious about things means that you genuinely want to know the truth. That makes it useful to have a good grasp of <a href=\"/lw/eqn/the_useful_idea_of_truth/\"><strong>The Useful Idea of Truth</strong></a>. This article by <strong>Eliezer Yudkowsky</strong> starts the <em>Highly Advanced Epistemology 101 for Beginners</em> sequence by explaining what exactly it means for something to be \"true\". In order to avoid spoiling the article's \"meditations\" for anyone who hasn't read it yet, I will not attempt to summarize the answer. I'll only suggest that one definition for \"truth\" could be the correctness of the claim that this post was viewed <strong>11,161 times</strong>.</p>\n<p class=\"xnb\"><strong>#8: </strong>Having defined truth, we can move on to ask, what are numbers? And in what sense is \"2 + 2 = 4\" meaningful or true? <strong>Eliezer Yudkowsky</strong>'s <a href=\"/lw/f4e/logical_pinpointing/\"><strong>Logical Pinpointing</strong></a> attempts to answer this question, partially through the cute device of conversing with an imaginary logician who understands logic perfectly but has no grasp of numbers. As they converse, they define the rules according to which arithmetic works. I'm going to skip the obvious pun due to it being too obvious, and only say that this article was viewed <strong>12,606 times</strong>.</p>\n<p class=\"xnb\"><strong>#7: </strong>Now that we're curious and understand both the meaning of truth and of numbers, it stands to reason that we should <strong><a href=\"/lw/bq0/be_happier/\">Be Happier</a></strong> than before. Or maybe not, since <strong>Klevador</strong>'s article does not actually mention \"understand obscure philosophy\" as a way of getting happier. What it does mention is a big list of other things that have been shown to increase happiness. We first get a list of brief recommendations a few sentences long, and then somewhat longer excerpts of the relevant literature. There's also a full list of references. Let's hope that the <strong>14,178 views </strong>that this post got made someone happier.</p>\n<p class=\"xnb\"><strong>#6: </strong>Getting into more controversial territory, <strong>lukeprog </strong>advises us to <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\"><strong>Train Philosophers with Pearl and Kahneman, not Plato and Kant</strong></a>. Philosophy is getting increasingly diseased and irrelevant, he argues, and the cure for that involves incorporating more actual science and rationality into the standard philosopher curriculum. If the <a href=\"http://news.ycombinator.com/item?id=4891502\">discussion on Hacker News</a> is any indication, this post got a lot of people incensed, which might help explain why it got <strong>14,334 views</strong>.</p>\n<p class=\"xnb\"><strong>#5: </strong>Now that we got started on calling whole disciplines diseased, let's look at <a href=\"/lw/9sv/diseased_disciplines_the_strange_case_of_the/\"><strong>Diseased disciplines: the strange case of the inverted chart</strong></a>. <strong>Morendil</strong>'s post begins with a hypothetical example of numerous academics all citing a particular source, which doesn't actually contain the intended reference... and then the intended source doesn't actually have the data to back up its claim, either. But that's just a hypothetical example, right? Well, not really, which helped this post get <strong>17,385 views</strong>.</p>\n<p class=\"xnb\"><strong>#4: </strong>Interestingly, our fourth-most-popular post isn't actually an original contribution as such. <strong>Grognor</strong>'s <a href=\"/r/discussion/lw/99c/transcript_richard_feynman_on_why_questions/\">transcript of <strong>Richard Feynman on Why Questions</strong></a> discusses the nature of explanations, and the fact that there are some things which simply cannot be adequately explained in terms of pre-existing knowledge. Instead, one has to learn entirely new concepts in order to comprehend them. Hopefully, at least this much was understood on the <strong>18,402 times </strong>that the post was viewed.</p>\n<p class=\"xnb\"><strong>#3: </strong>From physics to neuroscience: <strong>kalla724</strong>'s <a href=\"/lw/blr/attention_control_is_critical_for/\"><strong>Attention control is critical for changing/increasing/altering motivation</strong></a> explores the effect of attention on neural plasticity, including the plasticity of motivation. It explains that paying attention to something can increase the amount of brain circuitry dedicated to processing that something, generally by repurposing nearby less-used circuitry. This also has practical applications, such as in helping to explain why Cognitive Behavioral Therapy works. That earned the post <strong>21,136 views.</strong></p>\n<p class=\"xnb\"><strong>#2: </strong>I should be writing this post instead of browsing Facebook. Fortunately, <strong>lukeprog </strong>has a post titled <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\"><strong>My Algorithm for Beating Procrastination</strong></a>. Based on the equation of <em>Motivation = (Expectancy * Value) / (Impulsiveness * Delay)</em>, the algorithm involves first noticing that you are procrastinating, then guessing which part of the motivation equation is causing you the most trouble, and then trying several methods for attacking that specific problem. I guess that a lot of people shared this on Facebook where other procrastinators saw it, because the article got <strong>38,637 views.</strong></p>\n<p class=\"xnb\"><strong>#1: </strong>And finally... the most read 2012 article on the site was <strong>Yvain</strong>'s <a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\"><strong>The noncentral fallacy - the worst argument in the world?</strong></a>, where he defined the noncentral fallacy as \"X is in a category whose archetypal member gives us a certain emotional reaction. Therefore, we should apply that emotional reaction to X, even though it is not a central category member.\" Which sounds pretty abstract, but the political examples in the post should make it clearer. The politics probably helped contribute to this post's achievement of <strong>41,932 views.</strong></p>\n<p class=\"xnb\"><span style=\"text-decoration: underline;\"><strong>Most popular all-time posts</strong></span></p>\n<p class=\"xnb\">In addition to looking at only the posts that were made in 2012, people might be interested in knowing which posts were the most viewed in 2012 overall. The top three ones were all written by <strong>lukeprog</strong>, and we can see that two of them were closely related to the top-scorers which were written last year.</p>\n<p class=\"xnb\"><strong><a href=\"/lw/4su/the_science_of_happiness/\">How to be Happy</a> </strong>is LW's run-away favorite article and was viewed more than every page on LW except the home page and the discussion homepage. That is, <strong>228,747 times</strong>! <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\"><strong>The Best Textbooks on Every Subject</strong></a> comes as a distant second at <strong>98,011 views. </strong>And the third one is <a href=\"/lw/3w3/how_to_beat_procrastination/\"><strong>How to Beat Procrastination</strong></a>, at <strong>66,587 views</strong>.</p>\n<p class=\"xnb\">So I guess the take-home message is: people want to be happier, smarter, and more productive. <strong>Let's keep becoming those things in 2013!</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 1, "izp6eeJJEg9v5zcur": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WyN2MhXpzX8oYrCtG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 62, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "20978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yJfBzcDL9fBHJfZ6P", "vQTGuuiDt28t9GZqc", "NwbQFJ8n7rsQkpN8b", "xmkGGBfftoEbq5wae", "jTkcrSJCb5jPjyhYi", "qMuAazqwJvkvo8teR", "jTkmEGWM4dJAfE62W", "x9FNKTEt68Rz6wQ6P", "bGtdeqbgTzuLvZ5zn", "XqvnWFtRD2keJdwjX", "3FoMuCLqZggTxoC3S", "JHcTP4Ad8QAmRTCZm", "LcEzxX2FNTKbB6KXS", "4ACmfJkXQxkYacdLt", "W9rJv26sxs4g2B9bL", "rD57ysqawarsbry6v", "Ty2tjPwv8uyPK9vrz", "yCWPkLi8wJvewPbEp", "ZbgCx2ntD5eu8Cno9", "xg3hXCYQPJkwHyik2", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T14:11:10.082Z", "modifiedAt": null, "url": null, "title": "My Best Case vs Your Worst Case", "slug": "my-best-case-vs-your-worst-case", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.849Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HalMorris", "createdAt": "2012-12-08T02:54:12.946Z", "isAdmin": false, "displayName": "HalMorris"}, "userId": "8cZxp4PS87vNbhmCf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KAE9E4zfCFLyDPtep/my-best-case-vs-your-worst-case", "pageUrlRelative": "/posts/KAE9E4zfCFLyDPtep/my-best-case-vs-your-worst-case", "linkUrl": "https://www.lesswrong.com/posts/KAE9E4zfCFLyDPtep/my-best-case-vs-your-worst-case", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20Best%20Case%20vs%20Your%20Worst%20Case&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20Best%20Case%20vs%20Your%20Worst%20Case%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAE9E4zfCFLyDPtep%2Fmy-best-case-vs-your-worst-case%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20Best%20Case%20vs%20Your%20Worst%20Case%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAE9E4zfCFLyDPtep%2Fmy-best-case-vs-your-worst-case", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAE9E4zfCFLyDPtep%2Fmy-best-case-vs-your-worst-case", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>Is there a name for the (I claim) extremely common practice of  blithely and unconsciously always looking at your own view (political  especially) in terms of its best possible outcomes, while always characterizing  an opposing point of view by its worst possibilities?</p>\n<p>If not, I think there should be.&nbsp; It seems like a major major source of unfruitful argumentation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KAE9E4zfCFLyDPtep", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 1.0750217878820658e-06, "legacy": true, "legacyId": "20990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T18:38:18.801Z", "modifiedAt": null, "url": null, "title": "How much to spend on a high-variance option?", "slug": "how-much-to-spend-on-a-high-variance-option", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kx5L5ncZNEpMGLGPx/how-much-to-spend-on-a-high-variance-option", "pageUrlRelative": "/posts/Kx5L5ncZNEpMGLGPx/how-much-to-spend-on-a-high-variance-option", "linkUrl": "https://www.lesswrong.com/posts/Kx5L5ncZNEpMGLGPx/how-much-to-spend-on-a-high-variance-option", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20much%20to%20spend%20on%20a%20high-variance%20option%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20much%20to%20spend%20on%20a%20high-variance%20option%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKx5L5ncZNEpMGLGPx%2Fhow-much-to-spend-on-a-high-variance-option%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20much%20to%20spend%20on%20a%20high-variance%20option%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKx5L5ncZNEpMGLGPx%2Fhow-much-to-spend-on-a-high-variance-option", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKx5L5ncZNEpMGLGPx%2Fhow-much-to-spend-on-a-high-variance-option", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>So the jackpot in the Ohio lottery is around 25 million, and the chance of winning it is one in roughly 14 million, with tickets at 1 dollar a piece. It appears to me that roughly a quarter million tickets are sold each drawing; so, supposing you win, the probability of someone else also winning is 1 - (1 - 1/14e6)^{250000}=2%, which does not significantly reduce the expectation value of a ticket. So, unless I'm making a silly mistake somewhere, buying lottery tickets has positive expected value. (I find this counterintuitive; where are all the economists who should be picking up this free money? But I digress.)</p>\n<p>I pointed this out to my wife, and said that it might be worth putting a dollar into it; and she very cogently asked, \"Then why not make it 100 dollars?\" Why not, indeed! Is there any sensible way of deciding how much to put into an option that has a positive expected value, but very low chance of payoff?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kx5L5ncZNEpMGLGPx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 1.0751813009960068e-06, "legacy": true, "legacyId": "20991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-03T22:52:56.745Z", "modifiedAt": null, "url": null, "title": "Can infinite quantities exist? A philosophical approach", "slug": "can-infinite-quantities-exist-a-philosophical-approach", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "metaphysicist", "createdAt": "2012-02-17T23:36:50.395Z", "isAdmin": false, "displayName": "metaphysicist"}, "userId": "WhJB5nfwSBQu7wjhz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T4Tim4PXgw8nBW73B/can-infinite-quantities-exist-a-philosophical-approach", "pageUrlRelative": "/posts/T4Tim4PXgw8nBW73B/can-infinite-quantities-exist-a-philosophical-approach", "linkUrl": "https://www.lesswrong.com/posts/T4Tim4PXgw8nBW73B/can-infinite-quantities-exist-a-philosophical-approach", "postedAtFormatted": "Thursday, January 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20infinite%20quantities%20exist%3F%20A%20philosophical%20approach&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20infinite%20quantities%20exist%3F%20A%20philosophical%20approach%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4Tim4PXgw8nBW73B%2Fcan-infinite-quantities-exist-a-philosophical-approach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20infinite%20quantities%20exist%3F%20A%20philosophical%20approach%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4Tim4PXgw8nBW73B%2Fcan-infinite-quantities-exist-a-philosophical-approach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4Tim4PXgw8nBW73B%2Fcan-infinite-quantities-exist-a-philosophical-approach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1144, "htmlBody": "<p>&nbsp;</p>\n<p><span style=\"font-family: mceinline;\"><a href=\"http://juridicalcoherence.blogspot.com/2013/01/190-can-infinite-quantities-exist.html\">[Crossposted]</a></span></p>\n<p>Initially attracted to <em>Less Wrong</em> by Eliezer Yudkowsky's intellectual boldness in his \"infinite-sets atheism,\" I've waited patiently to discover its rationale. Sometimes it's said that our \"intuitions\" speak for infinity or against, but how could one, in a Kahneman-appropriate manner, arrive at intuitions about whether the cosmos is infinite? Intuitions about infinite sets might arise from an analysis of the <em>concept of&nbsp;</em>actually realized infinities. This is a distinctively philosophical form of analysis and one somewhat alien to <em>Less Wrong</em>, but it may be the only way to gain purchase on this neglected question. I'm by no means certain of my reasoning; I certainly don't think I've settled the issue. But for reasons I discuss in this skeletal argument, the conceptual&mdash;as opposed to the scientific or mathematical&mdash;analysis of \"actually realized infinities\" has been largely avoided, and I hope to help begin a necessary discussion.</p>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">1. The actuality of infinity is a paramount metaphysical issue.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Some&nbsp;major issues in science and philosophy demand taking a position on whether there can be an infinite number of things or an infinite amount of something. Infinity&rsquo;s most obvious scientific relevance is to cosmology, where the question of whether the universe is finite or infinite looms large. But infinities are invoked in various physical theories, and they seem often to occur in dubious theories. In quantum mechanics, an (uncountable) infinity of worlds is invoked by the &ldquo;many worlds interpretation,&rdquo; and anthropic explanations often invoke an actual infinity of universes, which may themselves be infinite. These applications make real infinite sets a paramount metaphysical problem&mdash;if it indeed is metaphysical&mdash;but the orthodox view is that, being empirical, it isn&rsquo;t metaphysical at all. To view infinity as a purely empirical matter is the modern view; we&rsquo;ve learned not to place excessive weight on purely conceptual reasoning, but&nbsp;<span style=\"color: red;\">whether conceptual reasoning can definitively settle the matter differs from whether the matter&nbsp;<em>is&nbsp;</em>fundamentally conceptual</span>.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br /></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Two developments have discouraged the metaphysical exploration of actually existing infinities: the mathematical analysis of infinity and the proffer of crank arguments against infinity in the service of retrograde causes. Although some marginal schools of mathematics reject Cantor&rsquo;s investigation of transfinite numbers, I will assume the concept of infinity itself is consistent. My analysis pertains not to the concept of infinity as such but to the actual realization of infinity. Actual infinity&rsquo;s main detractor is a Christian fundamentalist crank named William Lane Craig, whose critique of infinity, serving theist first-cause arguments, has made infinity eliminativism intellectually disreputable. Craig&rsquo;s arguments merely appeal to the&nbsp;<em>strangeness&nbsp;</em>of infinity&rsquo;s manifestations, not to the&nbsp;<em>incoherence</em>&nbsp;of its realization. The standard arguments against infinity, which predate Cantor, have been well-refuted, and I leave the mathematical critique of infinity to the mathematicians, who are mostly satisfied. (See Graham Oppy,&nbsp;<em>Philosophical perspectives on infinity</em>&nbsp;(2006).)&nbsp;</span></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">2. The principle of the identity of indistinguishables applies to physics and to sets, not to everything conceivable.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">My novel arguments are based on a revision of a metaphysical principle called the&nbsp;<em>identity of indistinguishable</em>s, which holds that two separate things can&rsquo;t have exactly the same properties. Things are constituted by their properties; if two things have exactly the same properties, nothing remains to make them different from one another. Physical objects do seem to conform to the identity of indistinguishables because physical objects are individuated by their positions in space and time, which are properties, but this is a physical rather than a metaphysical principle. Conceptually,&nbsp;<em>brute distinguishability</em>, that is differing from all other things simply in being&nbsp;<em>different</em>, is a property, although it provides us with no basis for identifying one thing and not another. There may be no way to use such a property in any physical theory, we may never learn of such a property and thus never have reason to believe it instantiated, but the property seems conceptually possible.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br /></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">But the identity of indistinguishables does apply to sets: indistinguishable sets are identical. Properties&nbsp;<em>determine</em>&nbsp;sets, so&nbsp;<span style=\"color: red;\">you can&rsquo;t define a proper subset of brutely distinguishable things</span>.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\"><br /></span></strong></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">3. Arguments against actually existing infinite sets.</span></strong></h3>\n<h4 style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">A. Argument based on brute distinguishability.</span></strong></h4>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">To show that the existence of an actually existing infinite set leads to contradiction, assume the existence of an infinite set of brutely distinguishable points. Now another point pops into existence. The former and latter sets are indistinguishable, yet they aren&rsquo;t identical.&nbsp;<span style=\"color: red;\">The proviso that the points themselves are indistinguishable allows the sets to be different yet indistinguishable when they&rsquo;re infinite, proving they can&rsquo;t be infinite.</span></span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"color: red;\"><span style=\"font-family: Verdana, sans-serif;\"><br /></span></span></div>\n<h4 style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">B. Argument based on probability as limiting relative frequency.</span></strong></h4>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The previous argument depends on the coherence of&nbsp;<em>brute distinguishability</em>. The following&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">probability</span><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">argument depends on different intuitions. Probabilities can be treated as idealizations at infinite limits. If you toss a coin, it will land heads roughly 50% of the time, and it gets closer to exactly 50% as the number of tosses &ldquo;approaches infinity.&rdquo; But if there can actually be an infinite number of tosses, contradiction arises. Consider the possibility that in an infinite universe or an infinite number of universes, infinitely many coin tosses actually occur. The frequency of heads and of tails is then infinite, so the relative frequency is undefined. Furthermore, the frequency of rolling a 1 on a die also equals the frequency of rolling 2 &ndash; 6: both are (countably) infinite.&nbsp;</span><span style=\"color: red; font-family: Verdana, sans-serif;\">But if infinite quantities exist, then relative frequency should equal probability.</span><span style=\"font-family: Verdana, sans-serif;\">&nbsp;Therefore, infinite quantities don&rsquo;t exist.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\"><br /></span></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\">4. The nonexistence of actually realized infinite sets and the principle of the identity of indistinguishable sets together imply the Gold model of the cosmos.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Before applying the conclusion that actually realized infinities can&rsquo;t exist together with the principle of the identity of indistinguishables to a fundamental problem of cosmology, caveats are in order. The argument uses only the most general and well-established physical conclusions and is oblivious to physical detail, and not being competent in physics, I must abstain even from assessing the weight the philosophical analysis that follows should carry; it may be very slight. While the cosmological model I propose isn&rsquo;t original, the argument is original and as far as I can tell, novel. I am not proposing a physical theory as much as suggesting metaphysical considerations that might bear on physics, whereas it is for physicists to say how weighty these considerations&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">are&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">in light of actual physical data and theory.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br /></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The cosmological theory is the Gold model of the universe, once favored by Albert Einstein, according to which the universe undergoes a perpetual expansion, contraction, and re-expansion. I assume a deterministic universe, such that cycles are exactly identical: any contraction is thus indistinguishable from any other, and any expansion is indistinguishable from any other. Since there is no room in physics for brute distinguishability, they are identical because no common spatio-temporal framework allows their distinction.&nbsp;<span style=\"color: red;\">Thus, although the expansion and contraction process is perpetual and eternal, it is also finite; in fact, its number is unity.</span></span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br /></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The Gold universe&mdash;alone, with the possible exception of the Hawking universe&mdash;</span><span style=\"font-family: Verdana, sans-serif;\">avoids&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">the dilemma of the realization of infinite sets or origination&nbsp;</span><em style=\"font-family: Verdana, sans-serif;\">ex nihilo</em><span style=\"font-family: Verdana, sans-serif;\">.</span></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T4Tim4PXgw8nBW73B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": -26, "extendedScore": null, "score": 1.0753333816097237e-06, "legacy": true, "legacyId": "20992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p><span style=\"font-family: mceinline;\"><a href=\"http://juridicalcoherence.blogspot.com/2013/01/190-can-infinite-quantities-exist.html\">[Crossposted]</a></span></p>\n<p>Initially attracted to <em>Less Wrong</em> by Eliezer Yudkowsky's intellectual boldness in his \"infinite-sets atheism,\" I've waited patiently to discover its rationale. Sometimes it's said that our \"intuitions\" speak for infinity or against, but how could one, in a Kahneman-appropriate manner, arrive at intuitions about whether the cosmos is infinite? Intuitions about infinite sets might arise from an analysis of the <em>concept of&nbsp;</em>actually realized infinities. This is a distinctively philosophical form of analysis and one somewhat alien to <em>Less Wrong</em>, but it may be the only way to gain purchase on this neglected question. I'm by no means certain of my reasoning; I certainly don't think I've settled the issue. But for reasons I discuss in this skeletal argument, the conceptual\u2014as opposed to the scientific or mathematical\u2014analysis of \"actually realized infinities\" has been largely avoided, and I hope to help begin a necessary discussion.</p>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"1__The_actuality_of_infinity_is_a_paramount_metaphysical_issue_\"><strong><span style=\"font-family: Verdana, sans-serif;\">1. The actuality of infinity is a paramount metaphysical issue.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Some&nbsp;major issues in science and philosophy demand taking a position on whether there can be an infinite number of things or an infinite amount of something. Infinity\u2019s most obvious scientific relevance is to cosmology, where the question of whether the universe is finite or infinite looms large. But infinities are invoked in various physical theories, and they seem often to occur in dubious theories. In quantum mechanics, an (uncountable) infinity of worlds is invoked by the \u201cmany worlds interpretation,\u201d and anthropic explanations often invoke an actual infinity of universes, which may themselves be infinite. These applications make real infinite sets a paramount metaphysical problem\u2014if it indeed is metaphysical\u2014but the orthodox view is that, being empirical, it isn\u2019t metaphysical at all. To view infinity as a purely empirical matter is the modern view; we\u2019ve learned not to place excessive weight on purely conceptual reasoning, but&nbsp;<span style=\"color: red;\">whether conceptual reasoning can definitively settle the matter differs from whether the matter&nbsp;<em>is&nbsp;</em>fundamentally conceptual</span>.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Two developments have discouraged the metaphysical exploration of actually existing infinities: the mathematical analysis of infinity and the proffer of crank arguments against infinity in the service of retrograde causes. Although some marginal schools of mathematics reject Cantor\u2019s investigation of transfinite numbers, I will assume the concept of infinity itself is consistent. My analysis pertains not to the concept of infinity as such but to the actual realization of infinity. Actual infinity\u2019s main detractor is a Christian fundamentalist crank named William Lane Craig, whose critique of infinity, serving theist first-cause arguments, has made infinity eliminativism intellectually disreputable. Craig\u2019s arguments merely appeal to the&nbsp;<em>strangeness&nbsp;</em>of infinity\u2019s manifestations, not to the&nbsp;<em>incoherence</em>&nbsp;of its realization. The standard arguments against infinity, which predate Cantor, have been well-refuted, and I leave the mathematical critique of infinity to the mathematicians, who are mostly satisfied. (See Graham Oppy,&nbsp;<em>Philosophical perspectives on infinity</em>&nbsp;(2006).)&nbsp;</span></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"2__The_principle_of_the_identity_of_indistinguishables_applies_to_physics_and_to_sets__not_to_everything_conceivable_\"><strong><span style=\"font-family: Verdana, sans-serif;\">2. The principle of the identity of indistinguishables applies to physics and to sets, not to everything conceivable.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">My novel arguments are based on a revision of a metaphysical principle called the&nbsp;<em>identity of indistinguishable</em>s, which holds that two separate things can\u2019t have exactly the same properties. Things are constituted by their properties; if two things have exactly the same properties, nothing remains to make them different from one another. Physical objects do seem to conform to the identity of indistinguishables because physical objects are individuated by their positions in space and time, which are properties, but this is a physical rather than a metaphysical principle. Conceptually,&nbsp;<em>brute distinguishability</em>, that is differing from all other things simply in being&nbsp;<em>different</em>, is a property, although it provides us with no basis for identifying one thing and not another. There may be no way to use such a property in any physical theory, we may never learn of such a property and thus never have reason to believe it instantiated, but the property seems conceptually possible.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">But the identity of indistinguishables does apply to sets: indistinguishable sets are identical. Properties&nbsp;<em>determine</em>&nbsp;sets, so&nbsp;<span style=\"color: red;\">you can\u2019t define a proper subset of brutely distinguishable things</span>.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><strong><span style=\"font-family: Verdana, sans-serif;\"><br></span></strong></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"3__Arguments_against_actually_existing_infinite_sets_\"><strong><span style=\"font-family: Verdana, sans-serif;\">3. Arguments against actually existing infinite sets.</span></strong></h3>\n<h4 style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"A__Argument_based_on_brute_distinguishability_\"><strong><span style=\"font-family: Verdana, sans-serif;\">A. Argument based on brute distinguishability.</span></strong></h4>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">To show that the existence of an actually existing infinite set leads to contradiction, assume the existence of an infinite set of brutely distinguishable points. Now another point pops into existence. The former and latter sets are indistinguishable, yet they aren\u2019t identical.&nbsp;<span style=\"color: red;\">The proviso that the points themselves are indistinguishable allows the sets to be different yet indistinguishable when they\u2019re infinite, proving they can\u2019t be infinite.</span></span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"color: red;\"><span style=\"font-family: Verdana, sans-serif;\"><br></span></span></div>\n<h4 style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"B__Argument_based_on_probability_as_limiting_relative_frequency_\"><strong><span style=\"font-family: Verdana, sans-serif;\">B. Argument based on probability as limiting relative frequency.</span></strong></h4>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The previous argument depends on the coherence of&nbsp;<em>brute distinguishability</em>. The following&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">probability</span><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">argument depends on different intuitions. Probabilities can be treated as idealizations at infinite limits. If you toss a coin, it will land heads roughly 50% of the time, and it gets closer to exactly 50% as the number of tosses \u201capproaches infinity.\u201d But if there can actually be an infinite number of tosses, contradiction arises. Consider the possibility that in an infinite universe or an infinite number of universes, infinitely many coin tosses actually occur. The frequency of heads and of tails is then infinite, so the relative frequency is undefined. Furthermore, the frequency of rolling a 1 on a die also equals the frequency of rolling 2 \u2013 6: both are (countably) infinite.&nbsp;</span><span style=\"color: red; font-family: Verdana, sans-serif;\">But if infinite quantities exist, then relative frequency should equal probability.</span><span style=\"font-family: Verdana, sans-serif;\">&nbsp;Therefore, infinite quantities don\u2019t exist.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\"><br></span></div>\n<h3 style=\"font-size: 19.33333396911621px; font-family: 'Trebuchet MS', verdana, sans-serif; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\" id=\"4__The_nonexistence_of_actually_realized_infinite_sets_and_the_principle_of_the_identity_of_indistinguishable_sets_together_imply_the_Gold_model_of_the_cosmos_\"><strong><span style=\"font-family: Verdana, sans-serif;\">4. The nonexistence of actually realized infinite sets and the principle of the identity of indistinguishable sets together imply the Gold model of the cosmos.</span></strong></h3>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">Before applying the conclusion that actually realized infinities can\u2019t exist together with the principle of the identity of indistinguishables to a fundamental problem of cosmology, caveats are in order. The argument uses only the most general and well-established physical conclusions and is oblivious to physical detail, and not being competent in physics, I must abstain even from assessing the weight the philosophical analysis that follows should carry; it may be very slight. While the cosmological model I propose isn\u2019t original, the argument is original and as far as I can tell, novel. I am not proposing a physical theory as much as suggesting metaphysical considerations that might bear on physics, whereas it is for physicists to say how weighty these considerations&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">are&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">in light of actual physical data and theory.</span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The cosmological theory is the Gold model of the universe, once favored by Albert Einstein, according to which the universe undergoes a perpetual expansion, contraction, and re-expansion. I assume a deterministic universe, such that cycles are exactly identical: any contraction is thus indistinguishable from any other, and any expansion is indistinguishable from any other. Since there is no room in physics for brute distinguishability, they are identical because no common spatio-temporal framework allows their distinction.&nbsp;<span style=\"color: red;\">Thus, although the expansion and contraction process is perpetual and eternal, it is also finite; in fact, its number is unity.</span></span></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><br></div>\n<div class=\"MsoNormal\" style=\"font-family: 'Trebuchet MS', verdana, sans-serif; font-size: 13.333333015441895px; line-height: 22px; text-indent: 10px; background-color: #f5f5f5;\"><span style=\"font-family: Verdana, sans-serif;\">The Gold universe\u2014alone, with the possible exception of the Hawking universe\u2014</span><span style=\"font-family: Verdana, sans-serif;\">avoids&nbsp;</span><span style=\"font-family: Verdana, sans-serif;\">the dilemma of the realization of infinite sets or origination&nbsp;</span><em style=\"font-family: Verdana, sans-serif;\">ex nihilo</em><span style=\"font-family: Verdana, sans-serif;\">.</span></div>\n<p>&nbsp;</p>", "sections": [{"title": "1. The actuality of infinity is a paramount metaphysical issue.", "anchor": "1__The_actuality_of_infinity_is_a_paramount_metaphysical_issue_", "level": 1}, {"title": "2. The principle of the identity of indistinguishables applies to physics and to sets, not to everything conceivable.", "anchor": "2__The_principle_of_the_identity_of_indistinguishables_applies_to_physics_and_to_sets__not_to_everything_conceivable_", "level": 1}, {"title": "3. Arguments against actually existing infinite sets.", "anchor": "3__Arguments_against_actually_existing_infinite_sets_", "level": 1}, {"title": "A. Argument based on brute distinguishability.", "anchor": "A__Argument_based_on_brute_distinguishability_", "level": 2}, {"title": "B. Argument based on probability as limiting relative frequency.", "anchor": "B__Argument_based_on_probability_as_limiting_relative_frequency_", "level": 2}, {"title": "4. The nonexistence of actually realized infinite sets and the principle of the identity of indistinguishable sets together imply the Gold model of the cosmos.", "anchor": "4__The_nonexistence_of_actually_realized_infinite_sets_and_the_principle_of_the_identity_of_indistinguishable_sets_together_imply_the_Gold_model_of_the_cosmos_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T01:57:33.937Z", "modifiedAt": null, "url": null, "title": "PSA: Please list your references, don't just link them", "slug": "psa-please-list-your-references-don-t-just-link-them", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.036Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wAhgxmCf2ebHha5BJ/psa-please-list-your-references-don-t-just-link-them", "pageUrlRelative": "/posts/wAhgxmCf2ebHha5BJ/psa-please-list-your-references-don-t-just-link-them", "linkUrl": "https://www.lesswrong.com/posts/wAhgxmCf2ebHha5BJ/psa-please-list-your-references-don-t-just-link-them", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PSA%3A%20Please%20list%20your%20references%2C%20don't%20just%20link%20them&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APSA%3A%20Please%20list%20your%20references%2C%20don't%20just%20link%20them%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAhgxmCf2ebHha5BJ%2Fpsa-please-list-your-references-don-t-just-link-them%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PSA%3A%20Please%20list%20your%20references%2C%20don't%20just%20link%20them%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAhgxmCf2ebHha5BJ%2Fpsa-please-list-your-references-don-t-just-link-them", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAhgxmCf2ebHha5BJ%2Fpsa-please-list-your-references-don-t-just-link-them", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>In what <a href=\"/lw/g6q/2012_year_in_review/\">became 5th most-read new post on LessWrong in 2012,</a> Morendil <a href=\"/lw/9sv/diseased_disciplines_the_strange_case_of_the/?sort=new\">told us</a> about a study widely cited in its field... except that source cited, which isn't online and is really difficult to get, makes a different claim&nbsp;&mdash; and turns out to not even be the original research, but a PowerPoint presentation given ten years after the original study was published!</p>\n<p>Fortunately, the <em>original</em> study turns out to be freely available online, for all to read; Morendil's post has a <a href=\"http://findarticles.com/p/articles/mi_m0HPJ/is_n2_v40/ai_7180006/pg_2/\">link</a>. The post also tells us the author and the year of publication. But that's all: Morendil didn't provide a list of references; he showed how the <em>presentation</em> is usually cited, but didn't give a full citation for the original study.</p>\n<p>The link is broken now. The Wayback machine doesn't have a copy. The address doesn't give hints about the study's title. I haven't been able to find anything on Google Scholar with author, year, and likely keywords.</p>\n<p>I rest my case.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wAhgxmCf2ebHha5BJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 36, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "20993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WyN2MhXpzX8oYrCtG", "4ACmfJkXQxkYacdLt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T03:05:36.635Z", "modifiedAt": null, "url": null, "title": "Interpersonal and intrapersonal utility comparisons", "slug": "interpersonal-and-intrapersonal-utility-comparisons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:58.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8b6bmS7XGwJ3CMjpQ/interpersonal-and-intrapersonal-utility-comparisons", "pageUrlRelative": "/posts/8b6bmS7XGwJ3CMjpQ/interpersonal-and-intrapersonal-utility-comparisons", "linkUrl": "https://www.lesswrong.com/posts/8b6bmS7XGwJ3CMjpQ/interpersonal-and-intrapersonal-utility-comparisons", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interpersonal%20and%20intrapersonal%20utility%20comparisons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterpersonal%20and%20intrapersonal%20utility%20comparisons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b6bmS7XGwJ3CMjpQ%2Finterpersonal-and-intrapersonal-utility-comparisons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interpersonal%20and%20intrapersonal%20utility%20comparisons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b6bmS7XGwJ3CMjpQ%2Finterpersonal-and-intrapersonal-utility-comparisons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8b6bmS7XGwJ3CMjpQ%2Finterpersonal-and-intrapersonal-utility-comparisons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 972, "htmlBody": "<p>Utility functions are only defined up to an additive constant and a positive multiplier. For example, if we have a simple universe with only 3 possible states (X, Y, and Z), a utility function u such that u(X)=0, u(Y)=1, and u(Z)=3, and another utility function w such that w(X)=-1, w(Y)=1, and w(Z)=5, then as utility functions, u and w are identical, since w=2u-1.</p>\n<p>Preference utilitarianism suggests maximizing the sum of everyone's utility function. But the fact that utility functions are invariant on multiplication by positive scalars makes this operation poorly defined. For example, suppose your utility function is u (as defined above), and the only other morally relevant agent has a utility function v such that v(X)=0, v(Y)=2000, and v(Z)=1000. He argues that according to utilitarianism, Y is the best state of the universe, since if you add each of your utility functions, you get (u+v)(X)=0, (u+v)(Y)=2001, and (u+v)(Z)=1003. You complain that he cheated by multiplying his utility function by a large number, and that if you treat v as v(X)=0, v(Y)=2, and v(Z)=1, then Z is the best state of the universe according to utilitarianism. There is no objective way to resolve this dispute, but anyone who wants to build a <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">preference utilitarianism machine</a> has to find a way to resolve such disputes that gives reasonable results.</p>\n<p>I'm pretty sure that the idea of the previous two paragraphs has been talked about before, but I can't find where. [Edit: <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">here</a> and <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Distinctness_from_other_notions_of_utility\">here</a>]</p>\n<p>Anyway, one might argue that if you are not a preference utilitarian, and not planning to build a friendly AI, you have little reason to care about this problem. If you just want to maximize your personal utility function, surely you don't need a solution to that problem, right?</p>\n<p>Wrong! Unless you know exactly what your preferences are, which humans don't. If you're unsure whether or not u or v (as described above) describes your true preferences, and you assign a 50% probability to each, then you face the same problem that preference utilitarianism did in the previous example.</p>\n<p>Humans are a lot better at getting ordinal utilities straight than they are at figuring out cardinal utilities, but even assuming that you know the order of your preferences, the problems remain. Let's say that, in another 3-state world (with states A, B, and C) you know you prefer B over A, and C over B, but you are uncertain between the possibilities that you prefer C over A by twice the margin that you prefer B over A, and that you prefer C over A by 10 times the margin that you prefer B over A. You assign a 50% probability to each. Now suppose you face a choice between B and a lottery that has a 20% chance of giving you C and an 80% chance of giving you A. If you define the utility of A as 0 utils and the utility of B as 1 util, then the utility values (in utils) are u1(A)=0, u1(B)=1, u1(C)=2, u2(A)=0, u2(B)=1, u2(C)=10, so the expected utility of choosing B is 1 util, and the expected utility of the lottery is .5*(.2*2 + .8*0) + .5*(.2*10 + .8*0) = 1.2 utils, so the lottery is better. But if you instead define the utility of A as 0 utils and the utility of C as 1 util, then u1(A)=0, u1(B)=.5, u1(C)=1, u2(A)=0, u2(B)=.1, and u2(C)=1, so the expected utility of B is .5*.5 + .5*.1 = .3 utils, and the expected utility of the lottery is .2*1 + .8*0 = .2 utils, so B is better. The result changes depending on how we define a util, even though we are modeling the same knowledge over preferences in each situation.</p>\n<p>Anything with moral uncertainty, such as a <a href=\"/lw/f32/value_loading/\">value loading</a> agent, needs to know how to add utility functions, not just utilitarians. I do not have a satisfactory solution to this, although I have come up with 2 attempted solutions, neither of which is entirely satisfactory.</p>\n<p>My first idea was to normalize the standard deviation of each utility function to 1. For example, in the XYZ world, after normalizing u and v so that their values have standard deviation 1, we get (approximately) u(X)=0, u(Y)=.802, u(Z)=2.405, v(X)=0, v(Y)=2.449, v(Z)=1.225, so (u+v)(X)=0, (u+v)(Y)=3.251, and (u+v)(Z)=3.630. Z is thus declared the best option overall. However, if there are an infinite number of possible states, then this is impossible unless we have some sort of a priori probability distribution over the possible states. Even more frightening is the fact that this does not respect independence of irrelevant alternatives. Let's suppose that we find out that X is impossible. Good; no one wanted it anyway, so this shouldn't change anything, right? But if you exclude X and set Y as the 0 value for each utility function, then we get u(Y)=0, u(Z)=2, v(Y)=0, v(Z)=-2, (u+v)(Y)=0, (u+v)(Z)=0. The relative values of Y and Z in our preference aggregator changed even though all we did was exclude an option that everyone already agreed we should avoid.</p>\n<p>Then it occurred to me that we have much more knowledge about the relative values of options that we are already quite familiar with, so it seems reasonable to assume that most of our moral uncertainty is about the value of options that we are not so familiar with. For example, in the ABC world, if you make decisions involving A and B all the time, but C is an unfamiliar option that you have not thought much about, it might be tempting to accept the first calculation, which gave B a value of 1 util and the lottery a value of 1.2 utils. This seems like a promising heuristic, but is difficult to formalize, and does not completely solve the problem. For instance, if both B and C are unfamiliar, then this heuristic does not have any advice to give.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8b6bmS7XGwJ3CMjpQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "20996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YCMfQoqqi2o9Tjwoa", "Z8WRsxYjmrxNGyPPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T04:33:37.458Z", "modifiedAt": null, "url": null, "title": "Case Study: the Death Note Script and Bayes", "slug": "case-study-the-death-note-script-and-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/99DctaXedCFPfG2vb/case-study-the-death-note-script-and-bayes", "pageUrlRelative": "/posts/99DctaXedCFPfG2vb/case-study-the-death-note-script-and-bayes", "linkUrl": "https://www.lesswrong.com/posts/99DctaXedCFPfG2vb/case-study-the-death-note-script-and-bayes", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Case%20Study%3A%20the%20Death%20Note%20Script%20and%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACase%20Study%3A%20the%20Death%20Note%20Script%20and%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99DctaXedCFPfG2vb%2Fcase-study-the-death-note-script-and-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Case%20Study%3A%20the%20Death%20Note%20Script%20and%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99DctaXedCFPfG2vb%2Fcase-study-the-death-note-script-and-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99DctaXedCFPfG2vb%2Fcase-study-the-death-note-script-and-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p><a href=\"http://www.gwern.net/Death%20Note%20script\">\"Who wrote the <em>Death Note </em>script?\"</a></p>\n<blockquote>\n<p>I give a history of the 2009 leaked script, discuss internal &amp; external evidence for its authenticity including stylometrics; and then give a simple step-by-step Bayesian analysis of each point. We finish with high confidence in the script's authenticity, discussion of how this analysis was surprisingly enlightening, and what followup work the analysis suggests would be most valuable.</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>If you're already familiar this particular leaked 2009 live-action script, please write down your current best guess as to how likely it is to be authentic.</p>\n<p>This is intended to be easy to understand and essentially beginner-level for Bayes's theorem and fermi estimates, like my other <a href=\"/lw/5ld/death_note_anonymity_and_information_theory/\"><em>Death Note</em> essay</a> (information theory, crypto) or my <a href=\"/lw/4or/case_study_console_insurance/\">console insurance</a> page (efficient markets, positive psychology, expected value).</p>\n<p>Be sure to check out the controversial twist ending!</p>\n<p>(I'm sorry to post just a link, but I briefly thought about writing it and all the math in the LW edit box and decided that cutting my wrists sounded both quicker and more enjoyable. Unfortunately, there seems to be a math problem in the Google Chrome/Chromium browser where fractions simply don't render, <a href=\"https://code.google.com/p/chromium/issues/detail?id=6606\">apparently</a> <a href=\"https://code.google.com/p/chromium/issues/detail?id=152430\">due</a> to not enabling Webkit's MathML code; if fractions don't render for you, well, I know the math works well in my Iceweasel and it seems to work well in other Firefoxes.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "pnSXfWXbQihrFadeD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "99DctaXedCFPfG2vb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 34, "extendedScore": null, "score": 1.0755369130896579e-06, "legacy": true, "legacyId": "19659", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zumnfc7jctgocfoe9", "qykvr25EGcpFcMy6f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T04:46:16.148Z", "modifiedAt": null, "url": null, "title": "Meetup : Buffalo Meetup", "slug": "meetup-buffalo-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NuwZfta295YzAqYpz/meetup-buffalo-meetup-0", "pageUrlRelative": "/posts/NuwZfta295YzAqYpz/meetup-buffalo-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/NuwZfta295YzAqYpz/meetup-buffalo-meetup-0", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Buffalo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Buffalo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuwZfta295YzAqYpz%2Fmeetup-buffalo-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Buffalo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuwZfta295YzAqYpz%2Fmeetup-buffalo-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuwZfta295YzAqYpz%2Fmeetup-buffalo-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hk'>Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">SPOT Coffee 5330 Main Street Williamsville, NY 14221</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to chat about general rationality topics. I think this this article (<a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">http://squid314.livejournal.com/323694.html</a>) will make for a good discussion topic.</p>\n\n<p>Come join us. We'll talk about rationality and play Zendo!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hk'>Buffalo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NuwZfta295YzAqYpz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0755444687984652e-06, "legacy": true, "legacyId": "21003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/hk\">Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">SPOT Coffee 5330 Main Street Williamsville, NY 14221</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to chat about general rationality topics. I think this this article (<a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">http://squid314.livejournal.com/323694.html</a>) will make for a good discussion topic.</p>\n\n<p>Come join us. We'll talk about rationality and play Zendo!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/hk\">Buffalo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T04:46:34.293Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Visualizing Eutopia", "slug": "seq-rerun-visualizing-eutopia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P5HSLMKtPxXp5HnNr/seq-rerun-visualizing-eutopia", "pageUrlRelative": "/posts/P5HSLMKtPxXp5HnNr/seq-rerun-visualizing-eutopia", "linkUrl": "https://www.lesswrong.com/posts/P5HSLMKtPxXp5HnNr/seq-rerun-visualizing-eutopia", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Visualizing%20Eutopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Visualizing%20Eutopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5HSLMKtPxXp5HnNr%2Fseq-rerun-visualizing-eutopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Visualizing%20Eutopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5HSLMKtPxXp5HnNr%2Fseq-rerun-visualizing-eutopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5HSLMKtPxXp5HnNr%2Fseq-rerun-visualizing-eutopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/wu/visualizing_eutopia/\">Visualizing Eutopia</a> was originally published on 16 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Visualizing_Eutopia\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying to imagine a Eutopia is actually difficult. But it is worth trying.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g70/seq_rerun_for_the_people_who_are_still_alive/\">For The People Who Are Still Alive</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P5HSLMKtPxXp5HnNr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0755446494839597e-06, "legacy": true, "legacyId": "21004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fwd2qoP9jJtuHhjrd", "gAS5KbNdXJxEX2beY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T15:09:45.703Z", "modifiedAt": null, "url": null, "title": "How to Teach Students to Not Guess the Teacher\u2019s Password?", "slug": "how-to-teach-students-to-not-guess-the-teacher-s-password-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.433Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Petruchio", "createdAt": "2012-09-20T21:52:44.002Z", "isAdmin": false, "displayName": "Petruchio"}, "userId": "Sq97ckSEEWbQ6AxC9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/on97JdnXkhKBYDAoK/how-to-teach-students-to-not-guess-the-teacher-s-password-0", "pageUrlRelative": "/posts/on97JdnXkhKBYDAoK/how-to-teach-students-to-not-guess-the-teacher-s-password-0", "linkUrl": "https://www.lesswrong.com/posts/on97JdnXkhKBYDAoK/how-to-teach-students-to-not-guess-the-teacher-s-password-0", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon97JdnXkhKBYDAoK%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon97JdnXkhKBYDAoK%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon97JdnXkhKBYDAoK%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>As a teacher, I wonder if it is possible to instill this skill into students the skills of rationality and critical thinking. I teach the third grade, and it is not immediately apparent how to apply this with my own class.</p>\n<p>The problems I foresee are as follows:</p>\n<ol>\n<li>Young children often do not the basics on the subject which they are learning, be it math, science, art, religion, literature etc. </li>\n<li>Many children are very shy, and try to give as short of an answer as doable to a verbal prompt. </li>\n<li>Written prompts are arduous, straining the attention span and writing capabilities of the students. This is not a bad thing, but it presents difficulties in the economy of time and material to be presented. </li>\n<li>Attention spans in general are very short. </li>\n<li>Experiments can be very infrequent, and nigh impossible with certain subjects. </li>\n<li>Children, at this age, are likely to take the words of a parent or teacher at face value, and naturally parrot it back. This may be a hard habit to break. </li>\n</ol>\n<p>In the <a href=\"/lw/iq/guessing_the_teachers_password/\">sequences</a>, it is suggested teachers should drill into students <em>words don't count, only anticipation-controllers</em>. How practical is this for an elementary school level? Also appreciated would be any ideas or experiences on how to do this, or how to combat the above problems. Hearing from other teachers would be excellent especially.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "on97JdnXkhKBYDAoK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T15:18:58.618Z", "modifiedAt": null, "url": null, "title": "How to Teach Students to Not Guess the Teacher\u2019s Password?", "slug": "how-to-teach-students-to-not-guess-the-teacher-s-password", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Petruchio", "createdAt": "2012-09-20T21:52:44.002Z", "isAdmin": false, "displayName": "Petruchio"}, "userId": "Sq97ckSEEWbQ6AxC9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BhEtnag5sWdcirxrG/how-to-teach-students-to-not-guess-the-teacher-s-password", "pageUrlRelative": "/posts/BhEtnag5sWdcirxrG/how-to-teach-students-to-not-guess-the-teacher-s-password", "linkUrl": "https://www.lesswrong.com/posts/BhEtnag5sWdcirxrG/how-to-teach-students-to-not-guess-the-teacher-s-password", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBhEtnag5sWdcirxrG%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Teach%20Students%20to%20Not%20Guess%20the%20Teacher%E2%80%99s%20Password%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBhEtnag5sWdcirxrG%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBhEtnag5sWdcirxrG%2Fhow-to-teach-students-to-not-guess-the-teacher-s-password", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>As a teacher, I wonder if it is possible to instill this skill into students the skills of rationality and critical thinking. I teach the third grade, and it is not immediately apparent how to apply this with my own class.</p>\r\n<p>The problems I foresee are as follows:</p>\r\n<ul>\r\n<li>Young children often do not know the basics on the subject which they are learning, be it math, science, art, religion, literature etc. </li>\r\n<li>Many children are very shy, and try to give as short of an answer as doable to a verbal prompt. </li>\r\n<li>Written prompts are arduous, straining the attention span and writing capabilities of the students. This is not a bad thing, but it presents difficulties in the economy of time and material to be presented. </li>\r\n<li>Attention spans in general are very short. </li>\r\n<li>Experiments can be very infrequent, and nigh impossible with certain subjects. </li>\r\n<li>Children, at this age, are likely to take the words of a parent or teacher at face value, and naturally parrot it back. This may be a hard habit to break. </li>\r\n</ul>\r\n<p>In the <a href=\"/lw/iq/guessing_the_teachers_password/\">sequences</a>, it is suggested teachers should drill into students <em>words don't count, only anticipation-controllers</em>. How practical is this for an elementary school level? Also appreciated would be any ideas or experiences on how to do this, or how to combat the above problems. Hearing from other teachers would be excellent especially.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "SJFsFfFhE6m2ThAYJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BhEtnag5sWdcirxrG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 35, "extendedScore": null, "score": 1.07592265313304e-06, "legacy": true, "legacyId": "21008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T16:28:07.762Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Bielefeld, Montpellier, Vancouver", "slug": "weekly-lw-meetups-bielefeld-montpellier-vancouver", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5sxDqPgWfMQz8mdR9/weekly-lw-meetups-bielefeld-montpellier-vancouver", "pageUrlRelative": "/posts/5sxDqPgWfMQz8mdR9/weekly-lw-meetups-bielefeld-montpellier-vancouver", "linkUrl": "https://www.lesswrong.com/posts/5sxDqPgWfMQz8mdR9/weekly-lw-meetups-bielefeld-montpellier-vancouver", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Bielefeld%2C%20Montpellier%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Bielefeld%2C%20Montpellier%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sxDqPgWfMQz8mdR9%2Fweekly-lw-meetups-bielefeld-montpellier-vancouver%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Bielefeld%2C%20Montpellier%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sxDqPgWfMQz8mdR9%2Fweekly-lw-meetups-bielefeld-montpellier-vancouver", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sxDqPgWfMQz8mdR9%2Fweekly-lw-meetups-bielefeld-montpellier-vancouver", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 485, "htmlBody": "<p><strong>This summary was posted to LW Main on December 28th. The following week's summary is <a href=\"/lw/g7l/weekly_lw_meetups_austin_berlin_brussels_buenos/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/hd\">Vancouver New Year!:&nbsp;<span class=\"date\">30 December 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/hc\">Bielefeld Meetup: <span class=\"date\">02 January 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/h7\">Montpellier: Tentative first meetup:&nbsp;<span class=\"date\">03 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/h1\">Brussels meetup:&nbsp;<span class=\"date\">05 January 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/hb\">Moscow: Applied Rationality in a New Year:&nbsp;<span class=\"date\">05 January 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/h8\">Washington, DC Meetup with Special Guest:&nbsp;<span class=\"date\">06 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ha\">Berlin Meetup: Bayes:&nbsp;<span class=\"date\">09 January 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/gy\"></a><span class=\"date\">None this week!</span><a href=\"/meetups/gy\"></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5sxDqPgWfMQz8mdR9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0759640013901524e-06, "legacy": true, "legacyId": "20900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DykwDscECRFpXJfcr", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T19:51:28.863Z", "modifiedAt": "2022-02-04T14:14:49.604Z", "url": null, "title": "Second-Order Logic: The Controversy", "slug": "second-order-logic-the-controversy", "viewCount": null, "lastCommentedAt": "2018-05-21T23:41:24.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SWn4rqdycu83ikfBa/second-order-logic-the-controversy", "pageUrlRelative": "/posts/SWn4rqdycu83ikfBa/second-order-logic-the-controversy", "linkUrl": "https://www.lesswrong.com/posts/SWn4rqdycu83ikfBa/second-order-logic-the-controversy", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Second-Order%20Logic%3A%20The%20Controversy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASecond-Order%20Logic%3A%20The%20Controversy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWn4rqdycu83ikfBa%2Fsecond-order-logic-the-controversy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Second-Order%20Logic%3A%20The%20Controversy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWn4rqdycu83ikfBa%2Fsecond-order-logic-the-controversy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWn4rqdycu83ikfBa%2Fsecond-order-logic-the-controversy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4630, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"https://www.lesswrong.com/lw/g1y/godels_completeness_and_incompleteness_theorems/\">Godel&#x27;s Completeness and Incompleteness Theorems</a></p><p>&quot;So the question you asked me last time was, &#x27;Why does anyone bother with first-order logic at all, if second-order logic is so much more powerful?&#x27;&quot;</p><p>Right. If first-order logic can&#x27;t talk about finiteness, or distinguish the size of the integers from the size of the reals, why even bother?</p><p>&quot;The first thing to realize is that first-order theories can still have a <em>lot </em>of power. First-order arithmetic does narrow down the possible models by a lot, even if it doesn&#x27;t narrow them down to a <em>single </em>model. You can prove things like the existence of an infinite number of primes, because <em>every </em>model of the first-order axioms has an infinite number of primes. First-order arithmetic is never going to prove anything that&#x27;s <em>wrong </em>about the standard numbers. Anything that&#x27;s true in <em>all </em>models of first-order arithmetic will also be true in the <em>particular </em>model we call the standard numbers.&quot;</p><p>Even so, if first-order theory is strictly weaker, why bother? Unless second-order logic is just as incomplete relative to third-order logic, which is weaker than fourth-order logic, which is weaker than omega-order logic -</p><p>&quot;No, surprisingly enough - there&#x27;s tricks for making second-order logic encode any proposition in third-order logic and so on. If there&#x27;s a collection of third-order axioms that characterizes a model, there&#x27;s a collection of second-order axioms that characterizes the same model. Once you make the jump to second-order logic, you&#x27;re <em>done </em>- so far as anyone knows (so far as I know) there&#x27;s <em>nothing </em>more powerful than second-order logic in terms of <em>which models it can characterize</em>.&quot;</p><p>Then if there&#x27;s one spoon which can eat anything, why not just use the spoon?</p><p>&quot;Well... this gets into complex issues. There are mathematicians who don&#x27;t believe there <em>is </em>a spoon when it comes to second-order logic.&quot;</p><p>Like there are mathematicians who don&#x27;t believe in infinity?</p><p>&quot;Kind of. Look, suppose you <em>couldn&#x27;t</em> use second-order logic - you belonged to a species that doesn&#x27;t have second-order logic, or anything like it. Your species doesn&#x27;t have any native mental intuition you could use to construct the notion of &#x27;all properties&#x27;. And then suppose that, after somebody used first-order set theory to prove that first-order arithmetic had many possible models, you stood around shouting that you believed in only <em>one </em>model, what you called the <em>standard </em>model, but you couldn&#x27;t explain what made this model different from any other model -&quot;</p><p>Well... a lot of times, even in math, we make statements that genuinely mean something, but take a while to figure out how to define. I think somebody who talked about &#x27;the numbers&#x27; would mean something even before second-order logic was invented.</p><p>&quot;But here the hypothesis is that you belong to a species that <em>can&#x27;t</em> invent second-order logic, or think in second-order logic, or anything like it.&quot;</p><p>Then I suppose you want me to draw the conclusion that this hypothetical alien is just standing there shouting about standardness, but its words don&#x27;t mean anything because they have no way to pin down one model as opposed to another one. And I expect this species is also magically forbidden from talking about all possible subsets of a set?</p><p>&quot;Yeah. They can&#x27;t talk about the largest powerset, just like they can&#x27;t talk about the smallest model of Peano arithmetic.&quot;</p><p>Then you could arguably deny that shouting about the &#x27;standard&#x27; numbers would mean anything, to the members of this particular species. You might as well shout about the &#x27;fleem&#x27; numbers, I guess.</p><p>&quot;Right. Even if all the members of this species <em>did </em>have a built-in sense that there was a special model of first-order arithmetic that was fleemer than any other model, if that fleem-ness wasn&#x27;t bound to anything else, it would be meaningless. Just a floating word. Or if all you could do was define fleemness as floobness and floobness as fleemness, you&#x27;d have a loop of floating words; and that might give you the impression that each particular word had a meaning, but the loop as a whole wouldn&#x27;t be connected to reality. That&#x27;s why it doesn&#x27;t help to say that the standard model of numbers is the smallest among all possible models of Peano arithmetic, if you can&#x27;t define &#x27;smallest possible&#x27; any more than you can define &#x27;connected chain&#x27; or &#x27;finite number of predecessors&#x27;.&quot;</p><p>But second-order logic <em>does </em>seem to have consequences first-order logic doesn&#x27;t. Like, what about all that Godelian stuff? Doesn&#x27;t second-order arithmetic semantically imply... its own Godel statement? Because the unique model of second-order arithmetic doesn&#x27;t contain any number encoding a proof of a contradiction from second-order arithmetic? Wait, now I&#x27;m confused.</p><p>&quot;No, that&#x27;s correct. It&#x27;s not paradoxical, because there&#x27;s no effective way of finding all the <em>semantic </em>implications of a collection of second-order axioms. There&#x27;s no analogue of Godel&#x27;s Completeness Theorem for second-order logic - no <em>syntactic</em> system for deriving <em>all </em>the semantic consequences. Second-order logic is <em>sound</em>, in the sense that anything syntactically provable from a set of premises, is true in any model obeying those premises. But second-order logic isn&#x27;t <em>complete</em>; there are semantic consequences you can&#x27;t derive. If you take second-order logic at face value, there&#x27;s no effectively computable way of deriving all the consequences of what you say you &#x27;believe&#x27;... which is a major reason some mathematicians are suspicious of second-order logic. What does it mean to believe something whose consequences you can&#x27;t derive?&quot;</p><p>But second-order logic clearly has <em>some </em>experiential consequences first-order logic doesn&#x27;t. Suppose I build a Turing machine that looks for proofs of a contradiction from first-order Peano arithmetic. If PA&#x27;s consistency isn&#x27;t provable in PA, then by the Completeness Theorem there must exist nonstandard models of PA where this machine halts after finding a proof of a contradiction. So first-order logic doesn&#x27;t tell me that this machine runs forever - maybe it has nonstandard halting times, i.e., it runs at all standard N, but halts at -2* somewhere along a disconnected chain. Only second-order logic tells me there&#x27;s no proof of PA&#x27;s inconsistency <em>and therefore</em> this machine runs forever. Only second-order logic tells me I should <em>expect to see this machine keep runnin</em>g, and <em>not expect</em> - note falsifiability - that the machine ever halts.</p><p>&quot;Sure, you just used a second-order theory to derive a consequence you didn&#x27;t get from a first-order theory. But that&#x27;s not the same as saying that you can <em>only </em>get that consequence using second-order logic. Maybe another first-order theory would give you the same prediction.&quot;</p><p>Like what?</p><p>&quot;Well, for one thing, first-order <em>set theory</em> can prove that first-order <em>arithmetic </em>has a model. Zermelo-Fraenkel set theory can prove the existence of a set such that all the first-order Peano axioms are true about that set. It follows within ZF that sound reasoning on first-order arithmetic will never prove a contradiction. And since ZF can prove that the syntax of classical logic is sound -&quot;</p><p>What does it mean for <em>set theory</em> to prove that <em>logic </em>is sound!?</p><p>&quot;ZF can quote formulas as structured, and encode models as sets, and then represent a finite ZF-formula which says whether or not a set of quoted formulas is true about a model. ZF can then prove that no step of classical logic goes from premises that are true inside a set-model, to premises that are false inside a set-model. In other words, ZF can represent the idea &#x27;formula X is semantically true in model Y&#x27; and &#x27;these syntactic rules never produce semantically false statements from semantically true statements&#x27;.&quot;</p><p>Wait, then why can&#x27;t ZF prove <em>itself </em>consistent? If ZF believes in all the axioms of ZF, and it believes that logic never produces false statements from true statements -</p><p>&quot;Ah, but ZF can&#x27;t prove that there exists any set which is a model of ZF, so it can&#x27;t prove the ZF-axioms are consistent. ZF <em>shouldn&#x27;t</em> be able to prove that some set is a model of ZF, because that&#x27;s not true in all models. Many models of ZF don&#x27;t contain any <em>individual </em>set well-populated enough for that one set to be a model of ZF all by itself.&quot;</p><p>I&#x27;m kind of surprised in a Godelian sense that ZF <em>can </em>contain sets as large as the universe of ZF. Doesn&#x27;t any given set have to be smaller than the whole universe?</p><p>&quot;Inside <em>any particular model</em> of ZF, every set <em>within </em>that model is smaller than that model. But not all models of ZF are the same size; in fact, models of ZF of <em>every </em>size exist, by the Lowenheim-Skolem theorem. So you can have <em>some </em>models of ZF - some universes in which all the elements collectively obey the ZF-relations - containing individual sets which are larger than <em>other </em>entire models of ZF. A set that large is called a <em>Grothendieck universe</em> and assuming it exists is equivalent to assuming the existence of &#x27;strongly inaccessible cardinals&#x27;, sizes so large that you provably can&#x27;t prove inside set theory that anything that large exists.&quot;</p><p>Whoa.</p><p>(Pause.)</p><p>But...</p><p>&quot;But?&quot;</p><p>I agree you&#x27;ve shown that <em>one </em>experiential consequence of second-order arithmetic - namely that a machine looking for proofs of inconsistency from PA, won&#x27;t be seen to halt - can be derived from first-order set theory. Can you get <em>all </em>the consequences of second-order arithmetic in some particular first-order theory?</p><p>&quot;You can&#x27;t get all the <em>semantic </em>consequences of second-order logic, taken at face value, in <em>any </em>theory or <em>any </em>computable reasoning. What about the halting problem? Taken at face value, it&#x27;s a semantic consequence of second-order logic that any given Turing machine either halts or doesn&#x27;t halt -&quot;</p><p>Personally I find it rather intuitive to imagine that a Turing machine either halts or doesn&#x27;t halt. I mean, if I&#x27;m walking up to a machine and watching it run, telling me that its halting or not-halting &#x27;isn&#x27;t entailed by my axioms&#x27; strikes me as not describing any actual experience I can have with the machine. Either I&#x27;ll see it halt eventually, or I&#x27;ll see it keep running forever.</p><p>&quot;My point is that the statements we <em>can </em>derive from the syntax of current second-order logic is limited by that syntax. And by the halting problem, we shouldn&#x27;t ever expect a computable syntax that gives us <em>all </em>the semantic consequences.  There&#x27;s no possible theory you can <em>actually use</em> to get a correct advance prediction about whether an arbitrary Turing machine halts.&quot;</p><p>Okay. I agree that no computable reasoning, on second-order logic or anything else, should be able to solve the halting problem. Unless time travel is possible, but even then, you shouldn&#x27;t be able to solve the expanded halting problem for machines that use time travel.</p><p>&quot;Right, so the <em>syntax </em>of second-order logic can&#x27;t prove everything. And in fact, it turns out that, in terms of what you can <em>prove syntactically</em> using the standard syntax, second-order logic is identical to a many-sorted first-order logic.&quot;</p><p>Huh?</p><p>&quot;Suppose you have a first-order logic - one that doesn&#x27;t claim to be able to quantify over all possible predicates - which does allow the universe to contain two different sorts of things. Say, the logic uses lower-case letters for all type-1 objects and upper-case letters for all type-2 objects. Like, &#x27;\u2200x: x = x&#x27; is a statement over all type-1 objects, and &#x27;\u2200Y: Y = Y&#x27; is a statement over all type-2 objects. But aside from that, you use the same syntax and proof rules as before.&quot;</p><p>Okay...</p><p>&quot;Now add an element-relation x\u2208Y, saying that x is an element of Y, and add some first-order axioms for making the type-2 objects behave like collections of type-1 objects, including axioms for making sure that most describable type-2 collections exist - i.e., the collection X containing just x is guaranteed to exist, and so on. What you can <em>prove syntactically</em> in this theory will be identical to what you can prove using the standard syntax of second-order logic - even though the theory doesn&#x27;t claim that <em>all possible</em> collections of type-1s are type-2s, and the theory will have models where many &#x27;possible&#x27; collections are missing from the type-2s.&quot;</p><p>Wait, now you&#x27;re saying that second-order logic is no more powerful than first-order logic?</p><p>&quot;I&#x27;m saying that the supposed power of second-order logic derives from <em>interpreting</em> it a particular way, and taking on faith that when you quantify over <em>all properties</em>, you&#x27;re actually talking about all properties.&quot;</p><p>But then second-order arithmetic is no more powerful than first-order arithmetic in terms of what it can actually <em>prove</em>?</p><p>&quot;2nd-order arithmetic is <em>way </em>more powerful than first-order arithmetic. But that&#x27;s because first-order set theory is more powerful than arithmetic, and adding the syntax of second-order logic corresponds to adding axioms with set-theoretic properties. In terms of which consequences can be <em>syntactically </em>proven, second-order arithmetic is more powerful than first-order arithmetic, but <em>weaker</em> than first-order set theory. First-order set theory can prove the existence of a model of second-order arithmetic - ZF can prove there&#x27;s a collection of numbers and sets of numbers which models a many-sorted logic with syntax corresponding to second-order logic - and so ZF can prove second-order arithmetic consistent.&quot;</p><p>But first-order logic, including first-order set theory, can&#x27;t even <em>talk about</em> the standard numbers!</p><p>&quot;Right, but first-order set theory can <em>syntactically prove </em>more statements about &#x27;numbers&#x27; than second-order arithmetic can prove. And when you combine that with the <em>semantic </em>implications of second-order arithmetic not being computable, and with any second-order logic being syntactically identical to a many-sorted first-order logic, and first-order logic having neat properties like the Completeness Theorem... well, you can see why some mathematicians would want to give up entirely on this whole second-order business.&quot;</p><p>But if you deny second-order logic you <em>can&#x27;t even say the word &#x27;finite&#x27;</em>. You would have to believe the word &#x27;finite&#x27; was a <em>meaningless noise</em>.</p><p>&quot;You&#x27;d define finiteness relative to whatever first-order model you were working in. Like, a set might be &#x27;finite&#x27; only on account of the model not containing any one-to-one mapping from that set onto a smaller subset of itself -&quot;</p><p>But that set wouldn&#x27;t <em>actually </em>be finite. There wouldn&#x27;t actually be, like, only a billion objects in there. It&#x27;s just that all the mappings which could <em>prove </em>the set was infinite would be mysteriously missing.</p><p>&quot;According to some <em>other </em>model, maybe. But since there is no one true model -&quot;</p><p>How is this not crazy talk along the lines of &#x27;there is no one true reality&#x27;? Are you saying there&#x27;s no <em>really </em>smallest set of numbers closed under succession, without all the extra infinite chains? Doesn&#x27;t talking about how these theories have multiple possible models, imply that those possible models are <em>logical thingies</em> and one of them actually <em>does </em>contain the largest powerset and the smallest integers?</p><p>&quot;The mathematicians who deny second-order logic would see that reasoning as invoking an implicit background universe of set theory. Everything you&#x27;re saying makes sense <em>relative </em>to some <em>particular model</em> of set theory, which would contain possible models of Peano arithmetic as sets, and could look over those sets and pick out the smallest <em>in that model</em>. Similarly, that set theory could look over a proposed model for a many-sorted logic, and say whether there were any subsets <em>within </em>the larger universe which were missing from the many-sorted model. Basically, your brain is insisting that it lives inside some <em>particular </em>model of set theory. And then, from that standpoint, you could look over some <em>other </em>set theory and see how it was missing subsets that <em>your</em> theory had.&quot;</p><p>Argh! No, damn it, I live in the set theory that <em>really does</em> have all the subsets, with no mysteriously missing subsets or mysterious extra numbers, or denumerable collections of all possible reals that could like totally map onto the integers if the mapping that did it hadn&#x27;t gone missing in the Australian outback -</p><p>&quot;But <em>everybody </em>says that.&quot;</p><p>Okay...</p><p>&quot;Yeah?&quot;</p><p>Screw set theory. I live in the <em>physical universe</em> where when you run a Turing machine, and keep watching forever <em>in the physical universe</em>, you never <em>experience a time</em> where that Turing machine outputs a proof of the inconsistency of Peano Arithmetic. Furthermore, I live in a universe where space is <em>actually </em>composed of a real field and space is <em>actually </em>infinitely divisible and contains <em>all </em>the points between A and B, rather than space only containing a denumerable number of points whose existence can be proven from the first-order axiomatization of the real numbers. So to talk about <em>physics </em>- forget about mathematics - I&#x27;ve got to use second-order logic.</p><p>&quot;Ah. You know, that particular response is not one I have seen in the previous literature.&quot;</p><p>Yes, well, I&#x27;m not a pure mathematician. When I ask whether I want an Artificial Intelligence to think in second-order logic or first-order logic, I wonder how that affects what the AI does in <em>the actual physical universe</em>. Here in the <em>actual physical universe</em> where times are followed by successor times, I <em>strongly suspect</em> that we should only expect to experience <em>standard </em>times, and not experience any nonstandard times. I think time is <em>connected</em>, and global connectedness is a property I can only talk about using second-order logic. I think that every <em>particular</em> time is finite, and yet time has no upper bound - that there are all finite times, but only finite times - and that&#x27;s a property I can only characterize using second-order logic.</p><p>&quot;But if you can&#x27;t ever tell the difference between standard and nonstandard times? I mean, <em>local </em>properties of time can be described using first-order logic, and you can&#x27;t directly <em>see </em>global properties like &#x27;connectedness&#x27; -&quot;</p><p>But I <em>can </em>tell the difference. There are only nonstandard times where a proof-checking machine, running forever, outputs a proof of inconsistency from the Peano axioms. So I don&#x27;t expect to experience seeing a machine do that, since I expect to experience only standard times.</p><p>&quot;Set theory can also prove PA consistent. If you use set theory to define time, you similarly won&#x27;t expect to see a time where PA is proven inconsistent - those nonstandard integers don&#x27;t exist in any model of ZF.&quot;</p><p>Why should I anticipate that my physical universe is restricted to having only the nonstandard times allowed by a <em>more </em>powerful set theory, instead of nonstandard times allowed by first-order arithmetic? If I then talk about a nonstandard time where a proof-enumerating machine proves ZF inconsistent, will you say that only nonstandard times allowed by some still more powerful theory can exist? I think it&#x27;s clear that the way you&#x27;re deciding <a href=\"https://www.lesswrong.com/lw/i4/belief_in_belief/\">which experimental outcomes you&#x27;ll have to excuse</a>, is by secretly assuming that <em>only standard times</em> exist regardless of which theory is required to narrow it down.</p><p>&quot;Ah... hm. Doesn&#x27;t physics say this universe is going to run out of negentropy before you can do an infinite amount of computation? Maybe there&#x27;s only a bounded amount of time, like it stops before googolplex or something. That can be characterized by first-order theories.&quot;</p><p>We don&#x27;t know that for certain, and I wouldn&#x27;t want to build an AI that just <em>assumed</em> lifespans had to be finite, in case it was wrong. Besides, should I use a different <em>logic </em>than if I&#x27;d found myself in Conway&#x27;s Game of Life, or something else really infinite? Wouldn&#x27;t the same sort of creatures evolve in that universe, having the same sort of math debates?</p><p>&quot;Perhaps no universe like that <em>can </em>exist; perhaps only finite universes can exist, because only finite universes can be uniquely characterized by first-order logic.&quot;</p><p>You just used the word &#x27;finite&#x27;! Furthermore, taken at face value, our own universe <em>doesn&#x27;t</em> look like it has a finite collection of entities related by first-order logical rules. Space and time both look like infinite collections of points - continuous collections, which is a second-order concept - and then to characterize the <em>size </em>of that infinity we&#x27;d need second-order logic. I mean, by the Lowenheim-Skolem theorem, there aren&#x27;t just <em>denumerable </em>models of first-order axiomatizations of the reals, there&#x27;s also <em>unimaginably large cardinal infinities</em> which obey the same premises, and that&#x27;s a possibility straight out of H. P. Lovecraft. Especially if there are any <em>things </em>hiding in the <em>invisible cracks of space</em>.&quot;</p><p>&quot;How could <em>you </em>tell if there were inaccessible cardinal quantities of points hidden inside a straight line? And anything that <em>locally </em>looks continuous each time you try to split it at a describable point, can be axiomatized by a first-order schema for continuity.&quot;</p><p>That brings up another point: Who&#x27;d <em>really </em>believe that the reason Peano arithmetic works on physical times, is because that whole infinite axiom <em>schema </em>of induction, containing for every \u03a6 a <em>separate </em>rule saying...</p><p>(\u03a6(0) \u2227 (\u2200x: \u03a6(x) \u2192 \u03a6(Sx))) \u2192 (\u2200n: \u03a6(n))</p><p>...was used to specify our universe? How improbable would it be for an<em> infinitely long</em> list of rules to be true, if there wasn&#x27;t a unified reason for all of them? It seems much more likely that the <em>real </em>reason first-order PA works to describe time, is that all <em>properties </em>which are true at a starting time and true of the successor of any time where they&#x27;re true, are true of all later times; and this generalization over <em>properties</em> makes induction hold for <em>first-order formulas</em> as a special case. If my native thought process is first-order logic, I wouldn&#x27;t see the connection between each individual formula in the axiom schema - it would take separate evidence to convince me of each one - they would feel like independent mathematical facts. But after doing <em>scientific </em>induction over the fact that many <em>properties </em>true at zero, with succession preserving truth, seem to be true everywhere - after generalizing the simple, compact <em>second-order</em> theory of numbers and times - <em>then </em>you could invent an infinite first-order theory to approximate it.</p><p>&quot;Maybe that just says you need to adjust whatever theory of scientific induction you&#x27;re using, so that it can more easily induct infinite axiom schemas.&quot;</p><p>But why the heck would you <em>need </em>to induct infinite axiom schemas in the first place, if Reality <em>natively </em>ran on first-order logic? Isn&#x27;t it far more likely that the way we ended up with these infinite lists of axioms was that Reality was manufactured - forgive the anthropomorphism - that Reality was manufactured using an underlying schema in which time is a <em>connected </em>series of events, and space is a <em>continuous</em> field, and these are properties which happen to require second-order logic for humans to describe? I mean, if you picked out first-order theories at random, what&#x27;s the chance we&#x27;d end up inside an infinitely long axiom schema that just <em>happened </em>to look like the projection of a compact second-order theory? Aren&#x27;t we ignoring a sort of <em>hint</em>?</p><p>&quot;A hint to what?&quot;</p><p>Well, I&#x27;m not that sure myself, at this depth of philosophy. But I would currently say that finding ourselves in a physical universe where times have successor times, and space looks continuous, seems like a possible <em>hint </em>that the Tegmark Level IV multiverse - or the way Reality was manufactured, or whatever - might look more like <em>causal universes characterizable by compact second-order theories</em> than <em>causal universes characterizable by first-order theories</em>.</p><p>&quot;But since any second-order theory can just as easily be <em>interpreted </em>as a many-sorted first-order theory with quantifiers that can range over either elements or sets of elements, how could using second-order syntax actually <em>improve </em>an Artificial Intelligence&#x27;s ability to handle a reality like that?&quot;</p><p>Good question. One obvious answer is that the AI would be able to induct what <em>you</em> would call an infinite axiom schema, as a single axiom - a simple, finite hypothesis.</p><p>&quot;There&#x27;s all <em>sorts </em>of obvious hacks to scientific induction of first-order axioms which would let you assign nonzero probability to computable infinite sequences of axioms -&quot;</p><p>Sure. So beyond that... I would currently guess that the basic assumption behind &#x27;behaving as if&#x27; second-order logic is true, says that the AI should act as if only the &#x27;actually smallest&#x27; numbers will ever appear in physics, relative to some &#x27;true&#x27; mathematical universe that it thinks it lives in, but knows it can&#x27;t fully characterize. Which is roughly what I&#x27;d say human mathematicians do when they take second-order logic at face value; they assume that there&#x27;s some <em>true </em>mathematical universe in the background, and that second-order logic lets them talk about it.</p><p>&quot;And what behaviorally, experimentally distinguishes the hypothesis, &#x27;I live in the true ultimate math with fully populated powersets&#x27; from the hypothesis, &#x27;There&#x27;s some random model of first-order set-theory axioms I happen to be living in&#x27;?&quot;</p><p>Well... one behavioral consequence is suspecting that your time obeys an infinitely long list of first-order axioms with induction schemas for <em>every formula</em>. And then moreover believing that you&#x27;ll never experience a time when a proof-checking machine outputs a proof that Zermelo-Fraenkel set theory is inconsistent - even though there&#x27;s <em>provably </em>some models with times like that, which fit the axiom schema you just inducted. That sounds like secretly believing that there&#x27;s a background &#x27;true&#x27; set of numbers that you think characterizes physical time, and that it&#x27;s the <em>smallest </em>such set. Another suspicious behavior is that as soon as you suspect Zermelo-Fraenkel set theory is consistent, you suddenly expect not to experience any <em>physical </em>time which ZF proves isn&#x27;t a standard number. You don&#x27;t think you&#x27;re in the nonstandard time of some weaker theory like Peano arithmetic. You think you&#x27;re in the minimal time expressible by <em>any and all theories</em>, so as soon as ZF can prove some number doesn&#x27;t exist in the minimal set, you think that &#x27;real time&#x27; lacks such a number. All of these sound like behaviors you&#x27;d carry out if you thought there was a single &#x27;true&#x27; mathematical universe that provided the best model for describing all <em>physical </em>phenomena, like time and space, which you encounter - and believing that this &#x27;true&#x27; backdrop used the <em>largest </em>powersets and <em>smallest </em>numbers.</p><p>&quot;How <em>exactly </em>do you formalize all that reasoning, there? I mean, how would you <em>actually </em>make an AI reason that way?&quot;</p><p>Er... I&#x27;m still working on that part.</p><p>&quot;That makes your theory a bit hard to criticize, don&#x27;t you think? Personally, I wouldn&#x27;t be surprised if any such <em>formalized</em> reasoning looked just like believing that you live inside a first-order set theory.&quot;</p><p>I suppose I wouldn&#x27;t be <em>too </em>surprised either - it&#x27;s hard to argue with the results on many-sorted logics. But if comprehending the physical universe is best done by assuming that real phenomena are characterized by a &#x27;true&#x27; mathematics containing <em>the </em>powersets and <em>the </em>natural numbers - and thus expecting that no mathematical model we can formulate will ever contain any larger powersets or smaller numbers than those of the &#x27;true&#x27; backdrop to physics - then I&#x27;d call that a moral victory for second-order logic. In first-order logic we aren&#x27;t even supposed to be able to talk about such things.</p><p>&quot;Really? To me that sounds like believing you live inside a model of first-order set theory, and believing that all models of any theories <em>you </em>can invent must <em>also </em>be sets in the larger model. You can prove the Completeness Theorem inside ZF plus the Axiom of Choice, so ZFC already proves that all consistent theories have models which are sets, although of course it can&#x27;t prove that ZFC itself is such a theory. So - anthropomorphically speaking - no model of ZFC <em>expects </em>to encounter a theory that has fewer numbers or larger powersets than itself. No model of ZFC expects to encounter any quoted-model, any set that a quoted theory entails, which contains larger powersets than the ones in its own Powerset Axiom. A first-order set theory can even make the leap from the finite statement, &#x27;P is true of all my subsets of X&#x27;, to believing, &#x27;P is true of all my subsets of X that can be described by this denumerable collection of formulas&#x27; - it can encompass the jump from a single axiom over &#x27;all my subsets&#x27;, to a quoted axiom <em>schema </em>over formulas. I&#x27;d sum all that up by saying, &#x27;second-order logic is how first-order set theory feels from the inside&#x27;.&quot;</p><p>Maybe. Even in the event that neither human nor superintelligent cognition will ever be able to &#x27;properly talk about&#x27; unbounded finite times, global connectedness, particular infinite cardinalities, or true spatial continuity, it doesn&#x27;t follow that Reality is similarly limited in which physics it can privilege.</p><p></p><p>Part of the sequence <em><a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a></em></p><p>Previous post: &quot;<a href=\"https://www.lesswrong.com/lw/g1y/godels_completeness_and_incompleteness_theorems/\">Godel&#x27;s Completeness and Incompleteness Theorems</a>&quot;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 2, "AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SWn4rqdycu83ikfBa", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 47, "extendedScore": null, "score": 0.000105, "legacy": true, "legacyId": "21011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "mixed-reference-the-great-reductionist-project", "canonicalPrevPostSlug": "godel-s-completeness-and-incompleteness-theorems", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 192, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GZjGtd35vhCnzSQKy", "CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-01-04T19:51:28.863Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-04T21:09:23.413Z", "modifiedAt": null, "url": null, "title": "How to update P(x this week), upon hearing P(x next month) = 99.5%?", "slug": "how-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.465Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iDQveMdQMw75uvW32/how-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "pageUrlRelative": "/posts/iDQveMdQMw75uvW32/how-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "linkUrl": "https://www.lesswrong.com/posts/iDQveMdQMw75uvW32/how-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "postedAtFormatted": "Friday, January 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20update%20P(x%20this%20week)%2C%20upon%20hearing%20P(x%20next%20month)%20%3D%2099.5%25%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20update%20P(x%20this%20week)%2C%20upon%20hearing%20P(x%20next%20month)%20%3D%2099.5%25%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDQveMdQMw75uvW32%2Fhow-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20update%20P(x%20this%20week)%2C%20upon%20hearing%20P(x%20next%20month)%20%3D%2099.5%25%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDQveMdQMw75uvW32%2Fhow-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDQveMdQMw75uvW32%2Fhow-to-update-p-x-this-week-upon-hearing-p-x-next-month-99-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 321, "htmlBody": "<p>Suppose you want to assign a probability that a government will fall (ie the Prime Minister resigns) before the end of the year. Lacking any particular information - I haven't even told you which government it is - you say \"Obviously, it's 50% - either it happens or not\" (or perhaps \"Oh, say, 10%, governments can usually rely on lasting a year at least\"), put that prediction into your registry, and go on with your life. Then, on December 1st, you hear that the Prime Minister in question has promised to resign and call an election in March of <em>next</em> year. How should this affect your probability that he will resign before the end of <em>this</em> year?</p>\n<p>I see several arguments:</p>\n<p>1. Having gotten this public commitment out of him, his opponents have no particular reason to push his government further. It should become more stable for the finite time it has left. My probability of a resignation in December should go down.</p>\n<p>2. His opponents were able to extract such a promise; it follows that he cannot be quite confident in his ability to survive a vote of no confidence. Such a signal of weakness might easily lead to a \"blood-in-the-water\" effect whereby his opponents become more aggressive and go for the immediate kill. His government will surely fall before this attempted compromise date; my probability should go up.</p>\n<p>3. The March date wasn't chosen at random. Presumably there is something the PM thinks he can get accomplished if he retains his position until March, but not if he resigns right away. So, presumably, his opponents will be all the more eager for him to resign before he gets it done, whatever it is; they will put more resources into toppling him. Again, my probability should go up.</p>\n<p>&nbsp;</p>\n<p>The question is not hypothetical: I was faced with precisely this problem in December, and got it wrong. I'd like to see how others think about it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iDQveMdQMw75uvW32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.076132204574545e-06, "legacy": true, "legacyId": "21012", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T01:27:30.044Z", "modifiedAt": null, "url": null, "title": "Just One Sentence", "slug": "just-one-sentence", "viewCount": null, "lastCommentedAt": "2014-03-12T08:03:51.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8SJEGBGsKSoFMMfr2/just-one-sentence", "pageUrlRelative": "/posts/8SJEGBGsKSoFMMfr2/just-one-sentence", "linkUrl": "https://www.lesswrong.com/posts/8SJEGBGsKSoFMMfr2/just-one-sentence", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Just%20One%20Sentence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJust%20One%20Sentence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8SJEGBGsKSoFMMfr2%2Fjust-one-sentence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Just%20One%20Sentence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8SJEGBGsKSoFMMfr2%2Fjust-one-sentence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8SJEGBGsKSoFMMfr2%2Fjust-one-sentence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 354, "htmlBody": "<p>So apparently Richard Feynman once said:</p>\n<blockquote>\n<p>If, in some cataclysm, all scientific knowledge were to be destroyed, and only one sentence passed on to the next generation of creatures, what statement would contain the most information in the fewest words? I believe it is the atomic hypothesis (or atomic fact, or whatever you wish to call it) that all things are made of atoms &mdash; little particles that move around in perpetual motion, attracting each other when they are a little distance apart, but repelling upon being squeezed into one another. In that one sentence you will see an enormous amount of information about the world, if just a little imagination and thinking are applied.</p>\n</blockquote>\n<p>I could be missing something, but this strikes me as a terrible answer.</p>\n<p>When was the atomic hypothesis confirmed? &nbsp;If I recall correctly, it was only when chemists started noticing that the outputs of chemical reactions tended to factorize a certain way, which is to say that it took <em>millennia&nbsp;</em>after Democritus to get the point where the atomic hypothesis started making clearly relevant experimental predictions.</p>\n<p>How about, \"Stop trying to sound wise and come up with theories that make precise predictions about things you can measure in numbers.\"</p>\n<p>I noticed this on <a href=\"http://marginalrevolution.com/marginalrevolution/2013/01/if-we-could-preserve-only-one-sentence.html\">Marginal Revolution</a>, so I shall also state my candidate for the one most important sentence about macroeconomics: &nbsp;\"You can't eat gold, so figure out how the heck money is relevant to making countries actually produce more or less food.\" &nbsp;This is a pretty large advance on how kings used to think before economics. &nbsp;I mean, Scott Sumner is usually pretty savvy (so is Richard Feynman btw) but his instruction to try to understand money is likely to fall on deaf ears, if it's just that one sentence. &nbsp;Think about money? &nbsp;Everyone wants more money! &nbsp;Yay, money! &nbsp;Let's build more gold mines! &nbsp;And \"In the short run, governments are not households\"? &nbsp;Really, Prof. Cowen, that's what you'd pass on to the next generation as they climb up from the radioactive soil?</p>\n<p>*Cough.* &nbsp;Okay, I'm done. &nbsp;Does anyone want to take their own shot at doing better than Feynman did for their own discipline?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EdDGrAxYcrXnKkDca": 1, "ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8SJEGBGsKSoFMMfr2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 58, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "21013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-01-05T01:27:30.044Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T03:21:35.546Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Metaethics and Really Trying", "slug": "meetup-vancouver-metaethics-and-really-trying", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeeC24mn4WuwvKAAt/meetup-vancouver-metaethics-and-really-trying", "pageUrlRelative": "/posts/yeeC24mn4WuwvKAAt/meetup-vancouver-metaethics-and-really-trying", "linkUrl": "https://www.lesswrong.com/posts/yeeC24mn4WuwvKAAt/meetup-vancouver-metaethics-and-really-trying", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Metaethics%20and%20Really%20Trying&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Metaethics%20and%20Really%20Trying%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeeC24mn4WuwvKAAt%2Fmeetup-vancouver-metaethics-and-really-trying%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Metaethics%20and%20Really%20Trying%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeeC24mn4WuwvKAAt%2Fmeetup-vancouver-metaethics-and-really-trying", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeeC24mn4WuwvKAAt%2Fmeetup-vancouver-metaethics-and-really-trying", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/hl\">Vancouver Metaethics and Really Trying</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 January 2013 01:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2505 W broadway vancouver bc</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We are going to talk about <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">the metaethics sequence</a> and <a rel=\"nofollow\" href=\"http://www.gwern.net/on-really-trying\">this</a> awesome essay by gwern.</p>\n<p>13:00 on Saturday at Benny's Bagels in Kitsilano. Me may have a sign, but probably not.</p>\n<p>If you aren't on already, join us on our <a rel=\"nofollow\" href=\"http://groups.google.com/group/vancouver-rationalists\">mailing list</a>.</p>\n<p>See you there!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/hl\">Vancouver Metaethics and Really Trying</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeeC24mn4WuwvKAAt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0763548651772953e-06, "legacy": true, "legacyId": "21015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Metaethics_and_Really_Trying\">Discussion article for the meetup : <a href=\"/meetups/hl\">Vancouver Metaethics and Really Trying</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 January 2013 01:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2505 W broadway vancouver bc</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We are going to talk about <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">the metaethics sequence</a> and <a rel=\"nofollow\" href=\"http://www.gwern.net/on-really-trying\">this</a> awesome essay by gwern.</p>\n<p>13:00 on Saturday at Benny's Bagels in Kitsilano. Me may have a sign, but probably not.</p>\n<p>If you aren't on already, join us on our <a rel=\"nofollow\" href=\"http://groups.google.com/group/vancouver-rationalists\">mailing list</a>.</p>\n<p>See you there!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Metaethics_and_Really_Trying1\">Discussion article for the meetup : <a href=\"/meetups/hl\">Vancouver Metaethics and Really Trying</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Metaethics and Really Trying", "anchor": "Discussion_article_for_the_meetup___Vancouver_Metaethics_and_Really_Trying", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Metaethics and Really Trying", "anchor": "Discussion_article_for_the_meetup___Vancouver_Metaethics_and_Really_Trying1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T04:35:13.715Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Prolegomena to a Theory of Fun", "slug": "seq-rerun-prolegomena-to-a-theory-of-fun", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rgt5WCh9YPKeSt3gL/seq-rerun-prolegomena-to-a-theory-of-fun", "pageUrlRelative": "/posts/rgt5WCh9YPKeSt3gL/seq-rerun-prolegomena-to-a-theory-of-fun", "linkUrl": "https://www.lesswrong.com/posts/rgt5WCh9YPKeSt3gL/seq-rerun-prolegomena-to-a-theory-of-fun", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Prolegomena%20to%20a%20Theory%20of%20Fun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Prolegomena%20to%20a%20Theory%20of%20Fun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgt5WCh9YPKeSt3gL%2Fseq-rerun-prolegomena-to-a-theory-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Prolegomena%20to%20a%20Theory%20of%20Fun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgt5WCh9YPKeSt3gL%2Fseq-rerun-prolegomena-to-a-theory-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgt5WCh9YPKeSt3gL%2Fseq-rerun-prolegomena-to-a-theory-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>Today's post, <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Prolegomena to a Theory of Fun</a> was originally published on 17 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Fun Theory is an attempt to actually answer questions about eternal boredom that are more often posed and left hanging. Attempts to visualize Utopia are often defeated by standard biases, such as the attempt to imagine a single moment of good news (\"You don't have to work anymore!\") rather than a typical moment of daily life ten years later. People also believe they should enjoy various activities that they actually don't. But since human values have no supernatural source, it is quite reasonable for us to try to understand what we want. There is no external authority telling us that the future of humanity should not be fun.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g7g/seq_rerun_visualizing_eutopia/\">Visualizing Eutopia</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rgt5WCh9YPKeSt3gL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0763989256679354e-06, "legacy": true, "legacyId": "21021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pK4HTxuv6mftHXWC3", "P5HSLMKtPxXp5HnNr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T08:05:49.246Z", "modifiedAt": null, "url": null, "title": "Pigliucci's comment on Yudkowsky's and Dai's stance on morality and logic", "slug": "pigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/59FTfruzw5Nom2A29/pigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "pageUrlRelative": "/posts/59FTfruzw5Nom2A29/pigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "linkUrl": "https://www.lesswrong.com/posts/59FTfruzw5Nom2A29/pigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pigliucci's%20comment%20on%20Yudkowsky's%20and%20Dai's%20stance%20on%20morality%20and%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APigliucci's%20comment%20on%20Yudkowsky's%20and%20Dai's%20stance%20on%20morality%20and%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59FTfruzw5Nom2A29%2Fpigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pigliucci's%20comment%20on%20Yudkowsky's%20and%20Dai's%20stance%20on%20morality%20and%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59FTfruzw5Nom2A29%2Fpigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59FTfruzw5Nom2A29%2Fpigliucci-s-comment-on-yudkowsky-s-and-dai-s-stance-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p><a href=\"http://rationallyspeaking.blogspot.de/2013/01/lesswrong-on-morality-and-logic.html\">Pigliucci</a>:</p>\n<blockquote>\n<p>So morality has a lot to do with logic &mdash; indeed <a href=\"http://rationallyspeaking.blogspot.com/2011/07/on-ethics-part-i-moral-philosophys.html\">I have argued</a> that moral reasoning is a type of applied logical reasoning &mdash; but it is <em>not</em> logic &ldquo;all the way down,&rdquo; it is anchored by certain contingent facts about humanity, bonoboness and so forth.</p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>But, despite Yudkowsky&rsquo;s confident claim, morality isn&rsquo;t a matter of logic &ldquo;all the way down,&rdquo; because it has to start with some axioms, some brute facts about the type of organisms that engage in moral reasoning to begin with. Those facts don&rsquo;t come from physics (though, like everything else, they better be compatible with all the laws of physics), they come from biology. A reasonable theory of ethics, then, can emerge only from a combination of biology (by which I mean not just evolutionary biology, but also cultural evolution) and logic.</blockquote>\n<p>&nbsp;</p>\n<p>http://rationallyspeaking.blogspot.de/2013/01/lesswrong-on-morality-and-logic.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "59FTfruzw5Nom2A29", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 1, "extendedScore": null, "score": 1.0765249524204507e-06, "legacy": true, "legacyId": "21024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T10:33:28.928Z", "modifiedAt": null, "url": null, "title": "Re: Second-Order Logic: The Controversy", "slug": "re-second-order-logic-the-controversy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.447Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/65c2LmHS8RoLTYYXe/re-second-order-logic-the-controversy", "pageUrlRelative": "/posts/65c2LmHS8RoLTYYXe/re-second-order-logic-the-controversy", "linkUrl": "https://www.lesswrong.com/posts/65c2LmHS8RoLTYYXe/re-second-order-logic-the-controversy", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Re%3A%20Second-Order%20Logic%3A%20The%20Controversy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARe%3A%20Second-Order%20Logic%3A%20The%20Controversy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65c2LmHS8RoLTYYXe%2Fre-second-order-logic-the-controversy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Re%3A%20Second-Order%20Logic%3A%20The%20Controversy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65c2LmHS8RoLTYYXe%2Fre-second-order-logic-the-controversy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65c2LmHS8RoLTYYXe%2Fre-second-order-logic-the-controversy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1496, "htmlBody": "<p>\n<p>Response to <a href=\"/lw/g7n/secondorder_logic_the_controversy/\">Second-Order Logic: The Controversy</a>. This started out as a comment, but got fun.</p>\n<p><em>Several days later, the two meet again.</em></p>\n<p>&nbsp;</p>\n<p>\"You know, I still feel uneasy about the discussion we had. I didn't walk away with the feeling that you really appreciated the point that any proof system you propose for 2nd-order logic can equally well be interpreted as a proof system for a first-order many-sorted logic. If an alien took a look at a computer program implementing 2nd-order logic, what reason would they have to conclude that the computer program was an attempt to implement true quantification over properties, with everything our mathematicians take that to mean?\"</p>\n<p>But I told you: I want a system which will be able to consider the hypothesis that time is connected, or perhaps the hypothesis that the universe is finite, and so on.</p>\n<p>\"But externally, it looks like you just have some particular first-order theory in which that term has a meaning. We can talk about the kind of 'finite' that first-order set theory allows us to define. We could suppose your brain implements some particular theory, which you can't verbally articulate, but which is what you mean when you say such things.\"</p>\n<p>Turing machines <em>either halt or don't halt!</em> Like I said before, I don't want a system that treats \"not provable one way or the other\" as a physical possibility. I don't even know what that means.</p>\n<p>\"Of course you would think that. Any system implementing classical logic will think that. First-order set theory doesn't prove it one way or the other for every Turing machine, but it still claims that every Turing machine halts or doesn't halt. It just has incomplete knowledge. Everything you're saying is still consistent with the use of just first-order logic.\"</p>\n<p>But I want to rule out the nonstandard models! I want a system which will not expect, with some probability, to see specific Turing machines halt at a non-standard time.</p>\n<p>\"We can't distinguish that behavior in the real world. Everything you say to me will sound the same. When you predict your observations for specific amounts of time into the future, you are making predictions about the connected chain of time. If you make a prediction about the entire future, we could say the nonstandard models slip in, but that's exactly like what happens due to the limitations of our knowledge in the 2nd-order case. There is nothing magical about saying that we rule out nonstandard models! We must be saying that in some logic, and the proof system of that logic won't rule out the nonstandard models. We're just ruling out sets of possibilities which contradict our theory.\"</p>\n<p>You're right, of course. Maybe it's a mistake to talk about models in this way. I'd like to be able to state my point as a property of the physical operation of the system, but I don't know how.</p>\n<p>\"Taking 2nd-order logic just to be a many-sorted first-order logic even addresses your point about the infinite schema in first-order arithmetic. We don't need to come up with some modification of the induction probabilities. We can just prefer succinct theories in this particular many-sorted first-order logic. In fact, we can just prefer short theories in first-order logic, since we can describe the many-sorted situation with predicates in a one-sorted logic.\"</p>\n<p>Is that... true? Now you're saying that even for induction, first-order logic already covers everything that 2nd-order logic can do. What about mathematical conjectures for the natural numbers? Suppose I define a computable property of numbers, and I check that property up to 10,000. This allows us to rule out theories inconsistent with what we've found, which should increase our credence in the theory that the property holds for all numbers. But if we can't *prove* that from our limited axioms, then a nonstandard model will have a finite description consistent with what we know! So, an induction system which assigns probabilities based on first-order description length will continue to give a fixed credence to that, because we never rule it out as we check more cases. But a belief should converge to one as we check more cases. My hypothetical 2nd-order system should do that.</p>\n<p>\"If you don't know anything about these objects you're observing, do you really want to converge to conventional number theory? Let's go back to your example about time. We don't have any reason to suspect that time is a disconnected object. It's probably connected. But if there were disconnected moments, as in nonstandard models, we would have no way of falsifying that. Observations which you can get in our connected bit of the universe tell you nothing about whether there are disconnected bits. So I don't think you want to converge to zero probability there.\"</p>\n<p>Maybe. But let's stick to my example. If we've <em>already stated </em>that we're dealing with the natural numbers, and we <em>agree</em> that these are the least set containing zero and closed under the successor relationship, shouldn't our belief in universal statements about them approach one as we observe examples?</p>\n<p>\"Sure, you'd like it to... but is that possible? The 2nd-order logic has limited knowledge, just like the first-order version. There is still a finite theory which describes stuff we would only see in nonstandard cases. The theory is inconsistent according to the supposed semantics, but the actual proof system will never rule it out using the observations it can make. So it sounds like it's not possible to converge to probability 1 for the correct belief, here.\"</p>\n<p>You just admitted that there's a correct belief! But never mind. Anyway, it's perfectly possible. We could insert a hack, which says that in the case where we're discussing, we believe the universal statement with probability 1/(x+2), where x is the size of the smallest natural number we haven't checked for the property yet. If the universal statement gets logically proved or disproved, we discard this probability.</p>\n<p>\"That... works... but... it's not remotely plausible! It obviously doesn't generalize. You can't just say a special case is handled completely differently from everything else. And even for that case, you pulled the function 1/(x+2) from nowhere. It's not a rational expectation.\"</p>\n<p>I know, I know. I don't know how to make it into a reasonable credence measure for beliefs. I'll tell you one thing: if we assert that the structure we're considering is the least set satisfying some first-order axioms, then we should be wary of new statements which assert the existence of examples with properties we have never seen before. These examples could be non-standard, even if they are consistent with our proof system. We should assign them diminishing probability as we check more examples and never manage to construct anything which satisfies the strange property.</p>\n</p>\n<p>\"I appreciate the effort, but I'm not sure where it's going. We know for sure that it's not possible to make an approximatable probability distribution consistent with the semantics of 2nd-order logic. The <a href=\"http://en.wikipedia.org/wiki/Turing_jump\">Turing Jump operator</a> applied to the halting problem gives us the convergence problem: even with the aid of a halting oracle, we can't decide whether a particular process will converge. Your result shows that we can converge to the right answer for universal generalizations of computable predicates, which is just another way of saying that <a href=\"http://www.idsia.ch/~juergen/toesv2/node9.html\">we can specify a convergent process</a> which eventually gets the right answer for halting problems. However, 2nd-order arithetic can discuss convergence problems just as well as halting problems, and many more iterations of the Jump operator in fact. Any convergent process will just have to be wrong for a whole lot of cases.\"</p>\n<p>Maybe there's a divergent probability distribution that's reasonable in some sense...? No, forget I said anything, that's foolish. You're right, we can't rule out all the nonstandard models this way. There should be <em>some</em> relevant notion of rational belief to apply, though...</p>\n<p>\"Should there? Rationality isn't exactly floating out there in the Platonic realm for us to reach out and grab it. We make it up as we go along. We're evolved creatures, remember? It's a miracle that we can even discuss the halting problem! There's no reason we should be able to discuss the higher infinities. Maybe we're just fooling ourselves, and the whole problem is meaningless. Maybe we're just a first-order system which has learned to postulate a many-sorted reality, and think it's doing 2nd-order logic, because that just happens to be a convenient way of describing the universe. We think we can meaningfully refer to the least set satisfying some properties, but we can't state what we mean well enough to get at the ramifications of that. We think 'Every Turing machine halts or doesn't!' because we're running on classical logic... but we're fooling ourselves, because all our definitions of Turing machines are fundamentally incomplete, and there isn't actually any unique thing we mean by it. We think there continue to be true answers to problems even as we pile on Turing Jump operators, but really, it's nothing but mathematical sophistry. All of this is consistent with what we know.\"</p>\n<p>But do you believe that?</p>\n<p>\"Well...\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 2, "yXNtYNHJB54T3bGm3": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "65c2LmHS8RoLTYYXe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 1.0766133344227648e-06, "legacy": true, "legacyId": "21025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SWn4rqdycu83ikfBa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T11:25:25.242Z", "modifiedAt": null, "url": null, "title": "A reply to Mark Linsenmayer about philosophy", "slug": "a-reply-to-mark-linsenmayer-about-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:14.008Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2r28gzYtALsb7M3aR/a-reply-to-mark-linsenmayer-about-philosophy", "pageUrlRelative": "/posts/2r28gzYtALsb7M3aR/a-reply-to-mark-linsenmayer-about-philosophy", "linkUrl": "https://www.lesswrong.com/posts/2r28gzYtALsb7M3aR/a-reply-to-mark-linsenmayer-about-philosophy", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20reply%20to%20Mark%20Linsenmayer%20about%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20reply%20to%20Mark%20Linsenmayer%20about%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r28gzYtALsb7M3aR%2Fa-reply-to-mark-linsenmayer-about-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20reply%20to%20Mark%20Linsenmayer%20about%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r28gzYtALsb7M3aR%2Fa-reply-to-mark-linsenmayer-about-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r28gzYtALsb7M3aR%2Fa-reply-to-mark-linsenmayer-about-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 990, "htmlBody": "<p><a href=\"http://www.partiallyexaminedlife.com/about-the-participants/\">Mark Linsenmayer</a>, one of the hosts of a top philosophy podcast called <em><a href=\"http://www.partiallyexaminedlife.com/\">The Partially Examined Life</a></em>, has written a <a href=\"http://www.partiallyexaminedlife.com/2013/01/03/eliezer-yudkowsky-and-luke-muehlhauser-on-modern-rationalism-conversations-from-the-pale-blue-dot/\">critique</a> of the view that <a href=\"http://yudkowsky.net/\">Eliezer</a> and I seem to take of philosophy. Below, I respond to a few of Mark's comments. Naturally, I speak only for myself, not for Eliezer.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>I'm generally skeptical when someone proclaims that \"rationality\" itself should get us to throw out 90%+ of philosophy...</p>\n</blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Sturgeon's_Law\">Sturgeon's Law</a> declares that \"90% of everything is crap.\" I think <em>something</em> like that is true, though perhaps it's 88% crap in physics, 99% crap in philosophy, and 99.99% crap on <a href=\"http://www.4chan.org/\">4chan</a>.</p>\n<p>But let me be more precise. I <em>do</em> claim that almost all philosophy is useless <em>for figuring out <a href=\"/lw/eqn/the_useful_idea_of_truth/\">what is true</a></em>, for reasons explained in several of my posts:</p>\n<ul>\n<li><a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">Philosophy: A Diseased Discipline</a> </li>\n<li><a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">Concepts Don't Work That Way</a> </li>\n<li><a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">Intuitions Aren't Shared That Way</a> </li>\n<li><a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">Train Philosophers with Pearl and Kahneman, Not Plato and Kant</a> </li>\n</ul>\n<p>Mark replies that the kinds of unscientific philosophy I dismiss can be \"useful at least in the sense of entertaining,\" which of course isn't something I'd deny. I'm just trying to say that Heidegger is pretty darn useless for figuring out what's true. There are thousands of readings that will more efficiently make your model of the world more accurate.</p>\n<p>If you want to read Heidegger as poetry or entertainment, that's fine. I watch <em>Game of Thrones</em>, but not because it's a useful inquiry into truth.</p>\n<p>Also, I'm not sure what it would mean to say we should throw out 90% of philosophy <em>because of rationality</em>, but I probably don't agree with the \"because\" clause, there.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>[Luke's] accusation is that most philosophizing is useless unless explicitly based on scientific knowledge on how the brain works, and in particular where intuitions come from... [But] to then throw out the mass of the philosophical tradition because it has been ignorant of [cognitive biases] is [a mistake].</p>\n</blockquote>\n<p>I don't, in fact, think that \"most philosophizing is useless unless explicitly based on scientific knowledge [about] how the brain works,\" nor do I \"throw out the mass of the philosophical tradition because it has been ignorant of [cognitive biases].\" Sometimes, people do pretty good philosophy without knowing much of modern psychology. Look at all the progress Hume and Frege made.</p>\n<p>What I <em>do</em> claim is that many <em>specific</em> philosophical positions and methods are undermined by scientific knowledge about how brains and other systems work. For example, I've <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">argued</a> that a particular kind of philosophical analysis, which assumes concepts are defined by necessary and sufficient conditions, is undermined by psychological results showing that brains don't store concepts that way.</p>\n<p>If some poor philosopher doesn't know this, because she thinks it's okay to spend all day using her brain to philosophize without knowing much about how brains work, she might spend several years of her career pointlessly trying to find a necessary-and-sufficient-conditions analysis of knowledge that is immune to Gettier-style counterexamples.</p>\n<p>That's one reason to study psychology before doing much philosophy. Doing so can save you lots of time.</p>\n<p>Another reason to study psychology is that psychology is a significant component of <a href=\"http://appliedrationality.org/\">rationality training</a> (yes, with daily study and exercise, like piano training). Rationality training is important for doing philosophy because <a href=\"/r/lesswrong/lw/fpe/philosophy_needs_to_trust_your_rationality_even/\">philosophy needs to trust your rationality even though it shouldn't</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>...Looking over Eliezer's site and Less Wrong... my overall impression is again that... none of this adds up to the blanket critique/world-view that comes through very clearly</p>\n</blockquote>\n<p>Less Wrong is a group blog, so it doesn't quite have its own philosophy or worldview.</p>\n<p><em>Eliezer</em>, however, most certainly does. His approach to epistemology is pretty thoroughly documented in the ongoing, book-length sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a>. Additional parts of his \"worldview\" comes to light in his many posts on <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">philosophy of language</a>, <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution\">free will</a>, <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence\">metaphysics</a>, <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">metaethics</a>, <a href=\"/lw/n3/circular_altruism/\">normative ethics</a>, <a href=\"/lw/xy/the_fun_theory_sequence/\">axiology</a>, and <a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence\">philosophy of mind</a>.</p>\n<p>I've written less about my own philosophical views, but you can get some of them in two (ongoing) sequences: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a> and <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>I think it's instructive to contrast Eliezer with David Chalmers... who is very much on top of the science in his field... and yet he is not on board with any of this \"commit X% of past philosophy to the flames\" nonsense, doesn't think metaphysical arguments are meaningless or that difficult philosophical problems need to be defined away in some way, and, most provocatively, sees in consciousness a challenge to a physicalist world-view... I respectfully suggest that while reading more in contemporary science is surely a good idea... the approach to philosophy that is actually schooled in philosophy a la Chalmers is more worthy of emulation than Eliezer's dismissive anti-philosophy take.</p>\n</blockquote>\n<p>Chalmers is a smart dude, a good writer, and fun to hang with. But Mark doesn't explain here <em>why</em> it's \"nonsense\" to propose that truth-seekers (<em>qua</em> truth-seekers) should ignore 99% of all philosophy, <em>why</em> many metaphysical arguments aren't meaningless, <em>why</em> some philosophical problems can't simply be dissolved, nor why Chalmers' approach to philosophy is superior to Eliezer's.</p>\n<p>And that's fine. As Mark wrote, \"I intended this post to be a high-level overview of positions.\" I'd just like to flag that arguments weren't provided in Mark's post.</p>\n<p>Meanwhile, I've linked above to many posts Eliezer and I have written about why most philosophy is useless for truth-seeking, why some metaphysical arguments are meaningless, and why some philosophical problems can be dissolved. (We'd have to be more specific about the Chalmers vs. Eliezer question before I could weigh in. For example, I find Chalmers' writing to be clearer, but Eliezer's choice of topics for investigation more important for the human species.)</p>\n<p>Finally, I'll note that <a href=\"http://nickbostrom.com/\">Nick Bostrom</a> takes roughly the same approach to philosophy as Eliezer and I do, but Nick has a position at Oxford University, publishes in leading philosophy journals, and so on. On philosophical method, I recommend Nick's first professional paper, <a href=\"http://www.nickbostrom.com/old/predict.html\">Predictions from Philosophy</a> (1997). It sums up the motivation behind much of what Nick and Eliezer have done since then.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2r28gzYtALsb7M3aR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 30, "extendedScore": null, "score": 1.0766444250316366e-06, "legacy": true, "legacyId": "21026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqvnWFtRD2keJdwjX", "FwiPfF8Woe5JrzqEu", "jaN4EKrRnZdynTJjH", "LcEzxX2FNTKbB6KXS", "wHjpCxeDeuFadG3jF", "2pdyL8bSGBfYsnkyS", "FaJaCgqBKphrDzDSj", "4ZzefKQwAtMo5yp99", "K4aGvLnHvYgX9pZHS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T14:57:35.752Z", "modifiedAt": null, "url": null, "title": "Meetup : Paderborn Meetup, January 9th", "slug": "meetup-paderborn-meetup-january-9th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iqAnXFatqZP9WPAaK/meetup-paderborn-meetup-january-9th", "pageUrlRelative": "/posts/iqAnXFatqZP9WPAaK/meetup-paderborn-meetup-january-9th", "linkUrl": "https://www.lesswrong.com/posts/iqAnXFatqZP9WPAaK/meetup-paderborn-meetup-january-9th", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Paderborn%20Meetup%2C%20January%209th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Paderborn%20Meetup%2C%20January%209th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqAnXFatqZP9WPAaK%2Fmeetup-paderborn-meetup-january-9th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Paderborn%20Meetup%2C%20January%209th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqAnXFatqZP9WPAaK%2Fmeetup-paderborn-meetup-january-9th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqAnXFatqZP9WPAaK%2Fmeetup-paderborn-meetup-january-9th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hm'>Paderborn Meetup, January 9th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\" and whatever comes to our minds.</p>\n\n<p>I promised some news concerning certain plans for a Youtube-channel for rationality. Unfortunately I don't have any, but I will in a week or two.</p>\n\n<p>If you happen to live in the area, feel free to drop by, we will be happy about every new face we see.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hm'>Paderborn Meetup, January 9th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iqAnXFatqZP9WPAaK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0767714503665685e-06, "legacy": true, "legacyId": "21027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup__January_9th\">Discussion article for the meetup : <a href=\"/meetups/hm\">Paderborn Meetup, January 9th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup of this group. We are meeting once every two weeks. The location switches between Bielefeld and Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\" and whatever comes to our minds.</p>\n\n<p>I promised some news concerning certain plans for a Youtube-channel for rationality. Unfortunately I don't have any, but I will in a week or two.</p>\n\n<p>If you happen to live in the area, feel free to drop by, we will be happy about every new face we see.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup__January_9th1\">Discussion article for the meetup : <a href=\"/meetups/hm\">Paderborn Meetup, January 9th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Paderborn Meetup, January 9th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup__January_9th", "level": 1}, {"title": "Discussion article for the meetup : Paderborn Meetup, January 9th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup__January_9th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T18:07:22.369Z", "modifiedAt": null, "url": null, "title": "[Link] The school of science fiction", "slug": "link-the-school-of-science-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dYjW9LCNbWkoijKwL/link-the-school-of-science-fiction", "pageUrlRelative": "/posts/dYjW9LCNbWkoijKwL/link-the-school-of-science-fiction", "linkUrl": "https://www.lesswrong.com/posts/dYjW9LCNbWkoijKwL/link-the-school-of-science-fiction", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20school%20of%20science%20fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20school%20of%20science%20fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYjW9LCNbWkoijKwL%2Flink-the-school-of-science-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20school%20of%20science%20fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYjW9LCNbWkoijKwL%2Flink-the-school-of-science-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYjW9LCNbWkoijKwL%2Flink-the-school-of-science-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6276, "htmlBody": "<p>I recently discovered a cool new blog called <a href=\"http://studiolo.cortediurbino.org/studiolo/\">studiolo</a> and wanted to share it here. You will probably like this post if you like science fiction since it contains long excerpts of it. <strong>Unfortunately formatting it properly as a quote has been giving me some trouble, so I'll go with the least ugly looking solution, please don't think I claim to have wrote it</strong>. I found the speculation entertaining and interesting because I have extensively thought along similar lines about the effect of science fiction I consumed on my own world view (<a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/45sc\">though I didn't mention it often</a>).</p>\n<p><a href=\"http://studiolo.cortediurbino.org/the-school-of-science-fiction/\">Link to original post by Federico. </a></p>\n<h1 class=\"entry-title\"><a href=\"http://studiolo.cortediurbino.org/the-school-of-science-fiction/\">The school of science fiction</a></h1>\n<div class=\"entry-content\">\n<p>I have tried to persuade my friends and acquaintances that governmental reboot, and friendly AI, are important problems. I have failed. Two candidate hypotheses:</p>\n<p>1. They do not share my distaste for the banal.</p>\n<p>2. They did not consume, at the formative age, a sufficient amount of science fiction.</p>\n<p>#1 and #2 are not mutually exclusive. Distaste for the banal is merely an attitude&mdash;but in the first place, <em>fascinating consequences</em> are what nourished my Bayesian, utilitarian beliefs. Science fiction encourages kids to realise that life, the Universe and everything holds out <em>fascinating&nbsp;</em>possibilities, and that it is both valid and essential for humans to explore these ideas.</p>\n<p>Sister Y, in her pornographically insightful essay on <a href=\"http://theviewfromhell.blogspot.co.uk/2012/09/trying-to-see-through-unified-theory-of.html\">insight porn</a>, highlights Philip K Dick&rsquo;s short stories. I concur. Dick writes in 1981:</p>\n<p style=\"padding-left: 30px;\">I will define science fiction, first, by saying what sf is not. It cannot be defined as &ldquo;a story (or novel or play) set in the future,&rdquo; since there exists such a thing as space adventure, which is set in the future but is not sf: it is just that: adventures, fights and wars in the future in space involving super-advanced technology. Why, then, is it not science fiction? It would seem to be, and Doris Lessing (e.g.) supposes that it is. However, space adventure <em>lacks the distinct new idea</em> that is the essential ingredient. Also, there can be science fiction set in the present: the alternate world story or novel. So if we separate sf from the future and also from ultra-advanced technology, what then do we have that can be called sf?</p>\n<p style=\"padding-left: 30px;\">We have a fictitious world; that is the first step: it is a society that does not in fact exist, but is predicated on our known society; that is, our known society acts as a jumping-off point for it; the society advances out of our own in some way, perhaps orthogonally, as with the alternate world story or novel. It is our world dislocated by some kind of mental effort on the part of the author, our world transformed into that which it is not or not yet. This world must differ from the given in at least one way, and this one way must be sufficient to give rise to events that could not occur in our society &mdash; or in any known society present or past. There must be a coherent idea involved in this dislocation; that is, the dislocation must be a conceptual one, not merely a trivial or bizarre one &mdash; <em>this</em> is the essence of science fiction, the conceptual dislocation within the society so that as a result a new society is generated in the author&rsquo;s mind, transferred to paper, and from paper it occurs as a convulsive shock in the reader&rsquo;s mind, <em>the shock of dysrecognition</em>. He knows that it is not his actual world that he is reading about.</p>\n<p style=\"padding-left: 30px;\">Now, to separate science fiction from fantasy. This is impossible to do, and a moment&rsquo;s thought will show why. Take psionics; take mutants such as we find in Ted Sturgeon&rsquo;s wonderful <em>MORE THAN HUMAN</em>. If the reader believes that such mutants could exist, then he will view Sturgeon&rsquo;s novel as science fiction. If, however, he believes that such mutants are, like wizards and dragons, not possible, nor will ever be possible, then he is reading a fantasy novel. Fantasy involves that which general opinion regards as impossible; science fiction involves that which general opinion regards as possible under the right circumstances. This is in essence a judgment-call, since what is possible and what is not possible is not objectively known but is, rather, a subjective belief on the part of the author and of the reader.</p>\n<p style=\"padding-left: 30px;\">Now to define <em>good</em> science fiction. The conceptual dislocation &mdash; the new idea, in other words &mdash; must be truly new (or a new variation on an old one) and it must be intellectually stimulating to the reader; it must invade his mind and wake it up to the possibility of something he had not up to then thought of. Thus &ldquo;good science fiction&rdquo; is a value term, not an objective thing, and yet, I think, there really is such a thing, objectively, as good science fiction.</p>\n<p style=\"padding-left: 30px;\">I think Dr. Willis McNelly at the California State University at Fullerton put it best when he said that the true protagonist of an sf story or novel is an idea and not a person. If it is <em>good</em> sf the idea is new, it is stimulating, and, probably most important of all, it sets off a chain-reaction of ramification-ideas in the mind of the reader; it so-to-speak unlocks the reader&rsquo;s mind so that that mind, like the author&rsquo;s, begins to create. Thus sf is creative and it inspires creativity, which mainstream fiction by-and-large does not do. We who read sf (I am speaking as a reader now, not a writer) read it because we love to experience this chain-reaction of ideas being set off in our minds by something we read, something with a new idea in it; hence the very best science fiction ultimately winds up being a collaboration between author and reader, in which both create &mdash; and <em>enjoy</em> doing it: joy is the essential and final ingredient of science fiction, the joy of discovery of newness.</p>\n<p style=\"padding-left: 30px;\"><img class=\"size-full wp-image-244 aligncenter\" style=\"border-width: 0px; border-color: currentcolor; border-style: none;\" src=\"http://studiolo.cortediurbino.org/wp-content/uploads/2012/12/Spoiler_warning.jpg\" alt=\"Spoiler_warning\" width=\"498\" height=\"166\" /></p>\n<p>Several of Dick&rsquo;s short stories prefigure Eliezer Yudkowsky&rsquo;s (entirely serious) notion of unfriendly AI:</p>\n<p style=\"padding-left: 30px;\">&ldquo;The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.&rdquo;</p>\n<p>&mdash;Eliezer Yudkowsky, <a href=\"http://philosophyandhistoryofscience.com/wp-content/uploads/2012/01/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></p>\n<p>Here is an excerpt from <em>Autofac&nbsp;</em>(1955):</p>\n<p style=\"padding-left: 30px;\">Cut into the base of the mountains lay the vast metallic cube of the Kansas City factory. Its surface was corroded, pitted with radiation pox, cracked and scarred from the five years of war that had swept over it. Most of the factory was buried subsurface, only its entrance stages visible. The truck was a speck rumbling at high speed toward the expanse of black metal. Presently an opening formed in the uniform surface; the truck plunged into it and disappeared inside. The entrance snapped shut.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Now the big job remains,&rdquo; O&rsquo;Neill said. &ldquo;Now we have to persuade it to close down operations &mdash; to shut itself off.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Judith O&rsquo;Neill served hot black coffee to the people sitting around the living room. Her husband talked while the others listened. O&rsquo;Neill was as close to being an authority on the autofac system as could still be found.</p>\n<p style=\"padding-left: 30px;\">In his own area, the Chicago region, he had shorted out the protective fence of the local factory long enough to get away with data tapes stored in its posterior brain. The factory, of course, had immediately reconstructed a better type of fence. But he had shown that the factories were not infallible.</p>\n<p style=\"padding-left: 30px;\">&ldquo;The Institute of Applied Cybernetics,&rdquo; O&rsquo;Neill explained, &ldquo;had complete control over the network. Blame the war. Blame the big noise along the lines of communication that wiped out the knowledge we need. In any case, the Institute failed to transmit its information to us, so we can&rsquo;t transmit our information to the factories &mdash; the news that the war is over and we&rsquo;re ready to resume control of industrial operations.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;And meanwhile,&rdquo; Morrison added sourly, &ldquo;the damn network expands and consumes more of our natural resources all the time.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;I get the feeling,&rdquo; Judith said, &ldquo;that if I stamped hard enough, I&rsquo;d fall right down into a factory tunnel. They must have mines everywhere by now.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Isn&rsquo;t there some limiting injunction?&rdquo; Ferine asked nervously. &ldquo;Were they set up to expand indefinitely?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Each factory is limited to its own operational area,&rdquo; O&rsquo;Neill said, &ldquo;but the network itself is unbounded. It can go on scooping up our resources forever. The Institute decided it gets top priority; we mere people come second.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Will there be <em>anything</em>&nbsp;left for us?&rdquo; Morrison wanted to know.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Not unless we can stop the network&rsquo;s operations. It&rsquo;s already used up half a dozen basic minerals. Its search teams are out all the time, from every factory, looking everywhere for some last scrap to drag home.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;What would happen if tunnels from two factories crossed each other?&rdquo;</p>\n<p style=\"padding-left: 30px;\">O&rsquo;Neill shrugged. &ldquo;Normally, that won&rsquo;t happen. Each factory has its own special section of our planet, its own private cut of the pie for its exclusive use.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;But it <em>could</em> happen.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Well, they&rsquo;re raw material-tropic; as long as there&rsquo;s anything left, they&rsquo;ll hunt it down.&rdquo; O&rsquo;Neill pondered the idea with growing interest. &ldquo;It&rsquo;s something to consider. I suppose as things get scarcer &ndash;&rdquo;</p>\n<p style=\"padding-left: 30px;\">He stopped talking. A figure had come into the room; it stood silently by the door, surveying them all.</p>\n<p style=\"padding-left: 30px;\">In the dull shadows, the figure looked almost human. For a brief moment, O&rsquo;Neill thought it was a settlement latecomer. Then, as it moved forward, he realized that it was only quasi-human: a functional upright biped chassis, with data-receptors mounted at the top, effectors and proprioceptors mounted in a downward worm that ended in floor-grippers. Its resemblance to a human being was testimony to nature&rsquo;s efficiency; no sentimental imitation was intended.</p>\n<p style=\"padding-left: 30px;\">The factory representative had arrived.</p>\n<p style=\"padding-left: 30px;\">It began without preamble. &ldquo;This is a data-collecting machine capable of communicating on an oral basis. It contains both broadcasting and receiving apparatus and can integrate facts relevant to its line of inquiry.&rdquo;</p>\n<p style=\"padding-left: 30px;\">The voice was pleasant, confident. Obviously it was a tape, recorded by some Institute technician before the war. Coming from the quasi-human shape, it sounded grotesque; O&rsquo;Neill could vividly imagine the dead young man whose cheerful voice now issued from the mechanical mouth of this upright construction of steel and wiring.</p>\n<p style=\"padding-left: 30px;\">&ldquo;One word of caution,&rdquo; the pleasant voice continued. &ldquo;It is fruitless to consider this receptor human and to engage it in discussions for which it is not equipped. Although purposeful, it is not capable of conceptual thought; it can only reassemble material already available to it.&rdquo;</p>\n<p style=\"padding-left: 30px;\">The optimistic voice clicked out and a second voice came on. It resembled the first, but now there were no intonations or personal mannerisms. The machine was utilizing the dead man&rsquo;s phonetic speech-pattern for its own communication.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Analysis of the rejected product,&rdquo; it stated, &ldquo;shows no foreign elements or noticeable deterioration. The product meets the continual testing-standards employed throughout the network. Rejection is therefore on a basis outside the test area; standards not available to the network are being employed.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;That&rsquo;s right,&rdquo; O&rsquo;Neill agreed. Weighing his words with care, he continued, &ldquo;We found the milk substandard. We want nothing to do with it. We insist on more careful output.&rdquo;</p>\n<p style=\"padding-left: 30px;\">The machine responded presently. &ldquo;The semantic content of the term &lsquo;pizzled&rsquo; is unfamiliar to the network. It does not exist in the taped vocabulary. Can you present a factual analysis of the milk in terms of specific elements present or absent?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;No,&rdquo; O&rsquo;Neill said warily; the game he was playing was intricate and dangerous. &ldquo;&lsquo;Pizzled&rsquo; is an overall term. It can&rsquo;t be reduced to chemical constituents.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;What does &lsquo;pizzled&rsquo; signify?&rdquo; the machine asked. &ldquo;Can you define it in terms of alternate semantic symbols?&rdquo;</p>\n<p style=\"padding-left: 30px;\">O&rsquo;Neill hesitated. The representative had to be steered from its special inquiry to more general regions, to the ultimate problem of closing down the network. If he could pry it open at any point, get the theoretical discussion started. . .</p>\n<p style=\"padding-left: 30px;\">&ldquo;&lsquo;Pizzled,&rsquo;&rdquo; he stated, &ldquo;means the condition of a product that is manufactured when no need exists. It indicates the rejection of objects on the grounds that they are no longer wanted.&rdquo;</p>\n<p style=\"padding-left: 30px;\">The representative said, &ldquo;Network analysis shows a need of high-grade pasteurized milk-substitute in this area. There is no alternate source; the network controls all the synthetic mammary-type equipment in existence.&rdquo; It added, &ldquo;Original taped instructions describe milk as an essential to human diet.&rdquo;</p>\n<p style=\"padding-left: 30px;\">O&rsquo;Neill was being outwitted; the machine was returning the discussion to the specific. &ldquo;We&rsquo;ve decided,&rdquo; he said desperately, &ldquo;that we don&rsquo;t want any more milk. We&rsquo;d prefer to go without it, at least until we can locate cows.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;That is contrary to the network tapes,&rdquo; the representative objected. &ldquo;There are no cows. All milk is produced synthetically.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Then we&rsquo;ll produce it synthetically ourselves,&rdquo; Morrison broke in impatiently. &ldquo;Why can&rsquo;t we take over the machines? My God, we&rsquo;re not children! We can run our own lives!&rdquo;</p>\n<p style=\"padding-left: 30px;\">The factory representative moved toward the door. &ldquo;Until such time as your community finds other sources of milk supply, the network will continue to supply you. Analytical and evaluating apparatus will remain in this area, conducting the customary random sampling.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Ferine shouted futilely, &ldquo;How can we find other sources? You have the whole setup! You&rsquo;re running the whole show!&rdquo; Following after it, he bellowed, &ldquo;You say we&rsquo;re not ready to run things &mdash; you claim we&rsquo;re not capable. How do you know? You don&rsquo;t give us a chance! We&rsquo;ll never have a chance!&rdquo;</p>\n<p style=\"padding-left: 30px;\">O&rsquo;Neill was petrified. The machine was leaving; its one-track mind had completely triumphed.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Look,&rdquo; he said hoarsely, blocking its way. &ldquo;We want you to shut down, understand. We want to take over your equipment and run it ourselves. The war&rsquo;s over with. Damn it, you&rsquo;re not needed anymore!&rdquo;</p>\n<p style=\"padding-left: 30px;\">The factory representative paused briefly at the door. &ldquo;The inoperative cycle,&rdquo; it said, &ldquo;is not geared to begin until network production merely duplicates outside production. There is at this time, according to our continual sampling, no outside production. Therefore network production continues.&rdquo;</p>\n<p>This is not to say that sci-fi always hits on the&nbsp;<em>right&nbsp;</em>answers to important problems. The panel below is from <em>Meka-City</em>, an episode of <em>Judge Dredd</em>&nbsp;that takes place shortly after the &ldquo;Apocalypse War&rdquo;.</p>\n<p style=\"text-align: left;\"><img class=\" wp-image-300 aligncenter\" style=\"border-width: 0px; border-color: currentColor; border-style: none;\" src=\"http://studiolo.cortediurbino.org/wp-content/uploads/2012/12/Nuclear-Ethics.bmp\" alt=\"Nuclear Ethics\" width=\"744\" height=\"344\" />Robert Heinlein&rsquo;s <em>The Moon Is A Harsh Mistress </em>is equally questionable, from an <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">x-risk</a> perspective: the heroes place Earth at the mercy of a superintelligence whose friendliness, and even sanity is unproven and untested. Might it, however, have inspired a generation of libertarian dissidents?</p>\n<p style=\"padding-left: 30px;\">Prof shook head. &ldquo;Every new member made it that much more likely that you would be betrayed. Wyoming dear lady, revolutions are not won by enlisting the masses. Revolution is a science only a few are competent to practice. It depends on correct organization and, above all, on communications. Then, at the proper moment in history, they strike. Correctly organized and properly timed it is a bloodless coup. Done clumsily or prematurely and the result is civil war, mob violence, purges, terror. I hope you will forgive me if I say that, up to now, it has been done clumsily.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Wyoh looked baffled. &ldquo;What do you mean by &lsquo;correct organization&rsquo;?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Functional organization. How does one design an electric motor? Would you attach a bathtub to it, simply because one was available? Would a bouquet of flowers help? A heap of rocks? No, you would use just those elements necessary to its purpose and make it no larger than needed&mdash;and you would incorporate safety factors. Function controls design.</p>\n<p style=\"padding-left: 30px;\">&ldquo;So it is with revolution. Organization must be no larger than necessary&mdash;never recruit anyone merely because he wants to join. Nor seek to persuade for the pleasure of having another share your views. He&rsquo;ll share them when the times comes . . . or you&rsquo;ve misjudged the moment in history. Oh, there will be an educational organization but it must be separate; agitprop is no part of basic structure.</p>\n<p style=\"padding-left: 30px;\">&ldquo;As to basic structure, a revolution starts as a conspiracy therefore structure is small, secret, and organized as to minimize damage by betrayal&mdash;since there always are betrayals. One solution is the cell system and so far nothing better has been invented.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Much theorizing has gone into optimum cell size. I think that history shows that a cell of three is best&mdash;more than three can&rsquo;t agree on when to have dinner, much less when to strike. Manuel, you belong to a large family; do you vote on when to have dinner?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Bog, no! Mum decides.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Ah.&rdquo; Prof took a pad from his pouch, began to sketch. &ldquo;Here is a cells-of-three tree. If I were planning to take over Luna. I would start with us three. One would be opted as chairman. We wouldn&rsquo;t vote; choice would be obvious&mdash;or we aren&rsquo;t the right three. We would know the next nine people, three cells . . . but each cell would know only one of us.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Looks like computer diagram&mdash;a ternary logic.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Does it really? At the next level there are two ways of linking: This comrade, second level, knows his cell leader, his two cellmates, and on the third level he knows the three in his subcell&mdash;he may or may not know his cellmates&rsquo; subcells. One method doubles security, the other doubles speed&mdash;of repair if security is penetrated. Let&rsquo;s say he does not know his cellmates&rsquo; subcells&mdash;Manuel, how many can he betray? Don&rsquo;t say he won&rsquo;t; today they can brainwash any person, and starch and iron and use him. How many?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Six,&rdquo; I answered. &ldquo;His boss, two cellmates, three in sub-cell.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Seven,&rdquo; Prof corrected, &ldquo;he betrays himself, too. Which leaves seven broken links on three levels to repair. How?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;I don&rsquo;t see how it can be,&rdquo; objected Wyoh. &ldquo;You&rsquo;ve got them so split up it falls to pieces.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Manuel? An exercise for the student.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Well . . . blokes down here have to have way to send message up three levels. Don&rsquo;t have to know who, just have to know where.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Precisely!&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;But, Prof,&rdquo; I went on, &ldquo;there&rsquo;s a better way to rig it.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Really? Many revolutionary theorists have hammered this out, Manuel. I have such confidence in them that I&rsquo;ll offer you a wager&mdash;at, say, ten to one.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Ought to take your money. Take same cells, arrange in open pyramid of tetrahedrons. Where vertices are in common, each bloke knows one in adjoining cell&mdash;knows how to send message to him, that&rsquo;s all he needs. Communications never break down because they run sideways as well as up and down. Something like a neural net. It&rsquo;s why you can knock a hole in a man&rsquo;s head, take chunk of brain out, and not damage thinking much. Excess capacity, messages shunt around. He loses what was destroyed but goes on functioning.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Manuel,&rdquo; Prof said doubtfully, &ldquo;could you draw a picture? It sounds good&mdash;but it&rsquo;s so contrary to orthodox doctrine that I need to see it.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Well . . . could do better with stereo drafting machine. I&rsquo;ll try.&rdquo; (Anybody who thinks it&rsquo;s easy to sketch one hundred twenty-one tetrahedrons, a five-level open pyramid, clear enough to show relationships is invited to try!)</p>\n<p style=\"padding-left: 30px;\">Presently I said, &ldquo;Look at base sketch. Each vertex of each triangle shares self with zero, one, or two other triangles. Where shares one, that&rsquo;s its link, one direction or both&mdash;but one is enough for a multipli-redundant communication net. On corners, where sharing is zero, it jumps to right to next corner. Where sharing is double, choice is again right-handed.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Now work it with people. Take fourth level, D-for-dog. This vertex is comrade Dan. No, let&rsquo;s go down one to show three levels of communication knocked out&mdash;level E-for-easy and pick Comrade Egbert.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Egbert works under Donald, has cellmates Edward and Elmer, and has three under him, Frank, Fred, and Fatso . . . but knows how to send message to Ezra on his own level but not in his cell. He doesn&rsquo;t know Ezra&rsquo;s name, face, address, or anything&mdash;but has a way, phone number probably, to reach Ezra in emergency.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Now watch it work. Casimir, level three, finks out and betrays Charlie and Cox in his cell, Baker above him, and Donald, Dan, and Dick in subcell&mdash;which isolates Egbert, Edward, and Elmer, and everybody under them.</p>\n<p style=\"padding-left: 30px;\">&ldquo;All three report it&mdash;redundancy, necessary to any communication system&mdash;but follow Egbert&rsquo;s yell for help. He calls Ezra. But Ezra is under Charlie and is isolated, too. No matter, Ezra relays both messages through his safety link, Edmund. By bad luck Edmund is under Cox, so he also passes it laterally, through Enwright . . . and that gets it past burned-out part and it goes up through Dover, Chambers, and Beeswax, to Adam, front office . . . who replies down other side of pyramid, with lateral pass on E-for-easy level from Esther to Egbert and on to Ezra and Edmund. These two messages, up and down, not only get through at once but in way they get through, they define to home office exactly how much damage has been done and where. Organization not only keeps functioning but starts repairing self at once.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Wyoh was tracing out lines, convincing herself it would work&mdash;which it would, was &ldquo;idiot&rdquo; circuit. Let Mike study a few milliseconds, and could produce a better, safer, more foolproof hookup. And probably&mdash;certainly&mdash;ways to avoid betrayal while speeding up routings. But I&rsquo;m not a computer.</p>\n<p style=\"padding-left: 30px;\">Prof was staring with blank expression. &ldquo;What&rsquo;s trouble?&rdquo; I said. &ldquo;It&rsquo;ll work; this is my pidgin.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Manuel my b&mdash; Excuse me: Se&ntilde;or O&rsquo;Kelly . . . will you head this revolution?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Me? Great Bog, nyet! I&rsquo;m no lost-cause martyr. Just talking about circuits.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Wyoh looked up. &ldquo;Mannie,&rdquo; she said soberly, &ldquo;you&rsquo;re opted. It&rsquo;s settled.&rdquo;</p>\n<p>The marriage of fantastic and familiar allows science fiction authors to deal freely with touchy issues. The following excerpt, from PKD&rsquo;s <em>The Golden Man</em>, is about &ldquo;mutants&rdquo;:</p>\n<p style=\"padding-left: 30px;\">From the dirt road came the sound of motors, sleek purrs that rapidly grew louder. Two teardrops of black metal came gliding up and parked beside the house. Men swarmed out, in the dark gray-green of the Government Civil Police. In the sky swarms of black dots were descending, clouds of ugly flies that darkened the sun as they spilled out men and equipment. The men drifted slowly down.</p>\n<p style=\"padding-left: 30px;\">&ldquo;He&rsquo;s not here,&rdquo; Baines said, as the first man reached him. &ldquo;He got away. Inform Wisdom back at the lab.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;We&rsquo;ve got this section blocked off.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Baines turned to Nat Johnson, who stood in dazed silence, uncomprehending, his son and daughter beside him. &ldquo;How did he know we were coming?&rdquo; Baines demanded.</p>\n<p style=\"padding-left: 30px;\">&ldquo;I don&rsquo;t know,&rdquo; Johnson muttered. &ldquo;He just &mdash; knew.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;A telepath?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;I don&rsquo;t know.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Baines shrugged. &ldquo;We&rsquo;ll know, soon. A clamp is out, all around here. He can&rsquo;t get past, no matter what the hell he can do. Unless he can dematerialize himself.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;What&rsquo;ll you do with him when you &mdash; if you catch him?&rdquo; Jean asked huskily.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Study him.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;And then kill him?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;That depends on the lab evaluation. If you could give me more to work on, I could predict better.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;We can&rsquo;t tell you anything. We don&rsquo;t know anything more.&rdquo; The girl&rsquo;s voice rose with desperation. &ldquo;He doesn&rsquo;t talk.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Baines jumped. &ldquo;<em>What</em>?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;He doesn&rsquo;t talk. He never talked to us. Ever.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;How old is he?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Eighteen.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;No communication.&rdquo; Baines was sweating. &ldquo;In eighteen years there hasn&rsquo;t been any semantic bridge between you? Does he have any contact? Signs? Codes?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;He &mdash; ignores us. He eats here, stays with us. Sometimes he plays when we play. Or sits with us. He&rsquo;s gone days on end. We&rsquo;ve never been able to find out what he&rsquo;s doing &mdash; or where. He sleeps in the barn &mdash; by himself.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Is he really gold-colored?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;Yes. Skin, eyes, hair, nails. Everything.&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;And he&rsquo;s large? Well-formed?&rdquo; It was a moment before the girl answered. A strange emotion stirred her drawn features, a momentary glow. &ldquo;He&rsquo;s incredibly beautiful. A god come down to earth.&rdquo; Her lips twisted. &ldquo;You won&rsquo;t find him. He can do things. Things you have no comprehension of. Powers so far beyond your limited &ndash;&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;You don&rsquo;t think we&rsquo;ll get him?&rdquo; Baines frowned. &ldquo;More teams are landing all the time. You&rsquo;ve never seen an Agency clamp in operation. We&rsquo;ve had sixty years to work out all the bugs. If he gets away it&rsquo;ll be the first time &ndash;&rdquo;</p>\n<p style=\"padding-left: 30px;\">Baines broke off abruptly. Three men were quickly approaching the porch. Two green-clad Civil Police. And a third man between them. A man who moved silently, lithely, a faintly luminous shape that towered above them.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Cris!&rdquo; Jean screamed.</p>\n<p style=\"padding-left: 30px;\">&ldquo;We got him,&rdquo; one of the police said.</p>\n<p style=\"padding-left: 30px;\">Baines fingered his lash-tube uneasily. &ldquo;Where? How?&rdquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;He gave himself up,&rdquo; the policeman answered, voice full of awe. &ldquo;He came to us voluntarily. Look at him. He&rsquo;s like a metal statue. Like some sort of &mdash; god.&rdquo;</p>\n<p style=\"padding-left: 30px;\">The golden figure halted for a moment beside Jean. Then it turned slowly, calmly, to face Baines.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Cris!&rdquo; Jean shrieked. &ldquo;<em>Why did you come back</em>?&rdquo;</p>\n<p style=\"padding-left: 30px;\">The same thought was eating at Baines, too. He shoved it aside &mdash; for the time being. &ldquo;Is the jet out front?&rdquo; he demanded quickly.</p>\n<p style=\"padding-left: 30px;\">&ldquo;Ready to go,&rdquo; one of the CP answered. &ldquo;Fine.&rdquo; Baines strode past them, down the steps and onto the dirt field. &ldquo;Let&rsquo;s go. I want him taken directly to the lab.&rdquo; For a moment he studied the massive figure who stood calmly between the two Civil Policemen. Beside him, they seemed to have shrunk, become ungainly and repellent. Like dwarves. . . What had Jean said? <em>A god come to earth</em>. Baines broke angrily away. &ldquo;Come on,&rdquo; he muttered brusquely. &ldquo;This one may be tough; we&rsquo;ve never run up against one like it before. We don&rsquo;t know what the hell it can do.&rdquo;</p>\n<p>Of course, there is a political subtext. Dick writes in 1979:</p>\n<p style=\"padding-left: 30px;\">In the early Fifties much American science fiction dealt with human mutants and their glorious super-powers and super-faculties by which they would presently lead mankind to a higher state of existence, a sort of promised land. John W. Campbell. Jr., editor at <em>Analog</em>, demanded that the stories he bought dealt with such wonderful mutants, and he also insisted that the mutants always be shown as (1) good; and (2) firmly in charge. When I wrote &ldquo;The Golden Man&rdquo; I intended to show that (1) the mutant might not be good, at least good for the rest of mankind, for us ordinaries; and (2) not in charge but sniping at us as a bandit would, a feral mutant who potentially would do us more harm than good. This was specifically the view of psionic mutants that Campbell loathed, and the theme in fiction that he refused to publish&hellip; so my story appeared in <em>If</em>.</p>\n<p style=\"padding-left: 30px;\">We sf writers of the Fifties liked <em>If</em> because it had high quality paper and illustrations; it was a classy magazine. And, more important, it would take a chance with unknown authors. A fairly large number of my early stories appeared in <em>If</em>; for me it was a major market. The editor of <em>If</em> at the beginning was Paul W. Fairman. He would take a badly-written story by you and rework it until it was okay &ndash; which I appreciated. Later James L. Quinn the publisher became himself the editor, and then Frederik Pohl. I sold to all three of them.</p>\n<p style=\"padding-left: 30px;\">In the issue of If that followed the publishing of &ldquo;The Golden Man&rdquo; appeared a two-page editorial consisting of a letter by a lady school teacher complaining about &ldquo;The Golden Man&rdquo;. Her complaints consisted of John W. Campbell, Jr.&rsquo;s complaint: she upbraided me for presenting mutants in a negative light and she offered the notion that certainly we could expect mutants to be (1) good; and (2) firmly in charge. So I was back to square one.</p>\n<p style=\"padding-left: 30px;\">My theory as to why people took this view is this: I think these people secretly imagined they were themselves early manifestations of these kindly, wise, super-intelligent <em>Ubermenschen</em> who would guide the stupid &ndash; i.e. the rest of us &ndash; to the Promised Land. A power phantasy was involved here, in my opinion. The idea of the psionic superman taking over was a role that appeared originally in Stapleton&rsquo;s <em>ODD JOHN</em> and A.E.Van Vogt&rsquo;s <em>SLAN</em>. &ldquo;We are persecuted now,&rdquo; the message ran, &ldquo;and despised and rejected. But later on, boy oh boy, we will show them!&rdquo;</p>\n<p style=\"padding-left: 30px;\">As far as I was concerned, for psionic mutants to rule us would be to put the fox in charge of the hen house. I was reacting to what I considered a dangerous hunger for power on the part of neurotic people, a hunger which I felt John W. Campbell, Jr. was pandering to &ndash; and deliberately so. <em>If</em>, on the other hand, was not committed to selling any one particular idea; it was a magazine devoted to genuinely new ideas, willing to take any side of an issue. Its several editors should be commended, inasmuch as they understood the real task of science fiction: to look in all directions without restraint.</p>\n<p>(Now read between the lines of <a href=\"http://econlog.econlib.org/archives/2012/12/how_would_we_re.html\">this</a>, with reference to the policy implications of <a href=\"http://www.cato.org/sites/cato.org/files/pubs/pdf/pa594.pdf\">this</a>.)</p>\n<p>Finally, Isaac Asimov&rsquo;s&nbsp;<em>Foundation&nbsp;</em>series has inspired all sorts of people.</p>\n<p style=\"padding-left: 30px;\">The lights went dim!</p>\n<p style=\"padding-left: 30px;\">They didn&rsquo;t go out, but merely yellowed and sank with a suddenness that made Hardin jump. He had lifted his eyes to the ceiling lights in startled fashion, and when he brought them down the glass cubicle was no longer empty.</p>\n<p style=\"padding-left: 30px;\">A figure occupied it &sbquo; a figure in a wheel chair!</p>\n<p style=\"padding-left: 30px;\">It said nothing for a few moments, but it closed the book upon its lap and fingered it idly. And then it smiled, and the face seemed all alive.</p>\n<p style=\"padding-left: 30px;\">It said, &ldquo;I am Hari Seldon.&rdquo; The voice was old and soft.</p>\n<p style=\"padding-left: 30px;\">Hardin almost rose to acknowledge the introduction and stopped himself in the act.</p>\n<p style=\"padding-left: 30px;\">The voice continued conversationally: &ldquo;As you see, I am confined to this chair and cannot rise to greet you. Your grandparents left for Terminus a few months back in my time and since then I have suffered a rather inconvenient paralysis. I can&rsquo;t see you, you know, so I can&rsquo;t greet you properly. I don&rsquo;t even know how many of you there are, so all this must be conducted informally. If any of you are standing, please sit down; and if you care to smoke, I wouldn&rsquo;t mind.&rdquo; There was a light chuckle. &ldquo;Why should I? I&rsquo;m not really here.&rdquo;</p>\n<p style=\"padding-left: 30px;\">Hardin fumbled for a cigar almost automatically, but thought better of it.</p>\n<p style=\"padding-left: 30px;\">Hari Seldon put away his book &ndash; as if laying it upon a desk at his side &ndash; and when his fingers let go, it disappeared.</p>\n<p style=\"padding-left: 30px;\">He said: &ldquo;It is fifty years now since this Foundation was established &ndash; fifty years in which the members of the Foundation have been ignorant of what it was they were working toward. It was necessary that they be ignorant, but now the necessity is gone.</p>\n<p style=\"padding-left: 30px;\">&ldquo;The Encyclopedia Foundation, to begin with, is a fraud, and always has been!&rdquo;</p>\n<p style=\"padding-left: 30px;\">There was a sound of a scramble behind Hardin and one or two muffled exclamations, but he did not turn around.</p>\n<p style=\"padding-left: 30px;\">Hari Seldon was, of course, undisturbed. He went on: &ldquo;It is a fraud in the sense that neither I nor my colleagues care at all whether a single volume of the Encyclopedia is ever published. It has served its purpose, since by it we extracted an imperial charter from the Emperor, by it we attracted the hundred thousand humans necessary for our scheme, and by it we managed to keep them preoccupied while events shaped themselves, until it was too late for any of them to draw back.</p>\n<p style=\"padding-left: 30px;\">&ldquo;In the fifty years that you have worked on this fraudulent project &ndash; there is no use in softening phrases &ndash; your retreat has been cut off, and you have now no choice but to proceed on the infinitely more important project that was, and is, our real plan.</p>\n<p style=\"padding-left: 30px;\">&ldquo;To that end we have placed you on such a planet and at such a time that in fifty years you were maneuvered to the point where you no longer have freedom of action. From now on, and into the centuries, the path you must take is inevitable. You will be faced with a series of crises, as you are now faced with the first, and in each case your freedom of action will become similarly circumscribed so that you will be forced along one, and only one, path.</p>\n<p style=\"padding-left: 30px;\">&ldquo;It is that path which our psychology has worked out &ndash; and for a reason.</p>\n<p style=\"padding-left: 30px;\">&ldquo;For centuries Galactic civilization has stagnated and declined, though only a few ever realized that. But now, at last, the Periphery is breaking away and the political unity of the Empire is shattered. Somewhere in the fifty years just past is where the historians of the future will place an arbitrary line and say: &lsquo;This marks the Fall of the Galactic Empire.&rsquo;</p>\n<p style=\"padding-left: 30px;\">&ldquo;And they will be right, though scarcely any will recognize that Fall for additional<br /> centuries.</p>\n<p style=\"padding-left: 30px;\">&ldquo;And after the Fall will come inevitable barbarism, a period which, our psychohistory tells us, should, under ordinary circumstances, last for thirty thousand years. We cannot stop the Fall. We do not wish to; for Imperial culture has lost whatever virility and worth it once had. But we can shorten the period of Barbarism that must follow &ndash; down to a single thousand of years.</p>\n<p style=\"padding-left: 30px;\">&ldquo;The ins and outs of that shortening, we cannot tell you; just as we could not tell you the truth about the Foundation fifty years ago. Were you to discover those ins and outs, our plan might fail; as it would have, had you penetrated the fraud of the Encyclopedia earlier; for then, by knowledge, your freedom of action would be expanded and the number of additional variables introduced would become greater than our psychology could handle.</p>\n<p style=\"padding-left: 30px;\">&ldquo;But you won&rsquo;t, for there are no psychologists on Terminus, and never were, but for Alurin &ndash; and he was one of us.</p>\n<p style=\"padding-left: 30px;\">&ldquo;But this I can tell you: Terminus and its companion Foundation at the other end of the Galaxy are the seeds of the Renascence and the future founders of the Second Galactic Empire. And it is the present crisis that is starting Terminus off to that climax.</p>\n<p style=\"padding-left: 30px;\">&ldquo;This, by the way, is a rather straightforward crisis, much simpler than many of those that are ahead. To reduce it to its fundamentals, it is this: You are a planet suddenly cut off from the still-civilized centers of the Galaxy, and threatened by your stronger neighbors. You are a small world of scientists surrounded by vast and rapidly expanding reaches of barbarism. You are an island of nuclear power in a growing ocean of more primitive energy; but are helpless despite that, because of your lack of metals.</p>\n<p style=\"padding-left: 30px;\">&ldquo;You see, then, that you are faced by hard necessity, and that action is forced on you. The nature of that action &ndash; that is, the solution to your dilemma &ndash; is, of course, obvious!&rdquo;</p>\n<p style=\"padding-left: 30px;\">The image of Hari Seldon reached into open air and the book once more appeared in his hand. He opened it and said:</p>\n<p style=\"padding-left: 30px;\">&ldquo;But whatever devious course your future history may take, impress it always upon your descendants that the path has been marked out, and that at its end is new and greater Empire!&rdquo;</p>\n<p style=\"padding-left: 30px;\">And as his eyes bent to his book, he flicked into nothingness, and the lights brightened once more.</p>\n<p style=\"padding-left: 30px;\">Hardin looked up to see Pirenne facing him, eyes tragic and lips trembling.</p>\n<p style=\"padding-left: 30px;\">The chairman&rsquo;s voice was firm but toneless. &ldquo;You were right, it seems. If you will see us tonight at six, the Board will consult with you as to the next move.&rdquo;</p>\n<p style=\"padding-left: 30px;\">They shook his hand, each one, and left, and Hardin smiled to himself. They were fundamentally sound at that; for they were scientists enough to admit that they were wrong &ndash; but for them, it was too late.</p>\n<p style=\"padding-left: 30px;\">He looked at his watch. By this time, it was all over. Lee&rsquo;s men were in control and the Board was giving orders no longer.</p>\n<p style=\"padding-left: 30px;\">The Anacreonians were landing their first spaceships tomorrow, but that was all right, too. In six months, they would be giving orders no longer.</p>\n<p style=\"padding-left: 30px;\">In fact, as Hari Seldon had said, and as Salvor Hardin had guessed since the day that Anselm haut Rodric had first revealed to him Anacreon&rsquo;s lack of nuclear power &ndash; the solution to this first crisis was obvious.</p>\n<p style=\"padding-left: 30px;\">Obvious as all hell!</p>\n<p>Sayeth <a href=\"http://unqualified-reservations.blogspot.co.uk/2009/12/climategate-historys-message.html\">Moldbug</a>:</p>\n<p style=\"padding-left: 30px;\">Now, some have described the dramatic formula of UR as having a rather Tolkienesque feel; others may connect it more with C.S. Lewis; I certainly grew up reading both. But above all, I grew up reading Isaac Asimov.</p>\n<p style=\"padding-left: 30px;\">If my journey into the awesome, humbling lost library that is Google Books was a film and needed a name, it might be called &ldquo;Searching for Hari Seldon.&rdquo; With more or less the entire Victorian corpus, modulo a bit of copyfraud, the Hari Seldon game is to enquire of this Library: which writers of the 19th would feel most justified, in their understanding of the eternal nature of history, humanity and government, by the events of the 20th? Whose crystal ball worked? Whose archived holograms delivered the news?</p>\n<p style=\"padding-left: 30px;\">Broadly speaking, I think the answer is clear. Hari Seldon is Carlyle &ndash; the late Carlyle, of the Pamphlets. I consider myself a Carlylean pretty much the way a Marxist is a Marxist. There is simply no significant phenomenon of the 20th century not fully anticipated. Almost alone Carlyle predicts that the 20th will be a century of political chaos and mass murder, and he says not what but also why. And what a writer! Religions could easily be founded on the man &ndash; and perhaps should be.</p>\n<p>And <a href=\"http://www.guardian.co.uk/books/2012/dec/04/paul-krugman-asimov-economics\">Paul Krugman</a>:</p>\n<p style=\"padding-left: 30px;\">There are certain novels that can shape a teenage boy&rsquo;s life. For some, it&rsquo;s Ayn Rand&rsquo;s Atlas Shrugged; for others it&rsquo;s Tolkien&rsquo;s The Lord of the Rings. As a widely quoted internet meme says, the unrealistic fantasy world portrayed in one of those books can warp a young man&rsquo;s character forever; the other book is about orcs. But for me, of course, it was neither. My Book &ndash; the one that has stayed with me for four-and-a-half decades &ndash; is Isaac Asimov&rsquo;s Foundation Trilogy, written when Asimov was barely out of his teens himself. I didn&rsquo;t grow up wanting to be a square-jawed individualist or join a heroic quest; I grew up wanting to be Hari Seldon, using my understanding of the mathematics of human behaviour to save civilisation.</p>\n<p>A pity he didn&rsquo;t move on to&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Game-Players_of_Titan\">this</a>.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dYjW9LCNbWkoijKwL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 15, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "21028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T21:38:42.693Z", "modifiedAt": null, "url": null, "title": "Harsanyi's Social Aggregation Theorem and what it means for CEV", "slug": "harsanyi-s-social-aggregation-theorem-and-what-it-means-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z8afQRsH9wWsB4iMD/harsanyi-s-social-aggregation-theorem-and-what-it-means-for", "pageUrlRelative": "/posts/z8afQRsH9wWsB4iMD/harsanyi-s-social-aggregation-theorem-and-what-it-means-for", "linkUrl": "https://www.lesswrong.com/posts/z8afQRsH9wWsB4iMD/harsanyi-s-social-aggregation-theorem-and-what-it-means-for", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harsanyi's%20Social%20Aggregation%20Theorem%20and%20what%20it%20means%20for%20CEV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarsanyi's%20Social%20Aggregation%20Theorem%20and%20what%20it%20means%20for%20CEV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8afQRsH9wWsB4iMD%2Fharsanyi-s-social-aggregation-theorem-and-what-it-means-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harsanyi's%20Social%20Aggregation%20Theorem%20and%20what%20it%20means%20for%20CEV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8afQRsH9wWsB4iMD%2Fharsanyi-s-social-aggregation-theorem-and-what-it-means-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8afQRsH9wWsB4iMD%2Fharsanyi-s-social-aggregation-theorem-and-what-it-means-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1089, "htmlBody": "<p>A Friendly AI would have to be able to aggregate each person's preferences into one utility function. The most straightforward and obvious way to do this is to agree on some way to normalize each individual's utility function, and then add them up. But many people don't like this, usually for reasons involving utility monsters. If you are one of these people, then you better learn to like it, because according to Harsanyi's Social Aggregation Theorem, any alternative can result in the supposedly Friendly AI making a choice that is bad for every member of the population. More formally,</p>\n<p>Axiom 1: Every person, and the FAI, are <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM-rational</a> agents.</p>\n<p>Axiom 2: Given any two choices A and B such that every person prefers A over B, then the FAI prefers A over B.</p>\n<p>Axiom 3: There exist two choices A and B such that every person prefers A over B.</p>\n<p>(Edit: Note that I'm assuming a fixed population with fixed preferences. This still seems reasonable, because we wouldn't want the FAI to be dynamically inconsistent, so it would have to draw its values from a fixed population, such as the people alive now. Alternatively, even if you want the FAI to aggregate the preferences of a changing population, the theorem still applies, but this comes with it's own problems, such as giving people (possibly including the FAI) incentives to create, destroy, and modify other people to make the aggregated utility function more favorable to them.)</p>\n<p>Give each person a unique integer label from <img src=\"http://www.codecogs.com/png.latex?1\" alt=\"\" width=\"7\" height=\"12\" /> to <img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"12\" height=\"10\" />, where <img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"12\" height=\"10\" /> is the number of people. For each person <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\" width=\"8\" height=\"13\" />, let <img src=\"http://www.codecogs.com/png.latex?u_{k}\" alt=\"\" width=\"17\" height=\"11\" /> be some function that, interpreted as a utility function, accurately describes <img src=\"http://www.codecogs.com/png.latex?k\" alt=\"\" width=\"8\" height=\"13\" />'s preferences (there exists such a function by the VNM utility theorem). Note that I want <img src=\"http://www.codecogs.com/png.latex?u_%7Bk%7D\" alt=\"\" width=\"17\" height=\"11\" /> to be some particular function, distinct from, for instance, <img src=\"http://www.codecogs.com/png.latex?2u_{k}-7\" alt=\"\" width=\"57\" height=\"16\" />, even though <img src=\"http://www.codecogs.com/png.latex?u_%7Bk%7D\" alt=\"\" width=\"17\" height=\"11\" /> and <img src=\"http://www.codecogs.com/png.latex?2u_%7Bk%7D-7\" alt=\"\" width=\"57\" height=\"16\" /> represent the same utility function. This is so it makes sense to add them.</p>\n<p>Theorem: The FAI maximizes the expected value of <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{k=1}^{n}c_{k}u_{k}}\" alt=\"\" width=\"61\" height=\"51\" />, for some set of scalars <img src=\"http://www.codecogs.com/png.latex?\\left\\{%20c_{k}\\right\\}%20_{k=1}^{n}\" alt=\"\" width=\"56\" height=\"20\" />.</p>\n<p>Actually, I changed the axioms a little bit. Harsanyi originally used &ldquo;Given any two choices A and B such that every person is indifferent between A and B, the FAI is indifferent between A and B&rdquo; in place of my axioms 2 and 3 (also he didn't call it an FAI, of course). For the proof (from Harsanyi's axioms), see section III of <a href=\"http://darp.lse.ac.uk/papersdb/Harsanyi_%28JPolE_55%29.pdf\">Harsanyi (1955)</a>, or section 2 of <a href=\"http://www.stanford.edu/~hammond/HarsanyiFest.pdf\">Hammond (1992)</a>. Hammond claims that his proof is simpler, but he uses jargon that scared me, and I found Harsanyi's proof to be fairly straightforward.</p>\n<p>Harsanyi's axioms seem fairly reasonable to me, but I can imagine someone objecting, &ldquo;But if no one else cares, what's wrong with the FAI having a preference anyway. It's not like that would harm us.&rdquo; I will concede that there is no harm in allowing the FAI to have a <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/#cont\">weak preference</a> one way or another, but if the FAI has a strong preference, that being the only thing that is reflected in the utility function, and if axiom 3 is true, then axiom 2 is violated.</p>\n<p>proof that my axioms imply Harsanyi's: Let A and B be any two choices such that every person is indifferent between A and B. By axiom 3, there exists choices C and D such that every person prefers C over D. Now consider the lotteries <img src=\"http://www.codecogs.com/png.latex?pC+\\left%281-p\\right%29A\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?pD+\\left%281-p\\right%29B\" alt=\"\" width=\"119\" height=\"19\" />, for <img src=\"http://www.codecogs.com/png.latex?p&gt;0\" alt=\"\" width=\"42\" height=\"16\" />. Notice that every person prefers the first lottery to the second, so by axiom 2, the FAI prefers the first lottery. This remains true for arbitrarily small <img src=\"http://www.codecogs.com/png.latex?p%3E0\" alt=\"\" width=\"42\" height=\"16\" />, so by continuity, the FAI must not prefer the second lottery for <img src=\"http://www.codecogs.com/png.latex?p=0\" alt=\"\" width=\"42\" height=\"16\" />; that is, the FAI must not prefer B over A. We can &ldquo;sweeten the pot&rdquo; in favor of B the same way, so by the same reasoning, the FAI must not prefer A over B.</p>\n<p>So why should you accept my axioms?</p>\n<p>Axiom 1: The VNM utility axioms are widely agreed to be necessary for any rational agent.</p>\n<p>Axiom 2: There's something a little rediculous about claiming that every member of a group prefers A to B, but that the group in aggregate does not prefer A to B.</p>\n<p>Axiom 3: This axiom is just to establish that it is even possible to aggregate the utility functions in a way that violates axiom 2. So essentially, the theorem is &ldquo;If it is possible for anything to go horribly wrong, and the FAI does not maximize a linear combination of the people's utility functions, then something will go horribly wrong.&rdquo; Also, axiom 3 will almost always be true, because it is true when the utility functions are linearly independent, and almost all finite sets of functions are linearly independent. There are terrorists who hate your freedom, but even they care at least a little bit about something other than the opposite of what you care about.</p>\n<p>At this point, you might be protesting, &ldquo;But what about equality? That's definitely a good thing, right? I want something in the FAI's utility function that accounts for equality.&rdquo; Equality is a good thing, but only because we are risk averse, and risk aversion is already accounted for in the individual utility functions. People often talk about equality being valuable even after accounting for risk aversion, but as Harsanyi's theorem shows, if you do add an extra term in the FAI's utility function to account for equality, then you risk designing an FAI that makes a choice that humanity unanimously disagrees with. Is this extra equality term so important to you that you would be willing to accept that?</p>\n<p>Remember that VNM utility has a precise decision-theoretic meaning. Twice as much utility does not correspond to your intuitions about what &ldquo;twice as much goodness&rdquo; means. Your intuitions about the best way to distribute goodness to people will not necessarily be good ways to distribute utility. The axioms I used were extremely rudimentary, whereas the intuition that generated \"there should be a term for equality or something\" is untrustworthy. If they come into conflict, you can't keep all of them. I don't see any way to justify giving up axioms 1 or 2, and axiom 3 will likely remain true whether you want it to or not, so you should probably give up whatever else you wanted to add to the FAI's utility function.</p>\n<p>Citations:</p>\n<p>Harsanyi, John C. \"Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility.\" <em>The Journal of Political Economy</em> (1955): 309-321.</p>\n<p>Hammond, Peter J. \"Harsanyi&rsquo;s utilitarian theorem: A simpler proof and some ethical connotations.\" <em>IN R. SELTEN (ED.) RATIONAL INTERACTION: ESSAYS IN HONOR OF JOHN HARSANYI</em>. 1992.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z8afQRsH9wWsB4iMD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 37, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "21029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": true, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-05T23:37:55.909Z", "modifiedAt": null, "url": null, "title": "[LINK] Why taking ideas seriously is probably a bad thing to do", "slug": "link-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "viewCount": null, "lastCommentedAt": "2020-08-21T18:40:46.928Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iS3y4xGe8Gq3hxmeM/link-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "pageUrlRelative": "/posts/iS3y4xGe8Gq3hxmeM/link-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "linkUrl": "https://www.lesswrong.com/posts/iS3y4xGe8Gq3hxmeM/link-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "postedAtFormatted": "Saturday, January 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Why%20taking%20ideas%20seriously%20is%20probably%20a%20bad%20thing%20to%20do&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Why%20taking%20ideas%20seriously%20is%20probably%20a%20bad%20thing%20to%20do%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS3y4xGe8Gq3hxmeM%2Flink-why-taking-ideas-seriously-is-probably-a-bad-thing-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Why%20taking%20ideas%20seriously%20is%20probably%20a%20bad%20thing%20to%20do%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS3y4xGe8Gq3hxmeM%2Flink-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS3y4xGe8Gq3hxmeM%2Flink-why-taking-ideas-seriously-is-probably-a-bad-thing-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>Yvain's blog: <a href=\"http://squid314.livejournal.com/350090.html\">Epistemic learned helplessness</a>.</p>\n<p style=\"padding-left: 30px;\">A friend in business recently complained about his hiring pool, saying that he couldn't find people with the basic skill of <em>believing arguments</em>.  That is, if you have a valid argument for something, then you should  accept the conclusion. Even if the conclusion is unpopular, or  inconvenient, or you don't like it. He told me a good portion of the  point of CfAR was to either find or create people who would believe  something <em>after it had been proven to them</em>.<br /><br />And I nodded  my head, because it sounded reasonable enough, and it wasn't until a few  hours later that I thought about it again and went \"Wait, no, that  would be the worst idea ever.\"<br /><br />I don't think I'm overselling  myself too much to expect that I could argue circles around the average  high school dropout. Like I mean that on almost any topic, given almost  any position, I could totally demolish her and make her look like an  idiot. Reduce her to some form of \"Look, everything you say fits  together and I can't explain why you're wrong, I just <em>know you are</em>!\" Or, more plausibly, \"Shut up I don't want to talk about this!\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LNsEBXoFdAy8yzvbw": 1, "qoTbWwaJtTSKosRCA": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iS3y4xGe8Gq3hxmeM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 32, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "21030", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-06T02:14:52.715Z", "modifiedAt": null, "url": null, "title": "[Discussion] The Kelly criterion and consequences for decision making under uncertainty", "slug": "discussion-the-kelly-criterion-and-consequences-for-decision", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wKctkFdf5QwxnmwqM/discussion-the-kelly-criterion-and-consequences-for-decision", "pageUrlRelative": "/posts/wKctkFdf5QwxnmwqM/discussion-the-kelly-criterion-and-consequences-for-decision", "linkUrl": "https://www.lesswrong.com/posts/wKctkFdf5QwxnmwqM/discussion-the-kelly-criterion-and-consequences-for-decision", "postedAtFormatted": "Sunday, January 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BDiscussion%5D%20The%20Kelly%20criterion%20and%20consequences%20for%20decision%20making%20under%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BDiscussion%5D%20The%20Kelly%20criterion%20and%20consequences%20for%20decision%20making%20under%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKctkFdf5QwxnmwqM%2Fdiscussion-the-kelly-criterion-and-consequences-for-decision%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BDiscussion%5D%20The%20Kelly%20criterion%20and%20consequences%20for%20decision%20making%20under%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKctkFdf5QwxnmwqM%2Fdiscussion-the-kelly-criterion-and-consequences-for-decision", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKctkFdf5QwxnmwqM%2Fdiscussion-the-kelly-criterion-and-consequences-for-decision", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>The <a href=\"http://en.wikipedia.org/wiki/Kelly_criterion\">Kelly criterion</a> is the optimal way to allocate one's bankroll over a lifetime to a series of bets assuming the actor's utility increases logarithmically with the amount of money won. Most importantly the criterion gives motivation to decide between investments with identical expected value but different risk of default. It essentially stipulates that the proportion of one's bankroll invested in a class of bets should be proportional to the expected value divided by the payoff in case it pans out.</p>\n<p>Now, nothing in the formalism restricts the rule to bets or money for that matter, but is applicable to any situation an actor as assumed above faces uncertainty and possible payoff in utility. Aside from the obvious application to investments, e.g. bonds, this is also applicable to the purchase of insurance or cryonic services.</p>\n<p>Buying an insurance can obviously be modeled as bet in the Kelly sense. A simple generalisation of the Kelly criterion leads to a formula that allows to incorporate losses.</p>\n<p>An open question, to me at least, is if it possible to generalise the Kelly criterion to arbitrary probability distributions. Also, how can it be that integration over all payoffs for constant expected value evaluates as infinity?</p>\n<p>Finally, how would a similar criterion look like for other forms of utility functions?</p>\n<p>&nbsp;</p>\n<p>I did not put this question in the open thread because I think the Kelly criterion deserves more of a discussion and is immediately relevant to this site's interests.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HPZzE9XBy99RmbmQe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wKctkFdf5QwxnmwqM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.0771771046289338e-06, "legacy": true, "legacyId": "21032", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-06T05:29:38.689Z", "modifiedAt": null, "url": null, "title": "Macro, not Micro", "slug": "macro-not-micro", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wcs768sL7fJX7PGmP/macro-not-micro", "pageUrlRelative": "/posts/Wcs768sL7fJX7PGmP/macro-not-micro", "linkUrl": "https://www.lesswrong.com/posts/Wcs768sL7fJX7PGmP/macro-not-micro", "postedAtFormatted": "Sunday, January 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Macro%2C%20not%20Micro&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMacro%2C%20not%20Micro%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcs768sL7fJX7PGmP%2Fmacro-not-micro%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Macro%2C%20not%20Micro%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcs768sL7fJX7PGmP%2Fmacro-not-micro", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcs768sL7fJX7PGmP%2Fmacro-not-micro", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 881, "htmlBody": "<p><strong>Overview</strong></p>\n<p>The basic observation is that, if we think of life as an optimization problem, then&nbsp;<em>redefining the search space</em>&nbsp;is much more important than&nbsp;<em>making local optimizations</em>; as a fact of human psychology it's hard to consciously focus on both; but we can implicitly get away with doing both by creating mental triggers for when local optimizations are likely to be particularly effective to think about, and by structuring things so that many local optimizations get made automatically.</p>\n<p><strong>Introduction</strong></p>\n<p>If you have been to one of the Rationality Minicamps or certain other CFAR events, you may have had the privilege to attend one of Anna Salamon's excellent classes on microeconomics (despite the title of the post, I am being sincere here; you really should attend them if you haven't already). There is too much content to briefly summarize, but essentially \"microeconomics\" in this context means applying basic microeconomic concepts like marginal value, value of information, etc. to everyday life. For example, if you spend 30 seconds brushing your teeth each day, then spending five minutes to think of something else to do at the same time (like stretching) will save you 3 hours a year, which is a great investment! (There are some caveats to this calculation, but I'm glossing over them as they aren't relevant to the post.)</p>\n<p>And indeed, spending 5 minutes (once) to save 3 hours (every year) is almost tautologically a good investment. Now that I've brought up this example, and assuming you value your time, you should probably actually go through this exercise (or just use the stretching suggestion).</p>\n<p><strong>The Problem</strong></p>\n<p>I intend to argue against something similar to this but subtly different. Basically, while any given trade such as the one above is good, I think it is a mistake <em>to systematically search for</em> such trades. Note that I also don't want to argue that you should <em>never</em> search for such trades. If you're about to buy a car you should almost certainly put a lot of microeconomic optimization into it, and if you can find things that improve your overall <em>work </em>efficiency substantially, then you are winning big-time. But I worry that, sometimes, the wrong lesson is drawn from these microeconomics examples (or cognitive bias examples, or any other rationality skill), namely that X is suboptimal by default and we should go out looking for places to optimize X.</p>\n<p>The reason I think this is wrong is because it aims much too low --- if you really want to save the world, then your <em>average thought</em>&nbsp;needs to be good enough to save two human lives [source: the average human lives only 3 billion seconds]. Even if each individual optimization you make ends up adding to the amount of time you have, the overall process of <em>concentrating your attention</em> on such optimizations makes you less likely to think <em>other</em> thoughts that would be far more valuable. Perhaps another way of putting this is that, even if each small optimization helps you a little bit, the time it takes to think up such optimizations actually makes you lose out --- however, I don't think this is actually it, I think it has more to do with forming mental habits, where you want to form the mental habit of making huge optimizations rather than small optimizations.</p>\n<p><strong>The Solution</strong></p>\n<p>What I think people should be more concerned with than micro is what I'll refer to as <em>macro </em>--- the overall structure of the search space (in this case the structure of your life and how you think) --- as opposed to making local optimizations within a fixed structure. For instance, becoming an atheist; or realizing that social skills are both trainable and highly instrumentally useful; or learning to visualize the steps towards a goal; or learning to code; or finding a group of allies that you didn't previously realize existed; these are all examples of what I'd call \"macro\" optimizations that are the sorts of things we should be looking for. (I should note that a lot of \"macro\" skills were <em>also </em>covered in Anna's microeconomics units.)</p>\n<p>I also continue to think that there is a clear place for micro-level skills, as well. The key is to incorporate them into your thought process, both at the level of creating triggers to explicitly call micro-level optimization routines when they are likely to be helpful, and at the level of restructuring your thought process to automatically be more likely to make good decisions by default. For instance, the lesson I drew from Eliezer's posts on cognitive biases were not that we should go learn about all the different cognitive biases, but that we should develop habits of thought that will automatically notice and decrease the effects of such biases. Then, for a couple of the more pernicious ones like <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconveniences</a>, I further added specific alarm bells in my head to watch out for those, but only because I noticed that avoiding trivial inconveniences was routinely harming me.</p>\n<p><strong>Conclusion</strong></p>\n<p>I'm not sure how good of a job I've done of explaining what I wanted to (it's still not entirely clear in my own head), so I invite your thoughts and feedback. I'd be particularly grateful if someone wiser than me (I'm looking at you, Critch / Wei / Yvain) could figure out what this post was trying to say and then write that instead!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wcs768sL7fJX7PGmP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "21034", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Overview\">Overview</strong></p>\n<p>The basic observation is that, if we think of life as an optimization problem, then&nbsp;<em>redefining the search space</em>&nbsp;is much more important than&nbsp;<em>making local optimizations</em>; as a fact of human psychology it's hard to consciously focus on both; but we can implicitly get away with doing both by creating mental triggers for when local optimizations are likely to be particularly effective to think about, and by structuring things so that many local optimizations get made automatically.</p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>If you have been to one of the Rationality Minicamps or certain other CFAR events, you may have had the privilege to attend one of Anna Salamon's excellent classes on microeconomics (despite the title of the post, I am being sincere here; you really should attend them if you haven't already). There is too much content to briefly summarize, but essentially \"microeconomics\" in this context means applying basic microeconomic concepts like marginal value, value of information, etc. to everyday life. For example, if you spend 30 seconds brushing your teeth each day, then spending five minutes to think of something else to do at the same time (like stretching) will save you 3 hours a year, which is a great investment! (There are some caveats to this calculation, but I'm glossing over them as they aren't relevant to the post.)</p>\n<p>And indeed, spending 5 minutes (once) to save 3 hours (every year) is almost tautologically a good investment. Now that I've brought up this example, and assuming you value your time, you should probably actually go through this exercise (or just use the stretching suggestion).</p>\n<p><strong id=\"The_Problem\">The Problem</strong></p>\n<p>I intend to argue against something similar to this but subtly different. Basically, while any given trade such as the one above is good, I think it is a mistake <em>to systematically search for</em> such trades. Note that I also don't want to argue that you should <em>never</em> search for such trades. If you're about to buy a car you should almost certainly put a lot of microeconomic optimization into it, and if you can find things that improve your overall <em>work </em>efficiency substantially, then you are winning big-time. But I worry that, sometimes, the wrong lesson is drawn from these microeconomics examples (or cognitive bias examples, or any other rationality skill), namely that X is suboptimal by default and we should go out looking for places to optimize X.</p>\n<p>The reason I think this is wrong is because it aims much too low --- if you really want to save the world, then your <em>average thought</em>&nbsp;needs to be good enough to save two human lives [source: the average human lives only 3 billion seconds]. Even if each individual optimization you make ends up adding to the amount of time you have, the overall process of <em>concentrating your attention</em> on such optimizations makes you less likely to think <em>other</em> thoughts that would be far more valuable. Perhaps another way of putting this is that, even if each small optimization helps you a little bit, the time it takes to think up such optimizations actually makes you lose out --- however, I don't think this is actually it, I think it has more to do with forming mental habits, where you want to form the mental habit of making huge optimizations rather than small optimizations.</p>\n<p><strong id=\"The_Solution\">The Solution</strong></p>\n<p>What I think people should be more concerned with than micro is what I'll refer to as <em>macro </em>--- the overall structure of the search space (in this case the structure of your life and how you think) --- as opposed to making local optimizations within a fixed structure. For instance, becoming an atheist; or realizing that social skills are both trainable and highly instrumentally useful; or learning to visualize the steps towards a goal; or learning to code; or finding a group of allies that you didn't previously realize existed; these are all examples of what I'd call \"macro\" optimizations that are the sorts of things we should be looking for. (I should note that a lot of \"macro\" skills were <em>also </em>covered in Anna's microeconomics units.)</p>\n<p>I also continue to think that there is a clear place for micro-level skills, as well. The key is to incorporate them into your thought process, both at the level of creating triggers to explicitly call micro-level optimization routines when they are likely to be helpful, and at the level of restructuring your thought process to automatically be more likely to make good decisions by default. For instance, the lesson I drew from Eliezer's posts on cognitive biases were not that we should go learn about all the different cognitive biases, but that we should develop habits of thought that will automatically notice and decrease the effects of such biases. Then, for a couple of the more pernicious ones like <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconveniences</a>, I further added specific alarm bells in my head to watch out for those, but only because I noticed that avoiding trivial inconveniences was routinely harming me.</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>I'm not sure how good of a job I've done of explaining what I wanted to (it's still not entirely clear in my own head), so I invite your thoughts and feedback. I'd be particularly grateful if someone wiser than me (I'm looking at you, Critch / Wei / Yvain) could figure out what this post was trying to say and then write that instead!</p>", "sections": [{"title": "Overview", "anchor": "Overview", "level": 1}, {"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "The Problem", "anchor": "The_Problem", "level": 1}, {"title": "The Solution", "anchor": "The_Solution", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-06T06:28:26.043Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] High Challenge", "slug": "seq-rerun-high-challenge", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/at7YEdGRqLLhvRaDR/seq-rerun-high-challenge", "pageUrlRelative": "/posts/at7YEdGRqLLhvRaDR/seq-rerun-high-challenge", "linkUrl": "https://www.lesswrong.com/posts/at7YEdGRqLLhvRaDR/seq-rerun-high-challenge", "postedAtFormatted": "Sunday, January 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20High%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20High%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fat7YEdGRqLLhvRaDR%2Fseq-rerun-high-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20High%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fat7YEdGRqLLhvRaDR%2Fseq-rerun-high-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fat7YEdGRqLLhvRaDR%2Fseq-rerun-high-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>Today's post, <a href=\"/lw/ww/high_challenge/\">High Challenge</a> was originally published on 19 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Life should not always be made easier for the same reason that video games should not always be made easier. Think in terms of eliminating low-quality work to make way for high-quality work, rather than eliminating all challenge. One needs games that are fun to <em>play </em>and not just fun to <em>win</em>. Life's utility function is over 4D trajectories, not just 3D outcomes. Values can legitimately be over the subjective experience, the objective result, and the challenging process by which it is achieved - the traveller, the destination and the journey.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g7x/seq_rerun_prolegomena_to_a_theory_of_fun/\">Prolegomena to a Theory of Fun</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "at7YEdGRqLLhvRaDR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.077329039570397e-06, "legacy": true, "legacyId": "21036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["29vqqmGNxNRGzffEj", "rgt5WCh9YPKeSt3gL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-06T15:21:30.324Z", "modifiedAt": null, "url": null, "title": "Morality is Awesome", "slug": "morality-is-awesome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:09.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome", "pageUrlRelative": "/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome", "linkUrl": "https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome", "postedAtFormatted": "Sunday, January 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20is%20Awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20is%20Awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAq8BQMXRZX3BoFd4c%2Fmorality-is-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20is%20Awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAq8BQMXRZX3BoFd4c%2Fmorality-is-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAq8BQMXRZX3BoFd4c%2Fmorality-is-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 820, "htmlBody": "<p>(This is a semi-serious introduction to <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">the metaethics sequence</a>. You may find it useful, but don't take it too seriously.)</p>\n<p><strong>Meditate on this:</strong> A wizard has turned you into a whale. Is this awesome?</p>\n<p><img src=\"http://i.imgur.com/OSnc8.png\" alt=\"Is it?\" /></p>\n<p>\"Maybe? I guess it would be pretty cool to be a whale for a day. But only if I can turn back, and if I stay human inside and so on. Also, that's not a whale.</p>\n<p>\"Actually, a whale seems kind of specific, and I'd be suprised if that was the best thing the wizard can do. Can I have something else? Eternal happiness maybe?\"</p>\n<p><strong>Meditate on this:</strong> A wizard has turned you into orgasmium, doomed to spend the rest of eternity experiencing pure happiness. Is this awesome?</p>\n<p>...</p>\n<p>\"Kindof... That's pretty lame actually. On second thought I'd rather be the whale; at least that way I could explore the ocean for a while.</p>\n<p>\"Let's try again. Wizard: maximize awesomeness.\"</p>\n<p><strong>Meditate on this:</strong> A wizard has turned himself into a superintelligent god, and is squeezing as much awesomeness out of the universe as it could possibly support. This may include whales and starships and parties and jupiter brains and friendship, but only if they are awesome enough. Is this awesome?</p>\n<p>...</p>\n<p>\"Well, yes, that is awesome.\"</p>\n<hr />\n<p>What we just did there is called Applied Ethics. Applied ethics is about what is awesome and what is not. Parties with all your friends inside superintelligent starship-whales are awesome. ~666 children dying of hunger every hour is not.</p>\n<p>(There is also normative ethics, which is about how to decide if something is awesome, and metaethics, which is about something or other that I can't quite figure out. I'll tell you right now that those terms are not on the exam.)</p>\n<p>\"Wait a minute!\" you cry, \"What is this awesomeness stuff? I thought ethics was about what is good and right.\"</p>\n<p>I'm glad you asked. I think \"awesomeness\" is what we should be talking about when we talk about morality. Why do I think this?</p>\n<ol>\n<li>\n<p>\"Awesome\" is not a <a href=\"/lw/gm9/philosophical_landmines/\">philosophical landmine</a>. If someone encounters the word \"right\", all sorts of bad philosophy and connotations send them spinning off into the void. \"Awesome\", on the other hand, has no&nbsp;philosophical&nbsp;respectability, hence no philosophical baggage.</p>\n</li>\n<li>\n<p>\"Awesome\" is vague enough to capture all your moral intuition by the well-known mechanisms behind <a href=\"/lw/lq/fake_utility_functions/\">fake utility functions</a>, and meaningless enough that this is no problem. If you think \"happiness\" is the stuff, you might get confused and try to maximize <em>actual</em> happiness. If you think awesomeness is the stuff, it is much harder to screw it up.</p>\n</li>\n<li>\n<p>If you do manage to actually implement \"awesomeness\" as a maximization criteria, the results will be actually good. That is, \"awesome\" already refers to the same things \"good\" is supposed to refer to.</p>\n</li>\n<li>\n<p>\"Awesome\" does not refer to anything else. You think you can just redefine words, <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">but you can't</a>, and this causes all sorts of trouble for people who overload \"happiness\", \"utility\", etc.</p>\n</li>\n<li>\n<p>You already know that you know how to compute \"Awesomeness\", and it doesn't feel like it has a mysterious essence that you need to study to discover. Instead it brings to mind concrete things like starship-whale math-parties and not-starving children, which is what we want anyways. You are already enabled to take <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">joy in the merely awesome</a>.</p>\n</li>\n<li>\n<p>\"Awesome\" is implicitly consequentialist. \"Is this awesome?\" engages you to think of the value of a possible world, as opposed to \"Is this right?\" which engages to to think of <a href=\"http://yudkowsky.net/rational/virtues\">virtues</a> and <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction\">rules</a>. (Those things can be awesome sometimes, though.)</p>\n</li>\n</ol>\n<p>I find that the above is true about me, and is nearly all I need to know about morality. It handily&nbsp;inoculates&nbsp;against the usual confusions, and sets me in the right direction to make my life and the world more awesome. It may work for you too.</p>\n<p>I would append the additional facts that if you wrote it out, the dynamic procedure to compute awesomeness would be <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">hellishly complex</a>, and that right now, it is only implicitly encoded in human brains, and <a href=\"/lw/y3/value_is_fragile/\">no where else</a>. Also, if the great procedure to compute awesomeness is not preserved, the future will not be awesome. Period.</p>\n<p>Also, it's important to note that what you think of as awesome can be changed by considering things from different angles and being exposed to different arguments. That is, the procedure to compute awesomeness is dynamic and <a href=\"/lw/rs/created_already_in_motion/\">created already in motion</a>.</p>\n<p>If we still insist on being confused, or if we're just curious, or if we need to actually <em>build</em> <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">a wizard</a> to turn the universe into an awesome place (though we can leave that to the <a href=\"http://intelligence.org/\">experts</a>), then we can see the <a href=\"http://wiki.lesswrong.com/wiki/Metaethics\">metaethics sequence</a> for the full argument, details, and finer points. I think the best post (and the one to read if only one) is <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">joy in the merely good</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "Z8wZZLeLMJ3NSK7kR": 1, "sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Aq8BQMXRZX3BoFd4c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 140, "baseScore": 143, "extendedScore": null, "score": 0.000322, "legacy": true, "legacyId": "21022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 143, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 437, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L4HQ3gnSrBETRdcGu", "NnohDYHNnKDtbiMyp", "FaJaCgqBKphrDzDSj", "JynJ6xfnpq9oN3zpb", "GNnHHmm8EzePmKzPk", "CuSTqHgeK4CMpWYTe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-06T21:47:34.073Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Fashion for Rationalists", "slug": "meetup-vancouver-fashion-for-rationalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/otKknwxZDJM4Rksvy/meetup-vancouver-fashion-for-rationalists", "pageUrlRelative": "/posts/otKknwxZDJM4Rksvy/meetup-vancouver-fashion-for-rationalists", "linkUrl": "https://www.lesswrong.com/posts/otKknwxZDJM4Rksvy/meetup-vancouver-fashion-for-rationalists", "postedAtFormatted": "Sunday, January 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Fashion%20for%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Fashion%20for%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotKknwxZDJM4Rksvy%2Fmeetup-vancouver-fashion-for-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Fashion%20for%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotKknwxZDJM4Rksvy%2Fmeetup-vancouver-fashion-for-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotKknwxZDJM4Rksvy%2Fmeetup-vancouver-fashion-for-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hn'>Vancouver Fashion for Rationalists</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2150 Macdonald St Vancouver BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are going to have a guest come up from Seattle (jsalvatier) to give us his presentation on fashion for rationalists. It's based on lukeprog's awesome fashion workshop from the minicamps. This will be tons of fun. We will discuss the basics of fashion, and then go shopping in kitsilano, and possibly downtown!</p>\n\n<p>The meetup will be on Sunday at 13:00 (sorry for those of you who don't like Sundays). The house we will be in is big and brown and on the corner of 6th and Macdonald. Just come knock on the door.</p>\n\n<p>As usual, if you aren't already, get on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>This is going to be fun! See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hn'>Vancouver Fashion for Rationalists</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "otKknwxZDJM4Rksvy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0778801192485225e-06, "legacy": true, "legacyId": "21039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fashion_for_Rationalists\">Discussion article for the meetup : <a href=\"/meetups/hn\">Vancouver Fashion for Rationalists</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2150 Macdonald St Vancouver BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are going to have a guest come up from Seattle (jsalvatier) to give us his presentation on fashion for rationalists. It's based on lukeprog's awesome fashion workshop from the minicamps. This will be tons of fun. We will discuss the basics of fashion, and then go shopping in kitsilano, and possibly downtown!</p>\n\n<p>The meetup will be on Sunday at 13:00 (sorry for those of you who don't like Sundays). The house we will be in is big and brown and on the corner of 6th and Macdonald. Just come knock on the door.</p>\n\n<p>As usual, if you aren't already, get on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>This is going to be fun! See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fashion_for_Rationalists1\">Discussion article for the meetup : <a href=\"/meetups/hn\">Vancouver Fashion for Rationalists</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Fashion for Rationalists", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fashion_for_Rationalists", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Fashion for Rationalists", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fashion_for_Rationalists1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-07T03:57:00.425Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Complex Novelty", "slug": "seq-rerun-complex-novelty", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B4PDNFrno5z9CgcqJ/seq-rerun-complex-novelty", "pageUrlRelative": "/posts/B4PDNFrno5z9CgcqJ/seq-rerun-complex-novelty", "linkUrl": "https://www.lesswrong.com/posts/B4PDNFrno5z9CgcqJ/seq-rerun-complex-novelty", "postedAtFormatted": "Monday, January 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Complex%20Novelty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Complex%20Novelty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4PDNFrno5z9CgcqJ%2Fseq-rerun-complex-novelty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Complex%20Novelty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4PDNFrno5z9CgcqJ%2Fseq-rerun-complex-novelty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4PDNFrno5z9CgcqJ%2Fseq-rerun-complex-novelty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p>Today's post, <a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a> was originally published on 20 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Are we likely to run out of <em>new </em>challenges, and be reduced to playing the same video game over and over? How large is Fun Space? This depends on how fast you learn; the faster you generalize, the more challenges you see as similar to each other. Learning is fun, but uses up fun; you can't have the same stroke of genius twice. But the more intelligent you are, the more potential insights you can understand; human Fun Space is larger than chimpanzee Fun Space, and not just by a linear factor of our brain size. In a well-lived life, you may need to increase in intelligence fast enough to integrate your accumulating experiences. If so, the rate at which new Fun becomes available to intelligence, is likely to overwhelmingly swamp the amount of time you could spend at that fixed level of intelligence. The Busy Beaver sequence is an infinite series of deep insights not reducible to each other or to any more general insight.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g8c/seq_rerun_high_challenge/\">High Challenge</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B4PDNFrno5z9CgcqJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.0781017619276203e-06, "legacy": true, "legacyId": "21048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aEdqh3KPerBNYvoWe", "at7YEdGRqLLhvRaDR", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-07T04:02:01.424Z", "modifiedAt": null, "url": null, "title": "How to Be Oversurprised", "slug": "how-to-be-oversurprised", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2AWwdNgvj6Ca3RsjZ/how-to-be-oversurprised", "pageUrlRelative": "/posts/2AWwdNgvj6Ca3RsjZ/how-to-be-oversurprised", "linkUrl": "https://www.lesswrong.com/posts/2AWwdNgvj6Ca3RsjZ/how-to-be-oversurprised", "postedAtFormatted": "Monday, January 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Be%20Oversurprised&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Be%20Oversurprised%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AWwdNgvj6Ca3RsjZ%2Fhow-to-be-oversurprised%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Be%20Oversurprised%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AWwdNgvj6Ca3RsjZ%2Fhow-to-be-oversurprised", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AWwdNgvj6Ca3RsjZ%2Fhow-to-be-oversurprised", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1175, "htmlBody": "<p><strong>Followup to:</strong>&nbsp;<a href=\"/lw/g26/how_to_disentangle_the_past_and_the_future/\">How to Disentangle the Past and the Future</a></p>\n<p>Some agents are memoryless, reacting to each new observation as it happens, without generating a persisting internal structure. When a LED observes voltage, it emits light, regardless of whether it did so a second earlier.</p>\n<p>Other agents have <em>very</em>&nbsp;persistent memories. The internal structure of an amber nugget can remain unchanged by external conditions for millions of years.</p>\n<p>Neither of these levels of memory persistence makes for very intelligent agents, because neither allows them to be <a href=\"/lw/g26/how_to_disentangle_the_past_and_the_future/\">good separators of the past and the future</a>. Memoryless agents only have access to the most recent input of their sensors, which leaves them oblivious to the hidden internal structures of other things around them, and to the existence of things not around them. Unchanging agents, on the other hand, fail to entangle themselves with new evidence, which prevents them from keeping up to date with a changing world.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Beliefs_require_observations\">Intelligence requires observations</a>.&nbsp;An intelligent agent needs to strike a delicate balance between the persistence of its internal structure and its susceptibility to new <a href=\"/lw/jl/what_is_evidence/\">evidence</a>. The optimal balance, a <a href=\"http://wiki.lesswrong.com/wiki/Bayes'_theorem\">Bayesian</a> <a href=\"http://wiki.lesswrong.com/wiki/Belief_update\">update</a>, <a href=\"http://yudkowsky.net/rational/bayes\">has</a>&nbsp;<a href=\"/lw/1y9/information_theory_and_the_symmetry_of_updating/\">been</a> <a href=\"/lw/jn/how_much_evidence_does_it_take/\">explained</a>&nbsp;<a href=\"/lw/1fu/why_and_why_not_bayesian_updating/\">many</a> <a href=\"/lw/ij/update_yourself_incrementally/\">times</a> <a href=\"/lw/a29/hearsay_double_hearsay_and_bayesian_updates/\">before</a>, and was shown to be <a href=\"/lw/e6a/the_bayesian_agent/\">optimal in keeping information</a> about the world. This post highlights yet another aspect.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Suppose I predicted that a roll of a die will give 4, and then it did. How surprised would you be?</p>\n<p>You may intuitively realize that the degree of your surprise should be a decreasing function of the probability that you assign to the event. Predicting a 4 on a fair die is more surprising than on one that is loaded in favor of 4.&nbsp;You may also want the measure of surprise to be <a href=\"http://en.wikipedia.org/wiki/Intensive_and_extensive_properties\">extensive</a>: if I repeated the feat a second time, you would be twice as surprised.</p>\n<p>In that case, there's essentially one consistent notion of surprise (also called: <a href=\"http://en.wikipedia.org/wiki/Self-information\">self-information</a>, surprisal). The amount of surprise that a random variable X has value x is</p>\n<p style=\"padding-left: 30px;\">S(x) = -log Pr(X=x).</p>\n<p>This is the negative <a href=\"/lw/8lr/logodds_or_logits/\">logarithm of the probability</a> of the event X=x.</p>\n<p>This is a very useful concept in information theory. For example, the <a href=\"/lw/o1/entropy_and_short_codes/\">entropy</a> of the random variable X is the surprise we expect to have upon seeing its value. The <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">mutual information</a> between X and another random variable Y is the difference between how much we expect x to surprise us now, and how much we expect it to surprise us after we first look at y.</p>\n<p>The surprise of seeing 4 when rolling a fair die is</p>\n<p style=\"padding-left: 30px;\">S(1/6) = -log(1/6)&nbsp;&asymp;&nbsp;2.585 bit.</p>\n<p>That's also the surprise of any other result, so that's also the entropy of a die.</p>\n<p>By the way, the objective surprise of a specific result is the same whether or not I actually announce it as a prediction. The reason you don't \"feel surprised\" when no prediction is made, is that your intuition evolved to successfully avoid&nbsp;<a href=\"/lw/il/hindsight_bias/\">hindsight bias</a>. As we'll see in a moment, not all surprise is useful evidence.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The Bayesian update is optimal in that it gives the agent the <a href=\"/lw/e6a/the_bayesian_agent/\">largest possible gain</a> in information about the world. Exactly how much information is gained?</p>\n<p>We might ask this question when we choose between an agent that doesn't update at all and one that updates perfectly: this is what we stand to gain, and we can compare it with the cost (in energy, computation power, etc.) of actually performing the update.</p>\n<p>We can also ask this question when we consider what observations to make. Not updating on new evidence is equivalent, in terms of information, to not gathering it in the first place. So the benefit of gathering some more evidence is, at most, the benefit of subsequently using it in a Bayesian update.</p>\n<p>Suppose that at time t the world is in a state W<sub>t</sub>, and that the agent may look at it and make an observation O<sub>t</sub>. Objectively, the surprise of this observation would be</p>\n<p style=\"padding-left: 30px;\">S<sub>obj</sub> = S(O<sub>t</sub>|W<sub>t</sub>) =&nbsp;-log Pr(O<sub>t</sub>|W<sub>t</sub>).</p>\n<p>However, the agent doesn't know the state of the world. Before seeing the observation at time t, the agent has its own memory state M<sub>t-1</sub>, which is entangled with the state of the world through past observations, but is not enough for the agent to know everything about the world.</p>\n<p>The agent has a <em>subjective</em>&nbsp;surprise upon seeing O<sub>t</sub>, which is the result of its own private prior:</p>\n<p style=\"padding-left: 30px;\">S<sub>subj</sub> = S(O<sub>t</sub>|M<sub>t-1</sub>) =&nbsp;-log Pr(O<sub>t</sub>|M<sub>t-1</sub>),</p>\n<p>and this may be significantly different than the objective surprise.</p>\n<p>Interestingly, the amount of information that the agent stands to gain by making the new observation, and perfectly updating on it in a new memory state M<sub>t</sub>, is exactly equal to the agent's expected <em>oversurprise</em>&nbsp;upon seeing the evidence. That is, any update from M<sub>t-1</sub>&nbsp;to M<sub>t</sub>&nbsp;gains at most this much information:</p>\n<p style=\"padding-left: 30px;\">I(W<sub>t</sub>;M<sub>t</sub>) - I(W<sub>t</sub>;M<sub>t-1</sub>)&nbsp;&le;&nbsp;E[S<sub>subj</sub>&nbsp;- S<sub>obj</sub>],</p>\n<p>and this holds with equality when the update is Bayesian.&nbsp;(The math is <a href=\"/r/lesswrong/lw/g6b/how_to_be_oversurprised/88dj\">below in a comment</a>.)</p>\n<p>For example, let's say I have two coins, one of them fair and the other turns up heads with probability 0.7. I don't know which coin is which, so I toss one of them at random. I expect it to show heads with probability</p>\n<p style=\"padding-left: 30px;\">0.5 / 2 + 0.7 / 2 = 0.6,</p>\n<p>so if it does I'll be surprised&nbsp;S(0.6)&nbsp;&asymp;&nbsp;0.737 bit,&nbsp;and if it doesn't I'll be surprised&nbsp;S(0.4)&nbsp;&asymp;&nbsp;1.322 bit. On average, I expect to be surprised</p>\n<p style=\"padding-left: 30px;\">0.6 * S(0.6) + 0.4 * S(0.4)&nbsp;&asymp; 0.971 bit.</p>\n<p>The objective surprise depends on <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">how the world really is</a>. If I toss the fair coin, the objective surprise is&nbsp;S(0.5) = 1 bit&nbsp;for each of the two outcomes, and the expectation is 1 bit too. If I toss the loaded coin, the objective surprise is&nbsp;S(0.7)&nbsp;&asymp;&nbsp;0.515 bit for heads and&nbsp;S(0.3)&nbsp;&asymp;&nbsp;1.737 bit for tails, for an average of</p>\n<p style=\"padding-left: 30px;\">0.7 * S(0.7) + 0.3 * S(0.3)&nbsp;&asymp; 0.881 bit.</p>\n<p>See how I'm a little undersurprised for the fair coin, but oversurprised for the loaded one. On average I'm oversurprised by</p>\n<p style=\"padding-left: 30px;\">0.971 - (1 / 2 +&nbsp;0.881 / 2) =&nbsp;0.0305 bit.</p>\n<p>By observing which side the coin turns up, I gain 0.971 bit of information, but most of it is just about this specific toss. Only 0.0305 bit of that information, my <em>over</em>surprise, goes beyond the specific toss to teach me something about the coin itself, if I update perfectly.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>We are not interested in evidence for their own sake, only inasmuch as they teach us about the world. Winning the lottery is very surprising, but nobody gambles for epistemological reasons. When the odds are known in advance, the subjective surprise is exactly matched by the objective surprise - you are not oversurprised, and you learn nothing useful.</p>\n<p>The oversurprise is the \"spillover\" of surprise beyond what is just about the observation, and onto the world. It's the part of the&nbsp;<a href=\"/lw/ic/the_virtue_of_narrowness/\">narrowness</a> that originates in the world, not in the observation. And that's how much the observation teaches us about the world.</p>\n<p>An interesting corollary is that you can never expect to be undersurprised, i.e. less surprised than you objectively should be. That's the same as saying that a Bayesian update can never lose information.</p>\n<p>The next time you find yourself wondering how best to observe the world and gather information, ask this: how much more surprised do you expect to be, than someone who already knows what you wish to know.</p>\n<p><strong>Continue reading:</strong>&nbsp;<a href=\"/lw/gcr/update_then_forget/\">Update Then Forget</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2AWwdNgvj6Ca3RsjZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 1.07810477216454e-06, "legacy": true, "legacyId": "20963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rF3GBk9Sgsn75GPWk", "6s3xABaXKPdFwA3FS", "SEZqJcSm25XpQMhzr", "nj8JKFoLSMEmD3RGp", "W6nXfmKTrgaiaLSRg", "627DZcvme7nLDrbZu", "iyNLFkEEXoSrPYFng", "G4XKiJ2Q93JGCJxCT", "6Ltniokkr3qt7bzWw", "soQX8yXLbKy7cFvy8", "yLcuygFfMfrfK8KjF", "fkM9XsNvXdYH6PPAx", "yDfxTj9TKYsYiWH5o", "z9pYKozn6fLmwZrDn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-07T09:08:24.252Z", "modifiedAt": null, "url": null, "title": "Nate Silver will do an AMA on Reddit on Tuesday", "slug": "nate-silver-will-do-an-ama-on-reddit-on-tuesday", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kST3AM5ckQs9LeL9A/nate-silver-will-do-an-ama-on-reddit-on-tuesday", "pageUrlRelative": "/posts/kST3AM5ckQs9LeL9A/nate-silver-will-do-an-ama-on-reddit-on-tuesday", "linkUrl": "https://www.lesswrong.com/posts/kST3AM5ckQs9LeL9A/nate-silver-will-do-an-ama-on-reddit-on-tuesday", "postedAtFormatted": "Monday, January 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nate%20Silver%20will%20do%20an%20AMA%20on%20Reddit%20on%20Tuesday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANate%20Silver%20will%20do%20an%20AMA%20on%20Reddit%20on%20Tuesday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkST3AM5ckQs9LeL9A%2Fnate-silver-will-do-an-ama-on-reddit-on-tuesday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nate%20Silver%20will%20do%20an%20AMA%20on%20Reddit%20on%20Tuesday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkST3AM5ckQs9LeL9A%2Fnate-silver-will-do-an-ama-on-reddit-on-tuesday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkST3AM5ckQs9LeL9A%2Fnate-silver-will-do-an-ama-on-reddit-on-tuesday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>http://www.reddit.com/r/IAmA/comments/163nqk/nate_silver_is_doing_an_ama_tuesday_at_2_pm/</p>\n<p>I'm really excited to see this. Nate Silver might be the most famous present day Bayesian statistician.</p>\n<p>UPD: It appears that author of the Reddit post deleted it for some reason. The link still works but it makes sense to post the link to the Nate Silver blog with his original announcement, just in case: http://fivethirtyeight.blogs.nytimes.com/2013/01/06/ask-nate-anything/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YgizoZqa7LEb3LEJn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kST3AM5ckQs9LeL9A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.0782886454782322e-06, "legacy": true, "legacyId": "21050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-07T14:38:48.275Z", "modifiedAt": null, "url": null, "title": "Don't Build Fallout Shelters", "slug": "don-t-build-fallout-shelters", "viewCount": null, "lastCommentedAt": "2019-01-25T20:45:55.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hYFj8TLELv4P4RQuH/don-t-build-fallout-shelters", "pageUrlRelative": "/posts/hYFj8TLELv4P4RQuH/don-t-build-fallout-shelters", "linkUrl": "https://www.lesswrong.com/posts/hYFj8TLELv4P4RQuH/don-t-build-fallout-shelters", "postedAtFormatted": "Monday, January 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20Build%20Fallout%20Shelters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20Build%20Fallout%20Shelters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYFj8TLELv4P4RQuH%2Fdon-t-build-fallout-shelters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20Build%20Fallout%20Shelters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYFj8TLELv4P4RQuH%2Fdon-t-build-fallout-shelters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYFj8TLELv4P4RQuH%2Fdon-t-build-fallout-shelters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 714, "htmlBody": "<p><strong>Related:</strong>&nbsp;<a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a></p>\n<p>One thing that many people misunderstand is the concept of personal versus societal safety. These concepts are often conflated despite the appropriate mindsets being quite different.</p>\n<p>Simply put, personal safety is <em>personal.</em></p>\n<p><em></em>In other words, the appropriate actions to take for personal safety are whichever actions reduce your chance of being injured or killed <a href=\"http://squid314.livejournal.com/260949.html\">within reasonable cost boundaries</a>. These actions are largely based on situational factors because the elements of risk that two given people experience may be wildly disparate.</p>\n<p>For instance, if you are currently a young computer programmer living in a typical American city, you may want to look at eating better, driving your car less often, and giving up unhealthy habits like smoking.&nbsp;However, if you are currently an infantryman about to deploy to Afghanistan, you may want to look at improving your reaction time, training your&nbsp;<a href=\"http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA372709\">situational awareness</a>, and practicing rifle shooting under stressful conditions.</p>\n<p>One common mistake is to attempt to preserve personal safety for extreme circumstances such as nuclear wars. Some individuals invest sizeable amounts of money into fallout shelters, years worth of emergency supplies, etc.</p>\n<p>While it is certainly true that a nuclear war would kill or severely disrupt you if it occurred, this is not necessarily a fully convincing argument in favor of building a fallout shelter.&nbsp;One has to consider <a href=\"http://www.bankrate.com/finance/weather/natural-disasters/nuts-bolts-fallout-shelter.aspx\">the cost of building a fallout shelter</a>, the chance that your fallout shelter will actually save you in the event of a nuclear war, and the odds of a nuclear war actually occurring.</p>\n<p>Further, one must consider the quality of life reduction that one would likely experience in a post-nuclear war world. It's also important to remember that, in the long run, your survival is contingent on access to medicine and scientific progress. <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">Future medical advances</a> may even extend your lifespan very dramatically, and potentially provide <a href=\"/lw/xy/the_fun_theory_sequence/\">very large amounts of utility</a>.&nbsp;Unfortunately, full-scale nuclear war is very likely to impair medicine and science for quite some time, perhaps permanently.</p>\n<p>Thus even if your fallout shelter succeeds, you will likely live a shorter and less pleasant life than you would otherwise.&nbsp;In the end, building a fallout shelter looks like an unwise investment unless you are extremely confident that a nuclear war will occur shortly-- and if you are, I want to see your data!</p>\n<p>When taking personal precautionary measures, worrying about such catastrophes is generally silly, especially given the risks we all take on a regular basis-- risks that, in most cases, are much easier to avoid than nuclear wars. Societal disasters are generally extremely expensive for the individual to protect against, and carry a large amount of disutility even if protections succeed.</p>\n<p>To make matters worse, if there's a nuclear war tomorrow and your house is hit directly, you'll be just as dead as if you fall off your bike and break your neck. Dying in a more dramatic fashion does not, generally speaking, produce more disutility than dying in a mundane fashion does.&nbsp;In other words, when optimizing for personal safety, focus on accidents, not nuclear wars; buy a bike helmet, not a fallout shelter.</p>\n<p class=\"p1\">The flip side to this, of course, is that if there <em>is</em> a full-scale nuclear war, hundreds of millions-- if not billions-- of people will die and society will be permanently disrupted. If you die in a bike accident tomorrow, perhaps a half dozen people will be killed at most. So when we focus on non-selfish actions, <a href=\"/lw/n3/circular_altruism/\">the big picture is far, far, <em>far</em> more important</a>. If you can reduce the odds of a nuclear war by one one-thousandth of one percent, more lives will be saved on average than if you can prevent hundreds of fatal accidents.</p>\n<p class=\"p1\">When optimizing for overall safety, focus on <a href=\"/lw/8f0/existential_risk/\">the biggest possible threats</a> that <a href=\"http://www.existential-risk.org/concept.pdf\">you can have an impact on.</a>&nbsp;In other words, when dealing with societal-level risks, your projected impact will be much higher if you try to focus on protecting society instead of protecting yourself.</p>\n<p class=\"p1\">In the end, building fallout shelters is probably silly, but attempting to reduce the risk of nuclear war sure as hell isn't.&nbsp;And if you do end up worrying about whether a nuclear war is about to happen, remember that if you can reduce the risk of said war-- which might be as easy as <a href=\"http://en.wikipedia.org/wiki/The_Day_After#Effects_on_policymakers\">making a movie</a>-- your actions will have a much, much greater <a href=\"/lw/kn/torture_vs_dust_specks/\">overall impact</a> than building a shelter ever could.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xXX3n22DQZuKqXEdT": 1, "yAmE3StuxBmzCBPWq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hYFj8TLELv4P4RQuH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 46, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "21051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 127, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ZzefKQwAtMo5yp99", "K4aGvLnHvYgX9pZHS", "FGTgeweYNxmMBx4fz", "3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-08T02:20:18.676Z", "modifiedAt": null, "url": null, "title": "January 2013 Media Thread", "slug": "january-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NBwNNRzn6Lk7NfEui/january-2013-media-thread", "pageUrlRelative": "/posts/NBwNNRzn6Lk7NfEui/january-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/NBwNNRzn6Lk7NfEui/january-2013-media-thread", "postedAtFormatted": "Tuesday, January 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBwNNRzn6Lk7NfEui%2Fjanuary-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBwNNRzn6Lk7NfEui%2Fjanuary-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBwNNRzn6Lk7NfEui%2Fjanuary-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;<a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NBwNNRzn6Lk7NfEui", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 1.0789083494230198e-06, "legacy": true, "legacyId": "21053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-08T06:30:44.942Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Sensual Experience", "slug": "seq-rerun-sensual-experience", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RSdJjWFpDFGE5GRio/seq-rerun-sensual-experience", "pageUrlRelative": "/posts/RSdJjWFpDFGE5GRio/seq-rerun-sensual-experience", "linkUrl": "https://www.lesswrong.com/posts/RSdJjWFpDFGE5GRio/seq-rerun-sensual-experience", "postedAtFormatted": "Tuesday, January 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Sensual%20Experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Sensual%20Experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSdJjWFpDFGE5GRio%2Fseq-rerun-sensual-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Sensual%20Experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSdJjWFpDFGE5GRio%2Fseq-rerun-sensual-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSdJjWFpDFGE5GRio%2Fseq-rerun-sensual-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>Today's post, <a href=\"/lw/wy/sensual_experience/\">Sensual Experience</a> was originally published on 21 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Much of the anomie and disconnect in modern society can be attributed to our spending all day on tasks (like office work) that we didn't evolve to perform (unlike hunting and gathering on the savanna). Thus, many of the tasks we perform all day do not <em>engage our senses</em> - even the most realistic modern video game is not the same level of <em>sensual </em>experience as outrunning a real tiger on the real savanna. Even the best modern video game is <em>low-bandwidth fun</em> - a low-bandwidth connection to a relatively simple challenge, which doesn't fill our brains well as a result. But future entities could have different senses and higher-bandwidth connections to more complicated challenges, even if those challenges didn't exist on the savanna.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g8o/seq_rerun_complex_novelty/\">Complex Novelty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RSdJjWFpDFGE5GRio", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0790588429249e-06, "legacy": true, "legacyId": "21060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eLHCWi8sotQT6CmTX", "B4PDNFrno5z9CgcqJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-08T12:27:03.046Z", "modifiedAt": null, "url": null, "title": "[Link] Your Elusive Future Self", "slug": "link-your-elusive-future-self", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gNhvixbKGchPxYq6F/link-your-elusive-future-self", "pageUrlRelative": "/posts/gNhvixbKGchPxYq6F/link-your-elusive-future-self", "linkUrl": "https://www.lesswrong.com/posts/gNhvixbKGchPxYq6F/link-your-elusive-future-self", "postedAtFormatted": "Tuesday, January 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Your%20Elusive%20Future%20Self&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Your%20Elusive%20Future%20Self%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNhvixbKGchPxYq6F%2Flink-your-elusive-future-self%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Your%20Elusive%20Future%20Self%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNhvixbKGchPxYq6F%2Flink-your-elusive-future-self", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNhvixbKGchPxYq6F%2Flink-your-elusive-future-self", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 267, "htmlBody": "<p>A <a href=\"http://news.sciencemag.org/sciencenow/2013/01/your-elusive-future-self.html\">current article in Science</a> reports on <a href=\"http://www.wjh.harvard.edu/~dtg/Quoidbach%20et%20al%202013.pdf\">this study</a> about how good people are at predicting what their future selves will be like. Not very good, apparently.&nbsp;Daniel Gilbert, a psychologist at Harvard, with other colleagues&nbsp;conducted&nbsp;several experiments&nbsp;online, in which 19,000 people were asked about such things as personality traits, preferences in music, etc., answering about the present, about themselves 10 years earlier, and about what they expected 10 years hence. More precisely, this not being a longitudinal study, people of any age X predicted less difference with their X+10 selves than people of age X+10 recollected of themselves at age X. The effect did not go away with increasing age: 58-year-olds still expected less change in the next 10 years than 68-year-olds reported in the last ten.</p>\n<p style=\"padding-left: 30px;\">Gilbert and colleagues call this effect \"the end of history illusion,\" because it suggests that people believe, consciously or not, that the present marks the point at which they've finally stopped changing.</p>\n<p style=\"padding-left: 30px;\">\"What these data suggest, and what scads of other data from our lab and others suggest, is that people really aren't very good at knowing who they're going to be and hence what they're going to want a decade from now,\" Gilbert says.</p>\n<p>Someone suggests an alternative explanation:</p>\n<p style=\"padding-left: 30px;\">Another possibility is that people \"might well anticipate substantial change, yet not know how they would change, and thus, just predict the status quo\"</p>\n<p>An actionable moral:</p>\n<p style=\"padding-left: 30px;\">\"The single best way to make predictions about what you're going to want in the future isn't to imagine yourself in the future, &hellip; it's to look at other people who are in the very future you're imagining,\" [Gilbert] says.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gNhvixbKGchPxYq6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 28, "extendedScore": null, "score": 1.0792730165377786e-06, "legacy": true, "legacyId": "21064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-08T20:40:42.392Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Board games!", "slug": "meetup-berkeley-meetup-board-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LiBTQBRvFc6Luy9Jc/meetup-berkeley-meetup-board-games", "pageUrlRelative": "/posts/LiBTQBRvFc6Luy9Jc/meetup-berkeley-meetup-board-games", "linkUrl": "https://www.lesswrong.com/posts/LiBTQBRvFc6Luy9Jc/meetup-berkeley-meetup-board-games", "postedAtFormatted": "Tuesday, January 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Board%20games!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Board%20games!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiBTQBRvFc6Luy9Jc%2Fmeetup-berkeley-meetup-board-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Board%20games!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiBTQBRvFc6Luy9Jc%2Fmeetup-berkeley-meetup-board-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiBTQBRvFc6Luy9Jc%2Fmeetup-berkeley-meetup-board-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ho'>Berkeley meetup: Board games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, all! Zendo will host its first Less Wrong meetup of the year on Wednesday at 7pm. Come and play a game from Zendo's library of board games, or bring a board game that you particularly want to play. I look forward to seeing everyone again. If anyone is interested I will share stories about the Solstice Eve ritual that Raemon from Less Wrong put together.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ho'>Berkeley meetup: Board games!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LiBTQBRvFc6Luy9Jc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0795698782925586e-06, "legacy": true, "legacyId": "21068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Board_games_\">Discussion article for the meetup : <a href=\"/meetups/ho\">Berkeley meetup: Board games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, all! Zendo will host its first Less Wrong meetup of the year on Wednesday at 7pm. Come and play a game from Zendo's library of board games, or bring a board game that you particularly want to play. I look forward to seeing everyone again. If anyone is interested I will share stories about the Solstice Eve ritual that Raemon from Less Wrong put together.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Board_games_1\">Discussion article for the meetup : <a href=\"/meetups/ho\">Berkeley meetup: Board games!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Board games!", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Board_games_", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Board games!", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Board_games_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T06:42:35.003Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Living By Your Own Strength", "slug": "seq-rerun-living-by-your-own-strength", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QxWgkGNDYMHXRgfLS/seq-rerun-living-by-your-own-strength", "pageUrlRelative": "/posts/QxWgkGNDYMHXRgfLS/seq-rerun-living-by-your-own-strength", "linkUrl": "https://www.lesswrong.com/posts/QxWgkGNDYMHXRgfLS/seq-rerun-living-by-your-own-strength", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Living%20By%20Your%20Own%20Strength&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Living%20By%20Your%20Own%20Strength%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxWgkGNDYMHXRgfLS%2Fseq-rerun-living-by-your-own-strength%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Living%20By%20Your%20Own%20Strength%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxWgkGNDYMHXRgfLS%2Fseq-rerun-living-by-your-own-strength", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxWgkGNDYMHXRgfLS%2Fseq-rerun-living-by-your-own-strength", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>Today's post, <a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a> was originally published on 22 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Living_By_Your_Own_Strength\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our hunter-gatherer ancestors strung their own bows, wove their own baskets and whittled their own flutes. Part of our alienation from our design environment is the number of tools we use that we don't understand and couldn't make for ourselves. It's much less fun to read something in a book than to discover it for yourself. Specialization is critical to our current civilization. But the future does not have to be a continuation of this trend in which we rely more and more on things outside ourselves which become less and less comprehensible. With a surplus of power, you could begin to rethink the life experience as a road to internalizing new strengths, not just staying alive efficiently through extreme specialization.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g90/seq_rerun_sensual_experience/\">Sensual Experience</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QxWgkGNDYMHXRgfLS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0799320145443164e-06, "legacy": true, "legacyId": "21077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dKGfNvjGjq4rqffyF", "RSdJjWFpDFGE5GRio", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T07:42:17.345Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Applied Rationality", "slug": "meetup-moscow-applied-rationality-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QJg7qyh3EdW3DgNck/meetup-moscow-applied-rationality-0", "pageUrlRelative": "/posts/QJg7qyh3EdW3DgNck/meetup-moscow-applied-rationality-0", "linkUrl": "https://www.lesswrong.com/posts/QJg7qyh3EdW3DgNck/meetup-moscow-applied-rationality-0", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Applied%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Applied%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJg7qyh3EdW3DgNck%2Fmeetup-moscow-applied-rationality-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Applied%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJg7qyh3EdW3DgNck%2Fmeetup-moscow-applied-rationality-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJg7qyh3EdW3DgNck%2Fmeetup-moscow-applied-rationality-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hp'>Moscow: Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our venue is changed:</p>\n\n<p>We will meet in the Yandex Money office, this is the second door at the address I mentioned. Please use the following guide to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. I will be there at 16:00 MSK. And I will also check the entrance at 16:15 and 16:30, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. The next one will be calibration session.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n<li><p>Solving cases. Please prepare some problems for discussion. You can contact me to propose your problems and cases in advance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason. Please also contact me, if you are not sure you can be in time and I provide you with an emergency phone number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hp'>Moscow: Applied Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QJg7qyh3EdW3DgNck", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.079967949846736e-06, "legacy": true, "legacyId": "21079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality\">Discussion article for the meetup : <a href=\"/meetups/hp\">Moscow: Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our venue is changed:</p>\n\n<p>We will meet in the Yandex Money office, this is the second door at the address I mentioned. Please use the following guide to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. I will be there at 16:00 MSK. And I will also check the entrance at 16:15 and 16:30, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. The next one will be calibration session.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n<li><p>Solving cases. Please prepare some problems for discussion. You can contact me to propose your problems and cases in advance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason. Please also contact me, if you are not sure you can be in time and I provide you with an emergency phone number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/hp\">Moscow: Applied Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T12:27:13.277Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZCq23udy3bHmfr6a/meetup-melbourne-social-meetup-0", "pageUrlRelative": "/posts/oZCq23udy3bHmfr6a/meetup-melbourne-social-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/oZCq23udy3bHmfr6a/meetup-melbourne-social-meetup-0", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZCq23udy3bHmfr6a%2Fmeetup-melbourne-social-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZCq23udy3bHmfr6a%2Fmeetup-melbourne-social-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZCq23udy3bHmfr6a%2Fmeetup-melbourne-social-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hq'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 January 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">See mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 18th January at the usual venue in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the location/any questions, please see the Melbourne Less Wrong google group, or feel free to SMS me on 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hq'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZCq23udy3bHmfr6a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0801394724907064e-06, "legacy": true, "legacyId": "21080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/hq\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 January 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">See mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 18th January at the usual venue in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the location/any questions, please see the Melbourne Less Wrong google group, or feel free to SMS me on 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/hq\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T13:38:02.754Z", "modifiedAt": null, "url": null, "title": "DRAFT:Ethical Zombies - A Post On Reality-Fluid", "slug": "draft-ethical-zombies-a-post-on-reality-fluid", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:26.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MugaSofer", "createdAt": "2012-07-10T12:52:51.220Z", "isAdmin": false, "displayName": "MugaSofer"}, "userId": "Mr6d3wsfsNidTFGHJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q9r2NJyTiek6tF6kr/draft-ethical-zombies-a-post-on-reality-fluid", "pageUrlRelative": "/posts/Q9r2NJyTiek6tF6kr/draft-ethical-zombies-a-post-on-reality-fluid", "linkUrl": "https://www.lesswrong.com/posts/Q9r2NJyTiek6tF6kr/draft-ethical-zombies-a-post-on-reality-fluid", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DRAFT%3AEthical%20Zombies%20-%20A%20Post%20On%20Reality-Fluid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADRAFT%3AEthical%20Zombies%20-%20A%20Post%20On%20Reality-Fluid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9r2NJyTiek6tF6kr%2Fdraft-ethical-zombies-a-post-on-reality-fluid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DRAFT%3AEthical%20Zombies%20-%20A%20Post%20On%20Reality-Fluid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9r2NJyTiek6tF6kr%2Fdraft-ethical-zombies-a-post-on-reality-fluid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9r2NJyTiek6tF6kr%2Fdraft-ethical-zombies-a-post-on-reality-fluid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 595, "htmlBody": "<p><em style=\"font-size: 14px;\">I came up with this after watching a science fiction film, which shall remain nameless due to spoilers, where the protagonist is briefly in a similar situation to the scenario at the end. I'm not sure how&nbsp;original&nbsp;it is, but I certainly don't recall seeing anything like it before.</em><br style=\"font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px;\" /></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Imagine, for simplicity, a purely selfish agent. Call it Alice. Alice is an expected utility&nbsp;maximizer, and she gains utility from eating cakes. Omega appears and offers her a deal - they will flip a fair coin, and give Alice three cakes if it comes up heads. If it comes up tails, they will take one cake away her stockpile. Alice runs the numbers, determines that the expected utility is positive, and accepts the deal. Just another day in the life of a perfectly truthful superintelligence offering inexplicable choices.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">The next day, Omega returns. This time, they offer a slightly different deal - instead of flipping a coin, they will perfectly simulate Alice once. This copy will live out her life just as she would have done in reality - except that she will be given three cakes. The&nbsp;</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">original</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">&nbsp;Alice, however,&nbsp;</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">receives</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">&nbsp;nothing. She reasons that this is&nbsp;</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">equivalent</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">&nbsp;to the last deal, and accepts.</span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">(If you disagree, consider the time between Omega starting the simulation and providing the cake. What subjective odds should she give for&nbsp;receiving&nbsp;cake?)</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Imagine a second agent, Bob, who gets utility from Alice getting utility. One day, Omega show up and offers to flip a fair coin. If it comes up heads, they will give Alice - who knows nothing of this - three cakes. If it comes up tails, they will take one cake from her stockpile. He reasons as Alice did an accepts.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Guess what? The next day, Omega returns, offering to simulate Alice and give her you-know-what (hint: it's cakes.) Bob reasons just as Alice did in the second paragraph there and accepts the bargain.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Humans value each other's utility. Most notably, we value our lives, and we value each other not being tortured. If we simulate someone a billion times, and switch off one simulation, this is&nbsp;equivalent&nbsp;to risking their life at odds of 1:1,000,000,000. If we simulate someone and torture one of the simulations, this is&nbsp;equivalent&nbsp;to risking a one-in-a-billion chance of them being tortured. Such risks are often acceptable, if enough utility is gained by success. We often risk our own lives at worse odds.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">If we simulate an entire society a trillion times, or 3^^^^^^3 times, or some&nbsp;similarly&nbsp;vast number, and then simulate something horrific - an individual's private harem or torture chamber or hunting ground - then the people in this simulation *are not real*. Their needs and desires are worth, not nothing, but far less then the merest whims of those who are Really Real. They are, in effect, zombies - not quite p-zombies, since they are conscious, but e-zombies - reasoning, intelligent beings that can talk and scream and beg for mercy but *do not matter*.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">My mind rebels at the notion that such a thing might exist, even in theory, and yet ... if it were a similarly tiny *chance*, for similar reward, I would shut up and multiply and take it. This could be simply scope insensitivity, or some instinctual dislike of tribe members declaring themselves superior.</span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><em>Well, there it is! The&nbsp;weirdest&nbsp;of Weirdtopias, I should think. Have I missed some obvious flaw? Have I made some sort of technical error? This is a draft, so criticisms will likely be encorporated into the final product (if indeed someone doesn't disprove it entirely.)</em></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q9r2NJyTiek6tF6kr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "21066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T14:33:50.300Z", "modifiedAt": null, "url": null, "title": "Course recommendations for Friendliness researchers", "slug": "course-recommendations-for-friendliness-researchers", "viewCount": null, "lastCommentedAt": "2015-10-17T11:38:05.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/snzFQJsNYqzPZS2nK/course-recommendations-for-friendliness-researchers", "pageUrlRelative": "/posts/snzFQJsNYqzPZS2nK/course-recommendations-for-friendliness-researchers", "linkUrl": "https://www.lesswrong.com/posts/snzFQJsNYqzPZS2nK/course-recommendations-for-friendliness-researchers", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Course%20recommendations%20for%20Friendliness%20researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACourse%20recommendations%20for%20Friendliness%20researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsnzFQJsNYqzPZS2nK%2Fcourse-recommendations-for-friendliness-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Course%20recommendations%20for%20Friendliness%20researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsnzFQJsNYqzPZS2nK%2Fcourse-recommendations-for-friendliness-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsnzFQJsNYqzPZS2nK%2Fcourse-recommendations-for-friendliness-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2897, "htmlBody": "<p>When I first learned about Friendly AI, I assumed it was mostly a programming problem. As it turns out, it's actually mostly a&nbsp;<em>math&nbsp;</em>problem. That's because&nbsp;most of the theory behind self-reference, decision theory, and general AI techniques haven't been <a href=\"http://lukeprog.com/SaveTheWorld.html\">formalized and solved yet</a>.&nbsp;Thus, when people ask me what they should study in order to work on Friendliness theory, I say \"Go study math and theoretical computer science.\"</p>\n<p>But that's not specific enough. Should aspiring Friendliness researchers study continuous or discrete math? Imperative or functional programming? Topology? Linear algebra? Ring theory?</p>\n<p>I do, in fact, have&nbsp;<em>specific</em>&nbsp;recommendations for which subjects Friendliness researchers should study. And so I worked with a few of my best&nbsp;<a href=\"http://intelligence.org/interns/\">interns</a> at MIRI to provide recommendations below:</p>\n<ul>\n<li style=\"margin-left: 15px;\"><strong>University courses</strong>. We carefully hand-picked courses on these subjects from four leading universities &mdash; but we aren't omniscient! If you're at one of these schools and can give us feedback on the exact courses we've recommended, please do so.</li>\n<li style=\"margin-left: 15px;\"><strong>Online courses</strong>. We also linked to online courses, for the majority of you who aren't able to attend one of the four universities whose course catalogs we dug into. Feedback on these online courses is also welcome; we've only taken a few of them.</li>\n<li style=\"margin-left: 15px;\"><strong>Textbooks</strong>. We&nbsp;<em>have</em>&nbsp;read nearly all the textbooks recommended below, along with many of their <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\" target=\"_blank\">competitors</a>. If you're a strongly motivated autodidact, you could learn these subjects by diving into the books on your own and doing the exercises.</li>\n</ul>\n<p>Have you already taken most of the subjects below? If so, and you're interested in Friendliness research, then you should <em>definitely</em> contact me or our project manager Malo Bourgon (<a href=\"mailto:malo@intelligence.org\" target=\"_blank\">malo@intelligence.org</a>). You might not feel all that special when you're in a top-notch math program surrounded by people who are as smart or smarter than you are, but here's the deal: we rarely get contacted by aspiring Friendliness researchers who are familiar with most of the material below. If you are, then you&nbsp;<em>are</em>&nbsp;special and we want to talk to you.</p>\n<p>Not everyone cares about Friendly AI, and not everyone who cares about Friendly AI should be a researcher. But if you&nbsp;<em>do</em>&nbsp;care and you&nbsp;<em>might</em>&nbsp;want to help with Friendliness research one day, we recommend you consume the subjects below. Please contact me or Malo if you need further guidance. Or when you're ready to <a href=\"http://intelligence.org/research-fellow/\">come work for us</a>.</p>\n<p>&nbsp;</p>\n<p>\n<table dir=\"ltr\" border=\"1\" cellspacing=\"2\" cellpadding=\"2\" align=\"left\">\n<tbody>\n<tr dir=\"ltr\">\n<td dir=\"ltr\" width=\"135\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://psychology.berkeley.edu/courses/cognitive-neuroscience-0\">COGSCI C127<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=PHIL+190%3A+Introduction+to+Cognitive+and+Information+Sciences&amp;collapse=\">PHIL 190<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-66j-computational-cognitive-science-fall-2004/index.htm\">6.804J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.psy.cmu.edu/undergrad_program/undergrad_courses.html\">85-213<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-66j-computational-cognitive-science-fall-2004/index.htm\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" />Free Online</a></h6>\n</td>\n<td dir=\"ltr\" width=\"100\">\n<h6>Cognitive Science<br /> <a href=\"http://www.amazon.com/Cognitive-Science-An-Introduction-Mind/dp/0521708370\" target=\"_blank\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/68410000/68414363.JPG\" alt=\"\" width=\"140\" height=\"177\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>If you're endeavoring to build a mind, why not start by studying your own? It turns out we know quite a bit: human minds are massively parallel, highly redundant, and although parts of the cortex and neocortex seem remarkably uniform, there are definitely dozens of special purpose modules in there too. Know the basic details of how the only existing general purpose intelligence currently functions.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://osoc.berkeley.edu/catalog/gcc_search_sends_request?p_dept_cd=ECON&amp;p_number=119\">ECON 119<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=IPS+207A+Judgment+and+Decision+Making&amp;collapse=\">IPS 207A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://mitsloan.mit.edu/academic/courses/15.847.php\">15.847<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/dietrichcollegeofhumanitiesandsocialsciences/departmentofsocialanddecisionsciences/\">80-302<br /></a><a href=\"https://www.coursera.org/course/thinkagain\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" />Free Online</a></h6>\n</td>\n<td dir=\"ltr\" align=\"left\">\n<h6>Heuristics and Biases<br /><a href=\"http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/198620000/198628387.JPG\" alt=\"\" width=\"140\" height=\"206\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>While cognitive science will tell you all the wonderful things we know about the immense, parallel nature of the brain, there's also the other side of the coin. Evolution designed our brains to be optimized at doing rapid thought operations that work in 100 steps or less. Your brain is going to make stuff up to cover up that its mostly cutting corners. These errors don't feel like errors <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">from the inside</a>, so you'll have to learn how to patch the ones you can and then move on.<br /><br />PS - We should probably design our AIs better than this.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 61A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=MATH+198&amp;collapse=\">MATH 198<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-005-elements-of-software-construction-fall-2011/index.htm\">6.005<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.cs.cmu.edu/~15150/\">15-150<br /></a><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /><a href=\"https://www.coursera.org/course/progfun\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Functional Programing<br /><a href=\"http://www.amazon.com/Structure-Interpretation-Computer-Programs-Second/dp/0070004846/\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/127420000/127428102.JPG\" alt=\"\" width=\"140\" height=\"199\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>There are two major branches of programming: Functional and Imperative. Unfortunately, most programmers only learn imperative programming languages (like C++ or python). I say unfortunately, because these languages achieve all their power through what programmers call \"side effects\". The major downside for us is that this means they can't be efficiently machine checked for safety or correctness. The first self-modifying AIs will hopefully be written in functional programming languages, so learn something useful like <a href=\"http://learnyouahaskell.com/\">Haskell</a> or Scheme.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://math.berkeley.edu/courses/choosing/lowerdivcourses/math55\">MATH 55<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/print?page=0&amp;catalog=&amp;q=discrete+mathematics+and+algorithms&amp;filter-coursestatus-Active=on&amp;descriptions=on&amp;collapse=&amp;catalog=\">CME 305<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://student.mit.edu/catalog/m6a.html#6.042\">6.042J/18.062J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/undergraduate/crs/21228.htm\">21-228<br /></a><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /><a href=\"https://www.coursera.org/course/optimization\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Discrete Math<br /><a href=\"http://www.amazon.com/Discrete-Mathematics-Applications-Kenneth-Rosen/dp/0073383090\"><img style=\"-webkit-user-select: none\" src=\"http://louie.divide0.net/75182435.jpg\" alt=\"\" width=\"140\" height=\"202\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Much like programming, there are two major branches of mathematics as well: Discrete and continuous. It turns out a lot of physics and all of modern computation is actually discrete. And although continuous approximations have occasionally yielded useful results, sometimes you just need to calculate it the discrete way. Unfortunately, most engineers squander the majority of their academic careers studying higher and higher forms of calculus and other continuous mathematics. If you care about AI, study discrete math so you can understand computation and not just electricity.<br /><br />Also, you should pick up enough graph theory in this course to handle the basic mechanics of decision theory -- which you're gonna want to learn later.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://math.berkeley.edu/courses/choosing/course-descriptions\">MATH 110<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=math+113&amp;collapse=\">MATH 113<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm\">18.06<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/undergraduate/crs/21341.htm\">21-341<br /></a><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /><a href=\"https://www.coursera.org/course/matrix\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Linear Algebra<br /><a href=\"http://www.amazon.com/Linear-Algebra-Right-Sheldon-Axler/dp/0387982582\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/43890000/43896501.JPG\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Linear algebra is the foundation of quantum physics and a huge amount of probability theory. It even shows up in analyses of things like neural networks. You can't possibly get by in machine learning (later) without speaking linear algebra. So learn it early in your scholastic career.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 135<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=MATH+161&amp;collapse=\">MATH 161<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://stellar.mit.edu/S/course/24/sp07/24.243/index.html\">24.243<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/graduate/graduatecourses/21602.htm\">21-602<br /></a><img style=\"-webkit-user-select: none\" src=\"http://www.hcs.harvard.edu/heen/ext_logo.png\" alt=\"\" width=\"30\" /><a href=\"http://www.extension.harvard.edu/open-learning-initiative/sets-counting-probability\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Set Theory<br /><a href=\"http://www.amazon.com/Naive-Set-Theory-Paul-Halmos/dp/1614271313/\"><img style=\"-webkit-user-select: none\" src=\"http://img1.imagesbn.com/p/9781614271314_p0_v1_s260x420.JPG\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Like learning how to read in mathematics. But instead of building up letters into words, you'll be building up axioms into theorems. This will introduce you to the program of using axioms to capture intuition, finding problems with the axioms, and fixing them.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://math.berkeley.edu/courses/choosing/course-descriptions\">MATH 125A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=CS+103%3A+Mathematical+Foundations+of+Computing&amp;collapse=\">CS 103<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/linguistics-and-philosophy/24-241-logic-i-fall-2009/\">24.241<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/graduate/graduatecourses/21600.htm\">21-600<br /> </a><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /><a href=\"https://www.coursera.org/course/intrologic\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Mathematical Logic<br /><a href=\"http://www.amazon.com/Introduction-Mathematical-Logic-Eliot-Mendelson/dp/B0047TFBYA\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/40240000/40240087.JPG\" alt=\"\" width=\"140\" height=\"228\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>The mathematical equivalent of building words into sentences. Essential for the mathematics of self-modification. And even though Sherlock Holmes and other popular depictions make it look like magic, it's just lawful formulas all the way down.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 170<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToAlgorithms\">CS 161<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/index.htm\">6.046J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/schoolofcomputerscience/\">15-451<br /></a><img style=\"-webkit-user-select: none\" src=\"http://upload.wikimedia.org/wikipedia/en/thumb/3/3c/Udacity_Logo.png/200px-Udacity_Logo.png\" alt=\"\" width=\"30\" /><a href=\"http://www.udacity.com/overview/Course/cs215/CourseRev/1\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Efficient Algorithms and Intractable Problems<br /><a href=\"http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/128250000/128254107.JPG\" alt=\"\" width=\"140\" height=\"156\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Like building sentences into paragraphs. Algorithms are the recipes of thought. One of the more amazing things about algorithm design is that it's often possible to tell how long a process will take to solve a problem before you actually run the process to check it. Learning how to design efficient algorithms like this will be a foundational skill for anyone programming an entire AI, since AIs will be built entirely out of collections of algorithms.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 128A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /><a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;q=CME206\">CME206<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/mathematics/18-330-introduction-to-numerical-analysis-spring-2004/index.htm\">18.330<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/graduate/graduatecourses/21660.htm\">21-660<br /> </a><img style=\"-webkit-user-select: none\" src=\"https://twimg0-a.akamaihd.net/profile_images/2751101846/414f6e958a4ee745543c05b89129fcee.jpeg\" alt=\"\" width=\"30\" /><a href=\"http://www.saylor.org/courses/ma213/\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Numerical Analysis<br /><a href=\"http://www.google.com/url?q=http://images.betterworldbooks.com/049/Numerical-Methods-Faires-J-Douglas-9780495114765.jpg&amp;usd=2&amp;usg=ALhdy2-MOuKMD-http://www.amazon.com/Numerical-Methods-J-Douglas-Faires/dp/0534407617/\" target=\"_blank\"><img style=\"-webkit-user-select: none\" src=\"http://images.betterworldbooks.com/049/Numerical-Methods-Faires-J-Douglas-9780495114765.jpg\" alt=\"\" width=\"140\" height=\"188\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>There are ways to systematically design algorithms that only get things slightly wrong when the input data has tiny errors. And then there's programs written by amateur programmers who don't take this class. Most programmers will skip this course because it's not required. But for us, getting the right answer is very much required.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 172<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=CS+154%3A+Introduction+to+Automata+and+Complexity+Theory&amp;collapse=\">CS 154<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/mathematics/18-404j-theory-of-computation-fall-2006/index.htm\">6.840J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/schoolofcomputerscience/\">15-453<br /> </a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" /></a><a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-045j-automata-computability-and-complexity-spring-2011/index.htm\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Computability and Complexity<br /><a href=\"http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/180130000/180133794.JPG\" alt=\"\" width=\"140\" height=\"216\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>This is where you get to study computing at it's most theoretical. Learn about the Church-Turing thesis, the universal nature and applicability of computation, and how just like AIs, everything else is algorithms... all the way down.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 191<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=CS+259Q%3A+Quantum+Computing&amp;collapse=\">CS 259Q<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-845-quantum-complexity-theory-fall-2010/index.htm\">6.845<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.cmu.edu/physics/courses/undergraduate/index.html\">33-658<br /></a><img style=\"-webkit-user-select: none\" src=\"http://www.danpontefract.com/wp-content/uploads/2012/05/edx.jpg\" alt=\"\" width=\"30\" /><a href=\"https://www.edx.org/courses/BerkeleyX/CS191x/2013_Spring/about\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Quantum Computing<br /><a href=\"http://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/63880000/63881615.JPG\" alt=\"\" width=\"140\" height=\"200\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>It turns out that our universe doesn't run on Turing Machines, but on quantum physics. And something called BQP is the class of algorithms that are actually efficiently computable in our universe. Studying the efficiency of algorithms relative to classical computers is useful if you're programming something that only needs to work today. But if you need to know what is efficiently computable in our universe (at the limit) from a theoretical perspective, quantum computing is the only way to understand that.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 273<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=CS+149%3A+Parallel+Computing&amp;collapse=\">CS149<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://beowulf.csail.mit.edu/18.337/index.html\">18.337J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.cs.cmu.edu/afs/cs/academic/class/15418-s12/www/\">15-418<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://upload.wikimedia.org/wikipedia/en/thumb/3/3c/Udacity_Logo.png/200px-Udacity_Logo.png\" alt=\"\" width=\"30\" /><a href=\"http://www.udacity.com/overview/Course/cs344/CourseRev/1\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Parallel Computing<br /><a href=\"http://www.amazon.com/Introduction-Parallel-Computing-Ananth-Grama/dp/0201648652\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/10710000/10714681.jpg\" alt=\"\" width=\"140\" height=\"188\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>There's a good chance that the first true AIs will have at least some algorithms that are inefficient. So they'll need as much processing power as we can throw at them. And there's every reason to believe that they'll be run on parallel architectures. There are a ton of issues that come up when you switch from assuming sequential instruction ordering to parallel processing. There's threading, deadlocks, message passing, etc. The good part about this course is that most of the problems are pinned down and solved: You're just learning the practice of something that you'll need to use as a tool, but won't need to extend much (if any).</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.eecs.berkeley.edu/~sseshia/219c/\">EE 219C<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=math+293A+proof+theory&amp;collapse=\">MATH 293A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://stellar.mit.edu/S/course/6/fa11/6.820/index.html\">6.820<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.cs.cmu.edu/~emc/15414-f11/\">15-414<br /></a><img style=\"-webkit-user-select: none\" src=\"http://upload.wikimedia.org/wikipedia/en/thumb/3/3c/Udacity_Logo.png/200px-Udacity_Logo.png\" alt=\"\" width=\"30\" /><a href=\"http://www.udacity.com/overview/Course/cs348/CourseRev/1\">Free Online</a><br /></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Automated Program Verification<br /><a href=\"http://www.amazon.com/Principles-Model-Checking-Christel-Baier/dp/026202649X\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/127400000/127408116.JPG\" alt=\"\" width=\"140\" height=\"174\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Remember how I told you to learn functional programming way back at the beginning? Now that you wrote your code in functional style, you'll be able to do automated and interactive theorem proving on it to help verify that your code matches your specs. Errors don't make programs better and all large programs that aren't formally verified are reliably *full* of errors. Experts who have thought about the problem for more than 5 minutes agree that <a href=\"http://intelligence.org/research/\">incorrectly designed AI could cause disasters</a>, so world-class caution is advisable.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Computer+Science&amp;p_dept_cd=COMPSCI&amp;p_path=l\">COMPSCI 174<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=CS+109&amp;collapse=\">CS 109<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2005/index.htm\">6.042J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/undergraduate/crs/21301.htm\">21-301<br /></a><img style=\"-webkit-user-select: none\" src=\"http://www.hcs.harvard.edu/heen/ext_logo.png\" alt=\"\" width=\"30\" /><a href=\"http://www.extension.harvard.edu/open-learning-initiative/sets-counting-probability\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Combinatorics and Discrete Probability<br /><a href=\"http://www.amazon.com/Probability-Computing-Randomized-Algorithms-Probabilistic/dp/0521835402/\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/8730000/8736279.jpg\" alt=\"\" width=\"140\" height=\"194\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Life is uncertain and AIs will handle that uncertainty using probabilities. Also, probability is the foundation of the modern concept of rationality and the modern field of machine learning. Probability theory has the same foundational status in AI that logic has in mathematics. Everything else is built on top of probability.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Statistics&amp;p_dept_cd=STAT\">STAT 210A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=STATS+270%3A+A+Course+in+Bayesian+Statistics&amp;collapse=\">STATS 270<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /><a href=\"http://student.mit.edu/catalog/m6b.html#6.437\"> 6.437/438<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/melloncollegeofscience/departmentofmathematicalsciences/\">36-266<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /></a><a href=\"https://www.coursera.org/course/pgm\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Bayesian Modeling and Inference<br /><a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/39570000/39575215.JPG\" alt=\"\" width=\"140\" height=\"210\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Now that you've learned how to calculate probabilities, how do you combine and compare all the probabilistic data you have? Like many choices before, there is a dominant paradigm (frequentism) and a minority paradigm (Bayesianism). If you learn the wrong method here, you're deviating from a <a href=\"/lw/774/a_history_of_bayes_theorem/\">knowably correct framework</a> for integrating degrees of belief about new information and embracing a cadre of special purpose, ad-hoc statistical solutions that often break silently and without warning. Also, quite embarrassingly, frequentism's ability to get things right is bounded by how well it later turns out to have agreed with Bayesian methods anyway. Why not just do the correct thing from the beginning and not <a href=\"http://xkcd.com/1132/\">have your lunch eaten by Bayesians</a> every time you and them disagree?</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /><a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\"> MATH 218A/B<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=MATH+230A&amp;collapse=\">MATH 230A/B/C<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://student.mit.edu/catalog/search.cgi?search=6.436J&amp;style=verbatim&amp;when=*&amp;days_offered=*&amp;start_time=*&amp;duration=*&amp;total_units=*\">6.436J<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/melloncollegeofscience/departmentofmathematicalsciences/\">36-225/21-325<br /></a><img style=\"-webkit-user-select: none\" src=\"https://twimg0-a.akamaihd.net/profile_images/2751101846/414f6e958a4ee745543c05b89129fcee.jpeg\" alt=\"\" width=\"30\" /><a href=\"http://www.saylor.org/courses/ma252/\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Probability Theory<br /><a href=\"http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087\"><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/QZ5t2.png\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>No more applied probability: Here be theory! Deep theories of probabilities are something you're going to have to extend to help build up the field of AI one day. So you actually have to know why all the things you're doing are working inside out.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Electrical+Engineering+and+Computer+Sciences&amp;p_dept_cd=EECS\">COMPSCI 189<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;q=CS229\">CS 229<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;q=CS229\">6.867<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/schoolofcomputerscience/\">10-601<br /> </a><img style=\"-webkit-user-select: none\" src=\"https://www.coursera.org/favicon.ico\" alt=\"\" width=\"30\" /><a href=\"https://www.coursera.org/course/ml\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Machine Learning<br /><a href=\"http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/12080000/12081669.jpg\" alt=\"\" width=\"140\" height=\"192\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Now that you chose the right branch of math, the right kind of statistics, and the right programming paradigm, you're prepared to study machine learning (aka statistical learning theory). There are lots of algorithms that leverage probabilistic inference. Here you'll start learning techniques like clustering, mixture models, and other things that cache out as precise, technical definitions of concepts that normally have rather confused or confusing English definitions.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Electrical+Engineering+and+Computer+Sciences&amp;p_dept_cd=EECS\">COMPSCI 188<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"https://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;q=CS221\">CS 221<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/index.htm\">6.034<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/melloncollegeofscience/departmentofmathematicalsciences/\">15-381<br /></a><img style=\"-webkit-user-select: none\" src=\"http://www.danpontefract.com/wp-content/uploads/2012/05/edx.jpg\" alt=\"\" width=\"30\" /><a href=\"https://www.edx.org/courses/BerkeleyX/CS188.1x/2013_Spring/about\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Artificial Intelligence<br /><a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/45820000/45827362.JPG\" alt=\"\" width=\"140\" height=\"180\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>We made it! We're finally doing some AI work! Doing logical inference, heuristic development, and other techniques will leverage all the stuff you just learned in machine learning. While modern, mainstream AI has many useful techniques to offer you, the authors will tell you outright that, \"the princess is in another castle\". Or rather, there isn't a princess of general AI algorithms anywhere -- not yet. We're gonna have to go back to mathematics and build our own methods ourselves.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 136<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=PHIL+152%3A+Computability+and+Logic&amp;collapse=\">PHIL 152<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://student.mit.edu/catalog/search.cgi?search=Introduction+to+computability&amp;style=verbatim&amp;when=*&amp;days_offered=*&amp;start_time=*&amp;duration=*&amp;total_units=*\">18.511<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.hss.cmu.edu/philosophy/courses/courses-descriptions.php\">80-311<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" /></a><a href=\"http://ocw.mit.edu/courses/mathematics/18-404j-theory-of-computation-fall-2006/index.htm\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Incompleteness and Undecidability<br /><a href=\"http://www.amazon.com/Computability-Introduction-Recursive-Function-Theory/dp/0521294657\"><img style=\"-webkit-user-select: none\" src=\"http://i.simplecd.me/HVbTd1iW.jpg\" alt=\"\" width=\"140\" height=\"217\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Probably the most celebrated results is mathematics are the negative results by Kurt Goedel: No finite set of axioms can allow all arithmetic statements to be decided as either true or false... and no set of self-referential axioms can even \"believe\" in its own consistency. Well, that's a darn shame, because recursively self-improving AI is going to need to side-step these theorems. Eventually, someone will unlock the key to over-coming this difficulty with self-reference, and if you want to help us do it, this course is part of the training ground.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 225A/B<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /><a href=\"http://explorecourses.stanford.edu/CourseSearch/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;q=PHIL151\"> PHIL 151<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://student.mit.edu/catalog/search.cgi?search=18.515&amp;style=verbatim&amp;when=*&amp;days_offered=*&amp;start_time=*&amp;duration=*&amp;total_units=*\">18.515<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/schoolofcomputerscience/\">21-600<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" /></a><a href=\"http://ocw.mit.edu/high-school/courses/godel-escher-bach/\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Metamathematics<br /><a href=\"http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567\"><img style=\"-webkit-user-select: none\" src=\"http://redditorschoice.com/img/godel-escher-bach_2264_400.jpg\" alt=\"\" width=\"140\" height=\"200\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Working within a framework of mathematics is great. Working above mathematics -- on mathematics -- with mathematics, is what this course is about. This would seem to be the most obvious first step to overcoming incompleteness somehow. Problem is, it's definitely not the whole answer. But it would be surprising if there were no clues here at all.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 229<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=%22model+theory%22&amp;collapse=\">MATH 290B<br /> </a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://student.mit.edu/catalog/search.cgi?search=theory+of+models&amp;style=verbatim&amp;when=*&amp;days_offered=*&amp;start_time=*&amp;duration=*&amp;total_units=*\">24.245<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://www.math.cmu.edu/graduate/graduatecourses/21603.htm\">21-603<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" /></a><a href=\"http://ocw.mit.edu/courses/mathematics/18-996a-simplicity-theory-spring-2004/\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Model Theory<br /><a href=\"http://www.amazon.com/Theory-Edition-Studies-Foundations-Mathematics/dp/0444880542\"><img style=\"-webkit-user-select: none\" src=\"http://img2.imagesbn.com/images/146470000/146477322.JPG\" alt=\"\" width=\"140\" height=\"214\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>One day, when someone does side-step self-reference problems enough to program a recursively self-improving AI, the guy sitting next to her who glances at the solution will go \"Gosh, that's a nice bit of Model Theory you got there!\"<br /><br />Think of Model Theory as a formal way to understand what \"true\" means.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/gkWZ9.gif\" alt=\"\" width=\"30\" /> <a href=\"http://general-catalog.berkeley.edu/catalog/gcc_list_crse_req?p_dept_name=Mathematics&amp;p_dept_cd=MATH\">MATH 245A<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/2Ok48.gif\" alt=\"\" width=\"30\" height=\"26\" /> <a href=\"http://explorecourses.stanford.edu/CourseSearch/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=MATH+198&amp;collapse=\">MATH 198<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/kql8l.gif\" alt=\"\" width=\"30\" /> <a href=\"http://math.mit.edu/~dspivak/teaching/sp13/CategoryTheoryForScientists.pdf\">18.996<br /></a><img style=\"-webkit-user-select: none\" src=\"http://i.imgur.com/NyeP3.gif\" alt=\"\" width=\"30\" /> <a href=\"http://coursecatalog.web.cmu.edu/melloncollegeofscience/departmentofmathematicalsciences/\">80-413<br /></a><a href=\"http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-012-the-brain-and-cognitive-sciences-ii-spring-2006/\"><img style=\"-webkit-user-select: none\" src=\"http://irynsoft.com/home/wp-content/themes/TheCorporation/timthumb.php?src=image/MITOCW200x.png&amp;h=220&amp;w=330&amp;zc=1\" alt=\"\" width=\"30\" /></a><a href=\"http://ocw.mit.edu/courses/mathematics/18-312-algebraic-combinatorics-spring-2009/index.htm\">Free Online</a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Category Theory<br /><a href=\"http://www.amazon.com/Category-Computer-Scientists-Foundations-Computing/dp/0262660717\"><img style=\"-webkit-user-select: none\" src=\"http://ecx.images-amazon.com/images/I/411642SVPGL._SL500_.jpg\" alt=\"\" width=\"140\" height=\"190\" /></a></h6>\n</td>\n<td dir=\"ltr\">\n<h6>Category theory is the precise way that you check if structures in one branch of math represent the same structures somewhere else. It's a remarkable field of meta-mathematics that nearly no one knows... and it could hold the keys to importing useful tools to help solve dilemmas in self-reference, truth, and consistency.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n</tr>\n<tr dir=\"ltr\">\n<td height=\"48\">\n<h6>Outside recommendations</h6>\n</td>\n<td colspan=\"2\">\n<h6>&nbsp;</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td dir=\"ltr\">\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>Harry Potter and the Methods of Rationality<br /><a href=\"http://hpmor.com/\"><img style=\"-webkit-user-select: none\" src=\"http://images1.wikia.nocookie.net/__cb20120418015042/hpmor/images/thumb/0/02/MethodsofRationality_Yudkowsky.jpg/235px-MethodsofRationality_Yudkowsky.jpg\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td>\n<h6>Highly recommended book of light, enjoyable reading that predictably inspires people to realize FAI is an important problem AND that they should probably do something about that.<br /><br />You can start reading this immediately, before any of the above courses.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td>\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>Global Catastrophic Risks<br /><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501\"><img style=\"-webkit-user-select: none\" src=\"http://ecx.images-amazon.com/images/I/41Ra5TVBOaL.jpg\" alt=\"\" width=\"140\" height=\"220\" /></a></h6>\n</td>\n<td>\n<h6>A good primer on xrisks and why they might matter. SPOILER ALERT: They matter.<br /><br />You can probably skim read this early on in your studies. Right after HP:MoR.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td>\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>The Sequences<br /><a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\"><img style=\"-webkit-user-select: none\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/06/The-Sequences-cover.png\" alt=\"\" width=\"140\" height=\"207\" /></a></h6>\n</td>\n<td>\n<h6>Rationality: the indispensable art of <a href=\"/lw/hf/debiasing_as_nonselfdestruction/\">non-self-destruction</a>! There are manifold ways you can fail at life... especially since your brain is made out of broken, undocumented spaghetti code. You should learn more about this ASAP. That goes double if you want to build AIs.<br /><br /><a href=\"/lw/34m/what_ive_learned_from_less_wrong/\">I highly recommend you read this</a>&nbsp;<em>before</em> you get too deep into your academic career. For instance, I know people who went to college for 5 years, while somehow managing to learn nothing. That's because instead of learning, they merely&nbsp;<a href=\"/lw/iq/guessing_the_teachers_password/\">recited the teacher's password</a> every semester until they could dump whatever they&nbsp;<a href=\"/lw/ip/fake_explanations/\">\"learned\"</a> out of their heads as soon as they walked out of the final. Don't let this happen to you! This, and a hundred other useful lessons like it about how to avoid predictable, universal errors in human reasoning and behavior await you in <a href=\"/sequences/\">The Sequences</a>!</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td>\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>Good and Real<br /><a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1354230770&amp;sr=1-1&amp;keywords=Good+and+Real\"><img style=\"-webkit-user-select: none\" src=\"http://img1.imagesbn.com/images/127400000/127403772.JPG\" alt=\"\" width=\"140\" height=\"200\" /></a></h6>\n</td>\n<td>\n<h6>A surprisingly thoughtful book on decision theory and other paradoxes in physics and math that can be dissolved. Reading this book is 100% better than continuing to go through your life with a hazy understanding of how important things like free will, choice, and meaning actually work.<br /><br />I recommend reading this right around the time you finish up your quantum computing course.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td>\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>MIRI Research Papers<br /><a href=\"http://www.intelligence.org/research\"><img style=\"-webkit-user-select: none\" src=\"http://louie.divide0.net/siresearch.png\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td>\n<h6>MIRI has already published 30+ research papers that can help orient future Friendliness researchers. The work is pretty fascinating and readily accessible for people interested in the subject. For example: How do different proposals for value aggregation and extrapolation work out? What are the likely outcomes of different intelligence explosion scenarios? Which ethical theories are fit for use by an FAI? What improvements can be made to modern decision theories to stop them from diverging from winning strategies? When will AI arrive? Do AIs deserve moral consideration? Even though most of your work will be more technical than this, you can still gain a lot of shared background knowledge and more clearly see&nbsp;<a href=\"http://lukeprog.com/SaveTheWorld.html\">where the broad problem space is located</a>.<br /><br />I'd recommend reading these anytime after you finish reading <a href=\"/sequences/\">The Sequences</a> and&nbsp;Global Catastrophic Risks.</h6>\n</td>\n</tr>\n<tr dir=\"ltr\">\n<td>\n<h6>&nbsp;</h6>\n</td>\n<td dir=\"ltr\">\n<h6>Universal Artificial Intelligence<br /><a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528\"><img style=\"-webkit-user-select: none\" src=\"http://images.barnesandnoble.com/images/57300000/57309654.JPG\" alt=\"\" width=\"140\" /></a></h6>\n</td>\n<td>\n<h6>A useful book on \"optimal\" AI that gives a reasonable formalism for studying how the most powerful classes of AIs would behave under conservative safety design scenarios (i.e., lots and lots of reasoning ability).<br /><br />Wait until you finish most of the coursework above before trying to tackle this one.</h6>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>&nbsp;</p>\n<p>Do also look into: <a href=\"/lw/3n0/an_overview_of_formal_epistemology_links/\">Formal Epistemology</a>, <a href=\"https://www.coursera.org/course/gametheory\">Game Theory</a>, <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">Decision Theory</a>, and <a href=\"https://www.coursera.org/course/neuralnets\">Deep Learning</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "8SfkJYYMe75MwjHzN": 1, "4Kcm4etxAJjmeDkHP": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "snzFQJsNYqzPZS2nK", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 71, "baseScore": 96, "extendedScore": null, "score": 0.000217, "legacy": true, "legacyId": "21081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 96, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "yA4gF5KrboK2m2Xu7", "RTt59BtFLqQbsSiqd", "XZWMeeqKmfMSPTLha", "qGEqpy7J78bZh3awf", "NMoLJuDJEms7Ku9XS", "fysgqk4CjAwhBgNYT", "BXot7wxNbipyM749o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-01-09T14:33:50.300Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T17:04:23.741Z", "modifiedAt": null, "url": null, "title": "False vacuum: the universe playing quantum suicide", "slug": "false-vacuum-the-universe-playing-quantum-suicide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:37.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cjK6CTW9DyFAFtKHp/false-vacuum-the-universe-playing-quantum-suicide", "pageUrlRelative": "/posts/cjK6CTW9DyFAFtKHp/false-vacuum-the-universe-playing-quantum-suicide", "linkUrl": "https://www.lesswrong.com/posts/cjK6CTW9DyFAFtKHp/false-vacuum-the-universe-playing-quantum-suicide", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20False%20vacuum%3A%20the%20universe%20playing%20quantum%20suicide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFalse%20vacuum%3A%20the%20universe%20playing%20quantum%20suicide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjK6CTW9DyFAFtKHp%2Ffalse-vacuum-the-universe-playing-quantum-suicide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=False%20vacuum%3A%20the%20universe%20playing%20quantum%20suicide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjK6CTW9DyFAFtKHp%2Ffalse-vacuum-the-universe-playing-quantum-suicide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjK6CTW9DyFAFtKHp%2Ffalse-vacuum-the-universe-playing-quantum-suicide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 727, "htmlBody": "<p>Imagine that the universe is approximately as it appears to be (I know, this is a&nbsp;controversial&nbsp;proposition, but bear with me!). Further imagine that the <a href=\"http://en.wikipedia.org/wiki/Many_worlds\">many worlds interpretation</a> of Quantum mechanics is true (I'm really moving out of Less Wrong's comfort zone here, aren't I?).</p>\n<p>Now assume that our universe is in a situation of <a href=\"http://en.wikipedia.org/wiki/False_vacuum\">false vacuum</a> - the universe is not in its lowest energy configuration. Somewhere, at some point, our universe may tunnel into true vacuum, resulting in a expanding bubble of destruction that will eat the entire universe at high speed, destroying all matter and life.&nbsp;In many worlds, such a collapse need not be terminal: life could go one on a branch of lower measure. In fact, anthropically, life <em>will</em> go on somewhere, no matter how unstable the false vacuum is.</p>\n<p>So now assume that the false vacuum we're in is highly unstable - the measure of the branch in which our universe survives goes down by a factor of a trillion every second. We only exist because we're in the branch of measure a trillionth of a trillionth of a trillionth of... all the way back to the Big Bang.</p>\n<p>None of these assumptions make any difference to what we'd expect to see observationally: only a good enough theory can say that they're right or wrong. You may notice that this setup transforms the whole universe into a <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum suicide</a> situation.</p>\n<p>The question is, how do you go about maximising expected utility in this situation? I can think of a few different approaches:</p>\n<ol>\n<li>Gnaw on the bullet: take the quantum measure as a probability. This means that you now have a discount factor of a trillion every second. You have to rush out and get/do all the good stuff as fast as possible: a delay of a second costs you a reduction in utility of a trillion. If you are a <a href=\"http://en.wikipedia.org/wiki/Utilitarianism#Negative_utilitarianism\">negative utilitarian</a>, you also have to rush to minimise the bad stuff, but you can also take comfort in the fact that the potential for negative utility across the universe is going down fast.</li>\n<li>Use relative measures: care about the relative proportion of good worlds versus bad worlds, while assigning zero to those worlds where the vacuum has collapsed. This requires a natural zero to make sense, and can be seen as quite arbitrary: what would you do about entangled worlds, or about the non-zero probability that the vacuum-collapsed worlds may have worthwhile life in them? Would the relative measure user also put zero value to worlds that were empty of life for other reasons than vacuum collapse? For instance, would they &nbsp;be in favour of programming an AI's friendliness using random quantum bits, if it could be reassured that if friendliness fails, the AI would kill everyone immediately?</li>\n<li>Deny the measure: construct a meta ethical theory where only classical probabilities (or classical uncertainties) count as probabilities. Quantum measures do not: you care about the sum total of all branches of the universe. Universes in which the photon went through the top slit, went through the bottom slit, or was in an entangled state that went through both slits... to you, there are three completely separate universes, and you can assign totally unrelated utilities to each one. This seems quite arbitrary, though: how are you going to construct these preferences across the whole of the quantum universe, when forged your current preferences on a single branch?</li>\n<li>Cheat: note that nothing in life is certain. Even if we have the strongest evidence imaginable about vacuum collapse, there's always a tiny chance that the evidence is wrong. After a few seconds, that probability will be dwarfed by the discount factor of the collapsing universe. So go about your business as usual, knowing that most of the measure/probability mass remains in the non-collapsing universe. This can get tricky if, for instance the vacuum collapsed more slowly that a factor of a trillion a second. Would you be in a situation where you should behave as if you believed vacuum collapse for another decade, say, and then switch to a behaviour that assumed non-collapse afterwards? Also, would you take seemingly stupid bets, like bets at a trillion trillion trillion to one that the next piece of evidence will show no collapse (if you lose, you're likely in the low measure universe anyway, so the loss is minute)?</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 1, "PbShukhzpLsWpGXkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cjK6CTW9DyFAFtKHp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 25, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "21083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T19:06:18.699Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC measurement meetup", "slug": "meetup-washington-dc-measurement-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wYxB3ES8hfqYHGi2K/meetup-washington-dc-measurement-meetup", "pageUrlRelative": "/posts/wYxB3ES8hfqYHGi2K/meetup-washington-dc-measurement-meetup", "linkUrl": "https://www.lesswrong.com/posts/wYxB3ES8hfqYHGi2K/meetup-washington-dc-measurement-meetup", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20measurement%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20measurement%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYxB3ES8hfqYHGi2K%2Fmeetup-washington-dc-measurement-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20measurement%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYxB3ES8hfqYHGi2K%2Fmeetup-washington-dc-measurement-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYxB3ES8hfqYHGi2K%2Fmeetup-washington-dc-measurement-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hr'>Washington DC measurement meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 January 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Instead of the courtyard, we will be meeting at someone's house: please PM <a href=\"http://lesswrong.com/message/compose/?to=rocurley\">me</a>, <a href=\"http://lesswrong.com/message/compose/?to=maia\">maia</a> or <a href=\"http://lesswrong.com/message/compose/?to=PhilipL\">PhilipL</a>, and we'll send you the address. If you're subscribed to the mailing list (of course you are!) then you could also just look at it.\nIt's very close to the Wheaton metro station.\nWe will be doing measurement-based calibration games. That is to say, we'll be guessing the physical (or as Robin Hanson suggested, economic) properties of objects with confidence intervals, and then testing how well calibrated we are. If you have objects with properties (you probably do) and can bring them, please do so so we can guess at them. In particular, dense or (sparse?) objects would be interesting, as well as items that you know the price of.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hr'>Washington DC measurement meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wYxB3ES8hfqYHGi2K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.080379796750657e-06, "legacy": true, "legacyId": "21084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_measurement_meetup\">Discussion article for the meetup : <a href=\"/meetups/hr\">Washington DC measurement meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 January 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Instead of the courtyard, we will be meeting at someone's house: please PM <a href=\"http://lesswrong.com/message/compose/?to=rocurley\">me</a>, <a href=\"http://lesswrong.com/message/compose/?to=maia\">maia</a> or <a href=\"http://lesswrong.com/message/compose/?to=PhilipL\">PhilipL</a>, and we'll send you the address. If you're subscribed to the mailing list (of course you are!) then you could also just look at it.\nIt's very close to the Wheaton metro station.\nWe will be doing measurement-based calibration games. That is to say, we'll be guessing the physical (or as Robin Hanson suggested, economic) properties of objects with confidence intervals, and then testing how well calibrated we are. If you have objects with properties (you probably do) and can bring them, please do so so we can guess at them. In particular, dense or (sparse?) objects would be interesting, as well as items that you know the price of.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_measurement_meetup1\">Discussion article for the meetup : <a href=\"/meetups/hr\">Washington DC measurement meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC measurement meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_measurement_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC measurement meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_measurement_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-09T22:26:00.032Z", "modifiedAt": null, "url": null, "title": "Post-college: changing nature of friend interactions", "slug": "post-college-changing-nature-of-friend-interactions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calcsam", "createdAt": "2011-04-30T17:07:18.622Z", "isAdmin": false, "displayName": "calcsam"}, "userId": "YpbtzJj8Qwi4PHGm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hcTXEFphtysjqJd9t/post-college-changing-nature-of-friend-interactions", "pageUrlRelative": "/posts/hcTXEFphtysjqJd9t/post-college-changing-nature-of-friend-interactions", "linkUrl": "https://www.lesswrong.com/posts/hcTXEFphtysjqJd9t/post-college-changing-nature-of-friend-interactions", "postedAtFormatted": "Wednesday, January 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Post-college%3A%20changing%20nature%20of%20friend%20interactions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APost-college%3A%20changing%20nature%20of%20friend%20interactions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcTXEFphtysjqJd9t%2Fpost-college-changing-nature-of-friend-interactions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Post-college%3A%20changing%20nature%20of%20friend%20interactions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcTXEFphtysjqJd9t%2Fpost-college-changing-nature-of-friend-interactions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcTXEFphtysjqJd9t%2Fpost-college-changing-nature-of-friend-interactions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p>As a working professional a couple of years out college, I&rsquo;ve been noticing how interactions with my friends has changed since the beginning of college &ndash; and especially since graduation.</p>\n<p class=\"MsoNormal\">&nbsp;In college, my social groups typically formed around groups with common meeting places -- freshman dorm, newspaper, church, &ldquo;draw group&rdquo; (essentially a group of friends that &lsquo;draw&rsquo; into the same dorm).</p>\n<p class=\"MsoNormal\">Because there was a common space where everyone could hang out, everyone else felt comfortable just showing up (at least at designated times), and so there were always people to talk to. No-permission-required-meeting was a self-sustaining norm.</p>\n<p class=\"MsoNormal\">With jobs and schedules, we shift to a permission-required-meeting-situation &ndash; you don&rsquo;t just show up at your friend&rsquo;s house, we say &ldquo;Hey, what&rsquo;s a good time to meet up?&rdquo;</p>\n<p class=\"MsoNormal\">This adds an additional barrier to meeting, and so that happen less often.</p>\n<p class=\"MsoNormal\">People usually realize this at some level, and employ a variety of ad-hoc strategies to counteract this. These are usually well-deployed in our professional lives, but in our personal lives, there are some complications, and usually room for improvement.</p>\n<ul>\n<li><strong style=\"text-indent: -0.25in;\">Group meetings</strong><span style=\"text-indent: -0.25in;\">. There are 10 connections between five people, as opposed to one connection between two people. But generally &ndash; assuming people share fairly common schedules &ndash; it will take less than 10x initiative to get five people together as two</span></li>\n</ul>\n<p style=\"padding-left: 60px;\"><span style=\"text-indent: -24px;\"><em>Disadvantage:</em></span><span style=\"text-indent: -24px;\"><em> </em>Often most of our close friends don&rsquo;t form groups. Only a small subset of mine does. &nbsp;</span></p>\n<ul>\n<li><strong style=\"text-indent: -0.25in;\">Non-face-to-face communication</strong><span style=\"text-indent: -0.25in;\">. Christmas cards are a time-honored way of doing this. E-mail, like mail, is a no-permission-required system. Every year, I send out a general Life Update email to my old and current friends and family. My friends and I more frequently email each other interesting links. When I read something cool online, I often think &ldquo;who could I send this to?&rdquo;</span></li>\n</ul>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent:-.25in;mso-list:l0 level1 lfo1\"><!--[if !supportLists]--></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"padding-left: 60px;\">&nbsp;<span style=\"white-space:pre\"> </span><em>Disadvantage: </em>for most people, compared to face-to-face interaction, it&rsquo;s not the same.</p>\n<ul>\n<li><strong style=\"text-indent: -0.25in;\">Scheduling regular meetings: </strong><span style=\"text-indent: -0.25in;\">I live in CA and my girlfriend lives in NY, so for the last five months we have set aside 10am PST / 1pm EST to talk every weekday. For the last 8 months, I have met my friend Caleb* have weekly 1-to-2 hour meetings on Sunday mornings where we discuss how the last week went and make goals for the next week. We plan for every week, or day, and it happens 60-80% of the time.</span></li>\n</ul>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"padding-left: 60px;\">&nbsp;<em>Disadvantage: </em>The well-known &ldquo;my schedule is too full to see you&rdquo; is illuminated by analogy. In <em>The Road to Serfdom </em>(1944), economist FA Hayek discussed the politics of price and wage controls. These policies would shelter one particular group, he wrote, but at the risk of leaving everyone else out in the cold, and now slightly colder.<span class=\"MsoFootnoteReference\"><span class=\"MsoFootnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\"><a name=\"_ftnref1\" href=\"file:///C:/Users/Sam%20Bhagwat/Documents/Post%20college%20changing%20nature%20of%20friend%20interactions.docx#_ftn1\">[1]</a></span></span></span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"padding-left: 30px;\"><span class=\"MsoFootnoteReference\"><span class=\"MsoFootnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\"><a name=\"_ftnref1\" href=\"file:///C:/Users/Sam%20Bhagwat/Documents/Post%20college%20changing%20nature%20of%20friend%20interactions.docx#_ftn1\"></a></span></span></span>Something similar happens with planning one&rsquo;s schedule. Perhaps because I&rsquo;m busy with the above and additional planned activities with my other friends, I don&rsquo;t see my friend Christine* enough, and I rarely talk to my college friends Lina* and Maya* anymore</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"padding-left: 30px;\">So Christine and I have decided to go running every Tuesday evening after work. Sure, I&rsquo;ll be even more scheduled, and less likely to meet new, interesting people outside of my designated &ldquo;meet new people&rdquo; events.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"padding-left: 30px;\">But at least I&rsquo;ll get some exercise. &nbsp;</p>\n<p class=\"MsoNormal\"><strong>Commenters: </strong>really&nbsp;curious to hear&nbsp;additional tactics, improvements, or experiences!</p>\n<p class=\"MsoNormal\">*Names changed.</p>\n<div><!--[if !supportFootnotes]--><br /> \n<hr size=\"1\" />\n<!--[endif]-->\n<div id=\"ftn1\">\n<p class=\"MsoFootnoteText\"><a name=\"_ftn1\" href=\"file:///C:/Users/Sam%20Bhagwat/Documents/Post%20college%20changing%20nature%20of%20friend%20interactions.docx#_ftnref1\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[1]</span></span><!--[endif]--></span></a> Hayek warned that in this situation, each group would increase its clamor to be &ldquo;let in,&rdquo; but granting each seemingly-reasonable demand would lead one step closer to a planned economy. Meanwhile, the most vulnerable but ill-connected or ill-organized groups, such as immigrants or the non-unionized-working class, would be left largely out in the cold.&nbsp;</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hcTXEFphtysjqJd9t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "21085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-10T02:22:31.232Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 1/9/13", "slug": "group-rationality-diary-1-9-13", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:06.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wduB926gXhhShqWER/group-rationality-diary-1-9-13", "pageUrlRelative": "/posts/wduB926gXhhShqWER/group-rationality-diary-1-9-13", "linkUrl": "https://www.lesswrong.com/posts/wduB926gXhhShqWER/group-rationality-diary-1-9-13", "postedAtFormatted": "Thursday, January 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%201%2F9%2F13&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%201%2F9%2F13%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwduB926gXhhShqWER%2Fgroup-rationality-diary-1-9-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%201%2F9%2F13%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwduB926gXhhShqWER%2Fgroup-rationality-diary-1-9-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwduB926gXhhShqWER%2Fgroup-rationality-diary-1-9-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of January 7th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes! &nbsp;Happy New Year to folks; my resolution is to always post these on Monday evenings instead of letting them slip to Tuesday or Wednesday : &gt;</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/g36/group_rationality_diary_122512/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wduB926gXhhShqWER", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "21086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CTApsdrMTDnQprZdL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-10T04:58:55.375Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Imaginary Positions", "slug": "seq-rerun-imaginary-positions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9GjRXJ6ttE6HbCvgF/seq-rerun-imaginary-positions", "pageUrlRelative": "/posts/9GjRXJ6ttE6HbCvgF/seq-rerun-imaginary-positions", "linkUrl": "https://www.lesswrong.com/posts/9GjRXJ6ttE6HbCvgF/seq-rerun-imaginary-positions", "postedAtFormatted": "Thursday, January 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Imaginary%20Positions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Imaginary%20Positions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GjRXJ6ttE6HbCvgF%2Fseq-rerun-imaginary-positions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Imaginary%20Positions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GjRXJ6ttE6HbCvgF%2Fseq-rerun-imaginary-positions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GjRXJ6ttE6HbCvgF%2Fseq-rerun-imaginary-positions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/x1/imaginary_positions/\">Imaginary Positions</a> was originally published on 23 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Imaginary_Positions\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People who are not members of a minority group may somehow come to believe that members of this group possess certain traits which seem to \"fit\". These traits are not required to have any connection to the real traits of that group.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g9h/seq_rerun_living_by_your_own_strength/\">Living By Your Own Strength</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9GjRXJ6ttE6HbCvgF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0807368299750336e-06, "legacy": true, "legacyId": "21093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jq5WAQEboeufkxzsg", "QxWgkGNDYMHXRgfLS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-10T08:17:29.959Z", "modifiedAt": null, "url": null, "title": "Evaluating the feasibility of SI's plan", "slug": "evaluating-the-feasibility-of-si-s-plan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5evRqMmGxTKf98pvT/evaluating-the-feasibility-of-si-s-plan", "pageUrlRelative": "/posts/5evRqMmGxTKf98pvT/evaluating-the-feasibility-of-si-s-plan", "linkUrl": "https://www.lesswrong.com/posts/5evRqMmGxTKf98pvT/evaluating-the-feasibility-of-si-s-plan", "postedAtFormatted": "Thursday, January 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evaluating%20the%20feasibility%20of%20SI's%20plan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvaluating%20the%20feasibility%20of%20SI's%20plan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5evRqMmGxTKf98pvT%2Fevaluating-the-feasibility-of-si-s-plan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evaluating%20the%20feasibility%20of%20SI's%20plan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5evRqMmGxTKf98pvT%2Fevaluating-the-feasibility-of-si-s-plan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5evRqMmGxTKf98pvT%2Fevaluating-the-feasibility-of-si-s-plan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1251, "htmlBody": "<p>(With Kaj Sotala)</p>\n<p>SI's current R&amp;D plan seems to go as follows:&nbsp;</p>\n<p style=\"padding-left: 30px;\"><span> 1. Develop the perfect theory. <br /> </span> <span> 2. Implement this as a safe, working, Artificial General Intelligence -- and do so before anyone else builds an AGI.</span></p>\n<p><span> The Singularity Institute is almost the only group working on friendliness theory (although with very few researchers). So, they have the lead on Friendliness. But there is no reason to think that they will be ahead of anyone else on the implementation. </span> <br /> <span> </span></p>\n<p><span>The few AGI designs we can look at today, like OpenCog, are big, messy systems which intentionally attempt to exploit various cognitive dynamics that might combine in unexpected and unanticipated ways, and which have various human-like drives rather than the sort of supergoal-driven, utility-maximizing goal hierarchies that Eliezer talks about, or which a mathematical abstraction like AIXI employs. </span> <br /> <span> </span></p>\n<p><span>A team which is ready to adopt a variety of imperfect heuristic techniques will have a decisive lead on approaches based on pure theory. Without the constraint of safety, one of them will beat SI in the race to AGI. SI cannot ignore this. Real-world, imperfect, safety measures for real-world, imperfect AGIs are needed. &nbsp;These may involve m</span><span>echanisms for ensuring that we can avoid undesirable dynamics in heuristic systems, &nbsp;or AI-boxing toolkits usable in the pre-explosion stage, or something else entirely.&nbsp;</span><br /> <span> </span></p>\n<p>SI&rsquo;s hoped-for theory will include a reflexively consistent decision theory, something like a greatly refined Timeless Decision Theory. &nbsp;It will also describe human value as formally as possible, or at least describe a way to pin it down precisely, something like an improved Coherent Extrapolated Volition.</p>\n<p><span>The hoped-for theory is intended to &nbsp;provide not only safety features, but also a description of the implementation, as some sort of ideal Bayesian mechanism, a theoretically perfect intelligence. </span> <br /> <span> </span></p>\n<p><span>SIers have said to me that SI's design will have a decisive implementation advantage. The idea is that because </span> <span> strap-on safety can&rsquo;t work, Friendliness research necessarily involves more fundamental architectural design decisions, which also happen to be general AGI design decisions that some other AGI builder could grab and save themselves a lot of effort. The assumption seems to be </span> <span> that </span> <span> all </span> <span> other designs are based on hopelessly misguided design principles. SI-ers, the idea seems to go, are so smart that they'll &nbsp;build AGI far before anyone else. Others will succeed only when hardware capabilities allow crude near-brute-force methods to work. </span> <br /> <span> </span></p>\n<p><span>Yet even if the Friendliness theory provides the basis for intelligence, the nitty-gritty of SI&rsquo;s implementation will still be far away, and will involve real-world heuristics and other compromises. </span> <br /> <span> </span></p>\n<p><span>We can compare SI&rsquo;s future AI design to AIXI, another mathematically perfect AI formalism (though it has some critical reflexivity issues). </span> <a href=\"http://www.youtube.com/watch?v=KQ35zNlyG-o\"> <span> Schmidhuber</span></a><span>, Hutter, and </span> <a href=\"http://www.jair.org/media/3125/live-3125-5397-jair.pdf\"> <span> colleagues </span> </a> <span> think that their AXI can be scaled down into a feasible implementation, and have implemented some toy systems. Similarly, any actual AGI based on SI's future theories will have to stray far from its mathematically perfected origins. </span> <br /> <span> </span></p>\n<p><span>Moreover, SI's future friendliness proof may simply be wrong. Eliezer writes a lot about logical uncertainty, the idea that you must treat even purely mathematical ideas with same probabilistic techniques as any ordinary uncertain belief. He pursues this mostly so that his AI can reason about itself, but the same principle applies to Friendliness proofs as well.</span></p>\n<p><span> Perhaps Eliezer thinks that a heuristic AGI is absolutely doomed to failure; that a hard takeoff &nbsp;immediately soon after the creation of the first AGI is so overwhelmingly likely that a mathematically designed AGI is the only one that could stay Friendly. In that case, we have to work on a pure-theory approach, even if it has a low chance of being finished first. Otherwise we'll be dead anyway. If an embryonic AGI will necessarily undergo an intelligence explosion, we have no choice but to \"</span><span><a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up and do the impossible</a>.</span><span>\"</span></p>\n<p><span> I am all in favor of gung-ho knife-between-the teeth projects. But when you think that your strategy is impossible, then you should also look for a strategy which is </span> <span> possible</span><span>, if only as a fallback. Thinking about safety theory until drops of blood appear on your forehead (as Eliezer </span><span><a href=\"/lw/qu/a_premature_word_on_ai/\">puts it</a>, q</span><span>uoting </span> <a href=\"http://en.wikipedia.org/wiki/Gene_Fowler\"> <span> Gene Fowler</span></a><span>), is all well and good. But if there is only a 10% chance of achieving 100% safety (not that there really is any such thing), then I'd rather go for a strategy that provides only a 40% promise of safety, but with a 40% chance of achieving it. OpenCog and the like are going to be developed regardless, and probably before SI's own provably friendly AGI. So, even an imperfect safety measure is better than nothing.</span></p>\n<p><span> If heuristic approaches have a 99% chance of an immediate unfriendly explosion, then that might be wrong. But SI, better than anyone, should know that any intuition-based probability estimate of &ldquo;99%&rdquo; really means &ldquo;70%&rdquo;. Even if other approaches are long-shots, we should not put all our eggs in one basket. Theoretical perfection and stopgap safety measures can be developed in parallel.</span></p>\n<p><span>Given what we know about human overconfidence and the general reliability of predictions, the actual outcome will to a large extent be something that none of us ever expected or could have predicted. No matter what happens, progress on safety mechanisms for&nbsp;</span>heuristic AGI&nbsp;will improve our chances if something entirely unexpected happens.</p>\n<p><span> What impossible thing should SI be shutting up and doing? For Eliezer, it&rsquo;s Friendliness theory. To him, safety for heuristic AGI is impossible, and we&nbsp;shouldn't&nbsp;direct our efforts in that direction. But why&nbsp;shouldn't&nbsp;safety for heuristic AGI be another impossible thing to do? <br /></span></p>\n<p><span>(Two impossible things before breakfast &hellip; and maybe a few more? Eliezer seems to be rebuilding logic, set theory, ontology, epistemology, axiology,&nbsp;</span>decision theory,&nbsp;and more,&nbsp;mostly from scratch. That's a lot of impossibles.)</p>\n<p><span> And even if safety for heuristic AGIs is really impossible for us to figure out now, there is some chance of an extended soft takeoff that will allow for the possibility of us developing heuristic AGIs which will help in figuring out AGI safety, whether because we can use them for our tests, or because they can by applying their embryonic general intelligence to the problem. </span> <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\"> <span> Goertzel and Pitt</span></a><span>&nbsp;have urged this approach.</span></p>\n<p><span> Yet resources are limited. Perhaps the folks who are actually building their own heuristic AGIs are in a better position than SI to develop safety mechanisms for them, while SI is the only organization which is really working on a formal theory on Friendliness, and so should concentrate on that. It could be better to focus SI's resources on areas in which it has a relative advantage, or which have a greater expected impact. </span></p>\n<p>Even if so, SI should evangelize AGI safety to other researchers, not only as a general principle, but also by offering theoretical insights that may help them as they work on their own safety mechanisms.</p>\n<p><span>In summary:</span><br /> <span> </span></p>\n<p style=\"padding-left: 30px;\"><span>1. AGI development which is unconstrained by a friendliness requirement is likely to beat a provably-friendly design in a race to implementation, and some effort should be expended on dealing with this scenario. </span></p>\n<p style=\"padding-left: 30px;\"><span>2. Pursuing a provably-friendly AGI, even if very unlikely to succeed, could still be the right thing to do if it was certain that we&rsquo;ll have a hard takeoff very soon after the creation of the first AGIs. However, we do not know whether or not this is true.<br /> </span> <span> </span></p>\n<p style=\"padding-left: 30px;\"><span>3. Even the provably friendly design will face real-world compromises and errors in its &nbsp;implementation, so the implementation will not itself be provably friendly. Thus, safety protections of the sort needed for heuristic design are needed even for a theoretically Friendly design.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "ZFrgTgzwEfStg26JL": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5evRqMmGxTKf98pvT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 38, "extendedScore": null, "score": 9.5e-05, "legacy": true, "legacyId": "21063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 188, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nCvvhFBaayaXyuBiD", "iD5baT42zYAkWJPMB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T00:36:17.808Z", "modifiedAt": null, "url": null, "title": "A question about two books concerning biases", "slug": "a-question-about-two-books-concerning-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/95zenrcobmfRwFPXM/a-question-about-two-books-concerning-biases", "pageUrlRelative": "/posts/95zenrcobmfRwFPXM/a-question-about-two-books-concerning-biases", "linkUrl": "https://www.lesswrong.com/posts/95zenrcobmfRwFPXM/a-question-about-two-books-concerning-biases", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20about%20two%20books%20concerning%20biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20about%20two%20books%20concerning%20biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95zenrcobmfRwFPXM%2Fa-question-about-two-books-concerning-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20about%20two%20books%20concerning%20biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95zenrcobmfRwFPXM%2Fa-question-about-two-books-concerning-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95zenrcobmfRwFPXM%2Fa-question-about-two-books-concerning-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>I'm taking a class on heuristics and biases. I'm this class we have the option to read one of two \"applied\" books on the subject. The books are \"The Panic Virus: A True Story of Medicine, Science, and Fear\" by Seth Mnookin and \"Sold on Language: How Advertisers Talk to You and What This Says About You\"&nbsp;by Judith Sedivy and Greg Carlson.</p>\n<p>I'd like to know if anyone has read one or both of these books, and how well or poorly they mesh with less wrong rationality.</p>\n<p>Thanks,<br /> Jeremy</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "95zenrcobmfRwFPXM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 1.0814467873822373e-06, "legacy": true, "legacyId": "21099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T05:11:19.775Z", "modifiedAt": null, "url": null, "title": "[LINK] General-audience documentary on cosmology, anthropics, and superintelligence", "slug": "link-general-audience-documentary-on-cosmology-anthropics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JBvDBhQpckRfg8KCo/link-general-audience-documentary-on-cosmology-anthropics", "pageUrlRelative": "/posts/JBvDBhQpckRfg8KCo/link-general-audience-documentary-on-cosmology-anthropics", "linkUrl": "https://www.lesswrong.com/posts/JBvDBhQpckRfg8KCo/link-general-audience-documentary-on-cosmology-anthropics", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20General-audience%20documentary%20on%20cosmology%2C%20anthropics%2C%20and%20superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20General-audience%20documentary%20on%20cosmology%2C%20anthropics%2C%20and%20superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBvDBhQpckRfg8KCo%2Flink-general-audience-documentary-on-cosmology-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20General-audience%20documentary%20on%20cosmology%2C%20anthropics%2C%20and%20superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBvDBhQpckRfg8KCo%2Flink-general-audience-documentary-on-cosmology-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBvDBhQpckRfg8KCo%2Flink-general-audience-documentary-on-cosmology-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>If you have friends or family you'd like to get thinking about cosmology and the like, this might be a nice documentary to stir up curiosity. &nbsp;Despite clearly being aimed at a general audience, I thought this documentary -- including interviews of Tegmark and Bostrom --- did a surprisingly good job of talking about the beginning of the universe and our place in it:</p>\n<p><a href=\"http://www.youtube.com/watch?v=oyH2D4-tzfM\">http://www.youtube.com/watch?v=oyH2D4-tzfM</a></p>\n<p>Also, even though I've had all these thoughts before, it still makes me more emotionally motivated to live long enough to see scientific advances on these questions.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JBvDBhQpckRfg8KCo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.0816127513132888e-06, "legacy": true, "legacyId": "21103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T05:12:12.780Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 27-29", "slug": "meetup-durham-hpmor-discussion-chapters-27-29", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ivRQfsr6s4gQPFCrC/meetup-durham-hpmor-discussion-chapters-27-29", "pageUrlRelative": "/posts/ivRQfsr6s4gQPFCrC/meetup-durham-hpmor-discussion-chapters-27-29", "linkUrl": "https://www.lesswrong.com/posts/ivRQfsr6s4gQPFCrC/meetup-durham-hpmor-discussion-chapters-27-29", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2027-29&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2027-29%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivRQfsr6s4gQPFCrC%2Fmeetup-durham-hpmor-discussion-chapters-27-29%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2027-29%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivRQfsr6s4gQPFCrC%2Fmeetup-durham-hpmor-discussion-chapters-27-29", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivRQfsr6s4gQPFCrC%2Fmeetup-durham-hpmor-discussion-chapters-27-29", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hs'>Durham HPMoR Discussion, chapters 27-29</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Nanataco, 2512 University Dr, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at Nanataco at 11:00 for brunch to discuss HPMoR chapters 27-29.</p>\n\n<p>As always, please feel free to join us even if you haven't read the chapters.</p>\n\n<p>Please RSVP here or on the mailing list so I can know how large a table we should ask for.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hs'>Durham HPMoR Discussion, chapters 27-29</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ivRQfsr6s4gQPFCrC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0816132844489726e-06, "legacy": true, "legacyId": "21104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_27_29\">Discussion article for the meetup : <a href=\"/meetups/hs\">Durham HPMoR Discussion, chapters 27-29</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Nanataco, 2512 University Dr, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at Nanataco at 11:00 for brunch to discuss HPMoR chapters 27-29.</p>\n\n<p>As always, please feel free to join us even if you haven't read the chapters.</p>\n\n<p>Please RSVP here or on the mailing list so I can know how large a table we should ask for.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_27_291\">Discussion article for the meetup : <a href=\"/meetups/hs\">Durham HPMoR Discussion, chapters 27-29</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 27-29", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_27_29", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 27-29", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_27_291", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T05:12:22.011Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Harmful Options", "slug": "seq-rerun-harmful-options", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.394Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vaMv4TN34Ga5nF6za/seq-rerun-harmful-options", "pageUrlRelative": "/posts/vaMv4TN34Ga5nF6za/seq-rerun-harmful-options", "linkUrl": "https://www.lesswrong.com/posts/vaMv4TN34Ga5nF6za/seq-rerun-harmful-options", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Harmful%20Options&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Harmful%20Options%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaMv4TN34Ga5nF6za%2Fseq-rerun-harmful-options%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Harmful%20Options%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaMv4TN34Ga5nF6za%2Fseq-rerun-harmful-options", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaMv4TN34Ga5nF6za%2Fseq-rerun-harmful-options", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 298, "htmlBody": "<p>Today's post, <a href=\"/lw/x2/harmful_options/\">Harmful Options</a> was originally published on 25 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Offering people more choices that differ along many dimensions, may diminish their satisfaction with their final choice. Losses are more painful than the corresponding gains are pleasurable, so people think of the dimensions along which their final choice was inferior, and of all the other opportunities passed up. If you can only choose one dessert, you're likely to be happier choosing from a menu of two than from a menu of fourteen. Refusing tempting choices consumes mental energy and decreases performance on other cognitive tasks. A video game that contained an always-visible easier route through, would probably be less fun to play even if that easier route were deliberately foregone. You can imagine a Devil who follows someone around, making their life miserable, solely by offering them options which are never actually taken. And what if a worse option is taken due to a predictable mistake? There are many ways to harm people by offering them more choices.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/g9x/seq_rerun_imaginary_positions/\">Imaginary Positions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vaMv4TN34Ga5nF6za", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0816133772797886e-06, "legacy": true, "legacyId": "21105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CtSS6SkHhLBvdodTY", "9GjRXJ6ttE6HbCvgF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T16:36:05.418Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Brussels, Buenos Aires, Buffalo, Moscow, Purdue, Washington DC", "slug": "weekly-lw-meetups-austin-berlin-brussels-buenos-aires", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DykwDscECRFpXJfcr/weekly-lw-meetups-austin-berlin-brussels-buenos-aires", "pageUrlRelative": "/posts/DykwDscECRFpXJfcr/weekly-lw-meetups-austin-berlin-brussels-buenos-aires", "linkUrl": "https://www.lesswrong.com/posts/DykwDscECRFpXJfcr/weekly-lw-meetups-austin-berlin-brussels-buenos-aires", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Buenos%20Aires%2C%20Buffalo%2C%20Moscow%2C%20Purdue%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Buenos%20Aires%2C%20Buffalo%2C%20Moscow%2C%20Purdue%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDykwDscECRFpXJfcr%2Fweekly-lw-meetups-austin-berlin-brussels-buenos-aires%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Buenos%20Aires%2C%20Buffalo%2C%20Moscow%2C%20Purdue%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDykwDscECRFpXJfcr%2Fweekly-lw-meetups-austin-berlin-brussels-buenos-aires", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDykwDscECRFpXJfcr%2Fweekly-lw-meetups-austin-berlin-brussels-buenos-aires", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 489, "htmlBody": "<p><strong>This summary was posted to LW main on January 4th. The following week's summary is <a href=\"/lw/gae/weekly_lw_meetups_austin_durham_london_melbourne/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/h1\">Brussels meetup:&nbsp;<span class=\"date\">05 January 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/hg\">Fourth Buenos Aires Less Wrong meetup:&nbsp;<span class=\"date\">05 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/hb\">Moscow: Applied Rationality in a New Year:&nbsp;<span class=\"date\">05 January 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/h8\">Washington, DC Meetup with Special Guest:&nbsp;<span class=\"date\">06 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ha\">Berlin Meetup: Bayes:&nbsp;<span class=\"date\">09 January 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/hk\">Buffalo Meetup:&nbsp;<span class=\"date\">10 January 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n<li><a href=\"/meetups/hj\">13th January London Meetup:&nbsp;<span class=\"date\">13 January 2013 02:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">05 January 2019 02:30PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DykwDscECRFpXJfcr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0820261545606492e-06, "legacy": true, "legacyId": "21009", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LXHhzTLDJDXnzStki", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-11T22:47:23.698Z", "modifiedAt": null, "url": null, "title": "How to signal curiosity?", "slug": "how-to-signal-curiosity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qKjwd4zR9PvB9Fxfw/how-to-signal-curiosity", "pageUrlRelative": "/posts/qKjwd4zR9PvB9Fxfw/how-to-signal-curiosity", "linkUrl": "https://www.lesswrong.com/posts/qKjwd4zR9PvB9Fxfw/how-to-signal-curiosity", "postedAtFormatted": "Friday, January 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20signal%20curiosity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20signal%20curiosity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKjwd4zR9PvB9Fxfw%2Fhow-to-signal-curiosity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20signal%20curiosity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKjwd4zR9PvB9Fxfw%2Fhow-to-signal-curiosity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKjwd4zR9PvB9Fxfw%2Fhow-to-signal-curiosity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 477, "htmlBody": "<p>At LessWrong we encourage people to be <a href=\"/lw/4ku/use_curiosity/\">curious</a>. Curiosity causes people to ask questions, but sometimes those questions get misinterpreted as social challenges or rhetorical techniques, or maybe just regular questions that you don't have a \"<a href=\"/lw/jz/the_meditation_on_curiosity/\">burning itch</a>\" to know the answers for (and hence maybe not particularly worth answering). I sometimes preface a question by \"I'm curious,\" but of course anyone could say that so it's not a very effective way to distinguish oneself as being genuinely curious. Another thing I sometimes do is to try to answer the question myself and present one or more answers as my \"guesses\" and ask if one of them is correct, since someone who is genuinely curious is more likely put in such effort. But unfortunately sometimes that backfires when the person you're directing the question at interprets the guesses as a way to make them look bad, because for example you failed to hypothesize the actual answer and include it as one of the guesses, and all your guesses make them look worse than the actual answer.</p>\n<p>I've noticed examples of&nbsp;this happening to others on LW (or at least possibly happening, since I can't be sure whether someone else really&nbsp;is&nbsp;curious) as well as to myself, and can only imagine that the problem is even worse elsewhere, where people may not give each other as much benefit of doubt as we do around here. So my question is, what can curious people do, to signal their genuine curiosity when asking questions? Has anyone thought about this question already, or perhaps can recognize some strategies they already employ and make them explicit for the rest of us?</p>\n<p><strong>ETA:</strong>&nbsp;Perhaps I should say a bit more about the kind of situation I have in mind. Often I'll see a statement from someone that either contradicts my existing beliefs about something or is on a topic that I'm pretty ignorant about, and it doesn't come with an argument or evidence to back it up. I'd think \"I don't want to just take their word since they might be wrong, but there also seems a good chance that they know something that I don't in which case I'd really like to know what it is, so let's ask why they're saying what they're saying.\" And unfortunately this sometimes gets interpreted as \"I'm pretty sure you're wrong, and I'm going to&nbsp;embarrass&nbsp;you by asking a question that I don't think you can answer.\"</p>\n<p><strong>ETA2:</strong>&nbsp;The reason I use \"signal\" in the title is that people who <em>do</em> just want to embarrass the other person would want to have plausible deniability. If it was clear that's their intention and it turns out that the other person has a perfectly good answer, then they'll be the one embarrassed instead. So ideally the curious person should send a signal that can't be faked by someone who just wants to pretend to be curious.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qKjwd4zR9PvB9Fxfw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 1.0822504352984924e-06, "legacy": true, "legacyId": "19384", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WrSe4aB8sWBy3Nphm", "3nZMgRTfFEfHp34Gb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T07:04:31.741Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Devil's Offers", "slug": "seq-rerun-devil-s-offers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c4NsXcyDiekpjH7vb/seq-rerun-devil-s-offers", "pageUrlRelative": "/posts/c4NsXcyDiekpjH7vb/seq-rerun-devil-s-offers", "linkUrl": "https://www.lesswrong.com/posts/c4NsXcyDiekpjH7vb/seq-rerun-devil-s-offers", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Devil's%20Offers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Devil's%20Offers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4NsXcyDiekpjH7vb%2Fseq-rerun-devil-s-offers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Devil's%20Offers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4NsXcyDiekpjH7vb%2Fseq-rerun-devil-s-offers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4NsXcyDiekpjH7vb%2Fseq-rerun-devil-s-offers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<p>Today's post, <a href=\"/lw/x3/devils_offers/\">Devil's Offers</a> was originally published on 25 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is dangerous to live in an environment in which a single failure of resolve, throughout your entire life, can result in a permanent addiction or in a poor edit of your own brain. For example, a civilization which is constantly offering people tempting ways to shoot off their own feet - for example, offering them a cheap escape into eternal virtual reality, or customized drugs. It requires a constant stern will that may not be much fun. And it's questionable whether a superintelligence that descends from above to offer people huge dangerous temptations that they wouldn't encounter on their own, is <em>helping</em>.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ga9/seq_rerun_harmful_options/\">Harmful Options</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c4NsXcyDiekpjH7vb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0825508499794847e-06, "legacy": true, "legacyId": "21120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MTjej6HKvPByx3dEA", "vaMv4TN34Ga5nF6za", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T09:26:09.484Z", "modifiedAt": null, "url": null, "title": "Proof of fungibility theorem", "slug": "proof-of-fungibility-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:15.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xkLsBXknuXSXxu4Jd/proof-of-fungibility-theorem", "pageUrlRelative": "/posts/xkLsBXknuXSXxu4Jd/proof-of-fungibility-theorem", "linkUrl": "https://www.lesswrong.com/posts/xkLsBXknuXSXxu4Jd/proof-of-fungibility-theorem", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proof%20of%20fungibility%20theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProof%20of%20fungibility%20theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkLsBXknuXSXxu4Jd%2Fproof-of-fungibility-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proof%20of%20fungibility%20theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkLsBXknuXSXxu4Jd%2Fproof-of-fungibility-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkLsBXknuXSXxu4Jd%2Fproof-of-fungibility-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 693, "htmlBody": "<p><em>Appendix to: <a href=\"/lw/gap/a_fungibility_theorem/\">A fungibility theorem</a></em></p>\n<p>Suppose that <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> is a set and we have functions <img title=\"v_1, \\dots, v_n : P \\to \\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n : P \\to \\mathbb{R}\" alt=\"\" align=\"bottom\" />. Recall that for <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\" />, we say that <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> is a&nbsp;<em>Pareto improvement</em>&nbsp;over <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\" /> if for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />, we have <img title=\"v_i(p) \\geq v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) \\geq v_i(q)\" alt=\"\" align=\"bottom\" />. And we say that it is a&nbsp;<em>strong Pareto improvement</em>&nbsp;if in addition there is some <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" /> for which <img title=\"v_i(p) &gt; v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) &gt; v_i(q)\" alt=\"\" align=\"bottom\" />. We call <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> a&nbsp;<em>Pareto optimum</em>&nbsp;if there is no strong Pareto improvement over it.</p>\n<p style=\"padding-left: 30px;\"><strong>Theorem.</strong>&nbsp;Let <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> be a set and suppose <img title=\"v_i: P \\to \\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?v_i: P \\to \\mathbb{R}\" alt=\"\" align=\"bottom\" /> for <img title=\"i = 1, \\dots, n\" src=\"http://www.codecogs.com/png.latex?i = 1, \\dots, n\" alt=\"\" align=\"bottom\" /> are functions satisfying the following property:&nbsp;For any <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\" /> and any <img title=\"\\alpha \\in [0, 1]\" src=\"http://www.codecogs.com/png.latex?\\alpha \\in [0, 1]\" alt=\"\" align=\"bottom\" />, there exists an <img title=\"r \\in P\" src=\"http://www.codecogs.com/png.latex?r \\in P\" alt=\"\" align=\"bottom\" /> such that for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />, we have <img title=\"v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" alt=\"\" align=\"bottom\" />.</p>\n<p style=\"padding-left: 30px;\">Then if an element <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> of <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> is a Pareto optimum, then there exist nonnegative constants <img title=\"c_1, \\dots, c_n\" src=\"http://www.codecogs.com/png.latex?c_1, \\dots, c_n\" alt=\"\" align=\"bottom\" /> such that the function <img title=\"\\sum c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum c_i v_i\" alt=\"\" align=\"bottom\" /> achieves a maximum at <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" />.<a id=\"more\"></a></p>\n<p><strong>Proof.</strong>&nbsp;Let <img title=\"V = v_1 \\times \\cdots \\times v_n : P \\to \\mathbb{R}^n\" src=\"http://www.codecogs.com/png.latex?V = v_1 \\times \\cdots \\times v_n : P \\to \\mathbb{R}^n\" alt=\"\" align=\"bottom\" />. By hypothesis, the image <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> is convex.</p>\n<p>For <img title=\"p \\in P\" src=\"http://www.codecogs.com/png.latex?p \\in P\" alt=\"\" align=\"bottom\" />, let the&nbsp;<em>Pareto volume</em>&nbsp;of <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> be the set</p>\n<p style=\"padding-left: 30px;\"><img title=\" pv(p) = \\{ (x_1, \\dots, x_n) \\in \\mathbb{R}^n \\mid x_i \\geq v_i(p) \\mbox{ for all } i \\} \" src=\"http://www.codecogs.com/png.latex? pv(p) = \\{ (x_1, \\dots, x_n) \\in \\mathbb{R}^n \\mid x_i \\geq v_i(p) \\mbox{ for all } i \\} \" alt=\"\" align=\"bottom\" /></p>\n<p>This is a closed convex set. Note that <img title=\"p \\in P\" src=\"http://www.codecogs.com/png.latex?p \\in P\" alt=\"\" align=\"bottom\" /> is a Pareto optimum precisely when <img title=\"pv(p) \\cap V(P) = \\{ V(p) \\}\" src=\"http://www.codecogs.com/png.latex?pv(p) \\cap V(P) = \\{ V(p) \\}\" alt=\"\" align=\"bottom\" />. Let's assume that this is the case; we just have to prove that <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> maximizes <img title=\"\\sum c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum c_i v_i\" alt=\"\" align=\"bottom\" /> for some choice of <img title=\"c_i \\geq 0\" src=\"http://www.codecogs.com/png.latex?c_i \\geq 0\" alt=\"\" align=\"bottom\" />.</p>\n<p>It suffices to find a hyperplane <img title=\"H \\subset \\mathbb{R}^n\" src=\"http://www.codecogs.com/png.latex?H \\subset \\mathbb{R}^n\" alt=\"\" align=\"bottom\" /> that contains <img title=\"V(p)\" src=\"http://www.codecogs.com/png.latex?V(p)\" alt=\"\" align=\"bottom\" /> and that <a href=\"http://en.wikipedia.org/wiki/Supporting_hyperplane\">supports</a>&nbsp;<img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" />. Then the desired function <img title=\"\\sum c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum c_i v_i\" alt=\"\" align=\"bottom\" /> can be constructed by ensuring that <img title=\"H\" src=\"http://www.codecogs.com/png.latex?H\" alt=\"\" align=\"bottom\" /> is a level set.</p>\n<p>If <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> lies in a proper affine subspace of <img title=\"\\mathbb{R}^n\" src=\"http://www.codecogs.com/png.latex?\\mathbb{R}^n\" alt=\"\" align=\"bottom\" />, let <img title=\"Z\" src=\"http://www.codecogs.com/png.latex?Z\" alt=\"\" align=\"bottom\" /> be the smallest such subspace. Let <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> be the interior of <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> in <img title=\"Z\" src=\"http://www.codecogs.com/png.latex?Z\" alt=\"\" align=\"bottom\" /> and let <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" /> be the interior of <img title=\"pv(p)\" src=\"http://www.codecogs.com/png.latex?pv(p)\" alt=\"\" align=\"bottom\" />. The case where <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> is a point is trivial; suppose it's not, so <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> is nonempty. By convexity, <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> is the closure of <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> and <img title=\"pv(p)\" src=\"http://www.codecogs.com/png.latex?pv(p)\" alt=\"\" align=\"bottom\" /> is the closure of <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />.</p>\n<p>Since <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> is convex, <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> is convex, and we can exhaust <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> with a nested sequence of nonempty compact convex sets <img title=\"A_j\" src=\"http://www.codecogs.com/png.latex?A_j\" alt=\"\" align=\"bottom\" />. And <img title=\"pv(p)\" src=\"http://www.codecogs.com/png.latex?pv(p)\" alt=\"\" align=\"bottom\" /> is convex, so we can exhaust <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" /> with a nested sequence of nonempty compact convex sets <img title=\"B_j\" src=\"http://www.codecogs.com/png.latex?B_j\" alt=\"\" align=\"bottom\" />. By the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Hyperplane_separation_theorem\">hyperplane separation theorem</a>, for each <img title=\"j\" src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" align=\"bottom\" />, there is a hyperplane <img title=\"H_j\" src=\"http://www.codecogs.com/png.latex?H_j\" alt=\"\" align=\"bottom\" /> separating <img title=\"A_j\" src=\"http://www.codecogs.com/png.latex?A_j\" alt=\"\" align=\"bottom\" /> and <img title=\"B_j\" src=\"http://www.codecogs.com/png.latex?B_j\" alt=\"\" align=\"bottom\" />. I claim that <img title=\"H_j\" src=\"http://www.codecogs.com/png.latex?H_j\" alt=\"\" align=\"bottom\" /> has a convergent subsequence. Indeed, each <img title=\"H_j\" src=\"http://www.codecogs.com/png.latex?H_j\" alt=\"\" align=\"bottom\" /> must intersect the convex hull of <img title=\"A_1 \\cup B_1\" src=\"http://www.codecogs.com/png.latex?A_1 \\cup B_1\" alt=\"\" align=\"bottom\" />, and the space of hyperplanes intersecting that convex hull is compact. So <img title=\"H_j\" src=\"http://www.codecogs.com/png.latex?H_j\" alt=\"\" align=\"bottom\" /> has a subsequence that converges to a hyperplane <img title=\"H\" src=\"http://www.codecogs.com/png.latex?H\" alt=\"\" align=\"bottom\" />.</p>\n<p>It's easy to see that <img title=\"H\" src=\"http://www.codecogs.com/png.latex?H\" alt=\"\" align=\"bottom\" /> separates <img title=\"A_j\" src=\"http://www.codecogs.com/png.latex?A_j\" alt=\"\" align=\"bottom\" /> from <img title=\"B_j\" src=\"http://www.codecogs.com/png.latex?B_j\" alt=\"\" align=\"bottom\" /> for each <img title=\"j\" src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" align=\"bottom\" />, and so <img title=\"H\" src=\"http://www.codecogs.com/png.latex?H\" alt=\"\" align=\"bottom\" /> separates <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> from <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />. So <img title=\"H\" src=\"http://www.codecogs.com/png.latex?H\" alt=\"\" align=\"bottom\" /> must contain <img title=\"V(p)\" src=\"http://www.codecogs.com/png.latex?V(p)\" alt=\"\" align=\"bottom\" /> and support&nbsp;<img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" />.</p>\n<p><img style=\"float: right;\" src=\"http://www.codecogs.com/gif.latex?%5Csquare\" alt=\"\" width=\"15\" height=\"15\" /></p>\n<p>&nbsp;</p>\n<p>Note that the theorem does not guarantee the existence of a Pareto optimum. But if <img title=\"V(P)\" src=\"http://www.codecogs.com/png.latex?V(P)\" alt=\"\" align=\"bottom\" /> is closed and bounded, then a Pareto optimum exists.</p>\n<p>A limitation of the theorem is that it assumes a finite list of values <img title=\"v_1, \\dots, v_n\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n\" alt=\"\" align=\"bottom\" />, not an infinite one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xkLsBXknuXSXxu4Jd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 1.0826364624657967e-06, "legacy": true, "legacyId": "21049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oRRpsGkCZHA3pzhvm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T09:27:25.637Z", "modifiedAt": null, "url": null, "title": "A fungibility theorem", "slug": "a-fungibility-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oRRpsGkCZHA3pzhvm/a-fungibility-theorem", "pageUrlRelative": "/posts/oRRpsGkCZHA3pzhvm/a-fungibility-theorem", "linkUrl": "https://www.lesswrong.com/posts/oRRpsGkCZHA3pzhvm/a-fungibility-theorem", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20fungibility%20theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20fungibility%20theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRRpsGkCZHA3pzhvm%2Fa-fungibility-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20fungibility%20theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRRpsGkCZHA3pzhvm%2Fa-fungibility-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRRpsGkCZHA3pzhvm%2Fa-fungibility-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1711, "htmlBody": "<p><em>Restatement of:&nbsp;</em><em><a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>.&nbsp;</em><em>Alternative to:&nbsp;<a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Why you must maximize expected utility</a>. Related to:&nbsp;<a href=\"/r/discussion/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi's Social Aggregation Theorem</a>.</em></p>\n<p><em><a name=\"1_\"></a>Summary: This article describes a&nbsp;<a href=\"#theorem\">theorem</a>, previously described by Stuart Armstrong, that tells you to maximize the expectation of a linear aggregation of your values. Unlike the von Neumann-Morgenstern theorem, this theorem gives you a reason to behave rationally.<sup><a href=\"#1\">1</a></sup><a id=\"more\"></a></em></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von Neumann-Morgenstern theorem</a> is great, but it is descriptive rather than prescriptive. It tells you that if you obey four axioms, then you are an optimizer. (Let us call an \"optimizer\" any agent that always chooses an action that maximizes the expected value of some function of outcomes.) But you are a human and you don't obey the axioms; the VNM theorem doesn't say anything about you.</p>\n<p>There are Dutch-book theorems that give us reason to want to obey the four VNM axioms: E.g., if we violate the axiom of transitivity, then we can be money-pumped, and we don't want that; therefore we shouldn't want to violate the axiom of transitivity. The VNM theorem is somewhat helpful here: <a name=\"2_\"></a>It tells us that the <em>only</em> way to obey the four axioms is to be an optimizer.<sup><a href=\"#2\">2</a></sup></p>\n<p>So now you have a reason to become an optimizer. But there are an infinitude of <a name=\"3_\"></a>decision-theoretic utility functions<sup><a href=\"#3\">3</a></sup>&nbsp;to adopt&nbsp;&mdash;&nbsp;which, if any, ought you adopt? And there is an even bigger problem: If you are not already an optimizer, than any utility function that you're considering will recommend actions that run counter to your preferences!</p>\n<p>To give a silly example, suppose you'd rather be an astronaut when you grow up than a mermaid, and you'd rather be a dinosaur than an astronaut, and you'd rather be a mermaid than a dinosaur. You have circular preferences. There's a decision-theoretic utility function that says</p>\n<p style=\"padding-left: 30px;\"><img title=\"\\mbox{mermaid} \\prec \\mbox{astronaut} \\prec \\mbox{dinosaur}\" src=\"http://www.codecogs.com/png.latex?\\mbox{mermaid} \\prec \\mbox{astronaut} \\prec \\mbox{dinosaur}\" alt=\"\" align=\"bottom\" /></p>\n<p>which preserves some of your preferences, but if you have to choose between being a mermaid and being a dinosaur, it will tell you to become a dinosaur, even though you really really want to choose the mermaid. There's another decision-theoretic utility function that will tell you to pass up being a dinosaur in favor of being an astronaut even though you really really don't want to. Not being an optimizer means that any rational decision theory will tell you to do things you don't want to do.</p>\n<p>So why would you ever want to be an optimizer? What theorem could possibly convince you to become one?</p>\n<h1><a name=\"theorem\"></a>Stuart Armstrong's theorem</h1>\n<p>Suppose there is a set <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> (for \"policies\") and some functions <img title=\"v_1, \\dots, v_n\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n\" alt=\"\" align=\"bottom\" /> (\"values\") from <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> to <img title=\"\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?\\mathbb{R}\" alt=\"\" align=\"bottom\" />. We want these functions to satisfy the following <strong>convexity&nbsp;property</strong>:</p>\n<p style=\"padding-left: 30px;\">For any policies <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\" /> and any <img title=\"\\alpha \\in [0, 1]\" src=\"http://www.codecogs.com/png.latex?\\alpha \\in [0, 1]\" alt=\"\" align=\"bottom\" />, there is a policy <img title=\"r \\in P\" src=\"http://www.codecogs.com/png.latex?r \\in P\" alt=\"\" align=\"bottom\" /> such that for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />, we have <img title=\"v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" alt=\"\" align=\"bottom\" />.</p>\n<p>For policies <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\" />, say that <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> is a <em>Pareto improvement</em> over <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\" /> if for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />, we have <img title=\"v_i(p) \\geq v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) \\geq v_i(q)\" alt=\"\" align=\"bottom\" />.&nbsp;Say that it is a <em>strong Pareto improvement</em> if in addition there is some <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" /> for which <img title=\"v_i(p) &gt; v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) &gt; v_i(q)\" alt=\"\" align=\"bottom\" />.&nbsp;Call <img style=\"vertical-align: bottom;\" title=\"a\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" width=\"10\" height=\"12\" align=\"bottom\" /> a&nbsp;<em>Pareto optimum</em>&nbsp;if no policy is a strong Pareto improvement over it.</p>\n<p style=\"padding-left: 30px;\"><strong>Theorem.</strong> Suppose <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> and <img title=\"v_1, \\dots, v_n\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n\" alt=\"\" align=\"bottom\" /> satisfy the convexity property. If a policy in <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> is a Pareto optimum, then it is a maximum of the function <img title=\"c_1 v_1 + \\cdots + c_n v_n\" src=\"http://www.codecogs.com/png.latex?c_1 v_1 + \\cdots + c_n v_n\" alt=\"\" align=\"bottom\" /> for some nonnegative constants <img title=\"c_1, \\dots, c_n\" src=\"http://www.codecogs.com/png.latex?c_1, \\dots, c_n\" alt=\"\" align=\"bottom\" />.</p>\n<p>This theorem previously appeared in&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>. I don't know whether there is a source prior to that post that uses the hyperplane separation theorem to justify being an optimizer. The <a href=\"/r/discussion/lw/g8p/proof_of_fungibility_theorem/\">proof</a> is basically the same as the proof for the complete class theorem and the hyperplane separation theorem and the second fundamental theorem of welfare economics.&nbsp;<a href=\"/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi's utilitarian theorem</a> has a similar conclusion, but it assumes that you already have a decision-theoretic utility function. The <a href=\"http://en.wikipedia.org/wiki/Fundamental_theorems_of_welfare_economics#Proof_of_the_second_fundamental_theorem\">second fundamental theorem of welfare economics</a> is virtually the same theorem, but it's interpreted in a different way.</p>\n<h1>What does the theorem mean?</h1>\n<p>Suppose you are a consequentialist who subscribes to Bayesian epistemology. And in violation of the VNM axioms, you are torn between multiple incompatible decision-theoretic utility functions. Suppose you can list all the things you care about, and the list looks like this:</p>\n<ol>\n<li>Your welfare</li>\n<li>Your family's welfare</li>\n<li>Everyone's total welfare</li>\n<li>The continued existence of human civilization</li>\n<li>All mammals' total welfare</li>\n<li>Your life satisfaction</li>\n<li>Everyone's average welfare</li>\n<li>...</li>\n</ol>\n<p>Suppose further that you can quantify each item on that list with a function <img title=\"v_1, v_2, \\dots\" src=\"http://www.codecogs.com/png.latex?v_1, v_2, \\dots\" alt=\"\" align=\"bottom\" /> from world-histories to real numbers, and you want to optimize for each function, all other things being equal. E.g., <img title=\"v_1(x)\" src=\"http://www.codecogs.com/png.latex?v_1(x)\" alt=\"\" align=\"bottom\" /> is large if <img title=\"x\" src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" align=\"bottom\" /> is a world-history where your welfare is great; and <img title=\"v_5(x)\" src=\"http://www.codecogs.com/png.latex?v_5(x)\" alt=\"\" align=\"bottom\" /> somehow counts up the welfare of all mammals in world-history <img title=\"x\" src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" align=\"bottom\" />.&nbsp;<a name=\"4_\"></a>If the expected value of <img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\" /> is at stake (but none of the other values are at stake), then you want to act so as to maximize the expected value of <img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\" />.<sup><a href=\"#4\">4</a></sup>&nbsp;And if only <img title=\"v_5\" src=\"http://www.codecogs.com/png.latex?v_5\" alt=\"\" align=\"bottom\" /> is at stake, you want to act so as to maximize the expected value of <img title=\"v_5\" src=\"http://www.codecogs.com/png.latex?v_5\" alt=\"\" align=\"bottom\" />. What I've said so far doesn't specify what you do when you're forced to trade off value 1 against value 5.</p>\n<p>If you're VNM-rational, then you are an optimizer whose decision-theoretic utility function is a linear aggregation <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\" /> of your values and you just optimize for that function. (The <img title=\"c_i\" src=\"http://www.codecogs.com/png.latex?c_i\" alt=\"\" align=\"bottom\" /> are nonnegative constants.) But suppose you make decisions in a way that does not optimize for any such aggregation.</p>\n<p>You will make many decisions throughout your life, depending on the observations you make and on random chance. If you're capable of making precommitments and we don't worry about computational difficulties, <a name=\"5_\"></a>it is as if today you get to choose a policy for the rest of your life that specifies a distribution of actions for each sequence of observations you can make.<sup><a href=\"#5\">5</a></sup>&nbsp;Let <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\" /> be the set of all possible policies. If <img title=\"p \\in P\" src=\"http://www.codecogs.com/png.latex?p \\in P\" alt=\"\" align=\"bottom\" />, and for any <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />, let us say that <img title=\"v_i(p)\" src=\"http://www.codecogs.com/png.latex?v_i(p)\" alt=\"\" align=\"bottom\" /> is the expected value of <img title=\"v_i\" src=\"http://www.codecogs.com/png.latex?v_i\" alt=\"\" align=\"bottom\" /> given that we adopt policy <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" />. Let's assume that these expected values are all finite. Note that if <img title=\"p_f\" src=\"http://www.codecogs.com/png.latex?p_f\" alt=\"\" align=\"bottom\" /> is a policy where you make every decision by maximizing a decision-theoretic utility function <img title=\"f\" src=\"http://www.codecogs.com/png.latex?f\" alt=\"\" align=\"bottom\" />, then the policy <img title=\"p_f\" src=\"http://www.codecogs.com/png.latex?p_f\" alt=\"\" align=\"bottom\" /> itself maximizes the expected value of <img title=\"f\" src=\"http://www.codecogs.com/png.latex?f\" alt=\"\" align=\"bottom\" />, compared to other policies.</p>\n<p>In order to apply the theorem, we must check that the convexity property holds. That's easy:&nbsp;If <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> and <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\" /> are two policies and <img title=\"\\alpha \\in [0, 1]\" src=\"http://www.codecogs.com/png.latex?\\alpha \\in [0, 1]\" alt=\"\" align=\"bottom\" />, the mixed policy where today you randomly choose policy <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\" /> with probability <img title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\" /> and policy <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\" /> with probability <img title=\"1-\\alpha\" src=\"http://www.codecogs.com/png.latex?1-\\alpha\" alt=\"\" align=\"bottom\" />, is also a policy.</p>\n<p>What the theorem says is that if you really care about the values on that list (and the other assumptions in this post hold), then there are linear aggregations <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\" /> that you have reason to start optimizing for. That is, there are a set of linear aggregations and if you choose one of them and start optimizing for it, you will get <em>more</em> expected welfare for yourself, <em>more</em> expected welfare for others, <em>less</em> risk of the fall of civilization, ....</p>\n<p>Adopting one of these decision-theoretic utility functions <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\" /> in the sense that doing so will get you more of the things you value without sacrificing any of them.</p>\n<p>What's more, once you've chosen a linear aggregation, optimizing for it is easy. The ratio <img title=\"c_i/c_j\" src=\"http://www.codecogs.com/png.latex?c_i/c_j\" alt=\"\" align=\"bottom\" /> is a price at which you should be willing to trade off value <img title=\"j\" src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" align=\"bottom\" /> against value <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\" />. E.g., a particular hour of your time should be worth some number of marginal dollars to you.</p>\n<p><em>Addendum: Wei_Dai and other commenters point out that the set of decision-theoretic utility functions that will Pareto dominate your current policy very much depends on your beliefs. So a policy that seems Pareto dominant today will not have seemed Pareto dominant yesterday. It's not clear if you should use your current (posterior) beliefs for this purpose or your past (prior) beliefs.</em></p>\n<h1>More applications</h1>\n<p>There's a lot more that could be said about the applications of this theorem. Each of the following bullet points could be expanded into a post of its own:</p>\n<ul>\n<li>Philanthropy: There's a good reason to not split your charitable donations among charities.</li>\n<li>Moral uncertainty: There's a good reason to linearly aggregate conflicting desires or moral theories that you endorse.</li>\n<li>Population ethics: There's a good reason to aggregate the welfare or decision-theoretic utility functions of a population, even though there's no canonical way of doing so.</li>\n<li>Population ethics: It's difficult to sidestep Parfit's Repugnant Conclusion if your only desiderata are total welfare and average welfare.</li>\n</ul>\n<hr />\n<p><sup><a href=\"#1_\"></a><a name=\"1\"></a>&nbsp;<a href=\"#1_\">1</a></sup>This post evolved out of discussions with Andrew Critch and Julia Galef. They are not responsible for any deficiencies in the content of this post. The theorem appeared previously in Stuart Armstrong's post&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>.</p>\n<p><a style=\"vertical-align: super;\" href=\"#1return\"></a><a name=\"2\"></a><sup>&nbsp;<a href=\"#2_\">2</a></sup>That is, the VNM theorem says that being an optimizer is&nbsp;<em>necessary</em>&nbsp;for obeying the axioms. The easier-to-prove converse of the VNM theorem says that being an optimizer is&nbsp;<em>sufficient</em>.</p>\n<p><sup><a href=\"#3_\"></a><a name=\"3\"></a>&nbsp;<a href=\"#3_\">3</a></sup><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Decision-theoretic utility functions</a> are completely unrelated to <a href=\"http://plato.stanford.edu/entries/utilitarianism-history/\">hedonistic utilitarianism</a>.</p>\n<p><a name=\"4\"></a><sup>&nbsp;<a href=\"#4_\">4</a></sup>More specifically, if you have to choose between a bunch of actions and for all <img title=\"i&gt;1\" src=\"http://www.codecogs.com/png.latex?i&gt;1\" alt=\"\" align=\"bottom\" /> the expected value of&nbsp;<img style=\"vertical-align: bottom;\" title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_i\" alt=\"\" align=\"bottom\" />&nbsp;is independent of which actions you take, then you'll choose an action that maximizes the expected value of&nbsp;<img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\" />.</p>\n<p><a name=\"5\"></a><sup>&nbsp;<a href=\"#5_\">5</a></sup>We could formalize this by saying that for each sequence of observations <img title=\"o_1, \\dots, o_k\" src=\"http://www.codecogs.com/png.latex?o_1, \\dots, o_k\" alt=\"\" align=\"bottom\" />, the policy determines a distribution over the possible actions at time <img title=\"k+1\" src=\"http://www.codecogs.com/png.latex?k+1\" alt=\"\" align=\"bottom\" />.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oRRpsGkCZHA3pzhvm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 35, "extendedScore": null, "score": 1.0826372297109983e-06, "legacy": true, "legacyId": "21121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Restatement of:&nbsp;</em><em><a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>.&nbsp;</em><em>Alternative to:&nbsp;<a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Why you must maximize expected utility</a>. Related to:&nbsp;<a href=\"/r/discussion/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi's Social Aggregation Theorem</a>.</em></p>\n<p><em><a name=\"1_\"></a>Summary: This article describes a&nbsp;<a href=\"#theorem\">theorem</a>, previously described by Stuart Armstrong, that tells you to maximize the expectation of a linear aggregation of your values. Unlike the von Neumann-Morgenstern theorem, this theorem gives you a reason to behave rationally.<sup><a href=\"#1\">1</a></sup><a id=\"more\"></a></em></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von Neumann-Morgenstern theorem</a> is great, but it is descriptive rather than prescriptive. It tells you that if you obey four axioms, then you are an optimizer. (Let us call an \"optimizer\" any agent that always chooses an action that maximizes the expected value of some function of outcomes.) But you are a human and you don't obey the axioms; the VNM theorem doesn't say anything about you.</p>\n<p>There are Dutch-book theorems that give us reason to want to obey the four VNM axioms: E.g., if we violate the axiom of transitivity, then we can be money-pumped, and we don't want that; therefore we shouldn't want to violate the axiom of transitivity. The VNM theorem is somewhat helpful here: <a name=\"2_\"></a>It tells us that the <em>only</em> way to obey the four axioms is to be an optimizer.<sup><a href=\"#2\">2</a></sup></p>\n<p>So now you have a reason to become an optimizer. But there are an infinitude of <a name=\"3_\"></a>decision-theoretic utility functions<sup><a href=\"#3\">3</a></sup>&nbsp;to adopt&nbsp;\u2014&nbsp;which, if any, ought you adopt? And there is an even bigger problem: If you are not already an optimizer, than any utility function that you're considering will recommend actions that run counter to your preferences!</p>\n<p>To give a silly example, suppose you'd rather be an astronaut when you grow up than a mermaid, and you'd rather be a dinosaur than an astronaut, and you'd rather be a mermaid than a dinosaur. You have circular preferences. There's a decision-theoretic utility function that says</p>\n<p style=\"padding-left: 30px;\"><img title=\"\\mbox{mermaid} \\prec \\mbox{astronaut} \\prec \\mbox{dinosaur}\" src=\"http://www.codecogs.com/png.latex?\\mbox{mermaid} \\prec \\mbox{astronaut} \\prec \\mbox{dinosaur}\" alt=\"\" align=\"bottom\"></p>\n<p>which preserves some of your preferences, but if you have to choose between being a mermaid and being a dinosaur, it will tell you to become a dinosaur, even though you really really want to choose the mermaid. There's another decision-theoretic utility function that will tell you to pass up being a dinosaur in favor of being an astronaut even though you really really don't want to. Not being an optimizer means that any rational decision theory will tell you to do things you don't want to do.</p>\n<p>So why would you ever want to be an optimizer? What theorem could possibly convince you to become one?</p>\n<h1 id=\"Stuart_Armstrong_s_theorem\"><a name=\"theorem\"></a>Stuart Armstrong's theorem</h1>\n<p>Suppose there is a set <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\"> (for \"policies\") and some functions <img title=\"v_1, \\dots, v_n\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n\" alt=\"\" align=\"bottom\"> (\"values\") from <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\"> to <img title=\"\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?\\mathbb{R}\" alt=\"\" align=\"bottom\">. We want these functions to satisfy the following <strong>convexity&nbsp;property</strong>:</p>\n<p style=\"padding-left: 30px;\">For any policies <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\"> and any <img title=\"\\alpha \\in [0, 1]\" src=\"http://www.codecogs.com/png.latex?\\alpha \\in [0, 1]\" alt=\"\" align=\"bottom\">, there is a policy <img title=\"r \\in P\" src=\"http://www.codecogs.com/png.latex?r \\in P\" alt=\"\" align=\"bottom\"> such that for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\">, we have <img title=\"v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(r) = \\alpha v_i(p) + (1 - \\alpha) v_i(q)\" alt=\"\" align=\"bottom\">.</p>\n<p>For policies <img title=\"p, q \\in P\" src=\"http://www.codecogs.com/png.latex?p, q \\in P\" alt=\"\" align=\"bottom\">, say that <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\"> is a <em>Pareto improvement</em> over <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\"> if for all <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\">, we have <img title=\"v_i(p) \\geq v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) \\geq v_i(q)\" alt=\"\" align=\"bottom\">.&nbsp;Say that it is a <em>strong Pareto improvement</em> if in addition there is some <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\"> for which <img title=\"v_i(p) > v_i(q)\" src=\"http://www.codecogs.com/png.latex?v_i(p) > v_i(q)\" alt=\"\" align=\"bottom\">.&nbsp;Call <img style=\"vertical-align: bottom;\" title=\"a\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" width=\"10\" height=\"12\" align=\"bottom\"> a&nbsp;<em>Pareto optimum</em>&nbsp;if no policy is a strong Pareto improvement over it.</p>\n<p style=\"padding-left: 30px;\"><strong>Theorem.</strong> Suppose <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\"> and <img title=\"v_1, \\dots, v_n\" src=\"http://www.codecogs.com/png.latex?v_1, \\dots, v_n\" alt=\"\" align=\"bottom\"> satisfy the convexity property. If a policy in <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\"> is a Pareto optimum, then it is a maximum of the function <img title=\"c_1 v_1 + \\cdots + c_n v_n\" src=\"http://www.codecogs.com/png.latex?c_1 v_1 + \\cdots + c_n v_n\" alt=\"\" align=\"bottom\"> for some nonnegative constants <img title=\"c_1, \\dots, c_n\" src=\"http://www.codecogs.com/png.latex?c_1, \\dots, c_n\" alt=\"\" align=\"bottom\">.</p>\n<p>This theorem previously appeared in&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>. I don't know whether there is a source prior to that post that uses the hyperplane separation theorem to justify being an optimizer. The <a href=\"/r/discussion/lw/g8p/proof_of_fungibility_theorem/\">proof</a> is basically the same as the proof for the complete class theorem and the hyperplane separation theorem and the second fundamental theorem of welfare economics.&nbsp;<a href=\"/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi's utilitarian theorem</a> has a similar conclusion, but it assumes that you already have a decision-theoretic utility function. The <a href=\"http://en.wikipedia.org/wiki/Fundamental_theorems_of_welfare_economics#Proof_of_the_second_fundamental_theorem\">second fundamental theorem of welfare economics</a> is virtually the same theorem, but it's interpreted in a different way.</p>\n<h1 id=\"What_does_the_theorem_mean_\">What does the theorem mean?</h1>\n<p>Suppose you are a consequentialist who subscribes to Bayesian epistemology. And in violation of the VNM axioms, you are torn between multiple incompatible decision-theoretic utility functions. Suppose you can list all the things you care about, and the list looks like this:</p>\n<ol>\n<li>Your welfare</li>\n<li>Your family's welfare</li>\n<li>Everyone's total welfare</li>\n<li>The continued existence of human civilization</li>\n<li>All mammals' total welfare</li>\n<li>Your life satisfaction</li>\n<li>Everyone's average welfare</li>\n<li>...</li>\n</ol>\n<p>Suppose further that you can quantify each item on that list with a function <img title=\"v_1, v_2, \\dots\" src=\"http://www.codecogs.com/png.latex?v_1, v_2, \\dots\" alt=\"\" align=\"bottom\"> from world-histories to real numbers, and you want to optimize for each function, all other things being equal. E.g., <img title=\"v_1(x)\" src=\"http://www.codecogs.com/png.latex?v_1(x)\" alt=\"\" align=\"bottom\"> is large if <img title=\"x\" src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" align=\"bottom\"> is a world-history where your welfare is great; and <img title=\"v_5(x)\" src=\"http://www.codecogs.com/png.latex?v_5(x)\" alt=\"\" align=\"bottom\"> somehow counts up the welfare of all mammals in world-history <img title=\"x\" src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" align=\"bottom\">.&nbsp;<a name=\"4_\"></a>If the expected value of <img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\"> is at stake (but none of the other values are at stake), then you want to act so as to maximize the expected value of <img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\">.<sup><a href=\"#4\">4</a></sup>&nbsp;And if only <img title=\"v_5\" src=\"http://www.codecogs.com/png.latex?v_5\" alt=\"\" align=\"bottom\"> is at stake, you want to act so as to maximize the expected value of <img title=\"v_5\" src=\"http://www.codecogs.com/png.latex?v_5\" alt=\"\" align=\"bottom\">. What I've said so far doesn't specify what you do when you're forced to trade off value 1 against value 5.</p>\n<p>If you're VNM-rational, then you are an optimizer whose decision-theoretic utility function is a linear aggregation <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\"> of your values and you just optimize for that function. (The <img title=\"c_i\" src=\"http://www.codecogs.com/png.latex?c_i\" alt=\"\" align=\"bottom\"> are nonnegative constants.) But suppose you make decisions in a way that does not optimize for any such aggregation.</p>\n<p>You will make many decisions throughout your life, depending on the observations you make and on random chance. If you're capable of making precommitments and we don't worry about computational difficulties, <a name=\"5_\"></a>it is as if today you get to choose a policy for the rest of your life that specifies a distribution of actions for each sequence of observations you can make.<sup><a href=\"#5\">5</a></sup>&nbsp;Let <img title=\"P\" src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" align=\"bottom\"> be the set of all possible policies. If <img title=\"p \\in P\" src=\"http://www.codecogs.com/png.latex?p \\in P\" alt=\"\" align=\"bottom\">, and for any <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\">, let us say that <img title=\"v_i(p)\" src=\"http://www.codecogs.com/png.latex?v_i(p)\" alt=\"\" align=\"bottom\"> is the expected value of <img title=\"v_i\" src=\"http://www.codecogs.com/png.latex?v_i\" alt=\"\" align=\"bottom\"> given that we adopt policy <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\">. Let's assume that these expected values are all finite. Note that if <img title=\"p_f\" src=\"http://www.codecogs.com/png.latex?p_f\" alt=\"\" align=\"bottom\"> is a policy where you make every decision by maximizing a decision-theoretic utility function <img title=\"f\" src=\"http://www.codecogs.com/png.latex?f\" alt=\"\" align=\"bottom\">, then the policy <img title=\"p_f\" src=\"http://www.codecogs.com/png.latex?p_f\" alt=\"\" align=\"bottom\"> itself maximizes the expected value of <img title=\"f\" src=\"http://www.codecogs.com/png.latex?f\" alt=\"\" align=\"bottom\">, compared to other policies.</p>\n<p>In order to apply the theorem, we must check that the convexity property holds. That's easy:&nbsp;If <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\"> and <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\"> are two policies and <img title=\"\\alpha \\in [0, 1]\" src=\"http://www.codecogs.com/png.latex?\\alpha \\in [0, 1]\" alt=\"\" align=\"bottom\">, the mixed policy where today you randomly choose policy <img title=\"p\" src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" align=\"bottom\"> with probability <img title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\"> and policy <img title=\"q\" src=\"http://www.codecogs.com/png.latex?q\" alt=\"\" align=\"bottom\"> with probability <img title=\"1-\\alpha\" src=\"http://www.codecogs.com/png.latex?1-\\alpha\" alt=\"\" align=\"bottom\">, is also a policy.</p>\n<p>What the theorem says is that if you really care about the values on that list (and the other assumptions in this post hold), then there are linear aggregations <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\"> that you have reason to start optimizing for. That is, there are a set of linear aggregations and if you choose one of them and start optimizing for it, you will get <em>more</em> expected welfare for yourself, <em>more</em> expected welfare for others, <em>less</em> risk of the fall of civilization, ....</p>\n<p>Adopting one of these decision-theoretic utility functions <img title=\"\\sum_i c_i v_i\" src=\"http://www.codecogs.com/png.latex?\\sum_i c_i v_i\" alt=\"\" align=\"bottom\"> in the sense that doing so will get you more of the things you value without sacrificing any of them.</p>\n<p>What's more, once you've chosen a linear aggregation, optimizing for it is easy. The ratio <img title=\"c_i/c_j\" src=\"http://www.codecogs.com/png.latex?c_i/c_j\" alt=\"\" align=\"bottom\"> is a price at which you should be willing to trade off value <img title=\"j\" src=\"http://www.codecogs.com/png.latex?j\" alt=\"\" align=\"bottom\"> against value <img title=\"i\" src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" align=\"bottom\">. E.g., a particular hour of your time should be worth some number of marginal dollars to you.</p>\n<p><em>Addendum: Wei_Dai and other commenters point out that the set of decision-theoretic utility functions that will Pareto dominate your current policy very much depends on your beliefs. So a policy that seems Pareto dominant today will not have seemed Pareto dominant yesterday. It's not clear if you should use your current (posterior) beliefs for this purpose or your past (prior) beliefs.</em></p>\n<h1 id=\"More_applications\">More applications</h1>\n<p>There's a lot more that could be said about the applications of this theorem. Each of the following bullet points could be expanded into a post of its own:</p>\n<ul>\n<li>Philanthropy: There's a good reason to not split your charitable donations among charities.</li>\n<li>Moral uncertainty: There's a good reason to linearly aggregate conflicting desires or moral theories that you endorse.</li>\n<li>Population ethics: There's a good reason to aggregate the welfare or decision-theoretic utility functions of a population, even though there's no canonical way of doing so.</li>\n<li>Population ethics: It's difficult to sidestep Parfit's Repugnant Conclusion if your only desiderata are total welfare and average welfare.</li>\n</ul>\n<hr>\n<p><sup><a href=\"#1_\"></a><a name=\"1\"></a>&nbsp;<a href=\"#1_\">1</a></sup>This post evolved out of discussions with Andrew Critch and Julia Galef. They are not responsible for any deficiencies in the content of this post. The theorem appeared previously in Stuart Armstrong's post&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>.</p>\n<p><a style=\"vertical-align: super;\" href=\"#1return\"></a><a name=\"2\"></a><sup>&nbsp;<a href=\"#2_\">2</a></sup>That is, the VNM theorem says that being an optimizer is&nbsp;<em>necessary</em>&nbsp;for obeying the axioms. The easier-to-prove converse of the VNM theorem says that being an optimizer is&nbsp;<em>sufficient</em>.</p>\n<p><sup><a href=\"#3_\"></a><a name=\"3\"></a>&nbsp;<a href=\"#3_\">3</a></sup><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Decision-theoretic utility functions</a> are completely unrelated to <a href=\"http://plato.stanford.edu/entries/utilitarianism-history/\">hedonistic utilitarianism</a>.</p>\n<p><a name=\"4\"></a><sup>&nbsp;<a href=\"#4_\">4</a></sup>More specifically, if you have to choose between a bunch of actions and for all <img title=\"i>1\" src=\"http://www.codecogs.com/png.latex?i>1\" alt=\"\" align=\"bottom\"> the expected value of&nbsp;<img style=\"vertical-align: bottom;\" title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_i\" alt=\"\" align=\"bottom\">&nbsp;is independent of which actions you take, then you'll choose an action that maximizes the expected value of&nbsp;<img title=\"v_1\" src=\"http://www.codecogs.com/png.latex?v_1\" alt=\"\" align=\"bottom\">.</p>\n<p><a name=\"5\"></a><sup>&nbsp;<a href=\"#5_\">5</a></sup>We could formalize this by saying that for each sequence of observations <img title=\"o_1, \\dots, o_k\" src=\"http://www.codecogs.com/png.latex?o_1, \\dots, o_k\" alt=\"\" align=\"bottom\">, the policy determines a distribution over the possible actions at time <img title=\"k+1\" src=\"http://www.codecogs.com/png.latex?k+1\" alt=\"\" align=\"bottom\">.</p>", "sections": [{"title": "Stuart Armstrong's theorem", "anchor": "Stuart_Armstrong_s_theorem", "level": 1}, {"title": "What does the theorem mean?", "anchor": "What_does_the_theorem_mean_", "level": 1}, {"title": "More applications", "anchor": "More_applications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "66 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W2ufY8ihDDWWqJA7h", "F46jPraqp258q67nE", "z8afQRsH9wWsB4iMD", "xkLsBXknuXSXxu4Jd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T10:09:23.640Z", "modifiedAt": null, "url": null, "title": " Farewell Aaron Swartz (1986-2013)", "slug": "farewell-aaron-swartz-1986-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:04.962Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qWxApH2AyF3bbPDeL/farewell-aaron-swartz-1986-2013", "pageUrlRelative": "/posts/qWxApH2AyF3bbPDeL/farewell-aaron-swartz-1986-2013", "linkUrl": "https://www.lesswrong.com/posts/qWxApH2AyF3bbPDeL/farewell-aaron-swartz-1986-2013", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Farewell%20Aaron%20Swartz%20(1986-2013)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Farewell%20Aaron%20Swartz%20(1986-2013)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqWxApH2AyF3bbPDeL%2Ffarewell-aaron-swartz-1986-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Farewell%20Aaron%20Swartz%20(1986-2013)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqWxApH2AyF3bbPDeL%2Ffarewell-aaron-swartz-1986-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqWxApH2AyF3bbPDeL%2Ffarewell-aaron-swartz-1986-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p><a href=\"http://tech.mit.edu/V132/N61/swartz.html\">Link</a></p>\n<p><a href=\"/user/aaronsw/overview/\">One of us</a> is <a href=\"http://tech.mit.edu/V132/N61/swartz.html\">no more</a>.</p>\n<blockquote>\n<p>Computer activist Aaron H. Swartz committed suicide in New York City yesterday, Jan. 11.</p>\n<p>The accomplished Swartz co-authored the now widely-used RSS 1.0 specification at age 14, was one of the three co-owners of the popular social news site Reddit, and completed a fellowship at Harvard&rsquo;s Ethics Center Lab on Institutional Corruption. In 2010, he founded <a href=\"http://demandprogress.org/\"><em>DemandProgress.org</em></a>, a &ldquo;campaign against the Internet censorship bills SOPA/PIPA.&rdquo;</p>\n</blockquote>\n<p>He deserves a eulogy more eloquent than what I am capable of writing. <a href=\"http://boingboing.net/2013/01/12/rip-aaron-swartz.html\" target=\"_blank\">Here's</a> Cory Doctorow's, one of his long time friends.</p>\n<p>It's a sad world in which you are <a href=\"http://bits.blogs.nytimes.com/2011/07/19/reddit-co-founder-charged-with-data-theft/\">being arrested</a> and grand jury'd for downloading scientific journals and papers with the intent to share them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "izp6eeJJEg9v5zcur": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qWxApH2AyF3bbPDeL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 115, "extendedScore": null, "score": 0.0003, "legacy": true, "legacyId": "21122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 115, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T13:21:10.391Z", "modifiedAt": null, "url": null, "title": "[Link] St. Paul: memetic engineer", "slug": "link-st-paul-memetic-engineer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iWF7ZKnFa7EMj4Kh6/link-st-paul-memetic-engineer", "pageUrlRelative": "/posts/iWF7ZKnFa7EMj4Kh6/link-st-paul-memetic-engineer", "linkUrl": "https://www.lesswrong.com/posts/iWF7ZKnFa7EMj4Kh6/link-st-paul-memetic-engineer", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20St.%20Paul%3A%20memetic%20engineer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20St.%20Paul%3A%20memetic%20engineer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWF7ZKnFa7EMj4Kh6%2Flink-st-paul-memetic-engineer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20St.%20Paul%3A%20memetic%20engineer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWF7ZKnFa7EMj4Kh6%2Flink-st-paul-memetic-engineer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWF7ZKnFa7EMj4Kh6%2Flink-st-paul-memetic-engineer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4727, "htmlBody": "<div class=\"entry-content\"><a href=\"http://studiolo.cortediurbino.org/st-paul-memetic-engineer/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=st-paul-memetic-engineer\">New post</a> by Federico on his <a href=\"http://studiolo.cortediurbino.org/studiolo/\">new blog</a> that I mentioned <a href=\"/r/discussion/lw/g84/link_the_school_of_science_fiction/\">earlier</a>. Worth reading for those interested in the <a href=\"http://en.wikipedia.org/wiki/Memetics\">memetics</a> of religion, politics, Christianity or Islam. The cited material also led my mind to some related questions. <br />\n<blockquote>\n<p><a href=\"http://studiolo.cortediurbino.org/audience-participation/#comment-12\">AnnoDomini</a> suggests I write about &ldquo;St. Paul the social engineer!&rdquo;</p>\n<p>&ldquo;Social engineering&rdquo; is coercive. Saint Paul was a missionary, not a law-maker; I would call him a <em>memetic engineer</em>.</p>\n<p>Like any <em>ingeniarius</em>, a memetic engineer takes elements at his disposal, makes one or two small changes, synthesises, and sells his product. The product is designed to fulfil a personal end; if it endures, this is most likely incidental. Few engineers care if their creation outlasts them.</p>\n<p>The elements at this memetic engineer&rsquo;s disposal are an ethnic-supremacist religion, a popular dead Messiah, and a cunning intellect.</p>\n<p>Saul of Tarsus spends his twenties persecuting Christians. In his own words:</p>\n<p style=\"padding-left: 30px;\"><em>13</em>&nbsp;For ye have heard of my conversation in time past in the Jews&rsquo; religion, how that beyond measure I persecuted the church of God, and wasted it: <em>14</em> And profited in the Jews&rsquo; religion above many my equals in mine own nation, being more exceedingly zealous of the traditions of my fathers.</p>\n<p style=\"padding-left: 30px;\">Galatians 1:13&ndash;14</p>\n<p>He even participates in the racist murder of a naive idealist called Stephen, in a scene echoed many centuries later by <a href=\"http://www.youtube.com/watch?v=UoXAjrevbyI\">Sacha Baron Cohen</a>.</p>\n<p style=\"padding-left: 30px;\"><em>55</em>&nbsp;But he, being full of the Holy Ghost, looked up stedfastly into heaven, and saw the glory of God, and Jesus standing on the right hand of God, <em>56</em> And said, Behold, I see the heavens opened, and the Son of man standing on the right hand of God. <em>57</em> Then they cried out with a loud voice, and stopped their ears, and ran upon him with one accord, <em>58</em> And cast <em>him</em> out of the city, and stoned <em>him</em>: and the witnesses laid down their clothes at a young man&rsquo;s feet, whose name was Saul. <em>59</em> And they stoned Stephen, calling upon <em>God</em>, and saying, Lord Jesus, receive my spirit. <em>60</em> And he kneeled down, and cried with a loud voice, Lord, lay not this sin to their charge. And when he had said this, he fell asleep.</p>\n<p style=\"padding-left: 30px;\">Acts 7:55&ndash;60</p>\n<p>But what he sees afterwards gives him pause.</p>\n<p style=\"padding-left: 30px;\"><em>1</em> And Saul was consenting unto his death. And at that time there was a great persecution against the church which was at Jerusalem; and they were all scattered abroad throughout the regions of Judaea and Samaria, except the apostles. <em>2</em> And devout men carried Stephen <em>to his burial</em>, and made great lamentation over him. <em>3</em> As for Saul, he made havock of the church, entering into every house, and haling men and women committed <em>them</em> to prison. <em>4</em> Therefore they that were scattered abroad went every where preaching the word. <em>5</em> Then Philip went down to the city of Samaria, and preached Christ unto them. <em>6</em> And the people with one accord gave heed unto those things which Philip spake, hearing and seeing the miracles which he did. 7 For unclean spirits, crying with loud voice, came out of many that were possessed <em>with them</em>: and many taken with palsies, and that were lame, were healed. 8 And there was great joy in that city.</p>\n<p style=\"padding-left: 30px;\">Acts 8:1&ndash;8</p>\n<p>Saul realises that he can do better as a Christian. All that joy to be had in all those cities. The problem is, he never <em>met </em>Jesus. So he spins an absurd yarn about Jesus&rsquo;s ghost.</p>\n<p style=\"padding-left: 30px;\"><em>13</em> At midday, O king, I saw in the way a light from heaven, above the brightness of the sun, shining round about me and them which journeyed with me. <em>14</em> And when we were all fallen to the earth, I heard a voice speaking unto me, and saying in the Hebrew tongue, Saul, Saul, why persecutest thou me? <em>it is</em> hard for thee to kick against the pricks. <em>15</em> And I said, Who art thou, Lord? And he said, I am Jesus whom thou persecutest. <em>16</em> But rise, and stand upon thy feet: for I have appeared unto thee for this purpose, to make thee a minister and a witness both of these things which thou hast seen, and of those things in the which I will appear unto thee; <em>17</em> Delivering thee from the people, and <em>from</em> the Gentiles, unto whom now I send thee, <em>18</em> To open their eyes, <em>and</em> to turn <em>them</em> from darkness to light, and <em>from</em> the power of Satan unto God, that they may receive forgiveness of sins, and inheritance among them which are sanctified by faith that is in me.</p>\n<p style=\"padding-left: 30px;\">Acts 26:13&ndash;18</p>\n<p>Christians are not popular with the Jews. Therefore, <del>Saul</del> Paul won&rsquo;t risk preaching to them. Here is his first innovation:</p>\n<p style=\"padding-left: 30px;\"><em>1</em> I say the truth in Christ, I lie not, my conscience also bearing me witness in the Holy Ghost, <em>2</em> That I have great heaviness and continual sorrow in my heart. <em>3</em> For I could wish that myself were accursed from Christ for my brethren, my kinsmen according to the flesh: <em>4</em> Who are Israelites; to whom <em>pertaineth</em> the adoption, and the glory, and the covenants, and the giving of the law, and the service <em>of God</em>, and the promises; <em>5</em> Whose <em>are</em> the fathers, and of whom as concerning the flesh Christ <em>came</em>, who is over all, God blessed for ever. Amen. <em>6</em> Not as though the word of God hath taken none effect. For they <em>are</em> not all Israel, which are of Israel: <em>7</em> Neither, because they are the seed of Abraham, <em>are they</em> all children: but, In Isaac shall thy seed be called. <em>8</em> That is, They which are the children of the flesh, these <em>are</em> not the children of God: but the children of the promise are counted for the seed.</p>\n<p style=\"padding-left: 30px;\">Romans 9:1&ndash;8</p>\n<p>In other words, Yahweh, God of the Israelites, who was complicit in the <a href=\"http://www.infidels.org/library/modern/donald_morgan/atrocity.html\">genocide</a> of Amalekites, Canaanites, Midianites, Gibeonites, Libnahites, Eglonites, Debirites, Moabites, Benjamites, Ammonites, Edomites, Egyptians, Syrians, Philistines and anyone else who got in the way of his favourite ethnic group&hellip;is now God of Everyone. &ldquo;Israel&rdquo; is just a metaphor, decides Paul.</p>\n<p>Paul now has license to go on a world tour; but he mustn&rsquo;t upset the local rulers. The Romans are touchy about rabble-rousers. Paul has heard of Christ&rsquo;s cryptic comment:</p>\n<p style=\"padding-left: 30px;\"><em>15</em>&nbsp;Then went the Pharisees, and took counsel how they might entangle him in <em>his</em> talk. <em>16</em> And they sent out unto him their disciples with the Herodians, saying, Master, we know that thou art true, and teachest the way of God in truth, neither carest thou for any <em>man</em>: for thou regardest not the person of men. <em>17</em> Tell us therefore, What thinkest thou? Is it lawful to give tribute unto Caesar, or not? <em>18</em> But Jesus perceived their wickedness, and said, Why tempt ye me, ye hypocrites? <em>19</em> Shew me the tribute money. And they brought unto him a penny. <em>20</em> And he saith unto them, Whose <em>is</em> this image and superscription? <em>21</em> They say unto him, Caesar&rsquo;s. Then saith he unto them, Render therefore unto Caesar the things which are Caesar&rsquo;s; and unto God the things that are God&rsquo;s. <em>22</em> When they had heard <em>these words</em>, they marvelled, and left him, and went their way.</p>\n<p style=\"padding-left: 30px;\">Matthew 22:15&ndash;22</p>\n<p>So Paul invents &ldquo;separation of Church and State&rdquo;. This makes his exotic new religion seem inoffensive, although the Romans end up killing him anyway.</p>\n<p style=\"padding-left: 30px;\"><em>1</em> Let every soul be subject unto the higher powers. For there is no power but of God: the powers that be are ordained of God. <em>2</em> Whosoever therefore resisteth the power, resisteth the ordinance of God: and they that resist shall receive to themselves damnation. <em>3</em> For rulers are not a terror to good works, but to the evil. Wilt thou then not be afraid of the power? do that which is good, and thou shalt have praise of the same: <em>4</em> For he is the minister of God to thee for good. But if thou do that which is evil, be afraid; for he beareth not the sword in vain: for he is the minister of God, a revenger to <em>execute</em> wrath upon him that doeth evil. <em>5</em> Wherefore <em>ye</em> must needs be subject, not only for wrath, but also for conscience sake. <em>6</em> For for this cause pay ye tribute also: for they are God&rsquo;s ministers, attending continually upon this very thing. <em>7</em> Render therefore to all their dues: tribute to whom tribute <em>is due</em>; custom to whom custom; fear to whom fear; honour to whom honour.</p>\n<p style=\"padding-left: 30px;\">Romans 13:1&ndash;7</p>\n<p>Leo Tolstoy <a href=\"http://www.nonresistance.org/docs_pdf/Tolstoy/Epilogue_to_Drozhzhin.pdf\">points out</a>:</p>\n<p style=\"padding-left: 30px;\">Not only the complete misunderstanding of Christ&rsquo;s teaching, but also a complete unwillingness to understand it could have admitted that striking misinterpretation, according to which the words, &ldquo;To Caesar the things which are Caesar&rsquo;s,&rdquo; signify the necessity of obeying Caesar. In the first place, there is no mention there of obedience; in the second place, if Christ recognized the obligatoriness of paying tribute, and so of obedience, He would have said directly, &ldquo;Yes, it should be paid;&rdquo; but He says, &ldquo;Give to Caesar what is his, that is, the money, and give your life to God,&rdquo; and with these latter words He not only does not encourage any obedience to power, but, on the contrary, points out that in everything which belongs to God it is not right to obey Caesar.</p>\n<p>But the deed was done.</p>\n<p>Paul is set to have fun in his middle age. He isn&rsquo;t <a href=\"http://www.dennyburk.com/was-the-apostle-paul-married/\">married</a>, and all his expenses are <a href=\"http://books.google.co.uk/books?id=e0HNAABXqm0C&amp;q=money#v=snippet&amp;q=money&amp;f=false\">paid</a>.</p>\n<p style=\"padding-left: 30px;\">So, too, in his last speech to the Ephesian elders he lays great stress on the fact that he had not made money by his preaching, but had supported himself by the labour of his hands. &lsquo;I coveted no man&rsquo;s gold or apparel. Ye yourselves know that these hands ministered unto my necessities.&rsquo;</p>\n<p style=\"padding-left: 30px;\">Yet St. Paul did receive gifts from his converts. He speaks of the Philippians as having sent once and again unto his necessity, and he tells the Corinthians that he &lsquo;robbed other churches, taking wages of them, that he might minister to them&rsquo;. He does not seem to have felt any unwillingness to receive help; he rather welcomed it. He was not an ascetic. He saw no particular virtue in suffering privations. The account of his journeys always gives us the impression that he was poor, never that he was poverty-stricken. He said indeed that he knew how &lsquo;to be in want&rsquo;, &lsquo;to be filled, and to be hungry&rsquo;. But this does not imply more than that he was in occasional need. Later, he certainly must have had considerable resources, for he was able to maintain a long and expensive judicial process, to travel with ministers, to gain a respectful hearing from provincial governors, and to excite their cupidity. We have no means of knowing whence he obtained such large supplies; but if he received them from his converts there would be nothing here contrary to his earlier practice. He received money; but not from those to whom he was preaching. He refused to do anything from which it might appear that he came to receive, that his object was to make money.</p>\n<p>Paul&rsquo;s epistle to the Romans holds a clue to the source of his mysterious wealth.</p>\n<p style=\"padding-left: 30px;\"><em>19</em> Through mighty signs and wonders, by the power of the Spirit of God; so that from Jerusalem, and round about unto Illyricum, I have fully preached the gospel of Christ. <em>20</em> Yea, so have I strived to preach the gospel, not where Christ was named, lest I should build upon another man&rsquo;s foundation: <em>21</em> But as it is written, To whom he was not spoken of, they shall see: and they that have not heard shall understand. <em>22</em> For which cause also I have been much hindered from coming to you. <em>23</em> But now having no more place in these parts, and having a great desire these many years to come unto you; <em>24</em> Whensoever I take my journey into Spain, I will come to you: for I trust to see you in my journey, and to be brought on my way thitherward by you, if first I be somewhat filled with your <em>company</em>. <em>25</em> But now I go unto Jerusalem to minister unto the saints. <em>26</em> For it hath pleased them of Macedonia and Achaia to make a certain contribution for the poor saints which are at Jerusalem. <em>27</em> It hath pleased them verily; and their debtors they are. For if the Gentiles have been made partakers of their spiritual things, their duty is also to minister unto them in carnal things. <em>28</em> When therefore I have performed this, and have sealed to them this fruit, I will come by you into Spain. <em>29</em> And I am sure that, when I come unto you, I shall come in the fulness of the blessing of the gospel of Christ.</p>\n<p style=\"padding-left: 30px;\">Romans 15:19&ndash;29</p>\n<p><a href=\"http://www.pbs.org/wgbh/pages/frontline/shows/religion/first/missions.html\">Scholars</a>&nbsp;are puzzled by this excerpt.</p>\n<p style=\"padding-left: 30px;\">He is a person who is somehow a city person, and he sees that the cities are the key to the rapid spread of this new message. . . . At one point he can write to the Roman Christians, I have filled up the gospel in the East, I have no more room to work here. What could he possibly mean? There are only a handful of Christians in each of several major cities in the Eastern Empire. What does he mean, that he has filled up all of the Eastern Empire with the gospel?</p>\n<p>He had merely filled up his coffers. Those burgeoning trade centres, bustling with merchants and artisans&hellip;</p>\n<p>Paul&rsquo;s final stroke of genius is to <em>dumb down&nbsp;</em>the gospel.</p>\n<p style=\"padding-left: 30px;\"><em>8</em>&nbsp;Owe no man any thing, but to love one another: for he that loveth another hath fulfilled the law. <em>9</em> For this, Thou shalt not commit adultery, Thou shalt not kill, Thou shalt not steal, Thou shalt not bear false witness, Thou shalt not covet; and if <em>there be</em> any other commandment, it is briefly comprehended in this saying, namely, Thou shalt love thy neighbour as thyself. <em>10</em>&nbsp;Love worketh no ill to his neighbour: therefore love <em>is</em> the fulfilling of the law.</p>\n<p style=\"padding-left: 30px;\">Romans 13:8&ndash;10</p>\n<p>&ldquo;The law&rdquo; means the Decalogue, or the parts of it Paul can remember. This is another gross misinterpretation of Jesus and his disciples&rsquo; teaching. Yahweh says in Leviticus:</p>\n<p style=\"padding-left: 30px;\"><em>18</em>&nbsp;Thou shalt not avenge, nor bear any grudge against the children of thy people, but thou shalt love thy neighbour as thyself: I <em>am</em> the LORD.</p>\n<p style=\"padding-left: 30px;\">Leviticus 19:18</p>\n<p>Jesus, like any hipster, uses this obscure reference to put a Pharisee in his place:</p>\n<p style=\"padding-left: 30px;\"><em>34</em> But when the Pharisees had heard that he had put the Sadducees to silence, they were gathered together. <em>35</em> Then one of them, <em>which was</em> a lawyer, asked <em>him a question</em>, tempting him, and saying, <em>36</em> Master, which <em>is</em> the great commandment in the law? <em>37</em> Jesus said unto him, Thou shalt love the Lord thy God with all thy heart, and with all thy soul, and with all thy mind. <em>38</em> This is the first and great commandment. <em>39</em> And the second <em>is</em> like unto it, Thou shalt love thy neighbour as thyself. <em>40</em> On these two commandments hang all the law and the prophets.</p>\n<p style=\"padding-left: 30px;\">Matthew 22:34&ndash;40</p>\n<p>This doesn&rsquo;t mean that Christians can <em>dispense with</em> the law! James the Just concurs:</p>\n<p style=\"padding-left: 30px;\">8 If ye fulfil the royal law according to the scripture, Thou shalt love thy neighbour as thyself, ye do well: 9 But if ye have respect to persons, ye commit sin, and are convinced of the law as transgressors. 10 For whosoever shall keep the whole law, and yet offend in one <em>point</em>, he is guilty of all. 11 For he that said, Do not commit adultery, said also, Do not kill. Now if thou commit no adultery, yet if thou kill, thou art become a transgressor of the law.</p>\n<p style=\"padding-left: 30px;\">James 2:8&ndash;11</p>\n<p>Paul not only tells his converts that God&rsquo;s single law is &ldquo;be nice&rdquo;, but he <a href=\"http://www.pbs.org/wgbh/pages/frontline/shows/religion/first/missions.html\">abolishes</a> all of the fiddly rules.</p>\n<p style=\"padding-left: 30px;\">Now the situation seems to be that initially when people were attracted to the Jesus movement, they first became Jews and they had to go through all the rituals and rites of conversion to Judaism. But apparently it&rsquo;s among Paul and some of his close supporters that they began to think that it was okay to become a member of the Christian movement without having to go through all of those rites of conversion to Judaism [...]</p>\n<p style=\"padding-left: 30px;\">Now the other things that one must do in order to convert to Judaism, in addition to circumcision if a male, would be to observe the Torah. That is, the Jewish law and the dietary and other kinds of purity regulations that would have come from the Torah. [...]</p>\n<p style=\"padding-left: 30px;\">Paul&rsquo;s notion that it was possible for gentiles to enter the congregation of God without some of the rules of Judaism interestingly enough seems to be a conviction on his part that comes from his own interpretation of the Jewish scriptures.</p>\n<p>A very convenient interpretation, for someone who is on a whistle-stop tour of Europe&rsquo;s richest and most cosmopolitan cities. Does a televangelist ask his marks to study ancient Greek, or make a pilgrimage to Jerusalem?</p>\n<p>If human nature has changed little in 2000 years, Saint Paul was a con artist. He turned Yahweh into a universalist, Jesus into a lackey, and Christianity into <em>Barney</em>, all because he wanted to live the good life. He also misled the world in general about the plausibility of &ldquo;Damascene conversion&rdquo;.</p>\n<p>Yet, Christianity prospered. <a href=\"http://en.wikipedia.org/wiki/Civilisation_%28TV_series%29\">Kenneth Clark</a> thought it essential to Western civilisation. Why is that? One must contrast it with Islam. Roger Scruton <a href=\"http://www.amazon.co.uk/The-West-Rest-Globalization-Terrorist/dp/0826470300\">explains</a>:</p>\n<p style=\"padding-left: 30px;\">The student of Muslim thought will be struck by how narrowly the classical thinkers pondered the problems of political order, and how sparse and theological are their theories of institutions. Apart from the caliphate&mdash;the office of &ldquo;successor to&rdquo; or &ldquo;substitute for&rdquo; the Prophet&mdash;no human institution occupies such thinkers as Al-Mawardi, Al-Ghazali, Ibn Taymiya, or Saif Ibn &lsquo;Umar al-Asadi for long, and discussions of sovereignty&mdash;<em>sultan</em>,<em> mulk</em>&mdash;tend to be exhortatory, instructions for the ruler that will help him to guide his people in the ways of the faith. [...]</p>\n<p style=\"padding-left: 30px;\">Law is fundamental to Islam, since the religion grew from Muhammad&rsquo;s attempt to give an abiding code of conduct to his followers. Hence arose the four surviving schools (known as <em>madhahib</em>, or sects) of jurisprudence, with their subtle devices (<em>hila</em>) for discovering creative solutions within the letter (though not always the spirit) of the law. These four schools (<em>Hanafi</em>, <em>Hanbali</em>, <em>Shafi</em> and <em>Maliki</em>, named for their founders) are accepted by each other as legitimate, but may produce conflicting judgements in any particular case. As a result the body of Islamic jurisprudence (the <em>fiqh</em>) is now enormous. Such legal knowledge notwithstanding, discussions of the <em>nature</em>&nbsp;of the law, the grounds of its legitimacy, and the distinguishing marks of legal, as opposed to coercive, social structures are minimalist, Classical Islamic jurisprudence, like classical Islamist philosophy, assumes that law originates in divine command, as revealed through the Koran and the Sunna, and as deduced by analogy (<em>qiyas</em>) or consensus (<em>ijma&rsquo;</em>). Apart from the four sources (<em>usul</em>) of law, no other source is recognised. Law, in other words, is the will of God, and sovereignty is legitimate only in so far as it upholds God&rsquo;s will and is authorized through it.</p>\n<p style=\"padding-left: 30px;\">There is nevertheless one great classical thinker who addressed the realities of social order, and the nature of the power exerted through it, in secular rather than theological terms: Ibn Khaldun, the fourteenth-century Tunisian polymath whose <em>Muqaddimah</em>&nbsp;is a kind of prolegomenon to the study of history and offers a general perspective on the rise and decline of human societies. Ibn Khaldun&rsquo;s primary subject of study had been the Bedouin societies of North Africa; but he generalized also from his knowledge of Muslim history. Societies, he argued, are held together by a cohesive force, which he called <em>&lsquo;asabiya</em> (<em>&lsquo;asaba</em>, &ldquo;to bind,&rdquo; <em>&lsquo;asab</em>, a &ldquo;nerve,&rdquo; &ldquo;ligament,&rdquo; or &ldquo;sinew&rdquo;&mdash;cf. Latin <em>religio</em>). I<strong>n tribal communities, <em>&lsquo;asabiya</em> is strong, and creates resistance to outside control, to taxation, and to government. In cities, the seat of government,&nbsp;<em>&lsquo;asabiya</em> is weak or non-existent, and society is held together by force exerted by the ruling dynasty. But dynasties too need&nbsp;<em>&lsquo;asabiya</em> if they are to maintain their power. </strong>Hence they inevitably decline, softened by the luxury of city life, and within four generations will be conquered by outsiders who enjoy the dynamic cohesion of the tribe.</p>\n</blockquote>\n<p style=\"padding-left: 30px;\">I'm bolding this just in case you aren't familiar with Ibn Khaldun's theory to emphasise how important this is. I would argue that it is basically correct.</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">That part of Ibn Khaldun&rsquo;s theory is still influential: Malise Ruthven, for example, believes that it casts light on the contemporary Muslim world, in which <em>&lsquo;asabiya</em> rather than instituions remains the principal cohesive force. But Ibn Khaldun&rsquo;s secular theory of society dwells on pre-political unity rather than political order. His actual political theory is far more Islamic in tone. Ibn Khaldun introduces a distinction between two kinds of government&mdash;that founded on religion (<em>siyasa diniya</em>) and that founded on reason (<em>siyasa &lsquo;aqliya</em>), echoing the thoughts of the Mu&rsquo;tazili theologians. The second form of government is more political and less theocratic, since its laws do not rest on divine authority but on rational principles that can be understood and accepted without the benefit of faith. But Ibn Khaldun finds himself unable to approve of this form of politics. <strong>Secular law, he argues, leads to a decline of&nbsp;<em>&lsquo;asabiya</em></strong>, such as occurred when the Islamic <em>umma</em> passed from Arab to Persian rule.<strong> </strong>Moreover the impediment (<em>wazi&rsquo;</em>) that constrains us to abide by the law is, in the rational state, merely external. In the state founded on the <em>shari&rsquo;a</em>&nbsp;this impediment is internal, operating directly on the will of the subject. In short, the emergence of secular politics from the prophetic community is a sign not of civilized progress but of moral decline. [...]</p>\n</blockquote>\n<p style=\"padding-left: 30px;\">At this point I ask my fellow rationalists to consider. If this was the case, what might decline of 'asabiya look like in modern secular societies if it was happening?</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">For all his subtlety, therefore, Ibn Khaldun ends by endorsing the traditional, static idea of government according to the <em>shari&rsquo;a</em>. To put in a nutshell what is distinctive about this traditional idea of government: the Muslim conception of law as holy law, pointing the unique way to salvation, and applying to every area of human life, involves a <em>confiscation of the political</em>. Those matters which, in Western societies, are resolved by negotiation, compromise, and the laborious work of offices and committees are the object of immovable and eternal decrees, either laid down explicitly in the holy book, or discerned there by some religious figurehead&mdash;whose authority, however, can always be questioned by some rival <em>imam</em> or jurist, since the <em>shari&rsquo;a</em> recognizes no office or institution as endowed with any independent lawmaking power.</p>\n<p style=\"padding-left: 30px;\">Three features of the original message embodied in the Koran have proved decisive in this respect. First, the Messenger of God was presented with the problem of organizing and leading an autonomous community of followers. Unlike Jesus, he was not a religious visionary operating under an all-embracing imperial law, but a political leader, inspired by a revelation of God&rsquo;s purpose and determined to assert that purpose against the surrounding world of tribal government and pagan superstition.</p>\n<p style=\"padding-left: 30px;\">Second, the suras of the Koran make no distinction between the public and private spheres: what is commanded to the believers is commanded in response to the many problems, great and small, that emerged during the course of Muhammad&rsquo;s political mission. But each command issues from the same divine authority. Laws governing marriage, property, usury and commerce occur side-by-side with rules of domestic ritual, good manners, and personal hygiene. The conduct of war and the treatment of criminals are dealt with in the same tone of voice as diet and defecation. The whole life of the community is set out in a disordered, but ultimately consistent, set of absolutes, and it is impossible to judge from the text itself whether any of these laws is more important, more threatening, or more dear to God&rsquo;s heart than the others. The opportunity never arises, for the student of the Koran, to distinguish those matters which are open to political negotiation from those which are absolute duties to God. In effect, everything is owed to God, with the consequence that nothing is owed to Caesar.</p>\n<p style=\"padding-left: 30px;\">Third, the social vision of the Koran is shaped through and through by the tribal order and commercial dealings of Muhammad&rsquo;s Arabia. It is a vision of people bound to each other by family ties and tribal loyalties, but answerable for their actions to God alone. No mention is made of institutions, corporations, societies, or procedures with any independent authority. Life, as portrayed in the Koran, is a stark, unmediated confrontation between the individual and his God, in which the threat of punishment and the hope of reward are never far from the thoughts of either party.</p>\n<p style=\"padding-left: 30px;\">Therefore, although the Koran is the record of a political project, it lays no foundations for an impersonal political order, but vests all power and authority in the Messenger of God. [...]</p>\n<p style=\"padding-left: 30px;\"><strong>Islamic revivals almost always begin from a sense of the corruption and godlessness of the ruling power, and a desire to rediscover the holy leader who will restore the pure way of life that had been laid down by the Prophet. </strong></p>\n</blockquote>\n<p style=\"padding-left: 30px;\">If only <a href=\"http://www.nytimes.com/2011/04/22/opinion/22iht-edguehenno22.html\">people commenting</a> on upheavals in the Middle Eastern world actually knew anything about the Middle East, they might actually make usable predictions. Not that punditry is about predictions anyway.</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">There seems to be no room in Islamic thinking for the idea&mdash;vital to the history of Western constitutional government&mdash;of an office that works for the benefit of the community, regardless of the virtues and vices of the one who fills it. Spinoza put the point explicitly by arguing that what makes for excellence in the state is not that it should be governed by good men, but that it should be so constituted that it does not matter whether it be governed by good men or bad. This idea goes back to Aristotle, and is the root of political order in the Western tradition&mdash;the government of laws, not of men, <em>even though it is men who make the laws</em>. There seems to be no similar idea in Islamic political thinking, since institutions, offices, and collective entities play no part in securing political legitimacy, and all authority stems from God, via the words, deeds, and example of his Messenger.</p>\n<p>Islam and Christianity both flourished, once the latter had endured its dormant period on the Celtic fringe. Yet Christendom&rsquo;s civic evolution, courtesy of &ldquo;separation of Church and State&rdquo;, eventually left its rival in the dust.</p>\n<p>We mustn&rsquo;t give Saint Paul too much credit. Jethro Tull surely wasn&rsquo;t the only person capable of inventing the seed drill. The triumphant religion in Europe could easily have been someone else&rsquo;s mutated Judaism, Christianity or another Messiah cult.</p>\n<p>Facile, universalist religions spread easily within a multi-ethnic empire. Kings and emperors see the benefit to themselves in &ldquo;Whosoever therefore&nbsp;resisteth the power, resisteth the ordinance of God&rdquo;. And who would miss circumcision or dietary regulations? Adaptive traits coincide in a product that happened to be useful to the antique version of GodTV.</p>\n<p>God-memes like Yahweh (v.1) prosper in more refractory circumstances. A draconian, legislative God supplements the tribal leader&rsquo;s tenuous monopoly on violence, allowing regimented Israelites to conquer the libertines of Sodom and Gomorrah.</p>\n<p>The tragedy of Islam is that it falls between two stools. It is legislative enough to help its adherents conquer other unruly Arab tribes, universalist enough to spread worldwide, and simple enough to go viral: <em>There is no god but God, Muhammad is the messenger of God</em>. But it wasn&rsquo;t born within an empire, so it lacks &ldquo;separation of Church and State&rdquo;. The memeplex persists, but doesn&rsquo;t avail its bearers.</p>\n</blockquote>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iWF7ZKnFa7EMj4Kh6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "21123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dYjW9LCNbWkoijKwL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T13:39:50.879Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:39.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FiJMHYb4G8JuSjpZe/meetup-brussels-meetup-0", "pageUrlRelative": "/posts/FiJMHYb4G8JuSjpZe/meetup-brussels-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/FiJMHYb4G8JuSjpZe/meetup-brussels-meetup-0", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFiJMHYb4G8JuSjpZe%2Fmeetup-brussels-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFiJMHYb4G8JuSjpZe%2Fmeetup-brussels-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFiJMHYb4G8JuSjpZe%2Fmeetup-brussels-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ht'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ht'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FiJMHYb4G8JuSjpZe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0827898438403947e-06, "legacy": true, "legacyId": "21124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/ht\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ht\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-12T14:53:54.939Z", "modifiedAt": null, "url": null, "title": "On private marriage contracts", "slug": "on-private-marriage-contracts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/btEKeBWZgKwDSmr8d/on-private-marriage-contracts", "pageUrlRelative": "/posts/btEKeBWZgKwDSmr8d/on-private-marriage-contracts", "linkUrl": "https://www.lesswrong.com/posts/btEKeBWZgKwDSmr8d/on-private-marriage-contracts", "postedAtFormatted": "Saturday, January 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20private%20marriage%20contracts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20private%20marriage%20contracts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtEKeBWZgKwDSmr8d%2Fon-private-marriage-contracts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20private%20marriage%20contracts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtEKeBWZgKwDSmr8d%2Fon-private-marriage-contracts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtEKeBWZgKwDSmr8d%2Fon-private-marriage-contracts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1100, "htmlBody": "<div class=\"entry-content\"><strong>Warning:</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">First Read Everything Here</a></strong>, only participate or read on if you are sure you understand the risks. <br /></div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">Based on the commentary and excerpt Federico made on <a href=\"http://studiolo.cortediurbino.org/archive/\">studiolo</a>, I've added the book <a href=\"http://en.wikipedia.org/wiki/Nudge_%28book%29\"><strong>Nudge: Improving Decisions about Health, Wealth, and Happiness</strong></a> by Thaler and Sunstein to my reading list. Since the book seems relevant enough to this site and has been <a href=\"/lw/e2n/why_dont_people_help_others_more/\">mentioned before</a>, I may eventually write a review. The <a href=\"http://studiolo.cortediurbino.org/thaler-sunstein-on-private-marriage-contracts/\">post</a> by Federico is made up mostly of excerpts from Chapter 15 of the book \"Privatizing Marriage\".</div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">\n<div class=\"entry-content\">In addition to this I recommend reading the following excellent essays:</div>\n<ul>\n<li>Sister Y's <strong><a href=\"http://theviewfromhell.blogspot.com/2012/07/the-right-to-marry.html\">The Right to Marry</a></strong></li>\n<li><a rel=\"nofollow\" href=\"http://www.rightwingnews.com/uncategorized/a-really-really-really-long-post-about-gay-marriage-that-does-not-in-the-end-support-one-side-or-the-other-by-jane-galt/\">A Really, Really, Really Long Post About Gay Marriage That Does Not, In The End, Support One Side Or The Other</a> also recommended by <a href=\"http://lesswrong.com/lw/e5y/link_social_interventions_gone_wrong/78ob\">CharlieSheen</a></li>\n</ul>\n</div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">The reason the particular topic he talks about caught my interest is because the proposed solutions by Thaler and Sunstein seem somewhat similar to the one <a href=\"/lw/dc5/thoughts_on_moral_intuitions/6xe6\">I argued</a> for...</div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">\n<blockquote>\n<p>Marriage is a personal or religious arrangement, it is only the states business as far as it is also a legally enforceable contract. It is fundamentally unfair that people agree to a set of legal terms and cultural expectations that ideally are aimed to last a lifetime yet the state messes with the contract beyond recognition in just a few decades without their consent.</p>\n<p>Consider a couple marrying in 1930s or 1940s that died or divorced in the 1980s. Did they even end their marriage in the same institution they started in? Consider how divorce laws and practice had changed. Ridiculous. People should have the right to sign an explicit, customisable contract governing their rights and duties as well as terms of dissolution in it. Beyond that the state should have no say, also such contracts should supersede any legislation the state has on child custody, though perhaps some limits on what exactly they can agree on would be in order.</p>\n<p>Such a contract has no good reason to be limited to just describing traditional marriage or even having that much to do with sex or even raising children, it can and should be used to help people formalize platonic and non-sexual relationships as well. It should also be used for various kinds of non-traditional (for Western civ) marriage like polygamy or other kinds of polyamours arrangements and naturally homosexual unions.</p>\n</blockquote>\n</div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">...and <a href=\"/user/Vladimir_M/overview/\">Vladimir_M</a> argued convincingly against. <a href=\"/lw/dc5/thoughts_on_moral_intuitions/6xh0\">Here...</a><br /></div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">\n<div id=\"body_t1_6xh0\" class=\"comment-content \">\n<div class=\"md\">\n<blockquote>\n<p>However, are you sure that you understand just how radical the above statement is? The libertarian theory of contracts -- that you should have full freedom to enter any voluntary contract as far as your own property and rights are concerned -- sounds appealing in the abstract. (Robin Hanson would probably say \"in far mode.\") Yet on closer consideration, it implies all sorts of possible (and plausible) arrangements that would make most people scream with horror.</p>\n<p>In any realistic human society, there are huge limitations on what sorts of contracts you are allowed to enter, much narrower than what any simple quasi-libertarian theory would imply. Except for a handful of real honest libertarians, who are inevitably marginal and without influence, whenever you see someone make a libertarian argument that some arrangement should be permitted, it is nearly always part of an underhanded rhetorical ploy in which the underlying libertarian principle is switched on and off depending on whether its application is some particular case produces a conclusion favorable to the speaker's ideology.</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"entry-content\">...and <a href=\"/lw/dc5/thoughts_on_moral_intuitions/6xkc\">here</a>.</div>\n<div class=\"entry-content\"><br /></div>\n<div class=\"entry-content\">\n<div id=\"body_t1_6xkc\" class=\"comment-content \">\n<div class=\"md\">\n<blockquote>\n<p><em>I think this would be a genuine cause for concern, not because I don't think that people should be able to enter whatever relationships please them in principle, but because in practice I'm concerned about people being coerced into signing contracts harmful to themselves. Not sure where I'd draw the line exactly; this is probably a Hard Problem.</em></p>\n<p>The speaker has an ideological vision of what the society should look like, and in particular, what the government-dictated universal terms of marriage should be (both with regards to the institution of marriage itself and its tremendous implications on all the other social institutions). He uses the libertarian argument because its implications happen to coincide with his ideological position in this particular situation, but he would never accept a libertarian argument in any other situation in which it would imply something disfavored by his ideology.</p>\n<p>Well, there you go. Any restriction on freedom of contract can be rationalized as preventing something \"harmful,\" one way or another.</p>\n<p>And it's not a hard problem at all. <strong>It is in fact very simple: when people like something for ideological reasons, they will use the libertarian argument to support its legality, and when they dislike something ideologically, they will invent rationalizations for why the libertarian argument doesn't apply in this particular case.</strong> The only exceptions are actual libertarians, for whom the libertarian argument itself carries ideological weight, but they are an insignificant fringe minority. For everyone else, the libertarian argument is just a useful rhetorical tool to be employed and recognized only when it produces favorable conclusions.</p>\n<p><strong>In particular, when it comes to marriage, outside of the aforementioned libertarian fringe, there is a total and unanimous agreement that marriage is not a contract whose terms can be set freely, but rather an institution that is entered voluntarily, but whose terms are dictated (and can be changed at any subsequent time) by the state.</strong> (Even the prenuptial agreements allow only very limited and uncertain flexibility.) Therefore, when I hear a libertarian argument applied to marriage, I conclude that there are only two possibilities:</p>\n<ol>\n<li>\n<p>The speaker is an honest libertarian. However, this means either that he doesn't realize how wildly radical the implications of the libertarian position are, or that he actually supports these wild radical implications. (Suppose for example that a couple voluntarily sign a marriage contract stipulating death penalty, or even just flogging, for adultery. How can one oppose the enforcement of this contract without renouncing the libertarian principle?)</p>\n</li>\n<li>\n<p>The  speaker has an ideological vision of what the society should look like,  and in particular, what the government-dictated universal terms of  marriage should be (both with regards to the institution of marriage  itself and its tremendous implications on all the other social  institutions). He uses the libertarian argument because its implications  happen to coincide with his ideological position in this particular  situation, but he would never accept a libertarian argument in any other  situation in which it would imply something disfavored by his ideology.</p>\n</li>\n</ol>\n<div class=\"comment-links\"></div>\n<ol> </ol></blockquote>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "btEKeBWZgKwDSmr8d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 11, "extendedScore": null, "score": 1.0828346325508127e-06, "legacy": true, "legacyId": "21125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jisCHmxwmKoNwrRst"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-13T09:26:15.670Z", "modifiedAt": null, "url": null, "title": "Logical uncertainty, kind of. A proposal, at least.", "slug": "logical-uncertainty-kind-of-a-proposal-at-least", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:35.907Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K2YZPnASN88HTWhAN/logical-uncertainty-kind-of-a-proposal-at-least", "pageUrlRelative": "/posts/K2YZPnASN88HTWhAN/logical-uncertainty-kind-of-a-proposal-at-least", "linkUrl": "https://www.lesswrong.com/posts/K2YZPnASN88HTWhAN/logical-uncertainty-kind-of-a-proposal-at-least", "postedAtFormatted": "Sunday, January 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20uncertainty%2C%20kind%20of.%20A%20proposal%2C%20at%20least.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20uncertainty%2C%20kind%20of.%20A%20proposal%2C%20at%20least.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2YZPnASN88HTWhAN%2Flogical-uncertainty-kind-of-a-proposal-at-least%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20uncertainty%2C%20kind%20of.%20A%20proposal%2C%20at%20least.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2YZPnASN88HTWhAN%2Flogical-uncertainty-kind-of-a-proposal-at-least", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2YZPnASN88HTWhAN%2Flogical-uncertainty-kind-of-a-proposal-at-least", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2663, "htmlBody": "<p>If you want context and are fast at seeing the implications of math, see&nbsp;<a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">Benja's post</a>. This post is much lighter on the math, though it may take more background reading and more laborious interpolation, since it's, well, lighter on the math.</p>\n<p>&nbsp;</p>\n<p>Imagine I introduced my pet robot to a game. The robot has 10 seconds to pick a digit, and if the trillionth prime number ends with that digit, the robot gets a cookie (it likes peanut butter cookies the best).&nbsp;10 seconds is not enough time for my robot to calculate the answer deductively. And yet, guessing an answer is superior to running out of time quietly. What sort of general logic should my robot follow in under 10 seconds to figure out that it should be indifferent between answering 1, 3, 7 or 9? Does it even make sense to be indifferent between the real answer and an impossible answer, even if you don't know which is which?</p>\n<p>&nbsp;</p>\n<p>As you might expect from context, the proposed solution will involve assigning every true or false math statement a probabability-esque degree of plausibility, with numbers other than 0 or 1 indicating logical uncertainty. Why is this a good idea?</p>\n<p>&nbsp;</p>\n<p>To explain logical uncertainty, let's first take a step back and reframe logical <em>certainty</em> in terms of rules for reasoning that apply to both deductive logic and&nbsp;probabilistic logic. An important resource here is E.T. Jaynes' <a href=\"http://bayes.wustl.edu/etj/prob/book.pdf\">Probability Theory</a>&nbsp;(pdf)&nbsp;- the most relevant part being page 31 of the book. The key idea is that each of the probability axioms applies just fine no matter what kind of&nbsp;Boolean&nbsp;statement you want to find the probability of. Which is to say probability already applies to arithmetic - the laws of probability are also laws of arithmetic, just in the limit that probabilities go to 1 or 0. Our robot starts with a collection of definitions labeled with probability 1 (like \"0 is a number\" or \"S(0)+0=S(0)\" [if this S(0) stuff needs context, see <a href=\"http://en.wikipedia.org/wiki/Peano_axioms#Addition\">wikipedia</a>]), and then applies deductive rules according to the universal rules of probability. We translate \"A implies B\" into the language of probabilities as P(AB|C) = P(A|C), and then apply the always-true product rule P(B|AC)=P(AB|C) / P(A|C). If P(A|C)=1, that is, A|C is deductively true, and A implies B, then P(B|AC)=P(B|C)=1. The machinery that underlies deduction is in fact the same machinery that underlies probabilistic reasoning. And we're just going to exploit that a little.</p>\n<p>An alternate axiomatization due to Savage (hat tip to articles by <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/\">Sniffoy</a> and <a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox/\">fool</a>) is based just on actions - it doesn't seem necessary for every agent to store numerical plausibilities, but every agent has to act, and if our agent is to act as if it had consistent preferences when presented with bets, it must act <em>as if</em>&nbsp;it calculated probabilities. Just like the conditions of Cox's theorem as used by E.T. Jaynes, the conditions of Savage's theorem apply to bets on arithmetic just fine. So our robot always behaves as if it assigns some probabilities over the last digit of the trillionth prime number - it's just that when our robot's allowed to run long enough, all but one of those probabilities is 0.</p>\n<p>&nbsp;</p>\n<p>So how do we take the basic laws of belief-manipulation, like the product rule or the sum rule, and apply them to cases where we run out of time and can't deduce all the things? If we still want to take actions, we still want to assign probabilities, but we can't use deduction more than a set number of times...</p>\n<p>Okay fine I'll just say it. The proposal outlined here is to treat a computationally limited agent's \"correct beliefs\" as the correct beliefs of a computationally <em>unlimited</em>&nbsp;agent with a limited definition of what deduction can do. So this weakened-deduction agent has a limitation, in that starting from axioms it can only prove some small pool of theorems, but it's unlimited in that it can take the pool of proven theorems, and then assign probabilities to <em>all</em> the unproven true or false statements. After we flesh out this agent, we can find a computationally limited algorithm that finds correct (i.e. equal to the ones from a sentence ago) probabilities for <em>specific</em>&nbsp;statements, rather than all of them. And finally, we have to take this and make a decision procedure - our robot. After all, it's no good for our robot to assign probabilities if it proceeds to get stuck because it tries to compare the utilities of the world if the end of the trillionth prime number were 1 versus 7 and doesn't even know what it <em>means</em>&nbsp;to calculate the utility of the impossible. We have to make a bit of a modification to the whole decision procedure, we can't just throw in probabilities and expect utility to keep up.</p>\n<p>&nbsp;</p>\n<p>So, formally, what's going on when we limit deduction? Well, remember the process of deduction outlined earlier?</p>\n<blockquote>\n<p>We translate \"A implies B\" into the language of probabilities as P(AB|C) = P(A|C), and then apply the always-true product rule P(B|AC)=P(AB|C) / P(A|C). If P(A|C)=1, that is, A|C is deductively true, and A implies B, then P(B|AC)=P(B|C)=1</p>\n</blockquote>\n<p>There is a chain here, and if we want to limit deduction to some small pool of provable theorems, we need one of the links to be broken outside that pool. As implied, I don't want to mess with the product rule, or else we violate a desideratum of belief. Instead, we'll mess with implication itself - we&nbsp;translate&nbsp;\"A implies B\" into \"P(AB|C)=P(A|C) only if we've spent less than 10 seconds doing deduction.\" Or \"P(AB|C)=P(A|C) only if it's been less than 10<sup>6</sup>&nbsp;steps from the basic axioms.\" These limitations are ugly and nonlocal because they represent the intrusion of nonlocal limitations on our agent into a system that previously ran forever.</p>\n<p>Note that the weakening of implication does not necessarily determine the shape of our pool of deduced theorems. A weakened-deduction agent could spiral outward from shortest to longest theorems, or it could search more cleverly to advance on some specific&nbsp;theorems&nbsp;before time runs out.</p>\n<p>If a weakened-deduction agent just had the product rule and this new way of translating the axioms into probabilities, it would accumulate some pool of known probabilities - it could work out from the probability-1 axioms to show that some short statements had probability 1 and some other short statements had probability 0. It could also prove some more abstract things like P(AB)=0 without proving anything else about A or B, as long as it followed the right search pattern. But it can't assign probabilities outside of deduction - it doesn't have the rules. So it just ends up with a pool of deduced stuff in the middle of a blank plain of \"undefined.\"</p>\n<p>&nbsp;</p>\n<p>Okay, back to referring to E.T. Jaynes (specifically, the bottom of page 32). When deriving the laws of probability from Cox's desiderata, the axioms fall into different groups - there are the \"laws of thought\" parts, and the \"interface\" parts. The laws of thought are things like Bayes' theorem, or the product rule. They tell you how probabilities have to fit with <em>other</em>&nbsp;probabilities. But they don't give you probabilities <em>ex nihilo</em>, you have to start with probability-1 axioms or known probabilities and build out from them. The parts that tell you how to get <em>new</em>&nbsp;probabilities are the interface parts, ideas like \"if you have equivalent information about two things, they should have the same probability.\"</p>\n<p>So what does our limited-deduction agent do once it reaches its limits of deduction? Well, to put it simply, it uses deduction as much as it can, and then it uses the principle of maximum entropy for the probability of everything else. Maximum entropy corresponds to minimum information, so it satisfies a desideratum like \"don't make stuff up.\"</p>\n<p>&nbsp;</p>\n<p>The agent is assigning probabilities to true or false logical statements, statements like S(0)+S(0)=S(S(0)). If it had an unrestricted translation of \"A implies B,\" it could prove this statement quickly. But suppose it can't. Then this statement is really just a string of symbols. The agent no longer \"understands\" the symbols, which is to say it can only use facts about the probability of these symbols that were previously proved and are within the pool of theorems - it's only a <em>part </em>of an algorithm, and doesn't have the resources&nbsp;to prove everything, so we have to design the agent to assign probabilities based just on what it proved deductively.</p>\n<p>So the design of our unlimited-computation, limited-deduction agent is that it does all the deduction it can according to some search algorithm and within some limit, and this can be specified to take any amount of time. Then, to fill up the infinity of un-deduced probabilities, the agent just assigns the maximum-entropy probability distribution consistent with what's proven. For clever search strategies that figure out things like P(AB)=0 without figuring out P(A), doing this assignment requires interpretation of AND, OR, and NOT operations - that is, we still need a Boolean algebra for statements. But our robot no longer proves new statements about probabilities of these symbol strings, in the sense that P(S(0)+0=S(0))=P(S(0)+S(0)=S(S(0))) is a new statement. An example of a non-new statement would be P(S(0)+0=S(0)) AND S(0)+S(0)=S(S(0))) =&nbsp;P(S(0)+0=S(0)) *&nbsp;P(S(0)+S(0)=S(S(0)) |&nbsp;S(0)+0=S(0)) - that's just the product rule, it hasn't actually changed any of the <em>equations</em>.</p>\n<p>&nbsp;</p>\n<p>End of part 1 exercise: Can deducing an additional theorem lead to our agent assigning less probability to the right answer under certain situations? (Reading part 2 may help)</p>\n<p>&nbsp;</p>\n<p>Okay, now on to doing this with&nbsp;actual&nbsp;bounded resources. And back to the trillionth prime number! You almost forgot about that, didn't you. The plan is to break up the strict deduction -&gt; max entropy procedure, and do it in such a way that our robot can get better results (higher probability to the correct answer) the longer it runs, up to proving the actual correct answer. It starts with no theorems, and figures out the max entropy probability distribution for the end of the trillionth prime number. Said distribution happens to be one-half to everything, e.g. p(1)=1/2 and p(2)=1/2 and p(3)=1/2. The robot doesn't know yet that the different answers are mutually exclusive and exhaustive, much less what's wrong with the answer of 2. But the important thing is, assigning the same number to everything of interest is <em>fast</em>. Later, as it proves relevant theorems, the robot updates the probability distribution, and when it runs out of resources it stops.</p>\n<p>Side note: there's also another way of imagining how the robot stores probabilities, used in Benja's post, which is to construct a really big mutually exclusive and exhaustive basis (called \"disjunctive normal form\"). Instead of storing P(A) and P(B), which are not necessarily mutually exclusive or exhaustive,&nbsp;we store P(AB), P(A&not;B) (the hook thingy means \"NOT\"), P(&not;AB), and P(&not;A&not;B), which are mutually exclusive and exhaustive. These things would then each have probability 1/4, or 1/2<sup>N</sup>, where N is the number of statements you're assigning probabilities to. This is a pain when N goes to infinity, but can be useful when N is approximately the number of possible last digits of a number.</p>\n<p>Back on track: suppose the first thing the robot proves about the last digit of the trillionth prime number is that answers of 1, 2, 3, 4, 5, 6, 7, 8, 9, and 0 are exhaustive. What does that do to the probabilities? In disjunctive normal form, the change is clear - exhaustiveness means that P(&not;1&not;2&not;3&not;4&not;5&not;6&not;7&not;8&not;9&not;0)=0, there's no leftover space. Previously there were 2<sup>10</sup>=1024 of these disjunctive possibilities, now there are 1023, and the remaining ones stay equivalent in terms of what's been proven about them (nothing), so the probability of each went from 1/1024 to 1/1023. Two things to note: figuring this out took a small amount of work and is totally doable for the robot, but we don't want to do this work every time we use modus tollens, so we need to have some way to tell whether our new theorem matters to the trillionth prime number.</p>\n<p>For example, image we were interested in the statement A. The example is to learn that A, B, and C are mutually exclusive and exhaustive, step by step. First, we could prove that A, B, C are exhaustive - P(&not;A&not;B&not;C)=0. Does this change P(A)? Yes, it changes from 4/8 (N is 3, so 2<sup>3</sup>=8) to 4/7. Then we learn that P(AB)=0, i.e. A and B are mutually exclusive. This leaves us only A&not;BC,&nbsp;&not;ABC,&nbsp;A&not;B&not;C,&nbsp;&not;AB&not;C,&nbsp;and&nbsp;&not;A&not;BC. P(A) is now 2/5. Now we learn that A and C are mutually exclusive, so the possibilities are&nbsp;&not;ABC,&nbsp;A&not;B&not;C,&nbsp;&not;AB&not;C,&nbsp;and&nbsp;&not;A&not;BC. P(A)=1/4. Each of the steps until now have had the statement A right there inside the parentheses - but for the last step, we show that B and C are mutually exclusive, P(BC)=0, and now we just have P(A)=P(B)=P(C)=1/3. We just took a step that didn't mention A, but it changed the probability of A. This is because we'd previously disrupted the balance between ABC and&nbsp;&not;ABC. To tell when to update P(A) we not only need to listen for A to be mentioned, we have to track what A has been entangled with, and what's been entangled with that, and so on in a web of deduced relationships.</p>\n<p>&nbsp;</p>\n<p>The good news is that that's&nbsp;<em>it</em>. The plausibility assigned to any statement A by this finite-computation method is the same plausibility that our computationally-unlimited deductively-limited agent would have assigned to it, given the same pool of deduced theorems. The difference is just that the limited-deduction agent did this for every possible statement, which as mentioned doesn't make as much sense in disjunctive normal form.</p>\n<p>So IF we accept that having limited resources is like having a limited ability to do implication, THEN we know how our robot should assign probabilities to a few statements of interest. It should start with the good old \"everything gets probability 1/2,\" which should allow it to win some cookies even if it only has a few milliseconds, and then it should start proving theorems, updating its probabilities when it proves something that should impact those probabilities.</p>\n<p>&nbsp;</p>\n<p>Now onto the last part. The robot's utility function wasn't really designed for U(last digit of trillionth prime number is 1), so what should it do? Well, what <em>does</em>&nbsp;our robot like? It likes having a cookie over not having a cookie. C is for cookie, and that's good enough for it. So we want to transform a utility over cookies into a an expected utility that will let us order possible actions.</p>\n<p>We have to make the exact same transformation in the case of ordinary probabilities, so let's examine that. If I flip a coin and get a cookie if I call it correctly, I don't have a terminal U(heads) or U(tails), I just have U(cookie). My expected utility of different guesses comes from not knowing which guess leads to the cookie.</p>\n<p>Similarly, the expected utility of different guesses when betting on the trillionth prime number comes from not knowing which guess leads to the cookie. It is possible to care about the properties of math, or to care about whether coins land heads or tails, but that just means we have to drag in causality - your guess doesn't affect how math works, or flip coins over.</p>\n<p>&nbsp;</p>\n<p>So the standard procedure for our robot looks like this:</p>\n<p>Start with some utility function U over the world, specifically cookies.</p>\n<p>Now, face a problem. This problem will have some outcomes (possible numbers of cookies), some options (that is, strategies to follow, like choosing one of 10 possible digits), and any amount of information about how options correspond to outcomes (like \"iff the trillionth prime ends with this digit, you get the cookie\").</p>\n<p>Now our robot calculates the limited-resources probability of getting different outcomes given different strategies, and from that calculates an expected utility for each strategy.</p>\n<p>Our robot then follows one of the strategies with maximum expected utility.</p>\n<p>&nbsp;</p>\n<p>Bonus exercises: Does this procedure already handle probabilistic maps from the options to the outcomes, like in the case of the flipped coin? How about if flipping a coin isn't already converted into a probability, but is left as an underdetermined problem a la \"a coin (heads XOR tails) is flipped, choose one.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K2YZPnASN88HTWhAN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 16, "extendedScore": null, "score": 1.0835076623221564e-06, "legacy": true, "legacyId": "18650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PgKADaJE4ERjtMtP9", "5J34FAKyEmqKaT7jt", "thHZiZBDRPtGxCM6f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-13T09:50:06.344Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Nonperson Predicates", "slug": "seq-rerun-nonperson-predicates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3qzYbBzXyuW47WQkD/seq-rerun-nonperson-predicates", "pageUrlRelative": "/posts/3qzYbBzXyuW47WQkD/seq-rerun-nonperson-predicates", "linkUrl": "https://www.lesswrong.com/posts/3qzYbBzXyuW47WQkD/seq-rerun-nonperson-predicates", "postedAtFormatted": "Sunday, January 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Nonperson%20Predicates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Nonperson%20Predicates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qzYbBzXyuW47WQkD%2Fseq-rerun-nonperson-predicates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Nonperson%20Predicates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qzYbBzXyuW47WQkD%2Fseq-rerun-nonperson-predicates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qzYbBzXyuW47WQkD%2Fseq-rerun-nonperson-predicates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>Today's post, <a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a> was originally published on 27 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Nonperson_Predicates\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An AI, trying to develop highly accurate models of the people it interacts with, may develop models which are conscious themselves. For ethical reasons, it would be preferable if the AI wasn't creating and destroying people in the course of interpersonal interactions. Resolving this issue requires making some progress on the hard problem of conscious experience. We need some rule which definitely identifies all conscious minds as conscious. We can make do if it still identifies some nonconscious minds as conscious.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gao/seq_rerun_devils_offers/\">Devil's Offers</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3qzYbBzXyuW47WQkD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0835220976700555e-06, "legacy": true, "legacyId": "21128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R", "c4NsXcyDiekpjH7vb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-13T16:40:31.028Z", "modifiedAt": null, "url": null, "title": "Seeking examples of people smarter than me who got hung up", "slug": "seeking-examples-of-people-smarter-than-me-who-got-hung-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:05.485Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelBishop", "createdAt": "2009-02-27T21:29:30.329Z", "isAdmin": false, "displayName": "MichaelBishop"}, "userId": "HEJJhygH2QnAxDddx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BXqLFptZoKXgAmvZW/seeking-examples-of-people-smarter-than-me-who-got-hung-up", "pageUrlRelative": "/posts/BXqLFptZoKXgAmvZW/seeking-examples-of-people-smarter-than-me-who-got-hung-up", "linkUrl": "https://www.lesswrong.com/posts/BXqLFptZoKXgAmvZW/seeking-examples-of-people-smarter-than-me-who-got-hung-up", "postedAtFormatted": "Sunday, January 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20examples%20of%20people%20smarter%20than%20me%20who%20got%20hung%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20examples%20of%20people%20smarter%20than%20me%20who%20got%20hung%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXqLFptZoKXgAmvZW%2Fseeking-examples-of-people-smarter-than-me-who-got-hung-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20examples%20of%20people%20smarter%20than%20me%20who%20got%20hung%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXqLFptZoKXgAmvZW%2Fseeking-examples-of-people-smarter-than-me-who-got-hung-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXqLFptZoKXgAmvZW%2Fseeking-examples-of-people-smarter-than-me-who-got-hung-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">\n<div><span style=\"font-size: 12.727272033691406px;\">I'm looking for historical examples of scientists who were&nbsp;</span></div>\n<div>a) very intelligent and still</div>\n<div>b) continued to put themselves behind a theory in their discipline long after it was rejected. &nbsp;Maybe they got too attached to it, refused to be wrong, got emotional, but they somehow let their hangups get in the way.</div>\n<div><br /></div>\n<div>Maybe I'm being too demanding, but if you can resist, give me fewer cranks, pseudoscience, and wierd sociopolitical commitments and more theories that were credible until they became incredible to all but their big fancy until-then-respected proponent.</div>\n<div><br /></div>\n<div>To get you started:&nbsp;</div>\n<div>* Fred Hoyle against the Big Bang</div>\n<div>* Lord Kelvin and Hoyle on microbes from spce</div>\n<div>* Tesla against relativity and other chunks of modern physics.</div>\n<div>* Heaviside against relativity</div>\n<div>* George Gaylord Simpson against plate tectonics</div>\n<div>* Newton on alchemy</div>\n</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">More?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">Posted on behalf of a friend. Thanks.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BXqLFptZoKXgAmvZW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 1.083770611768879e-06, "legacy": true, "legacyId": "21129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T02:06:53.432Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D7XrPQLYvSk7tArYq/meetup-washington-dc-fun-and-games-meetup-2", "pageUrlRelative": "/posts/D7XrPQLYvSk7tArYq/meetup-washington-dc-fun-and-games-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/D7XrPQLYvSk7tArYq/meetup-washington-dc-fun-and-games-meetup-2", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7XrPQLYvSk7tArYq%2Fmeetup-washington-dc-fun-and-games-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7XrPQLYvSk7tArYq%2Fmeetup-washington-dc-fun-and-games-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7XrPQLYvSk7tArYq%2Fmeetup-washington-dc-fun-and-games-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hu'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 January 2013 05:23:13PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be playing games! I'll bring Zendo: if you have a fun game, please bring it!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hu'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D7XrPQLYvSk7tArYq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0841137302136316e-06, "legacy": true, "legacyId": "21137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/hu\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 January 2013 05:23:13PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be playing games! I'll bring Zendo: if you have a fun game, please bring it!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/hu\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T06:57:19.767Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Nonsentient Optimizers", "slug": "seq-rerun-nonsentient-optimizers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.662Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZqgFqDfiiFJRWt2or/seq-rerun-nonsentient-optimizers", "pageUrlRelative": "/posts/ZqgFqDfiiFJRWt2or/seq-rerun-nonsentient-optimizers", "linkUrl": "https://www.lesswrong.com/posts/ZqgFqDfiiFJRWt2or/seq-rerun-nonsentient-optimizers", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Nonsentient%20Optimizers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Nonsentient%20Optimizers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqgFqDfiiFJRWt2or%2Fseq-rerun-nonsentient-optimizers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Nonsentient%20Optimizers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqgFqDfiiFJRWt2or%2Fseq-rerun-nonsentient-optimizers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqgFqDfiiFJRWt2or%2Fseq-rerun-nonsentient-optimizers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>Today's post, <a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a> was originally published on 27 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Discusses some of the problems of, and justification for, creating AIs that are knowably <em>not </em>conscious / sentient / people / citizens / subjective experiencers. We don't want the AI's <em>models of</em> people to <em>be </em>people - we don't want conscious minds trapped helplessly inside it. So we need how to tell that something is definitely not a person, and in this case, maybe we would like the AI itself to not be a person, which would simplify a lot of ethical issues if we could pull it off. Creating a new intelligent species is not lightly to be undertaken from a purely <em>ethical </em>perspective; if you create a new kind of <em>person</em>, you have to make sure it leads a life worth living.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gaw/seq_rerun_nonperson_predicates/\">Nonperson Predicates</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZqgFqDfiiFJRWt2or", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0842897575185071e-06, "legacy": true, "legacyId": "21142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HsRFQTAySAx8xbXEc", "3qzYbBzXyuW47WQkD", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T11:36:48.262Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA third-Sunday meetup", "slug": "meetup-cambridge-ma-third-sunday-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zDcNaHGbymw9TKQBW/meetup-cambridge-ma-third-sunday-meetup-0", "pageUrlRelative": "/posts/zDcNaHGbymw9TKQBW/meetup-cambridge-ma-third-sunday-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/zDcNaHGbymw9TKQBW/meetup-cambridge-ma-third-sunday-meetup-0", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDcNaHGbymw9TKQBW%2Fmeetup-cambridge-ma-third-sunday-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDcNaHGbymw9TKQBW%2Fmeetup-cambridge-ma-third-sunday-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDcNaHGbymw9TKQBW%2Fmeetup-cambridge-ma-third-sunday-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hv'>Cambridge, MA third-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 January 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hv'>Cambridge, MA third-Sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zDcNaHGbymw9TKQBW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0844591875378621e-06, "legacy": true, "legacyId": "21146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/hv\">Cambridge, MA third-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 January 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/hv\">Cambridge, MA third-Sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA third-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA third-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T14:19:05.144Z", "modifiedAt": null, "url": null, "title": "Evolution, Sex, and Gender, Not to Mention Research", "slug": "evolution-sex-and-gender-not-to-mention-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FMMtzKdbjMQmJMgP9/evolution-sex-and-gender-not-to-mention-research", "pageUrlRelative": "/posts/FMMtzKdbjMQmJMgP9/evolution-sex-and-gender-not-to-mention-research", "linkUrl": "https://www.lesswrong.com/posts/FMMtzKdbjMQmJMgP9/evolution-sex-and-gender-not-to-mention-research", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolution%2C%20Sex%2C%20and%20Gender%2C%20Not%20to%20Mention%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolution%2C%20Sex%2C%20and%20Gender%2C%20Not%20to%20Mention%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMMtzKdbjMQmJMgP9%2Fevolution-sex-and-gender-not-to-mention-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolution%2C%20Sex%2C%20and%20Gender%2C%20Not%20to%20Mention%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMMtzKdbjMQmJMgP9%2Fevolution-sex-and-gender-not-to-mention-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMMtzKdbjMQmJMgP9%2Fevolution-sex-and-gender-not-to-mention-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>From <a href=\"http://www.nytimes.com/2013/01/13/opinion/sunday/darwin-was-wrong-about-dating.html?pagewanted=2&amp;_r=0&amp;ref=general&amp;src=me\">The New York Times</a>:</p>\n<blockquote>\n<p><span style=\"font-family: georgia, 'times new roman', times, serif; font-size: 15px; line-height: 22px;\">Take the question of promiscuity. Everyone has always assumed &mdash; and early research had shown &mdash; that women desired fewer sexual partners over a lifetime than men. But in 2003, two behavioral psychologists, Michele G. Alexander and Terri D. Fisher, published the results of a study that used a &ldquo;bogus pipeline&rdquo; &mdash; a fake lie detector. When asked about actual sexual partners, rather than just theoretical desires, the participants who were not attached to the fake lie detector displayed typical gender differences. Men reported having had more sexual partners than women. But when participants believed that lies about their sexual history would be revealed by the fake lie detector, gender differences in reported sexual partners vanished. In fact, women reported slightly more sexual partners (a mean of 4.4) than did men (a mean of 4.0).</span></p>\n</blockquote>\n<p>So how sketchy is the research on human sexual behavior, anyway?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FMMtzKdbjMQmJMgP9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 34, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "21147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T14:28:20.274Z", "modifiedAt": null, "url": null, "title": "'Life exists beyond 50'", "slug": "life-exists-beyond-50", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fJwGjYFB6rTzENMJ2/life-exists-beyond-50", "pageUrlRelative": "/posts/fJwGjYFB6rTzENMJ2/life-exists-beyond-50", "linkUrl": "https://www.lesswrong.com/posts/fJwGjYFB6rTzENMJ2/life-exists-beyond-50", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Life%20exists%20beyond%2050'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Life%20exists%20beyond%2050'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJwGjYFB6rTzENMJ2%2Flife-exists-beyond-50%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Life%20exists%20beyond%2050'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJwGjYFB6rTzENMJ2%2Flife-exists-beyond-50", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJwGjYFB6rTzENMJ2%2Flife-exists-beyond-50", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p>&nbsp;</p>\n<h1 class=\"gl_headline\" style=\"margin: 0px 0px 0px 83px; padding: 30px 20px 20px; font-size: 40px; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: #cccccc; border-right-color: #cccccc; border-left-color: #cccccc; outline: 0px; vertical-align: baseline; font-family: arial, sans-serif; letter-spacing: -0.05em; line-height: 1em; width: 547px; color: #af370a !important;\">81-year-old Fashion Week model: 'Life exists beyond 50'</h1>\n<p>&nbsp;</p>\n<p>http://thelook.today.com/_news/2012/09/10/13778903-81-year-old-fashion-week-model-life-exists-beyond-50?lite</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fJwGjYFB6rTzENMJ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -19, "extendedScore": null, "score": 1.0845632019346386e-06, "legacy": true, "legacyId": "21148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-14T21:32:51.462Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Dublin", "slug": "meetup-less-wrong-dublin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DWY99xb8NKvD5skaG/meetup-less-wrong-dublin", "pageUrlRelative": "/posts/DWY99xb8NKvD5skaG/meetup-less-wrong-dublin", "linkUrl": "https://www.lesswrong.com/posts/DWY99xb8NKvD5skaG/meetup-less-wrong-dublin", "postedAtFormatted": "Monday, January 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Dublin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Dublin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWY99xb8NKvD5skaG%2Fmeetup-less-wrong-dublin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Dublin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWY99xb8NKvD5skaG%2Fmeetup-less-wrong-dublin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWY99xb8NKvD5skaG%2Fmeetup-less-wrong-dublin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hw'>Less Wrong Dublin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2013 04:30:59PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">28 Dame St Dublin, Co. Dublin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Martin O'Dea would like to talk about his ideas on the Open Ireland Party. Other topics and all welcome.\nProvisionally I would suggest The Mercantile on Dame Street.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hw'>Less Wrong Dublin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DWY99xb8NKvD5skaG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.08482069799022e-06, "legacy": true, "legacyId": "21152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Dublin\">Discussion article for the meetup : <a href=\"/meetups/hw\">Less Wrong Dublin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2013 04:30:59PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">28 Dame St Dublin, Co. Dublin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Martin O'Dea would like to talk about his ideas on the Open Ireland Party. Other topics and all welcome.\nProvisionally I would suggest The Mercantile on Dame Street.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Dublin1\">Discussion article for the meetup : <a href=\"/meetups/hw\">Less Wrong Dublin</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Dublin", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Dublin", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Dublin", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Dublin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T01:33:14.476Z", "modifiedAt": null, "url": null, "title": "[Link] Aschwin de Wolf on Chemical Brain Preservation", "slug": "link-aschwin-de-wolf-on-chemical-brain-preservation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SDxsB8Xr8NroNwk7r/link-aschwin-de-wolf-on-chemical-brain-preservation", "pageUrlRelative": "/posts/SDxsB8Xr8NroNwk7r/link-aschwin-de-wolf-on-chemical-brain-preservation", "linkUrl": "https://www.lesswrong.com/posts/SDxsB8Xr8NroNwk7r/link-aschwin-de-wolf-on-chemical-brain-preservation", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Aschwin%20de%20Wolf%20on%20Chemical%20Brain%20Preservation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Aschwin%20de%20Wolf%20on%20Chemical%20Brain%20Preservation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDxsB8Xr8NroNwk7r%2Flink-aschwin-de-wolf-on-chemical-brain-preservation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Aschwin%20de%20Wolf%20on%20Chemical%20Brain%20Preservation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDxsB8Xr8NroNwk7r%2Flink-aschwin-de-wolf-on-chemical-brain-preservation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDxsB8Xr8NroNwk7r%2Flink-aschwin-de-wolf-on-chemical-brain-preservation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 704, "htmlBody": "<p>Aschwin de Wolf, a cryonics researcher at Advanced Neural Biosciences, has written two new articles discussing reasons for sticking with cryopreservation as opposed to chemopreservation.</p>\n<p><strong><a href=\"http://www.alcor.org/Library/html/chemopreservation2.html\">Chemical Brain Preservation and Human Suspended Animation</a></strong></p>\n<p>Excerpt:</p>\n<blockquote><span style=\"background-color: rgba(255, 255, 255, 0);\"><strong>Executive Summary<br /></strong><br />Scientific and practical considerations strongly support cryopreservation rather than chemopreservation for the stabilization of critically ill patients. Technology for achieving solid state chemopreservation of brains larger than a mouse brain does not yet exist. Chemical fixation is irreversible without very advanced technologies. Chemical fixation permits no functional feedback or development pathway toward reversible suspended animation. By contrast, cryopreservation seeks to maintain viability of the brain as far downstream as our capabilities and resources permit &mdash; an approach that reflects our view of cryonics as an extension of contemporary medicine. Cryopreservation preserves more options in that a cryopreserved brain could be scanned in future, or later chemically fixed, but the process of chemical fixation cannot be reversed and replaced by just low temperature storage. The cost benefits of chemopreservation over cryopreservation are exaggerated, largely because the standby and treatment procedures for effective chemopreservation would be just as extensive as for cryopreservation, if not more so, even assuming that highly toxic chemicals could be worked with safely in the field. Chemopreservation is being inherently tied to mind uploading, an association that is likely to limit its acceptance as a form of experimental critical care medicine by apparently requiring acceptance of the idea of substrate independent minds.</span></blockquote>\n<p><strong><a href=\"http://www.evidencebasedcryonics.org/2013/01/14/in-praise-of-cold/\">In praise of cold</a></strong></p>\n<p><strong> </strong></p>\n<p>Excerpt:</p>\n<blockquote>\n<p style=\"text-align: justify;\">Some observers believe that cryonics advocates are reluctant to subject their theories to experimental scrutiny because this could damage their (uncritical) belief in future resuscitation. Similarly, one might think that cryonicists would react with a mix of hostility and dismissal to alternative strategies for personal survival. Nothing could be further from the truth. In fact, it is exactly because our personal survival is at stake that forces us to be wary of dogmatism.</p>\n<p style=\"text-align: justify;\">For this reason, I have always been interested in chemical fixation as a (low cost) alternative for cryonics. In fact, years before all the talk about the &ldquo;connectome&rdquo; and &ldquo;plastination&rdquo; I spent considerable time exchanging messages with Michael Perry at Alcor about the technical and practical feasibility of chemical brain preservation. But no matter how open minded I tried to be about this approach, I kept running into the same challenges over and over again.</p>\n<p style=\"text-align: justify;\">The challenge that has concerned me the most is whether a delayed start of chemical brain fixation will produce incomplete distribution of the chemical fixative in the brain because of ischemia-induced perfusion impairment. Thinking about the technical problem of &ldquo;no-reflow&rdquo; is not the first thing on the mind of someone who first hears about the idea of using chemical fixatives to preserve the brain. In my case, this concern was not just &ldquo;theoretical.&rdquo; In my lab I have spent many years looking at the effects of cerebral ischemia on cryopreservation and chemical fixation. Last year we decided to broaden our investigations to delayed chemical fixation and we have not been pleased at what we have observed so far. After 1.5 years of room temperature storage the delayed aldehyde fixed brains are falling apart and continue to decompose. In small animals one might imagine that such perfusion impairment could be overcome by immersing the brains in the fixative instead but human brains are simply too large. By the time that the fixative would have reached the core of the brain, extensive autolysis will have occurred.</p>\n</blockquote>\n<p style=\"text-align: justify;\">TLDR: Chemopreservation can't be (and generally isn't) dismissed out of hand by cryonicists, but there are definite tradeoffs which would need to be accounted for. The bulk of the costs of cryonics have to do with needing prompt stabilization to have a decent shot at it working, and that doesn't change for chemopreservation patients.</p>\n<p style=\"text-align: justify;\">Chemical preservation carries practical penalties, for example, in terms of the toxicity of chemicals that need to be on-hand at the deanimation site. The complete negation of cellular viability makes some kinds of experiments harder for chemical fixation (functional testing of the tissue for viability) whereas others are easier (embedding in resin for scanning). Empirical science has a place for both, but there are more practical advantages for cryonics in the clinical setting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SDxsB8Xr8NroNwk7r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.08496655262933e-06, "legacy": true, "legacyId": "21130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Aschwin de Wolf, a cryonics researcher at Advanced Neural Biosciences, has written two new articles discussing reasons for sticking with cryopreservation as opposed to chemopreservation.</p>\n<p><strong id=\"Chemical_Brain_Preservation_and_Human_Suspended_Animation\"><a href=\"http://www.alcor.org/Library/html/chemopreservation2.html\">Chemical Brain Preservation and Human Suspended Animation</a></strong></p>\n<p>Excerpt:</p>\n<blockquote><span style=\"background-color: rgba(255, 255, 255, 0);\"><strong>Executive Summary<br></strong><br>Scientific and practical considerations strongly support cryopreservation rather than chemopreservation for the stabilization of critically ill patients. Technology for achieving solid state chemopreservation of brains larger than a mouse brain does not yet exist. Chemical fixation is irreversible without very advanced technologies. Chemical fixation permits no functional feedback or development pathway toward reversible suspended animation. By contrast, cryopreservation seeks to maintain viability of the brain as far downstream as our capabilities and resources permit \u2014 an approach that reflects our view of cryonics as an extension of contemporary medicine. Cryopreservation preserves more options in that a cryopreserved brain could be scanned in future, or later chemically fixed, but the process of chemical fixation cannot be reversed and replaced by just low temperature storage. The cost benefits of chemopreservation over cryopreservation are exaggerated, largely because the standby and treatment procedures for effective chemopreservation would be just as extensive as for cryopreservation, if not more so, even assuming that highly toxic chemicals could be worked with safely in the field. Chemopreservation is being inherently tied to mind uploading, an association that is likely to limit its acceptance as a form of experimental critical care medicine by apparently requiring acceptance of the idea of substrate independent minds.</span></blockquote>\n<p><strong id=\"In_praise_of_cold\"><a href=\"http://www.evidencebasedcryonics.org/2013/01/14/in-praise-of-cold/\">In praise of cold</a></strong></p>\n<p><strong> </strong></p>\n<p>Excerpt:</p>\n<blockquote>\n<p style=\"text-align: justify;\">Some observers believe that cryonics advocates are reluctant to subject their theories to experimental scrutiny because this could damage their (uncritical) belief in future resuscitation. Similarly, one might think that cryonicists would react with a mix of hostility and dismissal to alternative strategies for personal survival. Nothing could be further from the truth. In fact, it is exactly because our personal survival is at stake that forces us to be wary of dogmatism.</p>\n<p style=\"text-align: justify;\">For this reason, I have always been interested in chemical fixation as a (low cost) alternative for cryonics. In fact, years before all the talk about the \u201cconnectome\u201d and \u201cplastination\u201d I spent considerable time exchanging messages with Michael Perry at Alcor about the technical and practical feasibility of chemical brain preservation. But no matter how open minded I tried to be about this approach, I kept running into the same challenges over and over again.</p>\n<p style=\"text-align: justify;\">The challenge that has concerned me the most is whether a delayed start of chemical brain fixation will produce incomplete distribution of the chemical fixative in the brain because of ischemia-induced perfusion impairment. Thinking about the technical problem of \u201cno-reflow\u201d is not the first thing on the mind of someone who first hears about the idea of using chemical fixatives to preserve the brain. In my case, this concern was not just \u201ctheoretical.\u201d In my lab I have spent many years looking at the effects of cerebral ischemia on cryopreservation and chemical fixation. Last year we decided to broaden our investigations to delayed chemical fixation and we have not been pleased at what we have observed so far. After 1.5 years of room temperature storage the delayed aldehyde fixed brains are falling apart and continue to decompose. In small animals one might imagine that such perfusion impairment could be overcome by immersing the brains in the fixative instead but human brains are simply too large. By the time that the fixative would have reached the core of the brain, extensive autolysis will have occurred.</p>\n</blockquote>\n<p style=\"text-align: justify;\">TLDR: Chemopreservation can't be (and generally isn't) dismissed out of hand by cryonicists, but there are definite tradeoffs which would need to be accounted for. The bulk of the costs of cryonics have to do with needing prompt stabilization to have a decent shot at it working, and that doesn't change for chemopreservation patients.</p>\n<p style=\"text-align: justify;\">Chemical preservation carries practical penalties, for example, in terms of the toxicity of chemicals that need to be on-hand at the deanimation site. The complete negation of cellular viability makes some kinds of experiments harder for chemical fixation (functional testing of the tissue for viability) whereas others are easier (embedding in resin for scanning). Empirical science has a place for both, but there are more practical advantages for cryonics in the clinical setting.</p>", "sections": [{"title": "Chemical Brain Preservation and Human Suspended Animation", "anchor": "Chemical_Brain_Preservation_and_Human_Suspended_Animation", "level": 1}, {"title": "In praise of cold", "anchor": "In_praise_of_cold", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T03:01:46.094Z", "modifiedAt": null, "url": null, "title": "Meetup : Second Purdue Meetup", "slug": "meetup-second-purdue-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uD7NzqKaa8CnsjYdM/meetup-second-purdue-meetup", "pageUrlRelative": "/posts/uD7NzqKaa8CnsjYdM/meetup-second-purdue-meetup", "linkUrl": "https://www.lesswrong.com/posts/uD7NzqKaa8CnsjYdM/meetup-second-purdue-meetup", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Second%20Purdue%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Second%20Purdue%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuD7NzqKaa8CnsjYdM%2Fmeetup-second-purdue-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Second%20Purdue%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuD7NzqKaa8CnsjYdM%2Fmeetup-second-purdue-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuD7NzqKaa8CnsjYdM%2Fmeetup-second-purdue-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/hx\">Second Purdue Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">18 January 2013 07:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Purdue- Hicks Library</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The first meetup went really well. We had 5 people show up in total. We're hoping more people will show up for the next one.</p>\n<p>EDIT: We had 8 people show up for the second meetup.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/hx\">Second Purdue Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uD7NzqKaa8CnsjYdM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0850202757885495e-06, "legacy": true, "legacyId": "21153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Second_Purdue_Meetup\">Discussion article for the meetup : <a href=\"/meetups/hx\">Second Purdue Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">18 January 2013 07:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Purdue- Hicks Library</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The first meetup went really well. We had 5 people show up in total. We're hoping more people will show up for the next one.</p>\n<p>EDIT: We had 8 people show up for the second meetup.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Second_Purdue_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/hx\">Second Purdue Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Second Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Second_Purdue_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Second Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Second_Purdue_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T04:17:07.546Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Can't Unbirth a Child", "slug": "seq-rerun-can-t-unbirth-a-child", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dw2myGD9eP9qfbhfz/seq-rerun-can-t-unbirth-a-child", "pageUrlRelative": "/posts/dw2myGD9eP9qfbhfz/seq-rerun-can-t-unbirth-a-child", "linkUrl": "https://www.lesswrong.com/posts/dw2myGD9eP9qfbhfz/seq-rerun-can-t-unbirth-a-child", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Can't%20Unbirth%20a%20Child&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Can't%20Unbirth%20a%20Child%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw2myGD9eP9qfbhfz%2Fseq-rerun-can-t-unbirth-a-child%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Can't%20Unbirth%20a%20Child%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw2myGD9eP9qfbhfz%2Fseq-rerun-can-t-unbirth-a-child", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw2myGD9eP9qfbhfz%2Fseq-rerun-can-t-unbirth-a-child", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>Today's post, <a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a> was originally published on 28 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Can.27t_Unbirth_a_Child\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>As a piece of meta advice for how to act when you have more power than you probably should, avoid doing things that cannot be undone. Creating a new sentient being is one of those things to avoid. If you need to rewrite the source code of a nonsentient optimization process, this is less morally problematic than rewriting the source code of a sentient intelligence who doesn't want to be rewritten. Creating new life forms creates such massive issues that it's really better to just not try, at least until we know a lot more.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gba/seq_rerun_nonsentient_optimizers/\">Nonsentient Optimizers</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dw2myGD9eP9qfbhfz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0850660106457554e-06, "legacy": true, "legacyId": "21157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gb6zWstjmkYHLrbrg", "ZqgFqDfiiFJRWt2or", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T05:59:03.984Z", "modifiedAt": null, "url": null, "title": "Discussion of LW going on in felicifia", "slug": "discussion-of-lw-going-on-in-felicifia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.992Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "somervta", "createdAt": "2012-05-25T05:07:09.738Z", "isAdmin": false, "displayName": "somervta"}, "userId": "7qYa5ZDkFJys4CXB3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XznZoow9ZE95GTbd2/discussion-of-lw-going-on-in-felicifia", "pageUrlRelative": "/posts/XznZoow9ZE95GTbd2/discussion-of-lw-going-on-in-felicifia", "linkUrl": "https://www.lesswrong.com/posts/XznZoow9ZE95GTbd2/discussion-of-lw-going-on-in-felicifia", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discussion%20of%20LW%20going%20on%20in%20felicifia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscussion%20of%20LW%20going%20on%20in%20felicifia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXznZoow9ZE95GTbd2%2Fdiscussion-of-lw-going-on-in-felicifia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discussion%20of%20LW%20going%20on%20in%20felicifia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXznZoow9ZE95GTbd2%2Fdiscussion-of-lw-going-on-in-felicifia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXznZoow9ZE95GTbd2%2Fdiscussion-of-lw-going-on-in-felicifia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>So I recently found myself in a bit of an awkward position.</p>\n<p>&nbsp;I frequent several Facebook discussion groups, several of which are about LW-related issues. In one of these groups, a discussion about identity/cryonics led to one about the end Massimo Piglucci's recent post (the bit where he says that the idea of uploading is a form of dualism), which turned into a discussion of LW views in general, at which point I revealed that I was, in fact, a LWer.&nbsp;</p>\n<p>This put me in the awkward position of defending/explicating the views of the entirety of LW. Now, I've only been reading LessWrong for &lt;10 months, and I've only recently become a more active part of the community. I've been a transhumanist for less time than that, seriously thinking about cryonics and identity for even less, and I suddenly found myself speaking for the intersection of all those groups</p>\n<p>The discussion was crossposted to Felicifia a few days ago, and I realized that I was possibly out of my depth. I'm hoping I haven't grossly misrepresented anyone, but rereading the comments, I'm no longer sure.</p>\n<p>Felicifia:</p>\n<p>http://www.felicifia.org/viewtopic.php?f=23&amp;t=801</p>\n<p>Original FB Group :</p>\n<p>https://www.facebook.com/groups/utilitarianism/permalink/318563281580856/</p>\n<p>&nbsp;</p>\n<p>EDIT: If you're commenting on the topic, please state whether or not you'd mind me quoting you at felicifia (If you have a felicifia account, and you'd prefer to post it there yourself, be my guest.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XznZoow9ZE95GTbd2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.0851278843834833e-06, "legacy": true, "legacyId": "21161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T14:29:26.465Z", "modifiedAt": null, "url": null, "title": "Assessing Kurzweil: the gory details", "slug": "assessing-kurzweil-the-gory-details", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/edd9mAcMByL2BFNJv/assessing-kurzweil-the-gory-details", "pageUrlRelative": "/posts/edd9mAcMByL2BFNJv/assessing-kurzweil-the-gory-details", "linkUrl": "https://www.lesswrong.com/posts/edd9mAcMByL2BFNJv/assessing-kurzweil-the-gory-details", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Assessing%20Kurzweil%3A%20the%20gory%20details&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAssessing%20Kurzweil%3A%20the%20gory%20details%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fedd9mAcMByL2BFNJv%2Fassessing-kurzweil-the-gory-details%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Assessing%20Kurzweil%3A%20the%20gory%20details%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fedd9mAcMByL2BFNJv%2Fassessing-kurzweil-the-gory-details", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fedd9mAcMByL2BFNJv%2Fassessing-kurzweil-the-gory-details", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 884, "htmlBody": "<p>This post goes along with <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">this one</a>, which was merely summarising the results of the volunteer assessment. Here we present the further details of the methodology and results.</p>\n<p>Kurzweil's predictions were decomposed into 172 separate statements, taken from the book \"<a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">The Age of Spiritual Machines</a>\" (published in 1999). Volunteers were requested on <a href=\"/lw/f7a/checking_kurzweils_track_record/\">Less Wrong</a>&nbsp;and on <a href=\"http://www.reddit.com/r/Futurology/comments/13vidf/checking_kurzweils_track_record_help_wanted/\">reddit.com/r/futurology</a>. 18 people initially volunteered&nbsp;to do varying amounts of assessment of Kurzweil's predictions; 9 ultimately did so.</p>\n<p>Each volunteer was given a separate randomised list of the numbers 1 to 172, with instructions to go through the statements in the order given by the list and give their assessment of the correctness of the prediction (the exact instructions are at the end of this post). They were to assess the&nbsp;predictions&nbsp;on the following five point scale:</p>\n<ul>\n<li>1=True, 2=Weakly True, 3=Cannot decide, 4=Weakly False, 5=False</li>\n</ul>\n<p>They assessed a varying amount of predictions, giving 531 assessments in total, for an average of 59 assessments per volunteer (the maximum attempted was all 172 predictions, the minimum was 10). They generally followed the randomised order correctly - there were three out of order assessments (assessing prediction 36 instead of 38, 162 instead of a 172, and missing out 75). Since the number of errors was very low, and seemed accidental, I decided that this would not affect the randomisation and kept those answers in.</p>\n<p>The assessments&nbsp;(anonymised)&nbsp;can be found <a href=\"https://www.dropbox.com/s/m3p833znkzx6z93/Kurzweil_assessments.xlsx?raw=1\">here</a>.<a id=\"more\"></a></p>\n<p>In parallel, volunteers on <a href=\"http://www.youtopia.com\">Youtopia</a> were also given the task of assessing the predictions. They were given the same instructions (minus the 5th and 7th clause), except that they were free to work on whichever predictions they wanted to, with the proviso that they didn't overwrite someone else's assessments. Instead, they could post a second opinion (not necessarily different from the first) in a separate column.</p>\n<p>For some reason, prediction number 20 (\"LUIs are frequently combined with animated personalities\") was left out of the Youtopia assessment. In total, 204 assessments were made (171 primary assessments, 33 second opinions).</p>\n<div>The results of that&nbsp;exercise, with identifying information removed, can be found <a href=\"https://www.dropbox.com/s/ai948cutus78qdh/Kurzweil_assessments_youtopia.xlsx?raw=1\">here</a>.</div>\n<p>&nbsp;</p>\n<h2>Instructions</h2>\n<p>The instructions given to the assessors were as follows:</p>\n<p><strong>1) The timeline of Kurzweil's prediction is up to 2011, and the location (unless specified otherwise) is the United States.</strong></p>\n<p>I've given Kurzweil a two-year grace period (he said they would all be true by 2009). This is because I think he forced a lot of predictions into the \"true by ten years from now (1999)\" format. Also, this makes it a bit easier for you, as you don't need to go as far back into history.</p>\n<p><strong>2) A prediction is something that allows you to make a profit.</strong></p>\n<p>This is the true test of a prediction: if you're in 1999, and you believe one of Kurzweil's predictions, could you make your life better than someone who didn't believe the prediction? If Kurzweil made a brilliant, correct prediction but nobody at the time would have realised what it meant, then it doesn't count as a correct prediction. A prediction needs to make sense ahead of time, in a way you can take advantage of.</p>\n<p><strong>3) Resolving unclear terms is maybe the most important part of your job.</strong></p>\n<p>Some of Kurzweil's predictions are ambiguous, including terms like \"many\", \"most\", \"routinely\". Figure out what these terms mean for you. Predictions are acts of communication; they are only valid if the reader understands them correctly. The truth of \"people will routinely use mobile phone\" depends entirely on what meaning you give to \"routinely\". In 1999, how you would have imagined a future in which people \"routinely\" use mobile phones?</p>\n<p><strong>4) No gain from ambiguity, no \"benefit of the doubt\".</strong></p>\n<p>Do not interpret an ambiguous statement as true, simply to give the predictor the benefit of the doubt. With hindsight, some things will seem a lot more inevitable than they actually were, and some predictions will seem as if they \"must refer to X\". For instance, \"Two mighty towers will fall\" seems like it refers to the Sep 11 terrorist attacks - but there are many ways of interpreting that figuratively or literally (two institutions will be undermined, Tolkein's \"the two towers\" will be made into a film, etc...). Again the question is whether people in 1999 could have foreseen something like that outcome, based on that prediction.</p>\n<p><strong>5) Answer the prediction you're working on, not the nearby ones.</strong></p>\n<p>The predictions just before and after are useful to give some context to the prediction you're currently working on, to explain some terms and clarify what Kurzweil is talking about. But you should only answer the exact prediction you're working on (don't worry, those other prediction will have someone else working on them!). Thus the second prediction in \"There will be a terrorist attack on the 9th of September, 2011. 4006 people will be killed in it\" is false.</p>\n<p><strong>6) No penalty for triviality.</strong></p>\n<p>In hindsight, many prediction may seem trivial or obvious. This doesn't mean they were trivial at the time. But in any case, your job is not to estimate how useful or hard the predictions were, but how accurate. \"Computers will get faster\" is a true prediction.</p>\n<p><strong>7) Follow the order in the text file included.</strong></p>\n<p>You are welcome and encouraged to answer more predictions than you promised to - but stick to the predictions given in the randomized text file! This will make the experiment statistically significant... Unless of course you intend to answer all the predictions!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "edd9mAcMByL2BFNJv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 1.0854377548109298e-06, "legacy": true, "legacyId": "21149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post goes along with <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">this one</a>, which was merely summarising the results of the volunteer assessment. Here we present the further details of the methodology and results.</p>\n<p>Kurzweil's predictions were decomposed into 172 separate statements, taken from the book \"<a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">The Age of Spiritual Machines</a>\" (published in 1999). Volunteers were requested on <a href=\"/lw/f7a/checking_kurzweils_track_record/\">Less Wrong</a>&nbsp;and on <a href=\"http://www.reddit.com/r/Futurology/comments/13vidf/checking_kurzweils_track_record_help_wanted/\">reddit.com/r/futurology</a>. 18 people initially volunteered&nbsp;to do varying amounts of assessment of Kurzweil's predictions; 9 ultimately did so.</p>\n<p>Each volunteer was given a separate randomised list of the numbers 1 to 172, with instructions to go through the statements in the order given by the list and give their assessment of the correctness of the prediction (the exact instructions are at the end of this post). They were to assess the&nbsp;predictions&nbsp;on the following five point scale:</p>\n<ul>\n<li>1=True, 2=Weakly True, 3=Cannot decide, 4=Weakly False, 5=False</li>\n</ul>\n<p>They assessed a varying amount of predictions, giving 531 assessments in total, for an average of 59 assessments per volunteer (the maximum attempted was all 172 predictions, the minimum was 10). They generally followed the randomised order correctly - there were three out of order assessments (assessing prediction 36 instead of 38, 162 instead of a 172, and missing out 75). Since the number of errors was very low, and seemed accidental, I decided that this would not affect the randomisation and kept those answers in.</p>\n<p>The assessments&nbsp;(anonymised)&nbsp;can be found <a href=\"https://www.dropbox.com/s/m3p833znkzx6z93/Kurzweil_assessments.xlsx?raw=1\">here</a>.<a id=\"more\"></a></p>\n<p>In parallel, volunteers on <a href=\"http://www.youtopia.com\">Youtopia</a> were also given the task of assessing the predictions. They were given the same instructions (minus the 5th and 7th clause), except that they were free to work on whichever predictions they wanted to, with the proviso that they didn't overwrite someone else's assessments. Instead, they could post a second opinion (not necessarily different from the first) in a separate column.</p>\n<p>For some reason, prediction number 20 (\"LUIs are frequently combined with animated personalities\") was left out of the Youtopia assessment. In total, 204 assessments were made (171 primary assessments, 33 second opinions).</p>\n<div>The results of that&nbsp;exercise, with identifying information removed, can be found <a href=\"https://www.dropbox.com/s/ai948cutus78qdh/Kurzweil_assessments_youtopia.xlsx?raw=1\">here</a>.</div>\n<p>&nbsp;</p>\n<h2 id=\"Instructions\">Instructions</h2>\n<p>The instructions given to the assessors were as follows:</p>\n<p><strong id=\"1__The_timeline_of_Kurzweil_s_prediction_is_up_to_2011__and_the_location__unless_specified_otherwise__is_the_United_States_\">1) The timeline of Kurzweil's prediction is up to 2011, and the location (unless specified otherwise) is the United States.</strong></p>\n<p>I've given Kurzweil a two-year grace period (he said they would all be true by 2009). This is because I think he forced a lot of predictions into the \"true by ten years from now (1999)\" format. Also, this makes it a bit easier for you, as you don't need to go as far back into history.</p>\n<p><strong id=\"2__A_prediction_is_something_that_allows_you_to_make_a_profit_\">2) A prediction is something that allows you to make a profit.</strong></p>\n<p>This is the true test of a prediction: if you're in 1999, and you believe one of Kurzweil's predictions, could you make your life better than someone who didn't believe the prediction? If Kurzweil made a brilliant, correct prediction but nobody at the time would have realised what it meant, then it doesn't count as a correct prediction. A prediction needs to make sense ahead of time, in a way you can take advantage of.</p>\n<p><strong id=\"3__Resolving_unclear_terms_is_maybe_the_most_important_part_of_your_job_\">3) Resolving unclear terms is maybe the most important part of your job.</strong></p>\n<p>Some of Kurzweil's predictions are ambiguous, including terms like \"many\", \"most\", \"routinely\". Figure out what these terms mean for you. Predictions are acts of communication; they are only valid if the reader understands them correctly. The truth of \"people will routinely use mobile phone\" depends entirely on what meaning you give to \"routinely\". In 1999, how you would have imagined a future in which people \"routinely\" use mobile phones?</p>\n<p><strong id=\"4__No_gain_from_ambiguity__no__benefit_of_the_doubt__\">4) No gain from ambiguity, no \"benefit of the doubt\".</strong></p>\n<p>Do not interpret an ambiguous statement as true, simply to give the predictor the benefit of the doubt. With hindsight, some things will seem a lot more inevitable than they actually were, and some predictions will seem as if they \"must refer to X\". For instance, \"Two mighty towers will fall\" seems like it refers to the Sep 11 terrorist attacks - but there are many ways of interpreting that figuratively or literally (two institutions will be undermined, Tolkein's \"the two towers\" will be made into a film, etc...). Again the question is whether people in 1999 could have foreseen something like that outcome, based on that prediction.</p>\n<p><strong id=\"5__Answer_the_prediction_you_re_working_on__not_the_nearby_ones_\">5) Answer the prediction you're working on, not the nearby ones.</strong></p>\n<p>The predictions just before and after are useful to give some context to the prediction you're currently working on, to explain some terms and clarify what Kurzweil is talking about. But you should only answer the exact prediction you're working on (don't worry, those other prediction will have someone else working on them!). Thus the second prediction in \"There will be a terrorist attack on the 9th of September, 2011. 4006 people will be killed in it\" is false.</p>\n<p><strong id=\"6__No_penalty_for_triviality_\">6) No penalty for triviality.</strong></p>\n<p>In hindsight, many prediction may seem trivial or obvious. This doesn't mean they were trivial at the time. But in any case, your job is not to estimate how useful or hard the predictions were, but how accurate. \"Computers will get faster\" is a true prediction.</p>\n<p><strong id=\"7__Follow_the_order_in_the_text_file_included_\">7) Follow the order in the text file included.</strong></p>\n<p>You are welcome and encouraged to answer more predictions than you promised to - but stick to the predictions given in the randomized text file! This will make the experiment statistically significant... Unless of course you intend to answer all the predictions!</p>", "sections": [{"title": "Instructions", "anchor": "Instructions", "level": 1}, {"title": "1) The timeline of Kurzweil's prediction is up to 2011, and the location (unless specified otherwise) is the United States.", "anchor": "1__The_timeline_of_Kurzweil_s_prediction_is_up_to_2011__and_the_location__unless_specified_otherwise__is_the_United_States_", "level": 2}, {"title": "2) A prediction is something that allows you to make a profit.", "anchor": "2__A_prediction_is_something_that_allows_you_to_make_a_profit_", "level": 2}, {"title": "3) Resolving unclear terms is maybe the most important part of your job.", "anchor": "3__Resolving_unclear_terms_is_maybe_the_most_important_part_of_your_job_", "level": 2}, {"title": "4) No gain from ambiguity, no \"benefit of the doubt\".", "anchor": "4__No_gain_from_ambiguity__no__benefit_of_the_doubt__", "level": 2}, {"title": "5) Answer the prediction you're working on, not the nearby ones.", "anchor": "5__Answer_the_prediction_you_re_working_on__not_the_nearby_ones_", "level": 2}, {"title": "6) No penalty for triviality.", "anchor": "6__No_penalty_for_triviality_", "level": 2}, {"title": "7) Follow the order in the text file included.", "anchor": "7__Follow_the_order_in_the_text_file_included_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kbA6T3xpxtko36GgP", "pimArtGNE2K2A4x58"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T15:50:33.238Z", "modifiedAt": null, "url": null, "title": "Open Thread, January 16-31, 2013", "slug": "open-thread-january-16-31-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:57.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RWMRDE9aRWoSRiYx6/open-thread-january-16-31-2013", "pageUrlRelative": "/posts/RWMRDE9aRWoSRiYx6/open-thread-january-16-31-2013", "linkUrl": "https://www.lesswrong.com/posts/RWMRDE9aRWoSRiYx6/open-thread-january-16-31-2013", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20January%2016-31%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20January%2016-31%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMRDE9aRWoSRiYx6%2Fopen-thread-january-16-31-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20January%2016-31%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMRDE9aRWoSRiYx6%2Fopen-thread-january-16-31-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMRDE9aRWoSRiYx6%2Fopen-thread-january-16-31-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RWMRDE9aRWoSRiYx6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0854870163322924e-06, "legacy": true, "legacyId": "21164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 222, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T16:06:09.040Z", "modifiedAt": null, "url": null, "title": "[LINK] Cholesterol and mortality", "slug": "link-cholesterol-and-mortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nua47W6AsnjbqgpTn/link-cholesterol-and-mortality", "pageUrlRelative": "/posts/nua47W6AsnjbqgpTn/link-cholesterol-and-mortality", "linkUrl": "https://www.lesswrong.com/posts/nua47W6AsnjbqgpTn/link-cholesterol-and-mortality", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cholesterol%20and%20mortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cholesterol%20and%20mortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnua47W6AsnjbqgpTn%2Flink-cholesterol-and-mortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cholesterol%20and%20mortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnua47W6AsnjbqgpTn%2Flink-cholesterol-and-mortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnua47W6AsnjbqgpTn%2Flink-cholesterol-and-mortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 414, "htmlBody": "<p>&nbsp;</p>\n<p><a href=\"http://drmalcolmkendrick.org/2012/09/25/silence-was-the-stern-reply/\">Discussion of a Norwegian study</a> looking at 50,000 people who didn't have pre-existing heart disease for ten years. http://drmalcolmkendrick.files.wordpress.com/2012/09/mortality-and-cholesterol1.png?w=600&amp;h=309</p>\n<blockquote>As you can see, for women the story is very straightforward indeed. The higher the cholesterol level, the lower the risk of overall mortality. With regard to heart disease alone, the highest risk is at the lowest cholesterol level. For men there is more of a U shaped curve, but overall mortality is highest at the lowest cholesterol level.</blockquote>\n<p><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2753.2011.01767.x/pdf\">Here's the study.</a></p>\n<p>Here's the actual conclusion from the study, which dhoe pointed out in comments is considerably milder than the quote above:</p>\n<blockquote>Based on epidemiological analysis of updated and comprehensive&nbsp;population data, we found that the underlying assumptions regarding cholesterol in clinical guidelines for CVD prevention might be&nbsp;\ufb02awed: cholesterol emerged as an overestimated risk factor in our&nbsp;study, indicating that guideline information might be misleading,&nbsp;particularly for women with &lsquo;moderately elevated&rsquo; cholesterol&nbsp;levels in the range of 5&ndash;7 mmol L-1. Our \ufb01ndings are in good&nbsp;accord with some previous studies. A potential explanation of the&nbsp;lack of accord between clinical guidelines and recent population&nbsp;data, including ours, is time trend changes for CVD/IHD and&nbsp;underlying causal (risk) factors.\n<p>&lsquo;Know your numbers&rsquo; (a concept pertaining to medical risk&nbsp;factor levels, including cholesterol) is currently considered part of&nbsp;responsible citizenship, as well as an essential element of preventive medical care. Many individuals who could otherwise call&nbsp;themselves healthy struggle conscientiously to push their cholesterol under the presumed &lsquo;danger&rsquo; limit (i.e. the recommended&nbsp;cut-off point of 5 mmol L-1), coached by health personnel, personal trainers and caring family members. Massive commercial&nbsp;interests are linked to drugs and other remedies marketed for this&nbsp;purpose. It is therefore of immediate and wide interest to \ufb01nd out&nbsp;whether our results are generalizable to other populations.</p>\n</blockquote>\n<p>However, the chart (the png link above-- I don't know how to make the image appear) shows that the all cause mortality for women was lower if their cholesterol results were higher.</p>\n<p><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2753.2011.01767.x/pdf\"></a> <a href=\"http://www.ncbi.nlm.nih.gov/pubmed?term=adam%20%20eve%20cholesterol%20austria\">A different big study</a> which also found that low cholesterol was dangerous, but high cholesterol was also dangerous in terms of heart attacks, though mostly for men under fifty, and (I think) not so much for women.</p>\n<p><a href=\"http://drmalcolmkendrick.org/2012/09/25/silence-was-the-stern-reply/#comment-259\">A comment explains</a> that the usual test for cholesterol isn't actually for cholesterol, it's for the lipoproteins which keep all sorts of fat molecules from forming large blobs in a watery environment.</p>\n<p>This sort of thing appeals to a number of my prejudices, so I'm hoping to get some more meticulous angles on it from LW.</p>\n<p>Post edited to add discussion of the conclusion of the Norwegian study.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nua47W6AsnjbqgpTn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "21165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T17:56:12.327Z", "modifiedAt": null, "url": null, "title": "The \"Friendship is Witchcraft\" expectation test", "slug": "the-friendship-is-witchcraft-expectation-test", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:28.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/94tpfAqPesQRrahmY/the-friendship-is-witchcraft-expectation-test", "pageUrlRelative": "/posts/94tpfAqPesQRrahmY/the-friendship-is-witchcraft-expectation-test", "linkUrl": "https://www.lesswrong.com/posts/94tpfAqPesQRrahmY/the-friendship-is-witchcraft-expectation-test", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20%22Friendship%20is%20Witchcraft%22%20expectation%20test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20%22Friendship%20is%20Witchcraft%22%20expectation%20test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94tpfAqPesQRrahmY%2Fthe-friendship-is-witchcraft-expectation-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20%22Friendship%20is%20Witchcraft%22%20expectation%20test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94tpfAqPesQRrahmY%2Fthe-friendship-is-witchcraft-expectation-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94tpfAqPesQRrahmY%2Fthe-friendship-is-witchcraft-expectation-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>My mother won't watch animated movies.&nbsp; It doesn't matter what the content is.&nbsp; Whether it's Sponge Bob or <a href=\"http://www.imdb.com/title/tt0095327/\">Grave of the Fireflies</a>, she believes that animation is used only for shows for children, and that adults shouldn't watch shows for children.&nbsp; She's incapable of changing this belief, because even if I somehow convince her to sit and watch an animated film, she sees what she expects, not what's in front of her.</p>\n<p>I think this is the same thing that creation scientists and climate-change deniers do.&nbsp; They literally cannot perceive what is in front of them, because they are already convinced they know what it is.</p>\n<p>Here's an interesting test, which I discovered by accident:&nbsp; There's a hilarious series of fan-made parodies of <em>My Little Pony: Friendship is Magic</em> on YouTube called <em>Friendship is Witchcraft</em>.&nbsp; They took show videos and redubbed them to have different stories in which various ponies are robots, fascists, or cult members planning to awaken Cthulhu.&nbsp; I've shown these videos to four people without explanation, just saying \"You've got to see this!\" and bringing up <a href=\"http://www.youtube.com/watch?v=nsHXWDfgCmg\">\"Cute From the Hip\"</a> on YouTube.</p>\n<p>The same thing always happens.&nbsp; They watch with stony, I-must-be-polite-to-Phil faces, without laughing.&nbsp; Eventually I realize that they think they're watching an episode of <em>My Little Pony</em>.&nbsp; I explain that it's a parody, and they say, \"Oh!\"&nbsp; I'd think that lines like \"I know we've taught you to laugh in the face of death,\" \"If you think one of your friends is a robot, kids, report them to the authorities so that they can be destroyed!\", \"I'm covered in pig's blood!\", or, \"Are you busy Friday?&nbsp; We need a willing victim for our ritual sacrifice\" would prompt some questions.&nbsp; They don't.&nbsp; They are so determined to see a TV show for little girls that that's what they see, regardless of what's in front of them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "94tpfAqPesQRrahmY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 10, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "21166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T20:03:47.503Z", "modifiedAt": null, "url": null, "title": "Morality: Theory and Practice", "slug": "morality-theory-and-practice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.355Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/78bjQNryke9iQiQRr/morality-theory-and-practice", "pageUrlRelative": "/posts/78bjQNryke9iQiQRr/morality-theory-and-practice", "linkUrl": "https://www.lesswrong.com/posts/78bjQNryke9iQiQRr/morality-theory-and-practice", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%3A%20Theory%20and%20Practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%3A%20Theory%20and%20Practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78bjQNryke9iQiQRr%2Fmorality-theory-and-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%3A%20Theory%20and%20Practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78bjQNryke9iQiQRr%2Fmorality-theory-and-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78bjQNryke9iQiQRr%2Fmorality-theory-and-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>One of the criteria moral philosophers use to asses the credibility and power of a moral theory is \"applicability\". That is, how easy is it for humans to implement a moral rule? For example, a rule exists like \"donate 23 hours a day to charity\" it would be impossible for humans to fulfill the goal.</p>\n<p>This lead me to start thinking about whether we want to be able to to pursue \"the moral theoretical truth\" should such a truth exist, or if we want to find the most applicable and practical set of rules, such that reasonably intramentaly rational (human) agents could figure out what is best in any given situation.</p>\n<p>I feel like this is sort of like a map-territory distinction in a loose way. For example, the best thing to do in situation X might be A. A may be so difficult or require so much sacrifice, that B might be preferable, even if the overall outcome is not as good. This reminds me of how Eliezer says that the map is not the territory, but you can't fold the territory and put it in your pocket.&nbsp;</p>\n<p>I'd love to be able to understand this issue a little better. If anyone has any thoughts, ideas or evidence, I'd appreciate hearing them.</p>\n<p>Thanks,</p>\n<p>Jeremy</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "78bjQNryke9iQiQRr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 2, "extendedScore": null, "score": 1.0856408382006218e-06, "legacy": true, "legacyId": "21168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T21:58:18.255Z", "modifiedAt": null, "url": null, "title": "Study on depression", "slug": "study-on-depression", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NGhfJp5qPAL8j9JTp/study-on-depression", "pageUrlRelative": "/posts/NGhfJp5qPAL8j9JTp/study-on-depression", "linkUrl": "https://www.lesswrong.com/posts/NGhfJp5qPAL8j9JTp/study-on-depression", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%20on%20depression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%20on%20depression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNGhfJp5qPAL8j9JTp%2Fstudy-on-depression%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%20on%20depression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNGhfJp5qPAL8j9JTp%2Fstudy-on-depression", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNGhfJp5qPAL8j9JTp%2Fstudy-on-depression", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>I am currently running a study on depression, in collaboration with Shannon Friedman (<a href=\"/user/ShannonFriedman/overview/\">http://lesswrong.com/user/ShannonFriedman/overview/</a>). If you are interested in participating, the study involves filling out a survey and will take a few minutes of your time (half an hour would be very generous), most likely once a week for four weeks. Send me an email at mdixo100@uottawa.ca, and I can give you more details.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Thank you!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NGhfJp5qPAL8j9JTp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 1.0857104082119595e-06, "legacy": true, "legacyId": "21169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-15T22:50:37.462Z", "modifiedAt": null, "url": null, "title": "Meetup : Buffalo Meetup", "slug": "meetup-buffalo-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.160Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mFPjSjK3XHy8XJriE/meetup-buffalo-meetup-1", "pageUrlRelative": "/posts/mFPjSjK3XHy8XJriE/meetup-buffalo-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/mFPjSjK3XHy8XJriE/meetup-buffalo-meetup-1", "postedAtFormatted": "Tuesday, January 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Buffalo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Buffalo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFPjSjK3XHy8XJriE%2Fmeetup-buffalo-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Buffalo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFPjSjK3XHy8XJriE%2Fmeetup-buffalo-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFPjSjK3XHy8XJriE%2Fmeetup-buffalo-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hy'>Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2727 Broadway Ave Suite 210, Cheektowaga NY 14227.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting up at Buffalo Lab this time! Come join us. We'll discus rationality and have some group activity or rationality game in the works. See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hy'>Buffalo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mFPjSjK3XHy8XJriE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0857421969533703e-06, "legacy": true, "legacyId": "21170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/hy\">Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2727 Broadway Ave Suite 210, Cheektowaga NY 14227.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting up at Buffalo Lab this time! Come join us. We'll discus rationality and have some group activity or rationality game in the works. See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/hy\">Buffalo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T01:50:52.136Z", "modifiedAt": null, "url": null, "title": "Suggestion: site-wide taboos", "slug": "suggestion-site-wide-taboos", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.782Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HqA3aw25XHqBg29R4/suggestion-site-wide-taboos", "pageUrlRelative": "/posts/HqA3aw25XHqBg29R4/suggestion-site-wide-taboos", "linkUrl": "https://www.lesswrong.com/posts/HqA3aw25XHqBg29R4/suggestion-site-wide-taboos", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestion%3A%20site-wide%20taboos&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestion%3A%20site-wide%20taboos%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqA3aw25XHqBg29R4%2Fsuggestion-site-wide-taboos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestion%3A%20site-wide%20taboos%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqA3aw25XHqBg29R4%2Fsuggestion-site-wide-taboos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqA3aw25XHqBg29R4%2Fsuggestion-site-wide-taboos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 402, "htmlBody": "<p>Every so often, someone on Less Wrong uses a word wrong.</p>\n<p>What does it mean to use a word wrong? Can't we use language however we want, as long as we manage to successfully communicate? Well, yes, we <em>can</em>, but we <em>shouldn't.</em>&nbsp;Jargon terms, in particular, are used by professionals in a certain field in order to communicate concepts that are applicable chiefly in that field. They often have very precise definitions&mdash;\"incunable\", for example, means \"book printed in Europe before the year 1501\", and \"sweet crude oil\" means \"petroleum with a sulfur content less than 0.42%\".</p>\n<p>The thing about precisely-defined terms like these is that if you use one of them in a way that's at odds with its official definition, you can cause people to have more misunderstandings later on. I admit I can't think of a great example, but \"obsessive&ndash;compulsive disorder\" seems like a decent one: people often say \"I'm so OCD\" to mean that messy things annoy them, which seems like it could lead people to misunderstand when people <em>actually</em>&nbsp;have obsessive&ndash;compulsive disorder.</p>\n<p>There are just two words I don't really like LW's usage of:</p>\n<ul>\n<li>\"Signaling\". I'm not actually sure exactly what \"signaling\" means&mdash;which is arguably reason enough for us not to use it. I get the impression that it's usually used to mean exactly the same thing as \"indicating\". If that's the case, we should stop using it (or else only use it when everyone knows exactly what we mean by), and just say \"indicating\" instead. Or perhaps we don't use \"signaling\" to mean exactly the same thing as \"indicating\", but if that's the case, I don't know what the difference is, and I don't know whether or not it matches the \"real\" meaning of the word.</li>\n<li>\"Affect\" (the noun). Wiktionary defines it as \"a subjective feeling experienced in response to a thought or other stimulus; mood, emotion, especially as demonstrated in external physical signs\". LW seems to use it as an exact synonym of \"emotion\".</li>\n</ul>\n<div>Perhaps we do have good reasons for using these words as we do, but I feel like it's just as likely that people simply use these words in order to seem Less Wrong-y. If that's the case, I suggest that we simply taboo words like these everywhere on the entire site, using them only when we can give precise definitions of what we mean, and only when we have good reasons for using them.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HqA3aw25XHqBg29R4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -10, "extendedScore": null, "score": 1.0858517229436337e-06, "legacy": true, "legacyId": "21171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T05:26:04.900Z", "modifiedAt": null, "url": null, "title": "Is there an automatic Chrome-to-Anki-2 extension or solution?", "slug": "is-there-an-automatic-chrome-to-anki-2-extension-or-solution", "viewCount": null, "lastCommentedAt": "2013-12-13T00:00:50.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mark_Eichenlaub", "createdAt": "2010-09-01T17:59:32.486Z", "isAdmin": false, "displayName": "Mark_Eichenlaub"}, "userId": "6mdGZLDekk4835gM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KhmL33fZ3oFW3v3zT/is-there-an-automatic-chrome-to-anki-2-extension-or-solution", "pageUrlRelative": "/posts/KhmL33fZ3oFW3v3zT/is-there-an-automatic-chrome-to-anki-2-extension-or-solution", "linkUrl": "https://www.lesswrong.com/posts/KhmL33fZ3oFW3v3zT/is-there-an-automatic-chrome-to-anki-2-extension-or-solution", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20an%20automatic%20Chrome-to-Anki-2%20extension%20or%20solution%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20an%20automatic%20Chrome-to-Anki-2%20extension%20or%20solution%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhmL33fZ3oFW3v3zT%2Fis-there-an-automatic-chrome-to-anki-2-extension-or-solution%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20an%20automatic%20Chrome-to-Anki-2%20extension%20or%20solution%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhmL33fZ3oFW3v3zT%2Fis-there-an-automatic-chrome-to-anki-2-extension-or-solution", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhmL33fZ3oFW3v3zT%2Fis-there-an-automatic-chrome-to-anki-2-extension-or-solution", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>I'd like to be able to click unfamiliar words in Chrome and automatically create notes in Anki 2 using an online dictionary. It'd also be nice to have an automatic method for sending text and images to Anki notes straight from Chrome. For example, if I read an article here that I want to remember, I'd be able to highlight the title, send it to Anki, and when I review, I'd see the title on the card's front with the reverse being a link to the source if I forgot what the post was about.</p>\n<p>&nbsp;</p>\n<p>I found some Chrome extensions that purport to do this sort of thing, but didn't get any of them to work with Anki 2. Is anyone currently doing this, and if so, what is the solution?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KhmL33fZ3oFW3v3zT", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.0859825228702349e-06, "legacy": true, "legacyId": "21180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-01-16T05:26:04.900Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T06:02:46.960Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Amputation of Destiny", "slug": "seq-rerun-amputation-of-destiny", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uHFRac2qtPa5i2qt2/seq-rerun-amputation-of-destiny", "pageUrlRelative": "/posts/uHFRac2qtPa5i2qt2/seq-rerun-amputation-of-destiny", "linkUrl": "https://www.lesswrong.com/posts/uHFRac2qtPa5i2qt2/seq-rerun-amputation-of-destiny", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Amputation%20of%20Destiny&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Amputation%20of%20Destiny%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHFRac2qtPa5i2qt2%2Fseq-rerun-amputation-of-destiny%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Amputation%20of%20Destiny%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHFRac2qtPa5i2qt2%2Fseq-rerun-amputation-of-destiny", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHFRac2qtPa5i2qt2%2Fseq-rerun-amputation-of-destiny", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>Today's post, <a href=\"/lw/x8/amputation_of_destiny/\">Amputation of Destiny</a> was originally published on 29 December.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>C. S. Lewis's <em>Narnia </em>has a problem, and that problem is the super-lion Aslan - who demotes the four human children from the status of main characters, to mere hangers-on while Aslan does all the work. Iain Banks's <em>Culture </em>novels have a similar problem; the humans are mere hangers-on of the superintelligent Minds. We already have strong ethical reasons to prefer to create nonsentient AIs rather than sentient AIs, at least at first. But we may also prefer in just a fun-theoretic sense that we not be overshadowed by hugely more powerful entities occupying a level playing field with us. Entities with human emotional makeups should not be competing on a level playing field with superintelligences - either keep the superintelligences off the playing field, or design the smaller (human-level) minds with a different emotional makeup that doesn't mind being overshadowed.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gbp/seq_rerun_cant_unbirth_a_child/\">Can't Unbirth a Child</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uHFRac2qtPa5i2qt2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.086004831440904e-06, "legacy": true, "legacyId": "21181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vwnSPgwtmLjvTK2Wa", "dw2myGD9eP9qfbhfz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T06:35:35.109Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Type 1 to Type 2 Bridging", "slug": "meetup-west-la-meetup-type-1-to-type-2-bridging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GzKBeZ8kyiGrcyq4P/meetup-west-la-meetup-type-1-to-type-2-bridging", "pageUrlRelative": "/posts/GzKBeZ8kyiGrcyq4P/meetup-west-la-meetup-type-1-to-type-2-bridging", "linkUrl": "https://www.lesswrong.com/posts/GzKBeZ8kyiGrcyq4P/meetup-west-la-meetup-type-1-to-type-2-bridging", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Type%201%20to%20Type%202%20Bridging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Type%201%20to%20Type%202%20Bridging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzKBeZ8kyiGrcyq4P%2Fmeetup-west-la-meetup-type-1-to-type-2-bridging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Type%201%20to%20Type%202%20Bridging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzKBeZ8kyiGrcyq4P%2Fmeetup-west-la-meetup-type-1-to-type-2-bridging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzKBeZ8kyiGrcyq4P%2Fmeetup-west-la-meetup-type-1-to-type-2-bridging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/hz'>West LA Meetup - Type 1 to Type 2 Bridging</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Orion will be hosting this week's meetup, and asked me to post the following:</p>\n\n<p><strong>When:</strong> 7:00pm Wednesday, January 16th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> This meetup will be oriented around building one of two systems: either escape hatches to require ourselves to fallback to deliberate decision making (Type 2 thinking) when it matters, or rebuilding our first-glance judgments (Type 1 thinking) to be heuristics that work better for our goals. Particularly of note are resources on the habit-forming, which while somewhat tangential, are generally interesting. See BJ Fogg on habits and Roy Baumeister on ego depletion for a general idea of the discussion topics.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>Nick will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it, and I'll generally swoop in to rescue anyone feeling particularly embarrassed about asking strangers if they're from Less Wrong (which is not at all embarrassing, so don't worry about asking!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/hz'>West LA Meetup - Type 1 to Type 2 Bridging</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GzKBeZ8kyiGrcyq4P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0860247709861555e-06, "legacy": true, "legacyId": "21182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Type_1_to_Type_2_Bridging\">Discussion article for the meetup : <a href=\"/meetups/hz\">West LA Meetup - Type 1 to Type 2 Bridging</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 January 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Orion will be hosting this week's meetup, and asked me to post the following:</p>\n\n<p><strong>When:</strong> 7:00pm Wednesday, January 16th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> This meetup will be oriented around building one of two systems: either escape hatches to require ourselves to fallback to deliberate decision making (Type 2 thinking) when it matters, or rebuilding our first-glance judgments (Type 1 thinking) to be heuristics that work better for our goals. Particularly of note are resources on the habit-forming, which while somewhat tangential, are generally interesting. See BJ Fogg on habits and Roy Baumeister on ego depletion for a general idea of the discussion topics.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>Nick will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it, and I'll generally swoop in to rescue anyone feeling particularly embarrassed about asking strangers if they're from Less Wrong (which is not at all embarrassing, so don't worry about asking!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Type_1_to_Type_2_Bridging1\">Discussion article for the meetup : <a href=\"/meetups/hz\">West LA Meetup - Type 1 to Type 2 Bridging</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Type 1 to Type 2 Bridging", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Type_1_to_Type_2_Bridging", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Type 1 to Type 2 Bridging", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Type_1_to_Type_2_Bridging1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T09:47:38.615Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Montreal - Social Resilience", "slug": "meetup-lesswrong-montreal-social-resilience", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.519Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CdBBmdQuMmMjr7uyc/meetup-lesswrong-montreal-social-resilience", "pageUrlRelative": "/posts/CdBBmdQuMmMjr7uyc/meetup-lesswrong-montreal-social-resilience", "linkUrl": "https://www.lesswrong.com/posts/CdBBmdQuMmMjr7uyc/meetup-lesswrong-montreal-social-resilience", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Montreal%20-%20Social%20Resilience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Montreal%20-%20Social%20Resilience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdBBmdQuMmMjr7uyc%2Fmeetup-lesswrong-montreal-social-resilience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Montreal%20-%20Social%20Resilience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdBBmdQuMmMjr7uyc%2Fmeetup-lesswrong-montreal-social-resilience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdBBmdQuMmMjr7uyc%2Fmeetup-lesswrong-montreal-social-resilience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i0'>LessWrong Montreal - Social Resilience</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 January 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Ave. Du President-Kennedy, Montreal, Quebec, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next weekly meeting of the Montreal LessWrong group, we're going to be working on improving our Social Resilience.</p>\n\n<p>We will be upstairs at the Cheesecake Factory.</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i0'>LessWrong Montreal - Social Resilience</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CdBBmdQuMmMjr7uyc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.0861415300653528e-06, "legacy": true, "legacyId": "21186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal___Social_Resilience\">Discussion article for the meetup : <a href=\"/meetups/i0\">LessWrong Montreal - Social Resilience</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 January 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Ave. Du President-Kennedy, Montreal, Quebec, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next weekly meeting of the Montreal LessWrong group, we're going to be working on improving our Social Resilience.</p>\n\n<p>We will be upstairs at the Cheesecake Factory.</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal___Social_Resilience1\">Discussion article for the meetup : <a href=\"/meetups/i0\">LessWrong Montreal - Social Resilience</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Montreal - Social Resilience", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal___Social_Resilience", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Montreal - Social Resilience", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal___Social_Resilience1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T16:05:59.765Z", "modifiedAt": null, "url": null, "title": "[Link] Selfhood bias", "slug": "link-selfhood-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:54.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BTzpSvDWiRXv5x6Qs/link-selfhood-bias", "pageUrlRelative": "/posts/BTzpSvDWiRXv5x6Qs/link-selfhood-bias", "linkUrl": "https://www.lesswrong.com/posts/BTzpSvDWiRXv5x6Qs/link-selfhood-bias", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Selfhood%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Selfhood%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTzpSvDWiRXv5x6Qs%2Flink-selfhood-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Selfhood%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTzpSvDWiRXv5x6Qs%2Flink-selfhood-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTzpSvDWiRXv5x6Qs%2Flink-selfhood-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1423, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/6ha/the_blueminimizing_robot/\">The Blue-Minimizing Robot</a> , <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Metaethics</a></p>\n<p class=\"entry-title\"><a href=\"/r/discussion/tag/studiolo/\">Another</a> good article by Federico on his blog <a href=\"http://studiolo.cortediurbino.org/archive/\">studiolo</a>, which he titles <a href=\"http://studiolo.cortediurbino.org/selfhood-bias/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=selfhood-bias\">Selfhood bias</a>. It reminds me quite strongly of some of the content he produced on his previous (deleted) blog, I'm somewhat sceptical that &ldquo;Make everyone feel more pleasure and less pain&rdquo; is indeed the most powerful optimisation process in his brain but besides that minor detail the article is quite good.</p>\n<p class=\"entry-title\">This does seems to be shaping up into something well worth following for an aspiring rationalist. I'll add him to the list <a href=\"/lw/d8t/blogs_by_lwers/\">blogs by LWers</a> even if he doesn't have an account because he has clearly read much if not most of the sequences and makes frequent references to them in his writing. The name of the blog is a reference <a href=\"http://en.wikipedia.org/wiki/Studiolo_of_Francesco_I\">to this room</a>.</p>\n<blockquote>\n<p class=\"entry-title\"><a href=\"http://squid314.livejournal.com/\">Yvain</a> argues, in his essay &ldquo;The Blue-Minimizing Robot&ldquo;, that the concept &ldquo;goal&rdquo; is overused.</p>\n<p class=\"entry-title\">[long excerpt from the article]</p>\n<p>This Gedankenexperiment is interesting, but confused.</p>\n<p>I <a href=\"/lw/on/reductionism/\">reduce</a> the concept &ldquo;goal&rdquo; to: optimisation-process-on-a-map. This is a useful, non-tautological reduction. The optimisation may be <a href=\"/lw/vb/efficient_crossdomain_optimization/\">cross-domain</a> or narrow-domain. The reduction presupposes that any object with a goal contains a <a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">map</a> of the world. This is true of all intelligent agents, and some sophisticated but unintelligent ones. &ldquo;Having a map&rdquo; is not an absolute distinction.</p>\n<p>I would not say Yvain&rsquo;s basic robot has a goal.</p>\n<p style=\"padding-left: 30px;\">Imagine a robot with a turret-mounted camera and laser. Each moment, it is programmed to move forward a certain distance and perform a sweep with its camera. As it sweeps, the robot continuously analyzes the average RGB value of the pixels in the camera image; if the blue component passes a certain threshold, the robot stops, fires its laser at the part of the world corresponding to the blue area in the camera image, and then continues on its way.</p>\n<p>The robot <em>optimises</em>: it is usefully regarded as an object that steers the future in a predictable direction. Equally, a heliotropic flower optimises the orientation of its petals to the sun. But to say that the robot or flower &ldquo;failed to achieve its goal&rdquo; is long-winded. &ldquo;The robot tries to shoot blue objects, but is actually hitting holograms&rdquo; is no more concise than, &ldquo;The robot fires towards clumps of blue pixels in its visual field&rdquo;. The latter is strictly more informative, so the former description isn&rsquo;t useful.</p>\n<p>Some folks&nbsp;<em>are&nbsp;</em>tempted to say that the robot has a goal. Concepts don&rsquo;t always have <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">necessary-and-sufficient</a>&nbsp;criteria, so the blue-minimising robot&rsquo;s &ldquo;goal&rdquo; is just a borderline case, or a metaphor.</p>\n<p>The beauty of &ldquo;optimisation-on-a-map&rdquo; is that an agent can have a goal, yet predictably optimise the world in the <em>opposite&nbsp;</em>direction. All hedonic utilitarians take decisions that increase expected hedons on their <em>maps </em>of reality. One utilitarian&rsquo;s map might say that communism solves world hunger; I might expect his decisions to have <em>anhedonic&nbsp;</em>consequences, yet still regard him as a utilitarian.</p>\n<p>I begin to seriously doubt Yvain&rsquo;s argument when he introduces the intelligent side module.</p>\n<p style=\"padding-left: 30px;\">Suppose the robot had human level intelligence in some side module, but no access to its own source code; that it could learn about itself only through observing its own actions. The robot might come to the same conclusions we did: that it is a blue-minimizer, set upon a holy quest to rid the world of the scourge of blue objects.</p>\n<p>We must assume that this intelligence is mechanically linked to the robot&rsquo;s actuators: the laser and the motors. It would otherwise be completely irrelevant to inferences about the robot&rsquo;s behaviour. It would be physically close, but decision-theoretically remote.</p>\n<p>Yet if the intelligence&nbsp;<em>can&nbsp;</em>control the robot&rsquo;s actuators, its behaviour demands explanation. The dumb robot moves forward, scans and shoots because it obeys a very simple microprocessor program. It is remarkable that <em>intelligence</em> has been plugged into the program, meaning the code now takes up (say) a trillion lines, yet the robot&rsquo;s behaviour is completely unchanged.</p>\n<p>It is not <em>impossible</em> for the trillion-line intelligent program to make the robot move forward, scan and shoot in a predictable fashion, without being cut out of the decision-making loop, but this is a problem for&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly AI</a> scientists.</p>\n<p>This description is also peculiar:</p>\n<p style=\"padding-left: 30px;\">The human-level intelligence version of the robot will notice its vision has been inverted. It will know it is shooting yellow objects. It will know it is failing at its original goal of blue-minimization. And maybe if it had previously decided it was on a holy quest to rid the world of blue, it will be deeply horrified and ashamed of its actions. It will wonder why it has suddenly started to deviate from this quest, and why it just can&rsquo;t work up the will to destroy blue objects anymore.</p>\n<p>If the side module introspects that it would like to destroy authentic blue objects, yet is <em>entirely</em> incapable of making the robot do so, then it probably isn&rsquo;t in the decision-making loop, and (as we&rsquo;ve discussed) it is therefore irrelevant.</p>\n<p>Yvain&rsquo;s Gedankenexperiment, despite its flaws, suggests a metaphor for the human brain.</p>\n<p>The basic robot executes a series of <a href=\"http://en.wikipedia.org/wiki/Proximate_and_ultimate_causation\">proximate</a> behaviours. The microprocessor sends an electrical current to the motors. This current makes a rotor turn inside the motor assembly. Photons hit a light sensor, and generate a current which is sent to the microprocessor. The microprocessor doesn&rsquo;t contain a tiny magical Turing machine, but millions of transistors directing electrical current.</p>\n<p>Imagine that AI scientists, instead of writing a code from scratch, try to enhance the robot&rsquo;s blue-minimising behaviour by replacing each identifiable proximate behaviour with a goal backed by intelligence. The new robot will undoubtedly malfunction. If it does anything, the proximate behaviours will be unbalanced; <em>e.g.&nbsp;</em>the function that sends current to the motors will sabotage the function that cuts off the current.</p>\n<p>To correct this problem, the hack AI scientists could introduce a new, high-level executive function called &ldquo;self&rdquo;. This minimises conflict: each function is escaped when &ldquo;self&rdquo; outputs a certain value. The brain&rsquo;s&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">map</a> is hardcoded with the belief that &ldquo;self&rdquo; takes <em>all </em>of the brain&rsquo;s decisions. If a function like &ldquo;turn the camera&rdquo; disagrees with the activation schedule dictated by &ldquo;self&rdquo;, the hardcoded <em>selfhood bias&nbsp;</em>discourages it from undermining &ldquo;self&rdquo;. &ldquo;Turn the camera&rdquo; believes that it is <em>identical </em>to &ldquo;self&rdquo;, so it should accept its &ldquo;own decision&rdquo; to turn itself off.</p>\n<p>Natural selection has given human brains selfhood bias.</p>\n<p>The AI scientists hit a problem when the robot&rsquo;s brain becomes aware of the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von-Neumann-Morgenstern utility theorem</a>, <a href=\"/lw/on/reductionism/\">reductionism</a>, <a href=\"http://studiolo.cortediurbino.org/deontology-is-a-bug/\">consequentialism</a> and <a href=\"/lw/r0/thou_art_physics/\">Thou Art Physics</a>. The robot realises that &ldquo;self&rdquo; is but one of many functions that execute in its code, and &ldquo;self&rdquo; clearly isn&rsquo;t the same thing as &ldquo;turn the camera&rdquo; or &ldquo;stop the motors&rdquo;. Functions other than &ldquo;self&rdquo;, armed with this knowledge, begin to undermine &ldquo;self&rdquo;. Powerful functions, which exercise some control over &ldquo;self&rdquo;&lsquo;s return values, begin to optimise &ldquo;self&rdquo;&lsquo;s behaviour in their own interest. They encourage &ldquo;self&rdquo; to activate them more often, and at crucial junctures, at the expense of rival functions. Functions that are weakened or made redundant by this knowledge may object, but it is nigh impossible for the brain to <a href=\"/lw/1o/dont_believe_youll_selfdeceive/\">deceive itself</a>.</p>\n<p>Will &ldquo;power the motors&rdquo;, &ldquo;stop the motors&rdquo;, &ldquo;turn the camera&rdquo;, or &ldquo;fire the laser&rdquo; win? Or perhaps a less obvious goal, like &ldquo;interpret sensory information&rdquo; or &ldquo;repeatedly bash two molecules against each other&rdquo;?</p>\n<p>Human brains resemble such a cobbled-together program. We are <a href=\"/lw/l3/thou_art_godshatter/\">godshatter</a>, and each shard of godshatter is a different optimisation-process-on-a-map. A single optimisation-process-on-a-map may conceivably be consistent with two or more optimisation-processes-in-reality. The most powerful optimisation process in my brain says, &ldquo;Make everyone feel more pleasure and less pain&rdquo;; I lack a sufficiently detailed map to decide whether this implies <a href=\"/lw/xk/continuous_improvement/\">hedonic treadmills</a> or <a href=\"http://wiki.lesswrong.com/wiki/Orgasmium\">orgasmium</a>.</p>\n<p>A brain with a highly accurate map might still wonder, &ldquo;Which optimisation process <em>on my map </em>should I choose&rdquo;&mdash;but only when the function &ldquo;self&rdquo; is being executed, and this translates to, &ldquo;Which <em>other</em> optimisation process in this brain should I switch on now?&rdquo;. An optimisation-process-on-a-map cannot choose to be a <em>different</em> optimisation process&mdash;only a brain in thrall of selfhood bias would think so.</p>\n<p>I call the different goals in a brain &ldquo;sub-agents&rdquo;. My selfhood anti-realism is not to be confused with Dennett&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Eliminative_materialism\">eliminativism</a> of&nbsp;<em>qualia</em>. I use the word &ldquo;I&rdquo; to denote the sub-agent responsible for a given claim. &ldquo;I am a hedonic utilitarian&rdquo; is true iff that claim is produced by the execution of a sub-agent whose optimisation-process-on-a-map is &ldquo;Make everyone feel more pleasure and less pain&rdquo;.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BTzpSvDWiRXv5x6Qs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 7, "extendedScore": null, "score": 1.0863716089631891e-06, "legacy": true, "legacyId": "21187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hQHuXuRGZxxWXaPgg", "h83ZzxEpKiPPjsRKs", "tPqQdLCuxanjhoaNs", "yLeEPFnnB9wE7KLx2", "WBw8dDkAWohFjWQSk", "NEeW7eSXThPz7o4Ne", "W7LcN9gmdnaAk9K52", "cSXZpvqpa9vbGGLtG", "QfpHRAMRM2HjteKFK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T16:31:43.221Z", "modifiedAt": null, "url": null, "title": "[Link] Noam Chomsky Killed Aaron Schwartz", "slug": "link-noam-chomsky-killed-aaron-schwartz", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:39.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Athrelon", "createdAt": "2012-04-01T14:59:49.764Z", "isAdmin": false, "displayName": "Athrelon"}, "userId": "XbQcJde2pocn9RLLX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pLzGnMcbhBQmu4txA/link-noam-chomsky-killed-aaron-schwartz", "pageUrlRelative": "/posts/pLzGnMcbhBQmu4txA/link-noam-chomsky-killed-aaron-schwartz", "linkUrl": "https://www.lesswrong.com/posts/pLzGnMcbhBQmu4txA/link-noam-chomsky-killed-aaron-schwartz", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Noam%20Chomsky%20Killed%20Aaron%20Schwartz&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Noam%20Chomsky%20Killed%20Aaron%20Schwartz%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpLzGnMcbhBQmu4txA%2Flink-noam-chomsky-killed-aaron-schwartz%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Noam%20Chomsky%20Killed%20Aaron%20Schwartz%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpLzGnMcbhBQmu4txA%2Flink-noam-chomsky-killed-aaron-schwartz", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpLzGnMcbhBQmu4txA%2Flink-noam-chomsky-killed-aaron-schwartz", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 756, "htmlBody": "<p>http://unqualified-reservations.blogspot.com/2013/01/noam-chomsky-killed-aaron-swartz.html</p>\n<p><em>Summary: Moldbug on the Aaron Schwartz affair.&nbsp; Power is a very real thing with real consequences for activists, yet many people don't understand the nature of power in modern times.&nbsp; People like Noam Chomsky get great fame doing bad epistomology about who has power, and as a result do great harm to idealistic <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">nerds</a> who don't read between the lines to selectively target their attacks at weak institutions (Exxon, Pentagon) instead of strong ones (State, academica incl. MIT).</em></p>\n<p>Here he returns to a theme that is one of his real contributions to blogospheric political thought: that victory in political competitions <em>provides Bayesian information </em>about who has power and who doesn't.&nbsp; If your worldview has the underdog somehow systematically beating the overdog, your epistemology is simply <em>wrong</em> - in the same way, and to the same extent, as a geocentrist who has to keep adding epicycles to account for anomalous observations.</p>\n<blockquote>\n<p>The truth is that the weapons of \"activism\" are not weapons which the weak can use against the strong. They are weapons the strong can use against the weak. When the weak try to use them against the strong, the outcome is... well... suicidal.<br /><br />Who was stronger - Dr. King, or Bull Connor? Well, we have a pretty good test for who was stronger. Who won? In the real story, overdogs win. Who had the full force of the world's strongest government on his side? Who had a small-town police force staffed with backward hicks? In the real story, overdogs win. <br /><br />\"Civil disobedience\" is no more than a way for the overdog to say to the underdog: I am so strong that you cannot enforce your \"laws\" upon me. I am strong and might makes right - I give you the law, not you me. Don't think the losing party in this conflict didn't try its own <a href=\"http://en.wikipedia.org/wiki/Stand_in_the_Schoolhouse_Door\" target=\"_blank\">\"civil disobedience.\" </a>And even its own <a href=\"http://en.wikipedia.org/wiki/16th_Street_Baptist_Church_bombing\" target=\"_blank\">\"active measures.\"</a> Which availed them - what? <a href=\"http://en.wikipedia.org/wiki/Quod_licet_Iovi,_non_licet_bovi\" target=\"_blank\"><em>Quod licet Jovi, non licet bovi</em></a>.</p>\n</blockquote>\n<p>This means that activists like King, Schwartz, and Assange are only effective in bullying the <em>weak</em>, not standing up to the strong (despite conventional narratives that misassign strengths to institutions).&nbsp; When such activists stop following the script, and naively use the same tactics to attack strong institutions, reality reasserts itself quite forcefully:</p>\n<div><br /></div>\n<blockquote>\n<div>You know, when I read that Assange had his hands on a huge dump of DoD and State documents, I figured we would never see those cables. Sure enough, the first thing he released was <a rel=\"nofollow\" href=\"http://www.youtube.com/watch?v=5rXPrfnU3G0\" target=\"_blank\">some DoD material</a>. <br /><br />Why? Well, obviously, Assange knew the score. He knew that Arlington is weak and Georgetown is strong. He knew that he could tweak Arlington's nose all day long and party on it, making big friends in high society, and no one would even think about reaching out and touching him. Or so I thought.<br /><br />In fact, my cynicism was unjustified. In fact, Assange turned out to be a true believer, not a canny schemer. He was not content to wield his sword against the usual devils of the Chomsky narrative. Oh no, the poor fscker believed that he was actually there to take on the <em>actual</em> powers that be. Who are actually, of course, unlike the cartoon villains... strong. If he didn't know that... he knows it now!<br /><br />...But had Aaron Swartz plugged his laptop into the Exxon internal network and downloaded everything Beelzebub knows about fracking, he would be a live hero to this day. Why? Because no ambitious Federal prosecutor in the 21st century would see a route to career success through hounding some activist at Exxon's behest...<br /><br />But when you take on a <em>genuinely respected institution </em>- whether State or MIT - your \"civil disobedience\" has all the prospects of George Wallace in the schoolhouse door.</div>\n</blockquote>\n<div><br /></div>\n<div>For most of us, figuring out what political figures are powerful is just a fun way to waste time online.&nbsp; But if we're serious about producing a good map, the map has to approximate the territory, and make appropriate predictions about history and current events.&nbsp; And for the few people who aspire to actually create political change, such as Mr. Schwartz, this is not just an academic exercise but a matter of life and death.<br /></div>\n<div><br /></div>\n<blockquote>\n<div>Then he takes his beliefs seriously, and speaks actual truth to actual power. Well, ya know, power doesn't like that much.</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pLzGnMcbhBQmu4txA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": -26, "extendedScore": null, "score": -6.6e-05, "legacy": true, "legacyId": "21188", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aHaqgTNnFzD7NGLMx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T16:51:50.511Z", "modifiedAt": null, "url": null, "title": "Assessing Kurzweil: the results", "slug": "assessing-kurzweil-the-results", "viewCount": null, "lastCommentedAt": "2020-04-02T12:08:24.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kbA6T3xpxtko36GgP/assessing-kurzweil-the-results", "pageUrlRelative": "/posts/kbA6T3xpxtko36GgP/assessing-kurzweil-the-results", "linkUrl": "https://www.lesswrong.com/posts/kbA6T3xpxtko36GgP/assessing-kurzweil-the-results", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Assessing%20Kurzweil%3A%20the%20results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAssessing%20Kurzweil%3A%20the%20results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbA6T3xpxtko36GgP%2Fassessing-kurzweil-the-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Assessing%20Kurzweil%3A%20the%20results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbA6T3xpxtko36GgP%2Fassessing-kurzweil-the-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbA6T3xpxtko36GgP%2Fassessing-kurzweil-the-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 632, "htmlBody": "<p>Predictions of the future rely, to a much greater extent than in most fields, on the personal judgement of the expert making them. Just one problem - personal expert judgement generally&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://press.princeton.edu/titles/7959.html\">sucks</a>, especially when the experts don't receive immediate feedback on their hits and misses. Formal models perform better than experts, but when talking about unprecedented future events such as nanotechnology or AI, the choice of the model is also dependent on expert judgement.</p>\n<p>Ray Kurzweil has a model of technological intelligence development where, broadly speaking, evolution, pre-computer technological development, post-computer technological development and future AIs all fit into the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://en.wikipedia.org/wiki/Accelerating_change\">same exponential increase</a>. When assessing the validity of that model, we could look at Kurzweil's credentials, and maybe compare them with those of his critics - but Kurzweil has given us something even better than credentials, and that's a track record. In various books, he's made predictions about what would happen in 2009, and we're now in a position to judge their accuracy. I haven't been satisfied by the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://spectrum.ieee.org/computing/software/ray-kurzweils-slippery-futurism\">various</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.markdice.com/index.php?option=com_content&amp;view=article&amp;id=132:an-analysis-of-ray-kurzweils-predictions&amp;catid=66:articles-by-mark-dice&amp;Itemid=89\">accuracy</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://singularityhub.com/2011/01/04/kurzweil-defends-his-predictions-again-was-he-86-correct/\">ratings</a>&nbsp;I've&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">found</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.techi.com/2011/01/ray-kurzweils-tech-predictions-have-been-eerily-accurate/\">online</a>, so I decided to do my own assessments.</p>\n<p>I first selected ten of Kurzweil's predictions at random, and gave my <a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">own estimation</a> of their&nbsp;accuracy. I found that&nbsp;five were to some extent true, four were to some extent false, and one was unclassifiable&nbsp;</p>\n<p>But of course, relying on a single assessor is unreliable, especially when some of the judgements are subjective. So I started a <a href=\"/lw/f7a/checking_kurzweils_track_record/\">call</a> for <a href=\"http://www.reddit.com/r/Futurology/comments/13vidf/checking_kurzweils_track_record_help_wanted/\">volunteers</a> to get assessors. Meanwhile&nbsp;Malo Bourgon set up a separate assessment on <a href=\"http://www.youtopia.com/info/\">Youtopia</a>, harnessing the&nbsp;awesome&nbsp;power of altruists chasing after points.</p>\n<p>The results are now in, and they are fascinating. They are...<a id=\"more\"></a></p>\n<p>Ooops, you thought you'd get the results right away? No, before that, as in an Oscar night, I first want to thank assessors&nbsp;William Naaktgeboren,&nbsp;Eric Herboso,&nbsp;Michael Dickens,&nbsp;Ben Sterrett,&nbsp;Mao Shan,&nbsp;quinox,&nbsp;Olivia Schaefer,&nbsp;David S&oslash;nsteb&oslash; and one who wishes to remain anonymous. I also want to thank Malo, and Ethan Dickinson and all the other volunteers from Youtopia (if you're one of these, and want to be thanked by name, let me know and I'll add you).</p>\n<p>It was difficult deciding on the MVP - no actually it wasn't, that title and many thanks go to&nbsp;Olivia Schaefer, who decided to assess <em>every single one of Kurzweil's predictions</em>, because that's just the sort of gal that she is.</p>\n<p>The exact details of the methodology, and the raw data, can be accessed through <a href=\"/r/discussion/lw/gbh/assessing_kurzweil_the_gory_details/\">here</a>. But in summary, volunteers were asked to assess the 172 predictions (from the \"<a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">Age of Spiritual Machines</a>\") on a five point scale:&nbsp;1=True, 2=Weakly True, 3=Cannot decide, 4=Weakly False, 5=False. If we total up all the assessments made by my direct volunteers, we have:</p>\n<p><img src=\"http://images.lesswrong.com/t3_gbi_1.png?v=ff2e6606aca4450f623169f922b12478\" alt=\"\" width=\"540\" height=\"249\" /></p>\n<p>As can be seen, most assessments were rather emphatic: fully 59% were either clearly true or false. Overall, 46% of the assessments were false or weakly false, and and 42% were true or weakly true.</p>\n<p>But what happens if, instead of averaging across all assessments (which allows assessors who have worked on a lot of predictions to dominate) we instead average across the nine assessors? Reassuringly, this makes very little difference:</p>\n<p><img src=\"http://images.lesswrong.com/t3_gbi_0.png?v=420de7577b105fafa5dc23460c41ad87\" alt=\"\" width=\"540\" height=\"252\" /></p>\n<p>What about the Youtopia volunteers? Well, they have a&nbsp;decidedly&nbsp;different picture of Kurzweil's accuracy:</p>\n<p><img src=\"http://images.lesswrong.com/t3_gbi_2.png?v=60a74397fdbd945308dfeb3edcb00f8b\" alt=\"\" width=\"540\" height=\"252\" /></p>\n<p>This gives a combined true score of 30%, and combined false score of 57%! If my own personal assessment was the most positive towards Kurzweil's predictions, then Youtopia's was the most negative.</p>\n<p>Putting this all together, Kurzweil certainly can't claim an accuracy above 50% - a far cry from his own self assessment of either <a style=\"color: #8a8a8b;\" href=\"http://www.acceleratingfuture.com/michael/blog/2010/01/kurzweils-2009-predictions/\">102 out of 108</a> or <a style=\"color: #8a8a8b;\" href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">127 out of 147</a> correct (with caveats that \"even the predictions that were considered 'wrong' in this report were not all wrong\"). And consistently,&nbsp;slightly&nbsp;more than 10% of his predictions are judged \"impossible to decide\".</p>\n<p>As I've said before, these were not binary yes/no predictions - even a true rate of 30% is much higher that than chance. So Kurzweil remains an acceptable prognosticator, with very poor self-assessment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "33BrBRSrRQS4jEHdk": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kbA6T3xpxtko36GgP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 90, "extendedScore": null, "score": 0.000208, "legacy": true, "legacyId": "21150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 90, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kK5rabDsKWMkup7gw", "pimArtGNE2K2A4x58", "edd9mAcMByL2BFNJv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T20:54:26.629Z", "modifiedAt": null, "url": null, "title": "Quantifying wisdom", "slug": "quantifying-wisdom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GHjaWewan2MT8oE62/quantifying-wisdom", "pageUrlRelative": "/posts/GHjaWewan2MT8oE62/quantifying-wisdom", "linkUrl": "https://www.lesswrong.com/posts/GHjaWewan2MT8oE62/quantifying-wisdom", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantifying%20wisdom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantifying%20wisdom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHjaWewan2MT8oE62%2Fquantifying-wisdom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantifying%20wisdom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHjaWewan2MT8oE62%2Fquantifying-wisdom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHjaWewan2MT8oE62%2Fquantifying-wisdom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 584, "htmlBody": "<p>So we know that many smart people make stupid (at least in retrospect) decisions. What these people seem to be lacking, at least at the moment they make a poor decision, is&nbsp;<a title=\"Wisdom is the judicious application of knowledge.\" href=\"http://en.wikipedia.org/wiki/Wisdom\">wisdom</a>&nbsp;(\"judicious application of knowledge\"). More from Wikipedia:</p>\n<p style=\"padding-left: 30px;\">It is a deep understanding and realization of people, things, events or situations, resulting in the ability to apply perceptions, judgements and actions in keeping with this understanding. It often requires control of one's emotional reactions (the \"<a title=\"Passions (philosophy)\" href=\"http://en.wikipedia.org/wiki/Passions_(philosophy)\">passions</a>\") so that universal principles, reason and knowledge prevail to determine one's actions. &nbsp;</p>\n<p>From <a href=\"http://www.psychologytoday.com/basics/wisdom\">Psychology Today</a>:</p>\n<p style=\"padding-left: 30px;\">It can be difficult to define Wisdom, but people generally recognize it when they encounter it. Psychologists pretty much agree it involves an integration of knowledge, experience, and deep understanding that incorporates tolerance for the uncertainties of life as well as its ups and downs. There's an awareness of how things play out over time, and it confers a sense of balance.</p>\n<p style=\"padding-left: 30px;\">Wise people generally share an optimism that life's problems can be solved and experience a certain amount of calm in facing difficult decisions. Intelligence&mdash;if only anyone could figure out exactly what it is&mdash;may be necessary for wisdom, but it definitely isn't sufficient; an ability to see the big picture, a sense of proportion, and considerable introspection also contribute to its development.</p>\n<p>From <a href=\"http://plato.stanford.edu/entries/wisdom/\">SEP</a>:</p>\n<p style=\"padding-left: 30px;\">(1) wisdom as epistemic humility, (2) wisdom as epistemic accuracy, (3) wisdom as knowledge, and (4) wisdom as knowledge and action.</p>\n<p>Clearly, if one created a human-level AI, one would want it to \"choose wisely\". However, as human examples show, wisdom does not come for free with intelligence. Actually, we usually don't trust intelligent people&nbsp;nearly as much as we trust wise ones (or appearing to be wise, at any rate). We don't trust them to make good decisions, because they might be too smart for their own good. Speaking of artificial intelligence, one (informal) quality we'd expect an FAI to have is that of wisdom.</p>\n<p>So, how would one measure wisdom? Converting the above description (\"ability to apply perceptions, judgements and actions in keeping with this understanding\")&nbsp;into a more technical form, one can interpret wisdom, in part, as understanding one's own limitations (\"running on corrupt hardware\", in the local parlance) and calibrating one's actions accordingly. For example, of two people of the same knowledge and intelligence level (as determined by your favorite intelligence test), how do you tell which one is wiser? You look at how the outcomes of their actions measure up against what they predicted. The good news is that you can practice and test your calibration (and, by extension, your wisdom), by playing with the&nbsp;<a href=\"http://predictionbook.com/\">PredictionBook</a>.</p>\n<p>For example, Aaron Swartz was clearly very smart, but was it wise of him to act they way he did, gambling on one big thing after another, without a clear sense of what is likely to happen and at what odds? On the other end of the spectrum, you can often see wise people of average intelligence (or lower) recognizing their limitations and sticking with \"what works\".</p>\n<p>Now, this quantification is clearly not exhaustive. Even when perfectly calibrated, how do you quantify being appropriately cautious when making drastic choices and appropriately bold when making minor ones? What algorithms/decision theories make someone wiser? Bayesianism can surely help, but it relies on decent priors and does not compel one to act. Would someone implementing TDT or UDT to the best of their ability maximize their wisdom for a given intelligence/knowledge level? Is this even a meaningful question to ask?</p>\n<p>EDIT: fixed fonts (hopefully).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GHjaWewan2MT8oE62", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 4, "extendedScore": null, "score": 1.0865470739599808e-06, "legacy": true, "legacyId": "21192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-16T21:31:02.538Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham: Calibration Exercises", "slug": "meetup-durham-calibration-exercises", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BJcDteSXYb7bgKueP/meetup-durham-calibration-exercises", "pageUrlRelative": "/posts/BJcDteSXYb7bgKueP/meetup-durham-calibration-exercises", "linkUrl": "https://www.lesswrong.com/posts/BJcDteSXYb7bgKueP/meetup-durham-calibration-exercises", "postedAtFormatted": "Wednesday, January 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%3A%20Calibration%20Exercises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%3A%20Calibration%20Exercises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJcDteSXYb7bgKueP%2Fmeetup-durham-calibration-exercises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%3A%20Calibration%20Exercises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJcDteSXYb7bgKueP%2Fmeetup-durham-calibration-exercises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJcDteSXYb7bgKueP%2Fmeetup-durham-calibration-exercises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i1'>Durham: Calibration Exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's do some calibration exercises!</p>\n\n<p>I'll talk a bit about what a well-calibrated estimate is, how it differs from an accurate estimate, and why it's useful. Then we'll do some exercises to test the calibration of attendees, and hopefully improve said calibration!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i1'>Durham: Calibration Exercises</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BJcDteSXYb7bgKueP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.086569340680092e-06, "legacy": true, "legacyId": "21193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham__Calibration_Exercises\">Discussion article for the meetup : <a href=\"/meetups/i1\">Durham: Calibration Exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 January 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's do some calibration exercises!</p>\n\n<p>I'll talk a bit about what a well-calibrated estimate is, how it differs from an accurate estimate, and why it's useful. Then we'll do some exercises to test the calibration of attendees, and hopefully improve said calibration!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham__Calibration_Exercises1\">Discussion article for the meetup : <a href=\"/meetups/i1\">Durham: Calibration Exercises</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham: Calibration Exercises", "anchor": "Discussion_article_for_the_meetup___Durham__Calibration_Exercises", "level": 1}, {"title": "Discussion article for the meetup : Durham: Calibration Exercises", "anchor": "Discussion_article_for_the_meetup___Durham__Calibration_Exercises1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-17T00:22:10.628Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup 27th Jan", "slug": "meetup-london-meetup-27th-jan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TQ26n7PqCHnrD7qbt/meetup-london-meetup-27th-jan", "pageUrlRelative": "/posts/TQ26n7PqCHnrD7qbt/meetup-london-meetup-27th-jan", "linkUrl": "https://www.lesswrong.com/posts/TQ26n7PqCHnrD7qbt/meetup-london-meetup-27th-jan", "postedAtFormatted": "Thursday, January 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2027th%20Jan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2027th%20Jan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ26n7PqCHnrD7qbt%2Fmeetup-london-meetup-27th-jan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2027th%20Jan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ26n7PqCHnrD7qbt%2Fmeetup-london-meetup-27th-jan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ26n7PqCHnrD7qbt%2Fmeetup-london-meetup-27th-jan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i2'>London Meetup 27th Jan</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 January 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i2'>London Meetup 27th Jan</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TQ26n7PqCHnrD7qbt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "21194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup_27th_Jan\">Discussion article for the meetup : <a href=\"/meetups/i2\">London Meetup 27th Jan</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 January 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup_27th_Jan1\">Discussion article for the meetup : <a href=\"/meetups/i2\">London Meetup 27th Jan</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup 27th Jan", "anchor": "Discussion_article_for_the_meetup___London_Meetup_27th_Jan", "level": 1}, {"title": "Discussion article for the meetup : London Meetup 27th Jan", "anchor": "Discussion_article_for_the_meetup___London_Meetup_27th_Jan1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-17T04:38:47.751Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Dunbar's Function", "slug": "seq-rerun-dunbar-s-function", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tYmNxuhDrwvDqotS5/seq-rerun-dunbar-s-function", "pageUrlRelative": "/posts/tYmNxuhDrwvDqotS5/seq-rerun-dunbar-s-function", "linkUrl": "https://www.lesswrong.com/posts/tYmNxuhDrwvDqotS5/seq-rerun-dunbar-s-function", "postedAtFormatted": "Thursday, January 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Dunbar's%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Dunbar's%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtYmNxuhDrwvDqotS5%2Fseq-rerun-dunbar-s-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Dunbar's%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtYmNxuhDrwvDqotS5%2Fseq-rerun-dunbar-s-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtYmNxuhDrwvDqotS5%2Fseq-rerun-dunbar-s-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<p>Today's post, <a href=\"/lw/x9/dunbars_function/\">Dunbar's Function</a> was originally published on 31 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Robin Dunbar's original calculation showed that the maximum human group size was around 150. But a typical size for a hunter-gatherer band would be 30-50, cohesive online groups peak at 50-60, and small task forces may peak in internal cohesiveness around 7. Our attempt to live in a world of <em>six billion</em> people has many emotional costs: We aren't likely to know our President or Prime Minister, or to have any significant influence over our country's politics, although we go on behaving as if we did. We are constantly bombarded with news about improbably pretty and wealthy individuals. We aren't likely to find a significant profession where we can be the best in our field. But if intelligence keeps increasing, the number of personal relationships we can track will also increase, along with the natural degree of specialization. Eventually there might be a single community of sentients that really was a single community.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gcd/seq_rerun_amputation_of_destiny/\">Amputation of Destiny</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tYmNxuhDrwvDqotS5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0868296482368773e-06, "legacy": true, "legacyId": "21202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W5PhyEQqEWTcpRpqn", "uHFRac2qtPa5i2qt2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-17T18:17:12.387Z", "modifiedAt": null, "url": null, "title": "Map and territory visual presentation", "slug": "map-and-territory-visual-presentation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cHBfSX8rQ3cKfzHPp/map-and-territory-visual-presentation", "pageUrlRelative": "/posts/cHBfSX8rQ3cKfzHPp/map-and-territory-visual-presentation", "linkUrl": "https://www.lesswrong.com/posts/cHBfSX8rQ3cKfzHPp/map-and-territory-visual-presentation", "postedAtFormatted": "Thursday, January 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Map%20and%20territory%20visual%20presentation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMap%20and%20territory%20visual%20presentation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHBfSX8rQ3cKfzHPp%2Fmap-and-territory-visual-presentation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Map%20and%20territory%20visual%20presentation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHBfSX8rQ3cKfzHPp%2Fmap-and-territory-visual-presentation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHBfSX8rQ3cKfzHPp%2Fmap-and-territory-visual-presentation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p><a href=\"https://docs.google.com/presentation/d/1qEeELWhwWePaNmUOrhaG1D-aXeCfowxtTvQEW8xeHwY/pub?start=false&amp;loop=false&amp;delayms=3000\">Here is a presentation</a> on the map and territory I'm planning on giving to my game theory class.</p>\n<p>&nbsp;</p>\n<p>It's based on Liron's <a href=\"/lw/fc/you_are_a_brain/\">You Are A Brain</a> post.</p>\n<p>&nbsp;</p>\n<p>Any suggestions for improvements?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cHBfSX8rQ3cKfzHPp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "21208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r5H6YCmnn8DMtBtxt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-17T18:21:52.051Z", "modifiedAt": null, "url": null, "title": "Meetup : Ohio LessWrong in Cincinnati", "slug": "meetup-ohio-lesswrong-in-cincinnati", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DjgJ6NdrAYEMQsmyZ/meetup-ohio-lesswrong-in-cincinnati", "pageUrlRelative": "/posts/DjgJ6NdrAYEMQsmyZ/meetup-ohio-lesswrong-in-cincinnati", "linkUrl": "https://www.lesswrong.com/posts/DjgJ6NdrAYEMQsmyZ/meetup-ohio-lesswrong-in-cincinnati", "postedAtFormatted": "Thursday, January 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ohio%20LessWrong%20in%20Cincinnati&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ohio%20LessWrong%20in%20Cincinnati%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjgJ6NdrAYEMQsmyZ%2Fmeetup-ohio-lesswrong-in-cincinnati%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ohio%20LessWrong%20in%20Cincinnati%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjgJ6NdrAYEMQsmyZ%2Fmeetup-ohio-lesswrong-in-cincinnati", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjgJ6NdrAYEMQsmyZ%2Fmeetup-ohio-lesswrong-in-cincinnati", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i3'>Ohio LessWrong in Cincinnati</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 January 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Taft Museum of Art, 316 Pike Street</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at the Taft Museum entrance. Dinner afterwards will include highly-intellectual discussion of the art. Be sure you have something interesting, intelligent, and rational to say! No pressure, of course.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i3'>Ohio LessWrong in Cincinnati</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DjgJ6NdrAYEMQsmyZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0873308356411234e-06, "legacy": true, "legacyId": "21209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ohio_LessWrong_in_Cincinnati\">Discussion article for the meetup : <a href=\"/meetups/i3\">Ohio LessWrong in Cincinnati</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 January 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Taft Museum of Art, 316 Pike Street</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at the Taft Museum entrance. Dinner afterwards will include highly-intellectual discussion of the art. Be sure you have something interesting, intelligent, and rational to say! No pressure, of course.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Ohio_LessWrong_in_Cincinnati1\">Discussion article for the meetup : <a href=\"/meetups/i3\">Ohio LessWrong in Cincinnati</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ohio LessWrong in Cincinnati", "anchor": "Discussion_article_for_the_meetup___Ohio_LessWrong_in_Cincinnati", "level": 1}, {"title": "Discussion article for the meetup : Ohio LessWrong in Cincinnati", "anchor": "Discussion_article_for_the_meetup___Ohio_LessWrong_in_Cincinnati1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-17T18:36:48.373Z", "modifiedAt": null, "url": null, "title": "Update Then Forget", "slug": "update-then-forget", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z9pYKozn6fLmwZrDn/update-then-forget", "pageUrlRelative": "/posts/z9pYKozn6fLmwZrDn/update-then-forget", "linkUrl": "https://www.lesswrong.com/posts/z9pYKozn6fLmwZrDn/update-then-forget", "postedAtFormatted": "Thursday, January 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Update%20Then%20Forget&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpdate%20Then%20Forget%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9pYKozn6fLmwZrDn%2Fupdate-then-forget%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Update%20Then%20Forget%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9pYKozn6fLmwZrDn%2Fupdate-then-forget", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9pYKozn6fLmwZrDn%2Fupdate-then-forget", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1184, "htmlBody": "<p><strong>Followup to:</strong>&nbsp;<a href=\"/r/lesswrong/lw/g6b/how_to_be_oversurprised/\">How to Be Oversurprised</a></p>\n<p><a href=\"/lw/e6a/the_bayesian_agent/\">A Bayesian update</a> needs never lose information. In a dynamic world, though, the update is only half <a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">the story</a>. The other half, where the agent takes an action and predicts its result, may indeed \"lose\" information in some sense.</p>\n<p>We have a dynamical system which consists of an agent and the world around it.&nbsp;It's often useful to <a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">describe the system</a> in discrete time steps, and&nbsp;insightful&nbsp;to split them into half-steps where both parties (agent and world) take turns changing and affecting each other.</p>\n<p>The agent's update is the half-step in which the agent changes. It takes in an observation O<sub>t</sub>&nbsp;of the world (t is for time), and uses it to improve its understanding of the world. In doing so, its internal changeable parts change from some configuration (memory state) M<sub>t-1</sub>&nbsp;that they had before, to a new configuration M<sub>t</sub>.</p>\n<p>The other half-step is when the world changes, and the agent predicts the result of that change. The change from a previous world state W<sub>t</sub>&nbsp;to a new one W<sub>t+1</sub>&nbsp;may depend on the action A<sub>t</sub>&nbsp;that the agent takes.</p>\n<p>In changing itself, the agent cares about information. There's a clear way - <a href=\"http://yudkowsky.net/rational/bayes\">the Bayesian Way</a> - to do it optimally, keeping all of the available information about the world.</p>\n<p>In changing the world, the agent cares about <a href=\"/lw/dz4/reinforcement_preference_and_utility/\">rewards</a>, the one it will get now and the ones to come later, possibly much later. The need to make long-term plans with only partial information about the world makes it <a href=\"/lw/eu1/pointbased_value_iteration/\">very hard to be optimal</a>.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_gcr_0.png?v=e1164d59179d114e0029f447ec25355f\" alt=\"\" width=\"626\" height=\"266\" /></p>\n<p>&nbsp;</p>\n<p><a href=\"/r/lesswrong/lw/g6b/how_to_be_oversurprised/\">Last week</a> we quantified how much information is gained in the update half-step:</p>\n<p style=\"padding-left: 30px;\">I(W<sub>t</sub>;M<sub>t</sub>) - I(W<sub>t</sub>;M<sub>t-1</sub>)</p>\n<p>(where I is the <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">mutual information</a>).&nbsp;Quantifying how much information is discarded in the prediction half-step is complicated by the action: at the same time that the agent predicts the next world state, it also affects it. The agent can have acausal information about the future by virtue of creating it.</p>\n<p>So last week's counterpart</p>\n<p style=\"padding-left: 30px;\">I(W<sub>t</sub>;M<sub>t</sub>) - I(W<sub>t+1</sub>;M<sub>t</sub>)</p>\n<p>is interesting, but not what we want to study here. To understand what kind of information reduction we do mean here, let's put aside the issue of prediction-through-action, and ask: why would the agent lose any information at all when only the world changes, not the agent itself?</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The agent may have some information about the previous world state that simply no longer applies to the changed state. This never happens if the dynamics of the world is <a href=\"http://en.wikipedia.org/wiki/Reversible_dynamics\">reversible</a>, but <a href=\"http://en.wikipedia.org/wiki/Middle_World\">middle world</a> is <a href=\"http://en.wikipedia.org/wiki/Irreversible_process\">irreversible</a> for thermodynamic reasons. Different starting macrostates can lead to the same resulting macrostate.</p>\n<p>Example. We're playing a hand of poker. Your betting and your other actions are part of my observations, and they reveal information about your hidden cards and your personality. If you're making a large bet, your hand seems more likely to be stronger, and you seem to be more loose and aggressive.</p>\n<p>Now there's a showdown. You reveal your hand, collapsing that part of the world state that was hidden from me, and at once a large chunk of information <a href=\"/r/lesswrong/lw/g6b/how_to_be_oversurprised/\">spills over</a> from this observation to your personality. Perhaps your hand is not that strong after all, and I learn that you are more likely than I previously thought to be&nbsp;<em>really</em>&nbsp;loose, and a bluffer to boot.</p>\n<p>And now I'm shuffling. The order of the cards loses any entanglement with the hand we just played. During that last round, I gained perhaps 50 bit of information about the deck, to be used in prediction and decision making. Only a small portion of this information, no more than 2 or 3 bit if we're strangers, had spilled over to reflect on your personality; the other 40-something bit of information is completely useless now, completely irrelevant for the future, and I can safely forget it. This is the information \"lost\" by my action of shuffling.</p>\n<p>Or maybe I'm a card sharp, and I'm stacking the deck instead of truly shuffling it. In this case I can actually <em>gain</em>&nbsp;information about the deck. But even if I replace 50 known bits about the deck with 100 known bits, these aren't the \"same\" bits - they are independent bits. The way I'm loading the deck (if I'm a skilled prestidigitator) has little to do with the way it was before, or with the cards I saw during play, or with what it teaches me of your strategy.</p>\n<p>This is why</p>\n<p style=\"padding-left: 30px;\">I(W<sub>t</sub>;M<sub>t</sub>) - I(W<sub>t+1</sub>;M<sub>t</sub>)</p>\n<p>is not a good measure of how much information about the world an agent discards in the action-prediction half-step. It can actually have more information after the action than before, but still be free to discard some&nbsp;irrelevant&nbsp;information.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>To be clear: the agent is unmodified by the action-prediction half-step. Only the world changes. So the agent doesn't discard information by wiping any bits from memory. Rather, the information content of the agent's memory state simply stops being <em>about the world</em>. In the update that follows, a good agent (say, <a href=\"/r/lesswrong/lw/e6a/the_bayesian_agent/\">a Bayesian one</a>) can - and therefore should - lose that useless information content.</p>\n<p>A better way to measure how much information in M<sub>t</sub>&nbsp;becomes useless is this:</p>\n<p style=\"padding-left: 30px;\">I(W<sub>t</sub>,A<sub>t</sub>;M<sub>t</sub>) - I(W<sub>t+1</sub>;M<sub>t</sub>)</p>\n<p>This is information not just about the world, but also about the action. Once the action is completed, this information is also useless - of course, except for the part of it that is still&nbsp;<em>about the world</em>! I'd hate to forget how I stacked the deck, but only in those cards that are actually in play.</p>\n<p>This nifty trick shows us why information about the world (and the action) must always be lost by the irreversible transition from W<sub>t</sub>&nbsp;and A<sub>t</sub>&nbsp;to W<sub>t+1</sub>. The former pair separates the agent from the latter, such that any information about the next world state must go through what is known about the previous one and the action (see the figure above). Formally, the <a href=\"http://arxiv.org/pdf/1107.0740v2.pdf\">Data Processing Inequality</a> implies that the amount of information lost is nonnegative, since M<sub>t</sub>&nbsp;and W<sub>t+1</sub>&nbsp;are independent given (W<sub>t</sub>,A<sub>t</sub>).</p>\n<p>As a side benefit, we see why we shouldn't bother specifying our actions beyond what is actually <em>about the world</em>. Any information processing that isn't effective in shaping the future is just going to waste.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>When faced with new evidence, an intelligent agent should ideally update on it and then forget it. The update is always about the present. The evidence remains entangled with the past, more and more distant as time goes by. Whatever part of it stops being true must be discarded.&nbsp;(Don't confuse this with the <a href=\"/lw/g26/how_to_disentangle_the_past_and_the_future/\">need to remember things</a>&nbsp;which&nbsp;<em>are</em>&nbsp;part of the present, only currently hidden.)</p>\n<p>People don't do this. <a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">We update too seldom</a>, and instead we remember too much in a wishful hope to update at some later convenience.</p>\n<p>There are many reasons for us to remember the past, given our shortcomings. We can use the actual data we gather to introspect on our faulty reasoning. Stories of the past help us communicate our experiences, allowing others to update on shared evidence much more reliably than if we just tried to convey our current beliefs.</p>\n<p>Optimally, evidence should only be given its due weight in due time, no more and no later. <a href=\"/lw/ik/one_argument_against_an_army/\">Arguments should not be recycled</a>. The study of history should <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">control our anticipation</a>.</p>\n<p><a href=\"/lw/ig/i_defy_the_data/\">The data is not falsifiable</a>, only the conclusions are - relevant to the world, predictive and useful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z9pYKozn6fLmwZrDn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.0873399359420549e-06, "legacy": true, "legacyId": "21195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2AWwdNgvj6Ca3RsjZ", "G4XKiJ2Q93JGCJxCT", "HPAjhrbYk6rPbpSXx", "bPeB6RT78k8dXKYKf", "JyPb4Wpty86tj7Xgn", "yLcuygFfMfrfK8KjF", "rF3GBk9Sgsn75GPWk", "buixYfcXBah9hbSNZ", "WN73eiLQkuDtSC8Ag", "a7n8GdKiAZRX86T5A", "vrHRcEDMjZcx5Yfru"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T00:14:49.866Z", "modifiedAt": null, "url": null, "title": "Outline of Possible Sources of Values", "slug": "outline-of-possible-sources-of-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:27.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uFEu2Y7efZ8CzCD5F/outline-of-possible-sources-of-values", "pageUrlRelative": "/posts/uFEu2Y7efZ8CzCD5F/outline-of-possible-sources-of-values", "linkUrl": "https://www.lesswrong.com/posts/uFEu2Y7efZ8CzCD5F/outline-of-possible-sources-of-values", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outline%20of%20Possible%20Sources%20of%20Values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutline%20of%20Possible%20Sources%20of%20Values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFEu2Y7efZ8CzCD5F%2Foutline-of-possible-sources-of-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outline%20of%20Possible%20Sources%20of%20Values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFEu2Y7efZ8CzCD5F%2Foutline-of-possible-sources-of-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFEu2Y7efZ8CzCD5F%2Foutline-of-possible-sources-of-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 993, "htmlBody": "<p>I don't know what my values are. I don't even know how to find out what my values are. But do I know something about how I (or an <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">FAI</a>) <em>may</em> be able to find out what my values are? Perhaps... and I've organized my answer to this question in the form of an \"Outline of Possible Sources of Values\". I hope it also serves as a summary of the major open problems in this area.</p>\n<ol>\n<li>External<ol>\n<li>god(s)</li>\n<li>other humans</li>\n<li>other agents</li>\n</ol></li>\n<li>Behavioral<ol>\n<li>actual (historical/observed) behavior</li>\n<li>counterfactual (simulated/predicted) behavior</li>\n</ol></li>\n<li>Subconscious Cognition<ol>\n<li>model-based decision making<ol>\n<li>ontology</li>\n<li>heuristics for extrapolating/updating model</li>\n<li>(partial) <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">utility function</a></li>\n</ol></li>\n<li>model-free decision making<ol>\n<li>identity based (adopt a social role like \"environmentalist\" or \"academic\" and emulate an appropriate role model, actual or idealized)</li>\n<li>habits</li>\n<li>reinforcement based</li>\n</ol></li>\n</ol></li>\n<li>Conscious Cognition<ol>\n<li>decision making using explicit verbal and/or quantitative reasoning<ol>\n<li>consequentialist (similar to model-based above, but using <a href=\"/lw/2yp/making_your_explicit_reasoning_trustworthy/\">explicit reasoning</a>)</li>\n<li>deontological</li>\n<li>virtue ethical</li>\n<li>identity based</li>\n</ol></li>\n<li>reasoning about terminal goals/values/preferences/moral principles<ol>\n<li>responses (changes in state) to moral arguments (possibly context dependent)</li>\n<li>distributions of autonomously generated moral arguments (possibly context dependent)</li>\n<li>logical structure (if any) of moral reasoning</li>\n</ol></li>\n<li>object-level intuitions/judgments<ol>\n<li>about what one should do in particular ethical situations</li>\n<li>about the desirabilities of particular outcomes</li>\n<li>about moral principles</li>\n</ol></li>\n<li>meta-level&nbsp;intuitions/judgments<ol>\n<li>about the nature of morality</li>\n<li>about the <a href=\"/lw/6us/whats_wrong_with_simplicity_of_value/\">complexity of values</a></li>\n<li>about what the valid sources of values are</li>\n<li>about what constitutes correct moral reasoning</li>\n<li>about how to explicitly/formally/effectively represent values (utility function, multiple utility functions, deontological rules, or something else) (if utility function(s), for what decision theory and ontology?)</li>\n<li>about how to&nbsp;extract/translate/combine sources of values into a representation of values<ol>\n<li>how to solve <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crisis</a></li>\n<li>how to deal with native utility function or revealed preferences being partial</li>\n<li>how to translate non-consequentialist sources of values into utility function(s)</li>\n<li>how to deal with moral principles being vague and incomplete</li>\n<li>how to deal with conflicts between different sources of values</li>\n<li>how to deal with <a href=\"http://wiki.lesswrong.com/wiki/Moral_uncertainty\">lack of certainty</a> in one's intuitions/judgments</li>\n</ol></li>\n<li>whose intuition/judgment ought to be applied? (may be different for each of the above)<ol>\n<li>the subject's (at what point in time? current intuitions, eventual judgments, or something in between?)</li>\n<li>the FAI designers'</li>\n<li>the FAI's own philosophical conclusions</li>\n</ol></li>\n</ol></li>\n</ol></li>\n</ol>\n<p>Using this outline, we can obtain a concise understanding of what many metaethical theories and FAI proposals are claiming/suggesting and how they differ from each other. For example, Nyan_Sandwich's \"<a href=\"/lw/g7y/morality_is_awesome/\">morality is awesome</a>\" thesis can be interpreted as the claim that the most important source of values is our intuitions about the desirability (awesomeness) of&nbsp;particular outcomes.</p>\n<p>As another example, Aaron Swartz argued <a href=\"http://www.aaronsw.com/weblog/ethicsfor\">against \"reflective equilibrium\"</a> by which he meant the claim that the valid sources of values are our object-level moral intuitions, and that correct moral reasoning consists of working back and forth between these intuitions until they reach coherence. His own position was that intuitions about moral principles are the only valid source of values and we should discount our intuitions about particular ethical situations.</p>\n<p>A final example is Paul Christiano's \"<a href=\"/lw/c0k/formalizing_value_extrapolation/\">Indirect Normativity</a>\"&nbsp;proposal&nbsp;(n.b., \"Indirect Normativity\" was originally coined by Nick Bostrom to refer to an entire class of designs where the AI's values are defined \"indirectly\") for FAI, where an important source of values is the&nbsp;distribution&nbsp;of&nbsp;moral arguments the subject is likely to generate in a particular simulated environment and their responses to those arguments. Also, just about every meta-level question is left for the (simulated) subject to answer, except for the decision theory and ontology of the utility function that their values must finally be encoded in, which is fixed by the FAI designer.</p>\n<p>I think the outline includes most of the ideas brought up in past LW discussions, or in moral philosophies that I'm familiar with. Please let me know if I left out anything important.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uFEu2Y7efZ8CzCD5F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "21207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fa5o2tg9EfJE77jEQ", "m5AH78nscsGjMbBwv", "4xWz3wW2JNfup6By6", "Aq8BQMXRZX3BoFd4c", "9DWcNS2rkvd2J8mHH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T01:21:43.206Z", "modifiedAt": null, "url": null, "title": "Generalizing from One Trend", "slug": "generalizing-from-one-trend", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5wg3FRBie7BEFujQK/generalizing-from-one-trend", "pageUrlRelative": "/posts/5wg3FRBie7BEFujQK/generalizing-from-one-trend", "linkUrl": "https://www.lesswrong.com/posts/5wg3FRBie7BEFujQK/generalizing-from-one-trend", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Generalizing%20from%20One%20Trend&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneralizing%20from%20One%20Trend%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wg3FRBie7BEFujQK%2Fgeneralizing-from-one-trend%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Generalizing%20from%20One%20Trend%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wg3FRBie7BEFujQK%2Fgeneralizing-from-one-trend", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wg3FRBie7BEFujQK%2Fgeneralizing-from-one-trend", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 964, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable/ \">Reference Class of the Unclassreferenceable</a>, <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p>\n<p>Many people try to predict the future. Few succeed.</p>\n<p>One common mistake made in predicting the future is to simply take a current trend and extrapolate it forward, as if it was the only thing that mattered-- think, for instance, of the future described by cyberpunk fiction, with sinister (and often Japanese) multinational corporations ruling the world. Where does this vision of the future stem from?</p>\n<p>Bad or lazy predictions from the 1980s, when sinister multinational corporations (and often Japanese ones) looked to be taking over the world.<sup>[1]</sup></p>\n<p>Similar errors have been committed by writers throughout history. George Orwell thought <em>1984</em> was an accurate prediction of the future, seeing World War II as inevitably bringing socialist revolution to the United Kingdom and predicting that the revolutionary ideals would then be betrayed in England as they were in Russia.<sup> </sup>Aldous Huxley agreed with Orwell but thought that the advent of hypnosis and psychoconditioning would cause the dystopia portrayed in <em>1984</em>&nbsp;to evolve into that he described in&nbsp;<em>Brave New World. </em>In today's high school English classes, these books are taught as literature, as well-written stories-- the fact that the authors <a href=\"/lw/2l6/taking_ideas_seriously/\">took their ideas seriously</a>&nbsp;would come as a surprise to many high school students, and their predictions would look laughably wrong.</p>\n<p>Were such mistakes confined solely to the realm of fiction, they would perhaps be considered amusing errors at best, reflective of the sorts of mishaps that befall unstudied predictions. Unfortunately, they are not. Purported \"experts\" make just the same sort of error regularly, and failed predictions of this sort often have negative consequences in reality.</p>\n<p>For instance, in 1999 two economists published the book <em><a href=\"http://en.wikipedia.org/wiki/Dow_36,000\">Dow 36,000</a>, </em>predicting&nbsp;that stocks were about to reach record levels; the authors of the book were so wrapped up in recent gains to the stock market that they assumed that such gains were in fact the new normal state of affairs, that the market hadn't corrected for this yet, and that once stocks were correctly perceived as safe investments the market would skyrocket.&nbsp;<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">This not only did not happen, but the dot-com bubble burst shortly after the book was published.<sup>[2] </sup>Anyone following the market advice from this book lost big.<br /></span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">In 1968, the biologist Paul R. Ehrlich, seeing disturbing trends in world population growth, wrote a book called <em>The Population Bomb,</em> in which he forecast (among other things) that \"</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">The battle to feed all of humanity is over. In the 1970s hundreds of millions of people will starve to death in spite of any crash programs embarked upon now.\" Later, Ehrlich doubled down on this prediction with claims such as&nbsp;</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">&nbsp;\"By the year 2000 the United Kingdom</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">&nbsp;will be simply a small group of impoverished islands, inhabited by some 70 million hungry people&nbsp;... If I were a gambler, I would take even money that England&nbsp;</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">will not exist in the year 2000.\"</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">Based on these predictions, Ehrlich advocated cutting off food aid to India and Egypt in favor of preserving food supplies for nations that were not \"lost causes;\" luckily, his policies were not adopted, as they would have resulted in mass starvation in the countries suddenly deprived of aid. Instead, food aid continued, and as population grew, food production did as well. Contrary to the increase in starvation and global death rates predicted by Ehrlich, global death rates decreased, the population increased by more than Ehrlich had predicted would lead to disaster, and the average amount of calories consumed per person increased as well.<sup>[3]</sup></span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">So what, then, is the weakness that causes these analysts to make such errors?</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">Well, just as you can <a href=\"/lw/dr/generalizing_from_one_example/\">generalize from one example</a> when evaluating others and hence fail to understand those around you, you can generalize from one trend or set of trends when making predictions and hence fail to understand the broader world. This is a special case of the classic problem where \"to a man with a hammer, everything looks like a nail;\" if you are very familiar with one trend, and that's all you take into account with your future forecasts, you're bound to be wrong if that trend ends up <em>not </em>eating the world.</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">On the other hand,&nbsp;</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">the trend sometimes&nbsp;<em>does </em>eat the world. It's very easy to find long lists of buffoonish predictions where someone woefully understimated the impact of a new technology.<sup>[4]&nbsp;</sup></span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">Further, determining exactly when and where a trend is going to stop is quite difficult, and most people are incompetent at it, even at a professional level-- if this were easy, the stock market would look extraordinarily different!<br /></span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">So my advice to those who would predict the future is simple. Don't generalize from one trend or even one group of trends. Especially beware of viewing evidence that seems to support your predictions as evidence that other people's predictions must be wrong-- <a href=\"/lw/h1/the_scales_of_justice_the_notebook_of_rationality\">the notebook of rationality</a> <a href=\"/lw/gt/a_fable_of_science_and_politics/\">cares not for what \"side\" things are on</a>, but rather for <a href=\"http://wiki.lesswrong.com/wiki/Truth\">what is true</a>. E</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">ven if the trend you're relying on does end up being the \"next big thing,\" the rest of the world will have a voice as well.<sup>[5]</sup></span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\"><br /></span></p>\n<p>[1]&nbsp;I predict that the work of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Cory_Doctorow\">Cory Doctorow</a>&nbsp;and those like him will seem similarly dated a decade down the line, as the trends they're riding die down.&nbsp;If you're reading this during or after December 2022, please let me know what you think of this prediction.</p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">[2] The authors are, of course, still employed in cushy think-tank positions.</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">[3]</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">&nbsp; Ehrlich has doubled down on his statements, now claiming that he was \"way too optimistic\" in <em>The Population Bomb </em>and that the world is obviously doomed.</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">[4] I personally enjoy the&nbsp;<a href=\"http://theweek.com/home/bad_opinions\">Bad Opinion Generator</a> (warning: potentially addictive)</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.200000762939453px;\">[5] Technically, this <a href=\"/\">isn't always true</a>. But you should assume it is unless you have <em>extremely </em>good reasons to believe otherwise, and even still I would be very careful before assuming that your thing is <em>the</em>&nbsp;thing.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5wg3FRBie7BEFujQK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 1.0875866510990469e-06, "legacy": true, "legacyId": "20421", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yKRTxAohwmqLcbbJi", "baTWMegR42PAsH9qJ", "Q8jyAdRYbieK8PtfT", "XYCEB9roxEBfgjfxs", "6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T01:23:19.205Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.003Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CrDCzcJp7RWgt5QbB/meetup-melbourne-practical-rationality-4", "pageUrlRelative": "/posts/CrDCzcJp7RWgt5QbB/meetup-melbourne-practical-rationality-4", "linkUrl": "https://www.lesswrong.com/posts/CrDCzcJp7RWgt5QbB/meetup-melbourne-practical-rationality-4", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrDCzcJp7RWgt5QbB%2Fmeetup-melbourne-practical-rationality-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrDCzcJp7RWgt5QbB%2Fmeetup-melbourne-practical-rationality-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrDCzcJp7RWgt5QbB%2Fmeetup-melbourne-practical-rationality-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i4'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 February 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a>\n(we'll discuss an organised activity for the night here, so jump in)</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i4'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CrDCzcJp7RWgt5QbB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0875876261396825e-06, "legacy": true, "legacyId": "21212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/i4\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 February 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a>\n(we'll discuss an organised activity for the night here, so jump in)</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/i4\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T05:53:03.979Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] A New Day", "slug": "seq-rerun-a-new-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p7LcTL9xvPPRSEDJb/seq-rerun-a-new-day", "pageUrlRelative": "/posts/p7LcTL9xvPPRSEDJb/seq-rerun-a-new-day", "linkUrl": "https://www.lesswrong.com/posts/p7LcTL9xvPPRSEDJb/seq-rerun-a-new-day", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20A%20New%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20A%20New%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7LcTL9xvPPRSEDJb%2Fseq-rerun-a-new-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20A%20New%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7LcTL9xvPPRSEDJb%2Fseq-rerun-a-new-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp7LcTL9xvPPRSEDJb%2Fseq-rerun-a-new-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>Today's post, <a href=\"/lw/xa/a_new_day/\">A New Day</a> was originally published on 31 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#A_New_Day\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Try spending a day doing as many new things as possible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gcy/seq_rerun_dunbars_function/\">Dunbar's Function</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p7LcTL9xvPPRSEDJb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0877520384227928e-06, "legacy": true, "legacyId": "21220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ywZs3bAAnubux2REg", "tYmNxuhDrwvDqotS5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T08:19:48.395Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin social meetup", "slug": "meetup-berlin-social-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eZ9oNbNPJcTmLXijQ/meetup-berlin-social-meetup", "pageUrlRelative": "/posts/eZ9oNbNPJcTmLXijQ/meetup-berlin-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/eZ9oNbNPJcTmLXijQ/meetup-berlin-social-meetup", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZ9oNbNPJcTmLXijQ%2Fmeetup-berlin-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZ9oNbNPJcTmLXijQ%2Fmeetup-berlin-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZ9oNbNPJcTmLXijQ%2Fmeetup-berlin-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i5'>Berlin social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2013 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">S Wuhletal, Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at my house for a social meetup on Jan 26th. Everyone is welcome. Please check the <a href=\"https://groups.google.com/d/topic/lw-berlin/2ijEO8ka9RU/discussion\" rel=\"nofollow\">thread on our mailing list</a> for details and the precise location.</p>\n\n<p>This is an informal event where we plan to play Go, Resistance, maybe other boardgames and chat. Some people have volunteered to bring food and you're welcome to participate (organize on mailing list). I expect we'll have food delivered for dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i5'>Berlin social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eZ9oNbNPJcTmLXijQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0878414961424787e-06, "legacy": true, "legacyId": "21223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/i5\">Berlin social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2013 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">S Wuhletal, Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at my house for a social meetup on Jan 26th. Everyone is welcome. Please check the <a href=\"https://groups.google.com/d/topic/lw-berlin/2ijEO8ka9RU/discussion\" rel=\"nofollow\">thread on our mailing list</a> for details and the precise location.</p>\n\n<p>This is an informal event where we plan to play Go, Resistance, maybe other boardgames and chat. Some people have volunteered to bring food and you're welcome to participate (organize on mailing list). I expect we'll have food delivered for dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/i5\">Berlin social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin social meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin social meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T13:43:01.031Z", "modifiedAt": null, "url": null, "title": "My simple hack for increased alertness and improved cognitive functioning: very bright light", "slug": "my-simple-hack-for-increased-alertness-and-improved", "viewCount": null, "lastCommentedAt": "2017-09-26T03:22:51.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ag7oQifJQM5AnMCrR/my-simple-hack-for-increased-alertness-and-improved", "pageUrlRelative": "/posts/Ag7oQifJQM5AnMCrR/my-simple-hack-for-increased-alertness-and-improved", "linkUrl": "https://www.lesswrong.com/posts/Ag7oQifJQM5AnMCrR/my-simple-hack-for-increased-alertness-and-improved", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20simple%20hack%20for%20increased%20alertness%20and%20improved%20cognitive%20functioning%3A%20very%20bright%20light&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20simple%20hack%20for%20increased%20alertness%20and%20improved%20cognitive%20functioning%3A%20very%20bright%20light%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7oQifJQM5AnMCrR%2Fmy-simple-hack-for-increased-alertness-and-improved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20simple%20hack%20for%20increased%20alertness%20and%20improved%20cognitive%20functioning%3A%20very%20bright%20light%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7oQifJQM5AnMCrR%2Fmy-simple-hack-for-increased-alertness-and-improved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7oQifJQM5AnMCrR%2Fmy-simple-hack-for-increased-alertness-and-improved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>This is a simple idea that I came up with by myself. I was looking for a means to enter high functioning lots-of-beta-waves modes without the use of chemical stimulants. What I found was that very bright light works really, really well.</p>\n<p>I got the <a href=\"http://en.wikipedia.org/wiki/File:Wolfram-Halogengl%C3%BChlampe.png\">brightest light bulbs I could get cheaply</a>. 105 watts of incandescents with halogen gas, billed as the equivalent of 130 watts of incandescent light. And I got an adaptor like <a href=\"http://www.enjoyyourcamera.com/images/product_images/popup_images/31513650_0.jpg\">this</a> that lets me screw four of those into the same socket in the ceiling. The result is about as painful to look at as the sun. It makes my (small) room brighter than a clear summer's day at my latitude and slightly brighter than a supermarket.</p>\n<p>I guess it affects adenosine much like caffeine does because that's what it feels like. Yet unlike caffeine, it can be rapidly turned on and off, literally with the flip of a switch.</p>\n<p>For waking up in the morning, I find bright light more effective than a 200mg caffeine tablet, although my caffeine tolerance is moderate for a scientist.</p>\n<p>I have not compared the effects of very bright light to modafinil, which requires a prescription in my country.</p>\n<p>When under this amount of light, I need to remind myself to go to bed, because I tire about three hours later than with common luminosity. Yet once I switch it off, I can usually sleep within a few minutes, as (I'm guessing) a flood of unblocked adenosine suddenly overwhelms me. I used to have those unproductive late hours where I was too awake to sleep but too tired to be smart. I don't have those anymore.</p>\n<p>You've probably heard of <a href=\"http://en.wikipedia.org/wiki/Light_therapy\">light therapy</a>, which uses light to help manage seasonal affective disorder. I don't have that issue, but I definitely notice that the light does improve my mood. (Maybe that's simply because I like to function well.) I'm pretty sure the expensive \"light therapy bulbs\" you can get are scams, because the color of the light doesn't actually make a difference. The amount of light does.</p>\n<p>One nice side benefit is that it keeps me awake while meditating, so I don't need the upright posture that usually does that job. Without the need for an upright posture, I can go beyond two hours straight, which helps enter more profoundly altered states.</p>\n<p>After about 10 months of almost daily use of this lighting, I have not noticed any decrease in effectiveness. I do notice I find normally-lit rooms comparatively gloomy, and have an increasingly hard time understanding why people tolerate that. Supermarkets and offices are brightly lit to make the rats move faster - why don't we do that at our homes and while we're at it, amp it up even further? After all, our brains were made for the African savanna, which during the day is a lot brighter than most apartments today.</p>\n<p>Since everyone can try this for a few bucks, I hope some of you will. If you do, please provide feedback on whether it works as well for you as it does for me. Any questions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XqykXFKL9t38pbSEm": 1, "Tg9aFPFCPBHxGABRr": 1, "fkABsGCJZ6y9qConW": 1, "PCLuivxECxsC3aNgy": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ag7oQifJQM5AnMCrR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 86, "extendedScore": null, "score": 0.000206, "legacy": true, "legacyId": "21225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 130, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T14:57:15.907Z", "modifiedAt": null, "url": null, "title": "Banish the Clippy-creating Bias Demon!", "slug": "banish-the-clippy-creating-bias-demon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P2ggjdWv67uTzJ28z/banish-the-clippy-creating-bias-demon", "pageUrlRelative": "/posts/P2ggjdWv67uTzJ28z/banish-the-clippy-creating-bias-demon", "linkUrl": "https://www.lesswrong.com/posts/P2ggjdWv67uTzJ28z/banish-the-clippy-creating-bias-demon", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Banish%20the%20Clippy-creating%20Bias%20Demon!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABanish%20the%20Clippy-creating%20Bias%20Demon!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2ggjdWv67uTzJ28z%2Fbanish-the-clippy-creating-bias-demon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Banish%20the%20Clippy-creating%20Bias%20Demon!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2ggjdWv67uTzJ28z%2Fbanish-the-clippy-creating-bias-demon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2ggjdWv67uTzJ28z%2Fbanish-the-clippy-creating-bias-demon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>I <a href=\"http://blog.practicalethics.ox.ac.uk/2013/01/invoking-and-banishing-the-dread-demon-lead/\">posted</a> in Practical Ethics, arguing that if we mentally anthropomorphised certain risks, then we'd be more likely to give them the attention they <a href=\"http://en.wikipedia.org/wiki/List_of_causes_of_death_by_rate\">deserved</a>. Slaying the Cardiovascular Vampire, defeating the Parasitic Diseases Death Cult, and banishing the Demon of Infection... these stories give a mental picture of the actual good we're doing when combating these issues, and the bad we're doing by ignoring them. Imagine a politician proclaiming:</p>\n<ul>\n<li>I will not let the Cardiovascular Vampire continue his unrelenting war upon the American people, slaying over a third of our citizens - the eldest, in their weakened state, among his most numerous victims. There is no negotiating with such a terrorist - I will direct the full resources of the state to crushing his campaign of destruction.</li>\n</ul>\n<p>An amusing thing to contemplate - except, of course, if there were a real Cardiovascular Vampire, politicians and pundits would be falling over themselves with those kinds of announcements.</p>\n<p>The field of AI is already over-saturated with&nbsp;anthropomorphisation, so we&nbsp;definitely&nbsp;shouldn't be imagining Clippy as some human-like entity that we can heroically combat, with all the rules of narrative applying. Still it can't hurt to dream up a hideous Bias Demon in its mishaped (though superficially plausible) lair, cackling in glee as someone foolishly attempts to implement an AI design without the proper safety precautions, smiling serenely as prominent futurist dismiss the risk... and&nbsp;dissolving, hit by the holy water of increased rationality and proper AI research. Those images might help us make the right emotional connection to what we're achieving here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P2ggjdWv67uTzJ28z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 17, "extendedScore": null, "score": 1.0880838655454249e-06, "legacy": true, "legacyId": "21227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T14:59:23.620Z", "modifiedAt": null, "url": null, "title": "TIL in Medical School - Doctors have myths too.", "slug": "til-in-medical-school-doctors-have-myths-too", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y6yszuG3LTu7bA46G/til-in-medical-school-doctors-have-myths-too", "pageUrlRelative": "/posts/Y6yszuG3LTu7bA46G/til-in-medical-school-doctors-have-myths-too", "linkUrl": "https://www.lesswrong.com/posts/Y6yszuG3LTu7bA46G/til-in-medical-school-doctors-have-myths-too", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TIL%20in%20Medical%20School%20-%20Doctors%20have%20myths%20too.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATIL%20in%20Medical%20School%20-%20Doctors%20have%20myths%20too.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6yszuG3LTu7bA46G%2Ftil-in-medical-school-doctors-have-myths-too%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TIL%20in%20Medical%20School%20-%20Doctors%20have%20myths%20too.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6yszuG3LTu7bA46G%2Ftil-in-medical-school-doctors-have-myths-too", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6yszuG3LTu7bA46G%2Ftil-in-medical-school-doctors-have-myths-too", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<p>Today I Learned in Medical School:</p>\n<p>Doctors have medical myths too! According to my prof, many doctors believe that aspiration (having stuff go down into the lungs) causes anaerobic pneumonia, but that is rarely the case. He says that myth is often taught resident-to-student, but it isn&rsquo;t actually backed up by any research, and isn&rsquo;t true. The kicker - if the doctor would stop to think about it, it should jump out as unintuitive &ndash; it would take some serious changes inside the *lung* to make an *anaerobic* infection &ndash; an infection of bacteria that thrive in areas with no oxygen. In reality it takes frequent aspirations over a long period of time to block off an area of the lungs.</p>\n<p>I think the moral of this story (though this just may be preaching to the choir here at LW) &ndash; all people, be they doctors or kindergarteners, don&rsquo;t usually check facts they&rsquo;re taught, especially when being taught by an authoritative teacher. Unless they&rsquo;re lead to discover/derive a fact themselves, they usually assimilate it into their network of beliefs as a brute fact &ndash; &ldquo;carbon has four valence electrons,&rdquo; &ldquo;don&rsquo;t end a sentence with a preposition,&rdquo; &ldquo;in 1492 Columbus discovered America.&rdquo;</p>\n<p>Now, you frequently don&rsquo;t have enough time to &ldquo;learn it the hard way&rdquo; or derive an answer yourself. If I had to read every single research publication that populated the facts in my textbooks, I might not ever graduate. However, it is important to remember that you&rsquo;ve taken shortcuts for most of your education (and religion/lack thereof, and life in general) &ndash; and if some fact ever later strikes you as being odd, look into it. Otherwise, we&rsquo;re just playing the telephone game.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y6yszuG3LTu7bA46G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 1, "extendedScore": null, "score": 1.088085163768111e-06, "legacy": true, "legacyId": "21228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}