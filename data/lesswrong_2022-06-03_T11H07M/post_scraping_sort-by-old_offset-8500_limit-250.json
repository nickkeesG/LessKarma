{"results": [{"createdAt": null, "postedAt": "2013-02-26T16:30:10.903Z", "modifiedAt": null, "url": null, "title": "Self-assessment in expert AI predictions", "slug": "self-assessment-in-expert-ai-predictions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KovmjL7fKyCuBtzgd/self-assessment-in-expert-ai-predictions", "pageUrlRelative": "/posts/KovmjL7fKyCuBtzgd/self-assessment-in-expert-ai-predictions", "linkUrl": "https://www.lesswrong.com/posts/KovmjL7fKyCuBtzgd/self-assessment-in-expert-ai-predictions", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-assessment%20in%20expert%20AI%20predictions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-assessment%20in%20expert%20AI%20predictions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKovmjL7fKyCuBtzgd%2Fself-assessment-in-expert-ai-predictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-assessment%20in%20expert%20AI%20predictions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKovmjL7fKyCuBtzgd%2Fself-assessment-in-expert-ai-predictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKovmjL7fKyCuBtzgd%2Fself-assessment-in-expert-ai-predictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p><em style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This brief post is written on behalf of <a href=\"/user/Kaj_Sotala/overview/\">Kaj Sotala</a>, due to deadline issues.</em></p>\n<p><span style=\"font-family: arial, sans-serif; color: #222222;\"><em style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"></em>The results of our prior analysis suggested that there was <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">little difference between experts and non-experts</a> in terms of predictive accuracy. There were <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/77dv\">suggestions</a>, though, that predictions published by self-selected experts would be different from those elicited from less selected groups, e.g. surveys at conferences.</span></p>\n<p><span style=\"font-family: arial, sans-serif; color: #222222;\">We have no real data to confirm this, but a single datapoint suggests the idea might be worth taking seriously. </span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Michie</span><span style=\"font-family: arial, sans-serif; color: #222222;\">&nbsp;<a href=\"http://www.nature.com/nature/journal/v241/n5391/pdf/241507a0.pdf\">conducted</a>&nbsp;an opinion poll of experts working in or around AI&nbsp;in 1973. The various experts </span><span style=\"font-size: 13px; color: #222222; font-family: arial, sans-serif;\">predicted adult-level human AI in:</span></p>\n<ul>\n<li><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">5 years: 0 experts</span></li>\n<li><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">10 years: 1 expert</span></li>\n<li><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">20 years: 16 experts</span></li>\n<li><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">50 years: 20 experts</span></li>\n<li><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">More than 50 years: 26 experts</span></li>\n</ul>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">On a quick visual inspection, these results look quite different from the distribution in the rest of the database giving a much more pessimistic prediction than the more self-selected experts:</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><img src=\"http://images.lesswrong.com/t3_gta_0.png?v=73322cd8c76e8f78cede37457c2d3534\" alt=\"\" width=\"691\" height=\"364\" /><br /></span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">But that could be an artifact from the way that the graph on page 12 breaks the predictions down to 5 year intervals while Michie breaks them down into intervals of 10, 20, 50, and 50+ years. Yet there seems to remain a clear difference once we group the predictions in a similar way [1]:</span></p>\n<p><img src=\"http://images.lesswrong.com/t3_gta_1.png?v=1d152f6007c571bfc3ca8e12ecf453f3\" alt=\"\" width=\"702\" height=\"365\" /></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This provides some support for the argument that \"the mainstream of expert opinion is reliably more pessimistic than the self-selected predictions that we keep hearing about\".</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">[1] Assigning each prediction to the closest category, so predictions of &lt;7&frac12; get assigned to 5, 7&frac12;&lt;=X&lt;15 get assigned to 10, 15&lt;=X&lt;35 get assigned to 20, 35&lt;=X&lt;50 get assigned to 50, and 50&lt; get assigned to over fifty.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KovmjL7fKyCuBtzgd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 22, "extendedScore": null, "score": 1.1233767836641349e-06, "legacy": true, "legacyId": "21790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["47ci9ixyEbGKWENwR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T17:20:57.126Z", "modifiedAt": null, "url": null, "title": "Givewell Survey - Opportunity to influence their research", "slug": "givewell-survey-opportunity-to-influence-their-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9eFoewq3boD8Z2ciL/givewell-survey-opportunity-to-influence-their-research", "pageUrlRelative": "/posts/9eFoewq3boD8Z2ciL/givewell-survey-opportunity-to-influence-their-research", "linkUrl": "https://www.lesswrong.com/posts/9eFoewq3boD8Z2ciL/givewell-survey-opportunity-to-influence-their-research", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Givewell%20Survey%20-%20Opportunity%20to%20influence%20their%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGivewell%20Survey%20-%20Opportunity%20to%20influence%20their%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9eFoewq3boD8Z2ciL%2Fgivewell-survey-opportunity-to-influence-their-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Givewell%20Survey%20-%20Opportunity%20to%20influence%20their%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9eFoewq3boD8Z2ciL%2Fgivewell-survey-opportunity-to-influence-their-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9eFoewq3boD8Z2ciL%2Fgivewell-survey-opportunity-to-influence-their-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><a href=\"http://blog.givewell.org/\">Givewell's blog</a> has recently begun a series of 5 self-evaluation posts (they are on the 4th right now) which discuss where the organization is at and where they're going. They're all worth a read. In particular, they build up to a <a href=\"http://www.givewell.org/feedback-on-future-research\">survey for Givewell followers</a> about how you'd like the organization to direct their research in the future, with options to emphasize existential risk and research even if the evidence is lower quality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9eFoewq3boD8Z2ciL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "21792", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T17:59:20.474Z", "modifiedAt": null, "url": null, "title": "How much do you value your current identity vs. your life?", "slug": "how-much-do-you-value-your-current-identity-vs-your-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6J7wS8bzYYmcxH9pf/how-much-do-you-value-your-current-identity-vs-your-life", "pageUrlRelative": "/posts/6J7wS8bzYYmcxH9pf/how-much-do-you-value-your-current-identity-vs-your-life", "linkUrl": "https://www.lesswrong.com/posts/6J7wS8bzYYmcxH9pf/how-much-do-you-value-your-current-identity-vs-your-life", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20much%20do%20you%20value%20your%20current%20identity%20vs.%20your%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20much%20do%20you%20value%20your%20current%20identity%20vs.%20your%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6J7wS8bzYYmcxH9pf%2Fhow-much-do-you-value-your-current-identity-vs-your-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20much%20do%20you%20value%20your%20current%20identity%20vs.%20your%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6J7wS8bzYYmcxH9pf%2Fhow-much-do-you-value-your-current-identity-vs-your-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6J7wS8bzYYmcxH9pf%2Fhow-much-do-you-value-your-current-identity-vs-your-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>I was reading about the effectiveness of bicycle helmet laws (<a href=\"http://www.nber.org/papers/w18773\">here</a>) and wondered how worthwhile it is to save your life at the expense of some aspect key to your current identity (Note that the paper linked doesn't say that this is the situation; this was just a tangential thought).</p>\n<p>Let's say that I perform some activity that carries a 10% chance that I will die but otherwise carries no risk of injury. There is some piece of safety gear that I can wear that cuts that risk in half, but for some reason adds a 10% chance that I will be permanently brain damaged such that I will not be \"me\" as I understand it now. Should I rate this as 15% fatal with the safety gear or is there some other way that this should be evaluated?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6J7wS8bzYYmcxH9pf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.1234343257868189e-06, "legacy": true, "legacyId": "21793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T18:01:40.545Z", "modifiedAt": null, "url": null, "title": "Exponent of Desire", "slug": "exponent-of-desire", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.238Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vohwa2bJ4LQ5S22uc/exponent-of-desire", "pageUrlRelative": "/posts/vohwa2bJ4LQ5S22uc/exponent-of-desire", "linkUrl": "https://www.lesswrong.com/posts/vohwa2bJ4LQ5S22uc/exponent-of-desire", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exponent%20of%20Desire&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExponent%20of%20Desire%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvohwa2bJ4LQ5S22uc%2Fexponent-of-desire%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exponent%20of%20Desire%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvohwa2bJ4LQ5S22uc%2Fexponent-of-desire", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvohwa2bJ4LQ5S22uc%2Fexponent-of-desire", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>I've been mostly lying in bed with fever for the last couple of days, and one night my starved for external stimuli semi-conscious mind produced the following mathematical construct, which I decided to share. This is not intended to be scientific or even all that serious.</p>\n<p>So, suppose you have something. Let's call it 's'. You like it, so you want to keep having it. This is a first-order want, let's call it w(s). You also want to want to have it, which is a second order want: w(w(s)), or w<sup>2</sup>(s). If you are perfectly content, this will be true for all higher order wants, as well, w<sup>n</sup>(s). Now, you don't worry nearly as much about higher orders, so let's discount their contribution to your thoughts and feelings by the factor n!. Finally, the sum total of your wants for s is</p>\n<p>(1+w+w<sup>2</sup>/2!+...w<sup>n</sup>/n!+...)(s)=e<sup>w</sup>(s).</p>\n<p>This is, of course, the standard way to construct functions of linear operators.</p>\n<p>So, if you love someone wholeheartedly and without reservation, you can call them the exponent of your desire. Hopefully they are geeky enough to appreciate it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vohwa2bJ4LQ5S22uc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 12, "extendedScore": null, "score": 1.1234358324955565e-06, "legacy": true, "legacyId": "21789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T22:47:23.219Z", "modifiedAt": null, "url": null, "title": "[Link] Is the Endowment Effect Real?", "slug": "link-is-the-endowment-effect-real", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rvDzx2HKiotEtbov6/link-is-the-endowment-effect-real", "pageUrlRelative": "/posts/rvDzx2HKiotEtbov6/link-is-the-endowment-effect-real", "linkUrl": "https://www.lesswrong.com/posts/rvDzx2HKiotEtbov6/link-is-the-endowment-effect-real", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Is%20the%20Endowment%20Effect%20Real%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Is%20the%20Endowment%20Effect%20Real%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvDzx2HKiotEtbov6%2Flink-is-the-endowment-effect-real%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Is%20the%20Endowment%20Effect%20Real%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvDzx2HKiotEtbov6%2Flink-is-the-endowment-effect-real", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvDzx2HKiotEtbov6%2Flink-is-the-endowment-effect-real", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 689, "htmlBody": "<p>Under fairly weak assumptions, the most a standard rational economic agent is willing to pay for an item they don't own (WTP) and the least they're willing to accept in exchange for that item if they already own it (WTA) should be identical. In experiments with humans, psychologists and economists have repeatedly found WTP-WTA gaps suggesting that humans aren't rational in at least this specific way. This has been interpreted as the <a title=\"http://en.wikipedia.org/wiki/Prospect_theory\" href=\"http://en.wikipedia.org/wiki/Prospect_theory\" target=\"_blank\">endowment effect</a>* and evidence for <a title=\"http://en.wikipedia.org/wiki/Prospect_theory\" href=\"http://en.wikipedia.org/wiki/Prospect_theory\" target=\"_blank\">prospect theory</a>. According to prospect theory, people are loss averse. Roughly this means that that, given their current ownership set, people value not losing stuff more highly than gaining stuff. Thus once someone gains ownership of something they suddenly value it much more highly. This \"endowment effect\"* on one's valuation of an item has been put forth as an explanation for the observed WTP - WTA gaps.</p>\n<p>*Wikipedia confusingly defines the endowment effect as the gap itself, i.e. as the phenomena to be explained instead of the explanation. I suspect this is a difference in terminology among economists and psychologists, where psychologists use the wiki definition and economists use the definition I give here. However, calling the WTP-WTA gap an \"endowment effect\" is a bit misleading because a priori the gap may not have anything to endowments at all.</p>\n<p>A <a title=\"http://simpsonm.public.iastate.edu/RandomStuff/PlottZeiler.pdf\" href=\"http://simpsonm.public.iastate.edu/RandomStuff/PlottZeiler.pdf\" target=\"_blank\">paper</a> (pdf) by Charlie Plott and Kathryn Zeiler investigates WTP-WTA gaps and it turns out that they may just be due to subjects not quite understanding the experimental protocols, particularly in the value elicitation process. Here's an important quote from their conclusion, but do read the paper for details:&nbsp;</p>\n<blockquote>\n<p>The issue explored here is not whether a WTP-WTA gap can be observed. Clearly, the experiments of KKT and others show not only that gaps can be observed, but also that they are replicable. Instead, our interest lies in the interpretation of observed gaps. The primary conclusion derived from the data reported here is that observed WTP-WTA gaps do not reflect a fundamental feature of human preferences. That is, endowment effect theory does not seem to explain observed gaps. In addition, our results suggest that observed gaps should not be interpreted as support for prospect theory.</p>\n<p>A review of the literature reveals that WTP-WTA gaps are not reliably observed across experimental designs. Given the nature of reported experimental designs, we posited that differences in experimental procedures might account for the differences across reported results. This conjecture prompted us to develop procedures to test for the robustness of the phenomenon. We conducted comparative experiments using procedures commonly used in studies that report observed gaps (i.e., KKT). We also employed a \"revealed theory\" methodology to identify procedures reported in the literature that provide clues about experimenter notions regarding subject misconceptions. We then conducted experiments that implemented the union of procedures used by experimentalists to control for subject misconceptions. The comparative experiments demonstrate that WTP-WTA gaps are indeed sensitive to experimental procedures. By implementing different procedures, the phenomenon can be turned on and off. When procedures used in studies that report the gap are employed, the gap is readily observed. When a full set of controls is implemented, the gap is not observed.</p>\n<p>The fact that the gap can be turned on and off demonstrates that interpreting gaps as support for endowment effect theory is problematic. The mere observation of the phenomenon does not support loss aversion-a very special form of preferences in which gains are valued less than losses. That the phenomenon can be turned on and off while holding the good constant supports a strong rejection of the claim that WTP-WTA gaps support a particular theory of preferences posited by prospect theory. Loss aversion might in some sense characterize preferences, but such a theory most likely does not explain observed WTP-WTA gaps. Exactly what accounts for observed WTP-WTA gaps? The thesis of this paper is that observed gaps are symptomatic of subjects' misconceptions about the nature of the experimental task. The differences reported in the literature reflect differences in experimental controls for misconceptions as opposed to differences in the nature of the commodity (e.g., candy, money, mugs, lotteries, etc.) under study.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rvDzx2HKiotEtbov6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.1236202615431246e-06, "legacy": true, "legacyId": "21795", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T02:56:39.785Z", "modifiedAt": null, "url": null, "title": "When should you give to multiple charities?", "slug": "when-should-you-give-to-multiple-charities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:29.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/27tTLHnG9dfPmPRxF/when-should-you-give-to-multiple-charities", "pageUrlRelative": "/posts/27tTLHnG9dfPmPRxF/when-should-you-give-to-multiple-charities", "linkUrl": "https://www.lesswrong.com/posts/27tTLHnG9dfPmPRxF/when-should-you-give-to-multiple-charities", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20should%20you%20give%20to%20multiple%20charities%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20should%20you%20give%20to%20multiple%20charities%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27tTLHnG9dfPmPRxF%2Fwhen-should-you-give-to-multiple-charities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20should%20you%20give%20to%20multiple%20charities%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27tTLHnG9dfPmPRxF%2Fwhen-should-you-give-to-multiple-charities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27tTLHnG9dfPmPRxF%2Fwhen-should-you-give-to-multiple-charities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 438, "htmlBody": "<p>The logic that you should donate only to a <a href=\"http://www.slate.com/id/2034/\">single top charity</a> is very strong.  But when faced with two ways of making the world better there's this urge <a href=\"http://blog.givewell.org/2009/11/19/denying-the-choice/\">deny the choice</a> and do both.  Is this urge irrational or is there something there?</p>\n<p>At the low end splitting up your giving can definitely be a problem. If you give $5 here and $10 there it's depressing how much of your donations will be <a href=\"http://www.givinggladly.com/2011/10/why-you-need-plan.html\">eaten up by processing costs</a>:</p>\n<blockquote>The most extreme case I've seen, from my days working at a nonprofit, was an elderly man who sent $3 checks to 75 charities. Since it costs more than that to process a donation, this poor guy was spending $225 to <em>take money</em> from his favorite organizations.</blockquote>\n<p>By contrast, at the high end you definitely need to divide your giving.  If a <a href=\"http://givingpledge.org/\">someone</a> decided to give $1B to the <a href=\"http://www.givewell.org/international/top-charities/AMF\">AMF</a> it would definitely do a lot of good. Because charities have limited <a href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a>, however, after the first $20M or so there are probably other anti-malaria organizations that could do more with the money.  And at some point we beat malaria and so other interventions start having a greater impact for your money.</p>\n<p>Most of us, however, are giving enough that our donations are well above the processing-cost level but not enough to satisfy an organization's room for more funding.  So what do you do?</p>\n<p>If one option is <a href=\"http://www.jefftk.com/news/2012-11-05\">much better than another</a> then you really do need to make the choice.  The best ones are <a href=\"http://opinionator.blogs.nytimes.com/2012/12/05/putting-charities-to-the-test/\">enough better</a> than the average ones that you need to <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">buckle down and pick the best</a>.</p>\n<p>But what about when you're not sure?  Even after going through all the evidence you can find you just can't decide whether it's more effective to take the <a href=\"http://www.givewell.org/international/top-charities/AMF\">sure thing</a> and help people now or support the extremely hard to evaluate but potentially crucial <a href=\"http://www.fhi.ox.ac.uk/\">work</a> of <a href=\"http://www.theatlantic.com/technology/archive/2012/03/were-underestimating-the-risk-of-human-extinction/253821/\">reducing the risk that our species wipes itself out</a>.  The strength of the economic argument for giving only to your top charity is proportional to the difference between it and your next choice.  If the difference is small enough and you find it painful to pick only one it's just not worth it: give to both.</p>\n<p>(It can also be worth it to give to multiple organizations because of what it indicates to other people.  I help fund <a href=\"http://80000hours.org/\">80,000 Hours</a> because I think spreading the idea of <a href=\"http://80000hours.org/what-is-an-effective-altruist\">effective altruism</a> is the most important thing I can do.  But it looks kind of sketchy to only give to <a href=\"http://www.jefftk.com/news/2012-06-05\">metacharities</a>, so I <a href=\"http://www.jefftk.com/donations\">divide my giving</a> between them and <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's top pick</a>.)</p>\n<p><em><small>I also posted this <a href=\"http://www.jefftk.com/news/2013-02-26\">on my blog</a></small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "27tTLHnG9dfPmPRxF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.1237812150582135e-06, "legacy": true, "legacyId": "21802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T05:06:38.467Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Nootropics", "slug": "meetup-west-la-meetup-nootropics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YxaEPtwX8L8RKM8L9/meetup-west-la-meetup-nootropics", "pageUrlRelative": "/posts/YxaEPtwX8L8RKM8L9/meetup-west-la-meetup-nootropics", "linkUrl": "https://www.lesswrong.com/posts/YxaEPtwX8L8RKM8L9/meetup-west-la-meetup-nootropics", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Nootropics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Nootropics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxaEPtwX8L8RKM8L9%2Fmeetup-west-la-meetup-nootropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Nootropics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxaEPtwX8L8RKM8L9%2Fmeetup-west-la-meetup-nootropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxaEPtwX8L8RKM8L9%2Fmeetup-west-la-meetup-nootropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jw'>West LA Meetup - Nootropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 February 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, February 27th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p>Discussion Topic: This week we will talk about <a href=\"http://en.wikipedia.org/wiki/Nootropic\" rel=\"nofollow\">Nootropics</a> and other associated things. This will range from discussion of the biochemistry to evaluating efficacy and safety of such things. The objective isn't to say \"chemicals are the best way to make me smart!\", but rather \"how can I tell if this or that chemical is worth my time and money as a means of fulfilling my goals?\"</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary;</em> this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jw'>West LA Meetup - Nootropics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YxaEPtwX8L8RKM8L9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1238651559988745e-06, "legacy": true, "legacyId": "21803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Nootropics\">Discussion article for the meetup : <a href=\"/meetups/jw\">West LA Meetup - Nootropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 February 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, February 27th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p>Discussion Topic: This week we will talk about <a href=\"http://en.wikipedia.org/wiki/Nootropic\" rel=\"nofollow\">Nootropics</a> and other associated things. This will range from discussion of the biochemistry to evaluating efficacy and safety of such things. The objective isn't to say \"chemicals are the best way to make me smart!\", but rather \"how can I tell if this or that chemical is worth my time and money as a means of fulfilling my goals?\"</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary;</em> this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Nootropics1\">Discussion article for the meetup : <a href=\"/meetups/jw\">West LA Meetup - Nootropics</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Nootropics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Nootropics", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Nootropics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Nootropics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T06:33:00.172Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] An Especially Elegant Evpsych Experiment", "slug": "seq-rerun-an-especially-elegant-evpsych-experiment", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cZ8CQHbeRpoeYXqke/seq-rerun-an-especially-elegant-evpsych-experiment", "pageUrlRelative": "/posts/cZ8CQHbeRpoeYXqke/seq-rerun-an-especially-elegant-evpsych-experiment", "linkUrl": "https://www.lesswrong.com/posts/cZ8CQHbeRpoeYXqke/seq-rerun-an-especially-elegant-evpsych-experiment", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20An%20Especially%20Elegant%20Evpsych%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20An%20Especially%20Elegant%20Evpsych%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZ8CQHbeRpoeYXqke%2Fseq-rerun-an-especially-elegant-evpsych-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20An%20Especially%20Elegant%20Evpsych%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZ8CQHbeRpoeYXqke%2Fseq-rerun-an-especially-elegant-evpsych-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZ8CQHbeRpoeYXqke%2Fseq-rerun-an-especially-elegant-evpsych-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/yj/an_especially_elegant_evpsych_experiment/\">An Especially Elegant Evpsych Experiment</a> was originally published on 13 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#An_Especially_Elegant_Evpsych_Experiment\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An experiment comparing expected parental grief at the death of a child at different ages, to the reproductive success rate of children at that age in a hunter gatherer tribe.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gsw/seq_rerun_the_evolutionarycognitive_boundary/\">The Evolutionary-Cognitive Boundary</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cZ8CQHbeRpoeYXqke", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1239209352423713e-06, "legacy": true, "legacyId": "21806", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J4vdsSKB7LzAvaAMB", "N7gwFeQ4CwHHMTiLQ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T07:22:09.782Z", "modifiedAt": null, "url": null, "title": "Why might the future be good?", "slug": "why-might-the-future-be-good", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bMeKwP9W5iRub9Jqd/why-might-the-future-be-good", "pageUrlRelative": "/posts/bMeKwP9W5iRub9Jqd/why-might-the-future-be-good", "linkUrl": "https://www.lesswrong.com/posts/bMeKwP9W5iRub9Jqd/why-might-the-future-be-good", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20might%20the%20future%20be%20good%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20might%20the%20future%20be%20good%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMeKwP9W5iRub9Jqd%2Fwhy-might-the-future-be-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20might%20the%20future%20be%20good%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMeKwP9W5iRub9Jqd%2Fwhy-might-the-future-be-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMeKwP9W5iRub9Jqd%2Fwhy-might-the-future-be-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2743, "htmlBody": "<p>&nbsp;</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(Cross-posted from&nbsp;<a href=\"http://www.rationalaltruist.com\">Rational Altruist</a>. See also recent posts on <a href=\"http://rationalaltruist.com/2013/02/22/four-flavors-of-time-discounting-i-endorse-and-one-i-do-not/\">time-discounting</a> and <a href=\"http://rationalaltruist.com/2013/02/23/self-driving-cars-as-a-target-for-philanthropy/\">self-driving cars</a>.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\"><span style=\"line-height: 19px;\">When talking about the future, I often encounter two (quite different) stories describing why the future might be good:</span></p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">\n<li>Decisions will be made by people whose lives are morally valuable and who want the best for themselves. They will bargain amongst each other and create a world that is good to live in. Because my values are roughly aligned with their aggregate preferences, I expect them to create a rich and valuable world (by my lights as well as theirs).</li>\n<li>Some people in the future will have altruistic values broadly similar to my own, and will use their influence to create a rich and valuable world (by my lights as well as theirs).</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Which of these pictures we take more seriously has implications for what we should do today. I often have object level disagreements which seem to boil down to disagreement about which of these pictures is more important, but rarely do I see serious discussion of that question. (When there is discussion, it seems to turn into a contest of political ideologies rather than facts.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If we take picture (1) seriously, we may be interested in ensuring that society continues to function smoothly, that people are aware of and pursue what really makes them happy, that governments are effective, markets are efficient, externalities are successfully managed, etc. If we take picture (2) seriously, we are more likely to be concerned with changing what the people of the future value, bolstering the influence of people who share our values, and ensuring that altruists are equipped to embark on their projects successfully.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I'm mostly concerned with the very long run---I am wondering what conditions will prevail for most of the people who live in the future, and I expect most of them to be alive very far from now.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">It seems to me that there are two major factors that control the relative importance of pictures (1) and (2): how prominent should we expect altruism to be in the future, and how efficiently are altruistic vs. selfish resources being used to create value? My answer to the second question is mostly vague hand-waving, but I think I have something interesting to say on the first question.</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>How much altruism do we expect?</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I often hear people talking about the future, and the present for that matter, as if we are falling towards a Darwinian attractor of cutthroat competition and vanishing empathy (at least as a default presumption, which might be averted by an extraordinary effort). I think this picture is essentially mistaken, and my median expectation is that the future is much&nbsp;<em>more</em>&nbsp;altruistic than the present.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>Dose natural selection select for self-interest?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">In the world of today, it may seem that humans are essentially driven by self-interest, that this self-interest was a necessary product of evolution, that good deeds are principally pursued instrumentally in service of self-interest, and that altruism only exists at all because it is too hard for humans to maintain a believable sociopathic facade.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If we take this situation and project it towards a future in which evolution has had more time to run its course, creating automations and organizations less and less constrained by folk morality, we may anticipate an outcome in which natural selection has stripped away all empathy in favor of self-interest and effective manipulation. Some may view this outcome as unfortunate but inevitable, others may view it as a catastrophe which we should work to avert, and still others might view it as a positive outcome in which individuals are free to bargain amongst themselves and create a world which serves their collective interest.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">But evolution itself does not actually seem to favor self-interest at all.&nbsp;<em>No matter what your values</em>, if you care about the future you are incentivized to survive, to acquire resources for yourself and your descendants, to defend yourself from predation, etc. etc. If I care about filling the universe with happy people and you care about filling the universe with copies of yourself, I'm not going to set out by trying to make people happy while allowing you and your descendants to expand throughout the universe unchecked. Instead, I will pursue a similar strategy of resource acquisition (or coordinate with others to stop your expansion), to ensure that I maintain a reasonable share of the available resources which I can eventually spend to help shape a world I consider value. (See&nbsp;<a href=\"http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\">here</a>&nbsp;for a similar discussion.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">This doesn't seem to match up with what we've seen historically, so if I claim that it's relevant to the future I have some explaining to do.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>Historical distortions</strong></h3>\n<h4 style=\"font-size: 13.333333969116211px; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><em>Short-range consequentialism</em></h4>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">One reason we haven't seen this phenomenon historically is that animals don't actually make decisions by backwards-chaining from a desired outcome. When animals (including humans) engage in goal-oriented behavior, it tends to be pretty local, without concern for consequences which are distant in time or space. To the extent that animal behavior is goal-oriented at a large scale, those goals are largely an emergent property of an interacting network of drives, heuristics, etc. So we should expect animals to have goals which lead them to multiply and acquire resources, even when those drives are pursued short-sightedly. And indeed, that's what we see. But it's not the fault of evolution alone---it is a product of evolution&nbsp;<em>given</em>&nbsp;nature's inability to create consequentialist reasoners.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Casually, we seem to observe a similar situation with respect to human organizations---organizations which value expansion for its own sake (or one of its immediate consequences) are able to expand aggressively, while organizations which don't value expansion have a much harder time deciding to expand for instrumental reasons without compromising their values.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Hopefully, this situation is exceptional in history. If humans ever manage to build systems which are properly consequentialist---organizations or automations which are capable of expanding because it is&nbsp;<em>instrumentally</em>&nbsp;useful---we should not expect natural selection to discriminate at all on the basis of those systems' values.</p>\n<h4 style=\"font-size: 13.333333969116211px; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><em>Value drift</em></h4>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Human's values are also distorted by the process of reproduction. A perfect consequentialist would prefer to have descendants who share their values. (Even if I value diversity or freedom of choice, I would like my children to at least share those values, at least if I want that freedom and diversity to last more than one generation!) But humans don't have this option---the only way we can expand our influence is by creating very lossy copies. And so each generation is populated by a fresh batch of humans with a fresh set of values, and the values of our ancestors only have an extremely indirect effect on the world of today.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Again, a similar problem afflicts human organizations. If I create a foundation that I would like to persist for generations, the only way it can expand its influence is by hiring new staff. And since those staff have a strong influence over what my foundation will do, the implicit values of my foundation will slowly but surely be pulled back to the values of the pool of human employees that I have to draw from.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">These constraints distort evolution, causing selection to act only those traits which can be reliably passed on from one generation to the next. In particular, this exacerbates the problem from the preceding section---even to the extent that humans&nbsp;<em>can</em>&nbsp;engage in goal-oriented reasoning and expand their own influence instrumentally, these tendencies can not be very well encoded in genes or passed on to the next generation in other ways. This is perhaps the most fundamental change which would result from the development of machine intelligences. If it were possible to directly control the characteristics and values of the next generation, evolution would be able to act on those characteristics and values directly.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>So what&nbsp;<em>does</em>&nbsp;natural selection select for?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If the next generation is created by the current generation, guided by the current generation's values, then the properties of the next generation will be disproportionately affected by those who care most strongly about the future.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">In finance: if investors have different time preferences, those who are more patient will make higher returns and eventually accumulate much wealth. In demographics: if some people care more about the future, they may have more kids as a way to influence it, and therefore be overrepresented in future generations. In government: if some people care about what government looks like in 100 years, they will use their political influence to shape what the government looks like in 100 years rather than trying to win victories today.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">What natural selection selects for is patience. In a thousand years, given efficient natural selection, the most influential people will be those who today cared what happens in a thousand years. Preferences about what happens to&nbsp;<em>me</em>&nbsp;(at least for a narrow conception of personal identity) will eventually die off, dominated by preferences about what society looks like on the longest timescales.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I think this picture is reasonably robust. There are ways that natural selection (/ efficient markets) can be frustrated, and I would not be&nbsp;<em>too</em>&nbsp;surprised if these frustrations persisted indefinitely, but nevertheless this dynamic seems like one of the most solid features of an uncertain future.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>What values are we starting with?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Most of people's preferences today seem to concern what happens to them in the near term. If we take the above picture seriously, these values will eventually have little influence over society. Then the question becomes: if we focus only on humanity's collective preferences&nbsp;<em>over the long term</em>, what do those preferences look like? (Trying to characterize preferences as \"altruistic\" or not no longer seems useful as we zoom in.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">This is an empirical question, which I am not very well-equipped to value. But I can make a few observations that ring true to me (though my data is mostly drawn from academics and intellectuals, who may fail to be representative of normal people in important ways even after conditioning on the \"forward-looking\" part of people's values):</p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">\n<li>When people think about the far future (and thus when they articulate their preferences for the far future) they seem to engage a different mode of reasoning, more strongly optimized to produce socially praise-worthy (and thus prosocial) judgments. This might be characterized as a&nbsp;<a href=\"http://www.overcomingbias.com/2013/02/beware-far-values.html\">bias</a>, but to the extent we can talk about human preferences at all they seem to be a result of these kinds of processes (and to the extent that I am using my own altruistic values to judge futures, they are produced by a similar process). This effect seems to persist even when we are not directly accountable for our actions.</li>\n<li>People mostly endorse their own enlightened preferences, and look discouragingly at attempts to lock-in hastily considered values (though they often seem to have overconfident views about what their enlightened preferences will look like, which admittedly might interfere with their attempts at reflection).</li>\n<li>I find myself sympathetic to very many people's accounts of their own preferences about the future, even where those accounts different significantly from my own. I would be surprised if the distribution of moral preferences was too scattered.</li>\n<li>To the extent that people care especially about their species, their nation, their family, themselves, etc. : they seem to be sensitive to fairness considerations (and rarely wish e.g. to spend a significant fraction of civilization's resources on themselves), their preferences seem to be only a modest distortion of aggregative values (wanting people with property X to flourish is not so different from wanting people to flourish, if property X is some random characteristic without moral significance), and human preferences seem to somewhat reliably drift in the direction of more universal concern as basic needs are addressed and more considerations are considered.</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">After cutting away all near-term interests, I expect that contemporary human society's collective preferences are similar to their stated moral preferences, with significant disagreement on many moral judgments. However, I expect that these values support reflection, that upon reflection the distribution of values is not too broad, and that for the most part these values are reasonably well-aligned. With successful bargaining, I expect a mixture of humanity's long-term interests to be only modestly (perhaps a factor of 10, probably not a factor of 1000) worse than my own values (as judged by my own values).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Moreover, I have strong intuitions to emphasize those parts of my values which are&nbsp;<em>least</em>&nbsp;historically contingent.&nbsp;(I accept that all of my values are contingent, but am happier to accept those values that are contingent on my biological identity than those that are contingent on my experiences as a child, and happier to accept those that are contingent on my experiences as a child than those that are contingent on my current blood sugar.) And I have strong reciprocity intuitions that exacerbate this effect and lead me to be more supportive of my peers' values. These effects make me more optimistic about a world determined by humanity's aggregate preferences than I otherwise would be.</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>How important is altruism?</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(The answer to this question, unlike the first one, depends on your values: how important to what? I will answer from my own perspective. I have roughly aggregative values, and think that the goodness of a world with twice as many happy people is twice as high.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Even if we know a society's collective preferences, it is not obvious what their relative&nbsp;<em>importance</em>&nbsp;is. At what level of prevalence would the contributions of explicit altruism become the source of value? If altruists are 10% of the influence-weighted population, do the contributions of the altruists matter? What if altruists are 1% of the population? A priori, it seems clear that the explicit altruists should do&nbsp;<em>at least</em>&nbsp;as much good--on the altruistic account--as any other population (otherwise they could decide to jump ship and become objectivists, or whatever). But beyond that, it isn't clear that altruists should create&nbsp;<em>much</em>&nbsp;more value--even on the altruistic account--than people with other values.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I suspect that explicit altruistic preferences create many times more value than self-interest or other nearly orthogonal preferences. So in addition to expecting a future in which altruistic preferences play a very large role, I think that altruistic preferences would be responsible for most of the value even if they controlled only 1% of the resources.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">One significant issue is population growth. Self-interest may lead people to create a world which is good for themselves, but it is unlikely to inspire people to create as many new people as they could, or use resources efficiently to support future generations. But it seems to me that the existence of large populations is a huge source of value. A barren universe is not a happy universe.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">A second issue is that population&nbsp;<em>characteristics</em>&nbsp;may also be an important factor in the goodness of the world, and self-interest is unlikely to lead people to ensure that each new generation has the sorts of characteristics which would cause them to lead happy lives. It may happen by good fortune that the future is full of people who are well-positioned to live rich lives, but I don't see any particular reason this would happen. Instead, we might have a future \"population\" in which almost all resources support automation that doesn't experience anything, or a world full of minds which crave survival but experience no joy, or etc.; \"self-interest\" wouldn't lead any of these populations to change themselves to experience more happiness. It's not clear why we would avoid these outcomes except by a law of nature that said that productive people were happy people (which seems implausible to me) or by coordinating to avoid these outcomes.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(If you have different values, such that there&nbsp;<em>is</em>&nbsp;a law [or at least guideline] of nature: \"productive people are morally valuable people,\" then this analysis may not apply to you. I know several such people, but I have a hard time sympathizing with their ethics.)</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\"><strong>Conclusion</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I think that the goodness of a world is mostly driven by the amount of explicit optimization that is going on to try and make the world good (this is all relative to my values, though a similar analysis seems to carry with respect to other aggregative values). This seems to be true even if relatively little optimization is going on. Fortunately, I also think that the future will be characterized by&nbsp;<em>much</em>&nbsp;higher influence for altruistic values. If I thought altruism was unlikely to win out, I would be concerned with changing that. As it is, I am instead more concerned with ensuring that the future proceeds without disruptions. (Though I still think it is worth it to try and increase the prevalence of altruism faster, most of all because this seems like a good approach to minimizing the probability of undesired disruptions.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bMeKwP9W5iRub9Jqd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 1.123952688976869e-06, "legacy": true, "legacyId": "21807", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(Cross-posted from&nbsp;<a href=\"http://www.rationalaltruist.com\">Rational Altruist</a>. See also recent posts on <a href=\"http://rationalaltruist.com/2013/02/22/four-flavors-of-time-discounting-i-endorse-and-one-i-do-not/\">time-discounting</a> and <a href=\"http://rationalaltruist.com/2013/02/23/self-driving-cars-as-a-target-for-philanthropy/\">self-driving cars</a>.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\"><span style=\"line-height: 19px;\">When talking about the future, I often encounter two (quite different) stories describing why the future might be good:</span></p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">\n<li>Decisions will be made by people whose lives are morally valuable and who want the best for themselves. They will bargain amongst each other and create a world that is good to live in. Because my values are roughly aligned with their aggregate preferences, I expect them to create a rich and valuable world (by my lights as well as theirs).</li>\n<li>Some people in the future will have altruistic values broadly similar to my own, and will use their influence to create a rich and valuable world (by my lights as well as theirs).</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Which of these pictures we take more seriously has implications for what we should do today. I often have object level disagreements which seem to boil down to disagreement about which of these pictures is more important, but rarely do I see serious discussion of that question. (When there is discussion, it seems to turn into a contest of political ideologies rather than facts.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If we take picture (1) seriously, we may be interested in ensuring that society continues to function smoothly, that people are aware of and pursue what really makes them happy, that governments are effective, markets are efficient, externalities are successfully managed, etc. If we take picture (2) seriously, we are more likely to be concerned with changing what the people of the future value, bolstering the influence of people who share our values, and ensuring that altruists are equipped to embark on their projects successfully.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I'm mostly concerned with the very long run---I am wondering what conditions will prevail for most of the people who live in the future, and I expect most of them to be alive very far from now.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">It seems to me that there are two major factors that control the relative importance of pictures (1) and (2): how prominent should we expect altruism to be in the future, and how efficiently are altruistic vs. selfish resources being used to create value? My answer to the second question is mostly vague hand-waving, but I think I have something interesting to say on the first question.</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"How_much_altruism_do_we_expect_\"><strong>How much altruism do we expect?</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I often hear people talking about the future, and the present for that matter, as if we are falling towards a Darwinian attractor of cutthroat competition and vanishing empathy (at least as a default presumption, which might be averted by an extraordinary effort). I think this picture is essentially mistaken, and my median expectation is that the future is much&nbsp;<em>more</em>&nbsp;altruistic than the present.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"Dose_natural_selection_select_for_self_interest_\"><strong>Dose natural selection select for self-interest?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">In the world of today, it may seem that humans are essentially driven by self-interest, that this self-interest was a necessary product of evolution, that good deeds are principally pursued instrumentally in service of self-interest, and that altruism only exists at all because it is too hard for humans to maintain a believable sociopathic facade.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If we take this situation and project it towards a future in which evolution has had more time to run its course, creating automations and organizations less and less constrained by folk morality, we may anticipate an outcome in which natural selection has stripped away all empathy in favor of self-interest and effective manipulation. Some may view this outcome as unfortunate but inevitable, others may view it as a catastrophe which we should work to avert, and still others might view it as a positive outcome in which individuals are free to bargain amongst themselves and create a world which serves their collective interest.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">But evolution itself does not actually seem to favor self-interest at all.&nbsp;<em>No matter what your values</em>, if you care about the future you are incentivized to survive, to acquire resources for yourself and your descendants, to defend yourself from predation, etc. etc. If I care about filling the universe with happy people and you care about filling the universe with copies of yourself, I'm not going to set out by trying to make people happy while allowing you and your descendants to expand throughout the universe unchecked. Instead, I will pursue a similar strategy of resource acquisition (or coordinate with others to stop your expansion), to ensure that I maintain a reasonable share of the available resources which I can eventually spend to help shape a world I consider value. (See&nbsp;<a href=\"http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\">here</a>&nbsp;for a similar discussion.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">This doesn't seem to match up with what we've seen historically, so if I claim that it's relevant to the future I have some explaining to do.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"Historical_distortions\"><strong>Historical distortions</strong></h3>\n<h4 style=\"font-size: 13.333333969116211px; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"Short_range_consequentialism\"><em>Short-range consequentialism</em></h4>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">One reason we haven't seen this phenomenon historically is that animals don't actually make decisions by backwards-chaining from a desired outcome. When animals (including humans) engage in goal-oriented behavior, it tends to be pretty local, without concern for consequences which are distant in time or space. To the extent that animal behavior is goal-oriented at a large scale, those goals are largely an emergent property of an interacting network of drives, heuristics, etc. So we should expect animals to have goals which lead them to multiply and acquire resources, even when those drives are pursued short-sightedly. And indeed, that's what we see. But it's not the fault of evolution alone---it is a product of evolution&nbsp;<em>given</em>&nbsp;nature's inability to create consequentialist reasoners.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Casually, we seem to observe a similar situation with respect to human organizations---organizations which value expansion for its own sake (or one of its immediate consequences) are able to expand aggressively, while organizations which don't value expansion have a much harder time deciding to expand for instrumental reasons without compromising their values.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Hopefully, this situation is exceptional in history. If humans ever manage to build systems which are properly consequentialist---organizations or automations which are capable of expanding because it is&nbsp;<em>instrumentally</em>&nbsp;useful---we should not expect natural selection to discriminate at all on the basis of those systems' values.</p>\n<h4 style=\"font-size: 13.333333969116211px; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"Value_drift\"><em>Value drift</em></h4>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Human's values are also distorted by the process of reproduction. A perfect consequentialist would prefer to have descendants who share their values. (Even if I value diversity or freedom of choice, I would like my children to at least share those values, at least if I want that freedom and diversity to last more than one generation!) But humans don't have this option---the only way we can expand our influence is by creating very lossy copies. And so each generation is populated by a fresh batch of humans with a fresh set of values, and the values of our ancestors only have an extremely indirect effect on the world of today.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Again, a similar problem afflicts human organizations. If I create a foundation that I would like to persist for generations, the only way it can expand its influence is by hiring new staff. And since those staff have a strong influence over what my foundation will do, the implicit values of my foundation will slowly but surely be pulled back to the values of the pool of human employees that I have to draw from.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">These constraints distort evolution, causing selection to act only those traits which can be reliably passed on from one generation to the next. In particular, this exacerbates the problem from the preceding section---even to the extent that humans&nbsp;<em>can</em>&nbsp;engage in goal-oriented reasoning and expand their own influence instrumentally, these tendencies can not be very well encoded in genes or passed on to the next generation in other ways. This is perhaps the most fundamental change which would result from the development of machine intelligences. If it were possible to directly control the characteristics and values of the next generation, evolution would be able to act on those characteristics and values directly.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"So_what_does_natural_selection_select_for_\"><strong>So what&nbsp;<em>does</em>&nbsp;natural selection select for?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">If the next generation is created by the current generation, guided by the current generation's values, then the properties of the next generation will be disproportionately affected by those who care most strongly about the future.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">In finance: if investors have different time preferences, those who are more patient will make higher returns and eventually accumulate much wealth. In demographics: if some people care more about the future, they may have more kids as a way to influence it, and therefore be overrepresented in future generations. In government: if some people care about what government looks like in 100 years, they will use their political influence to shape what the government looks like in 100 years rather than trying to win victories today.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">What natural selection selects for is patience. In a thousand years, given efficient natural selection, the most influential people will be those who today cared what happens in a thousand years. Preferences about what happens to&nbsp;<em>me</em>&nbsp;(at least for a narrow conception of personal identity) will eventually die off, dominated by preferences about what society looks like on the longest timescales.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I think this picture is reasonably robust. There are ways that natural selection (/ efficient markets) can be frustrated, and I would not be&nbsp;<em>too</em>&nbsp;surprised if these frustrations persisted indefinitely, but nevertheless this dynamic seems like one of the most solid features of an uncertain future.</p>\n<h3 style=\"font-size: 1.17em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"What_values_are_we_starting_with_\"><strong>What values are we starting with?</strong></h3>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Most of people's preferences today seem to concern what happens to them in the near term. If we take the above picture seriously, these values will eventually have little influence over society. Then the question becomes: if we focus only on humanity's collective preferences&nbsp;<em>over the long term</em>, what do those preferences look like? (Trying to characterize preferences as \"altruistic\" or not no longer seems useful as we zoom in.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">This is an empirical question, which I am not very well-equipped to value. But I can make a few observations that ring true to me (though my data is mostly drawn from academics and intellectuals, who may fail to be representative of normal people in important ways even after conditioning on the \"forward-looking\" part of people's values):</p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">\n<li>When people think about the far future (and thus when they articulate their preferences for the far future) they seem to engage a different mode of reasoning, more strongly optimized to produce socially praise-worthy (and thus prosocial) judgments. This might be characterized as a&nbsp;<a href=\"http://www.overcomingbias.com/2013/02/beware-far-values.html\">bias</a>, but to the extent we can talk about human preferences at all they seem to be a result of these kinds of processes (and to the extent that I am using my own altruistic values to judge futures, they are produced by a similar process). This effect seems to persist even when we are not directly accountable for our actions.</li>\n<li>People mostly endorse their own enlightened preferences, and look discouragingly at attempts to lock-in hastily considered values (though they often seem to have overconfident views about what their enlightened preferences will look like, which admittedly might interfere with their attempts at reflection).</li>\n<li>I find myself sympathetic to very many people's accounts of their own preferences about the future, even where those accounts different significantly from my own. I would be surprised if the distribution of moral preferences was too scattered.</li>\n<li>To the extent that people care especially about their species, their nation, their family, themselves, etc. : they seem to be sensitive to fairness considerations (and rarely wish e.g. to spend a significant fraction of civilization's resources on themselves), their preferences seem to be only a modest distortion of aggregative values (wanting people with property X to flourish is not so different from wanting people to flourish, if property X is some random characteristic without moral significance), and human preferences seem to somewhat reliably drift in the direction of more universal concern as basic needs are addressed and more considerations are considered.</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">After cutting away all near-term interests, I expect that contemporary human society's collective preferences are similar to their stated moral preferences, with significant disagreement on many moral judgments. However, I expect that these values support reflection, that upon reflection the distribution of values is not too broad, and that for the most part these values are reasonably well-aligned. With successful bargaining, I expect a mixture of humanity's long-term interests to be only modestly (perhaps a factor of 10, probably not a factor of 1000) worse than my own values (as judged by my own values).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Moreover, I have strong intuitions to emphasize those parts of my values which are&nbsp;<em>least</em>&nbsp;historically contingent.&nbsp;(I accept that all of my values are contingent, but am happier to accept those values that are contingent on my biological identity than those that are contingent on my experiences as a child, and happier to accept those that are contingent on my experiences as a child than those that are contingent on my current blood sugar.) And I have strong reciprocity intuitions that exacerbate this effect and lead me to be more supportive of my peers' values. These effects make me more optimistic about a world determined by humanity's aggregate preferences than I otherwise would be.</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"How_important_is_altruism_\"><strong>How important is altruism?</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(The answer to this question, unlike the first one, depends on your values: how important to what? I will answer from my own perspective. I have roughly aggregative values, and think that the goodness of a world with twice as many happy people is twice as high.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">Even if we know a society's collective preferences, it is not obvious what their relative&nbsp;<em>importance</em>&nbsp;is. At what level of prevalence would the contributions of explicit altruism become the source of value? If altruists are 10% of the influence-weighted population, do the contributions of the altruists matter? What if altruists are 1% of the population? A priori, it seems clear that the explicit altruists should do&nbsp;<em>at least</em>&nbsp;as much good--on the altruistic account--as any other population (otherwise they could decide to jump ship and become objectivists, or whatever). But beyond that, it isn't clear that altruists should create&nbsp;<em>much</em>&nbsp;more value--even on the altruistic account--than people with other values.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I suspect that explicit altruistic preferences create many times more value than self-interest or other nearly orthogonal preferences. So in addition to expecting a future in which altruistic preferences play a very large role, I think that altruistic preferences would be responsible for most of the value even if they controlled only 1% of the resources.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">One significant issue is population growth. Self-interest may lead people to create a world which is good for themselves, but it is unlikely to inspire people to create as many new people as they could, or use resources efficiently to support future generations. But it seems to me that the existence of large populations is a huge source of value. A barren universe is not a happy universe.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">A second issue is that population&nbsp;<em>characteristics</em>&nbsp;may also be an important factor in the goodness of the world, and self-interest is unlikely to lead people to ensure that each new generation has the sorts of characteristics which would cause them to lead happy lives. It may happen by good fortune that the future is full of people who are well-positioned to live rich lives, but I don't see any particular reason this would happen. Instead, we might have a future \"population\" in which almost all resources support automation that doesn't experience anything, or a world full of minds which crave survival but experience no joy, or etc.; \"self-interest\" wouldn't lead any of these populations to change themselves to experience more happiness. It's not clear why we would avoid these outcomes except by a law of nature that said that productive people were happy people (which seems implausible to me) or by coordinating to avoid these outcomes.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">(If you have different values, such that there&nbsp;<em>is</em>&nbsp;a law [or at least guideline] of nature: \"productive people are morally valuable people,\" then this analysis may not apply to you. I know several such people, but I have a hard time sympathizing with their ethics.)</p>\n<h2 style=\"font-size: 1.5em; color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 18.99305534362793px;\" id=\"Conclusion\"><strong>Conclusion</strong></h2>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13.333333969116211px; line-height: 18.99305534362793px;\">I think that the goodness of a world is mostly driven by the amount of explicit optimization that is going on to try and make the world good (this is all relative to my values, though a similar analysis seems to carry with respect to other aggregative values). This seems to be true even if relatively little optimization is going on. Fortunately, I also think that the future will be characterized by&nbsp;<em>much</em>&nbsp;higher influence for altruistic values. If I thought altruism was unlikely to win out, I would be concerned with changing that. As it is, I am instead more concerned with ensuring that the future proceeds without disruptions. (Though I still think it is worth it to try and increase the prevalence of altruism faster, most of all because this seems like a good approach to minimizing the probability of undesired disruptions.)</p>\n<p>&nbsp;</p>", "sections": [{"title": "How much altruism do we expect?", "anchor": "How_much_altruism_do_we_expect_", "level": 1}, {"title": "Dose natural selection select for self-interest?", "anchor": "Dose_natural_selection_select_for_self_interest_", "level": 2}, {"title": "Historical distortions", "anchor": "Historical_distortions", "level": 2}, {"title": "Short-range consequentialism", "anchor": "Short_range_consequentialism", "level": 3}, {"title": "Value drift", "anchor": "Value_drift", "level": 3}, {"title": "So what\u00a0does\u00a0natural selection select for?", "anchor": "So_what_does_natural_selection_select_for_", "level": 2}, {"title": "What values are we starting with?", "anchor": "What_values_are_we_starting_with_", "level": 2}, {"title": "How important is altruism?", "anchor": "How_important_is_altruism_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T11:57:09.574Z", "modifiedAt": null, "url": null, "title": "Need help with an MLP fanfiction with a transhumanist theme.", "slug": "need-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Salivanth", "createdAt": "2012-01-10T03:57:43.636Z", "isAdmin": false, "displayName": "Salivanth"}, "userId": "TEvWu3CMdGH5RiKMs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sPWbNqfXz3BzZNZzD/need-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "pageUrlRelative": "/posts/sPWbNqfXz3BzZNZzD/need-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "linkUrl": "https://www.lesswrong.com/posts/sPWbNqfXz3BzZNZzD/need-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Need%20help%20with%20an%20MLP%20fanfiction%20with%20a%20transhumanist%20theme.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeed%20help%20with%20an%20MLP%20fanfiction%20with%20a%20transhumanist%20theme.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPWbNqfXz3BzZNZzD%2Fneed-help-with-an-mlp-fanfiction-with-a-transhumanist-theme%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Need%20help%20with%20an%20MLP%20fanfiction%20with%20a%20transhumanist%20theme.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPWbNqfXz3BzZNZzD%2Fneed-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPWbNqfXz3BzZNZzD%2Fneed-help-with-an-mlp-fanfiction-with-a-transhumanist-theme", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1149, "htmlBody": "<p>&nbsp;</p>\n<p><strong>EDIT: I am now taking arguments for alicornism. Alicornism being the placeholder term I've given to the stance that all ponies should be alicorns. Please PM me or post here if you have a good one, or an argument against one of anti-alicornism's strongest points: Overpopulation/over-use of resources, magical abuse/existential risk, or upheaval of the respect ponies have for their rulers due to their alicorn status. I would prefer general arguments for alicornism over counter-arguments if possible. Deathist / anti-alicornist arguments are still fine to post here.</strong><br /><br />Disclaimer: I'm not sure if this is worthy of a discussion post, but I figured, given the amount of people on LW who like My Little Pony, it would have at least as many potentially interested people as a regional meet-up thread would, so I figured I'd give it a shot. If this is too trivial or frivolous for LW, feel free to tell me and/or downvote, and I'll refrain from such threads in future. A place where I could go to find some help instead of the Discussion section would also be greatly appreciated in such a case.<br /><br />So I had an idea for a one-shot or small novella, depending on how the plot developed, about an argument between Twilight and Celestia. Twilight finds out she's immortal now that she's an alicorn, and Twilight then decides that, given the standard anti-death concepts that immortality is good, death is bad, and so on, they should turn everyone who wants to be an alicorn into one.</p>\n<p>The problem is, I'm having a very difficult time coming up with actual arguments for Celestia.</p>\n<p>- Celestia herself is immortal, she's lived for well over a thousand years, and she isn't horrifically depressed, so clearly, immortal life is worth living and there's enough stuff to do with an extended lifespan.</p>\n<p>- For the purposes of this fic, it's possible to turn anypony into an alicorn. I'm likely going to go with the idea that the spell can only be used a few times a year, but that's still enough to turn anyone who wants it into an alicorn within a couple of decades via exponentiation: The first targets can all be gifted unicorns who can be easily trained to use the magic.</p>\n<p>- In most of the \"Immortality sucks\" fics I've read, the only real argument that immortality sucks is that you have to watch everyone else grow up and die. If a large majority of the population were turned alicorn, this wouldn't be a problem anymore.</p>\n<p>- Nothing in canon suggests that there's any sort of religion in Equestria. Even in fanfics I've read, I've only read one fanfic where someone made up an afterlife that some ponies believed in, and in many more that I've read, Celestia's name is actually used in place of God in various sentences, like \"Oh for Celestia's sake!\" Thus, it's unlikely they'd believe in an afterlife: Both in canon and the majority of fanon, the closest thing to a God appears to be Celestia herself.</p>\n<p>I've come up with arguments for Celestia by roleplaying the argument out by myself, but I haven't come up with anything that Twilight can't just shoot down, and I'd prefer if the argument wasn't just Celestia getting steamrolled, and I'd like to do this by strengthening Celestia's side, not weakening Twilight's.</p>\n<p>Is the argument for deathism really that weak? I've read over the Harry vs. Dumbledore deathism argument in HPMOR several times looking for ideas, and IIRC Eliezer actually claimed he steel-manned Dumbledore's position, but I don't find anything Dumbledore says convincing in the slightest, and ended that chapter feeling that Harry was the clear winner in that debate, and that's with Dumbledore having access to arguments that Celestia doesn't, given that in the Potterverse, nobody actually knows what it's like to be immortal, and Dumbledore believes in an afterlife.</p>\n<p>Some other arguments I've come up with for Celestia:</p>\n<p>Argument: We can't just have a massive ruling class. <br /><br />Response: There's no need for alicorns to be royalty. \"Princess = Alicorn, Alicorn = Princess\" is only something that law and tradition dictate: They can be changed. After all, Blueblood is a prince and not an alicorn, and it's certainly possible for an alicorn to NOT be royalty, if the princesses wanted.</p>\n<p>Argument:&nbsp;Harder to keep the populace in line, if everyone has more power. <br /><br />Response: Celestia's not exactly going around fighting criminals herself with her alicorn powers, so Celestia being much more powerful than others isn't necessary to keep the peace. If anything, an alicornified populace is MORE likely to be able to govern itself: Atm, a pegasus criminal can only be pursued effectively by about one-third of police officers, for example.</p>\n<p>Argument: Overpopulation. <br /><br />Response: One response to this is the idea that, starting a year or so from a royal edict, ponies who wish to be changed into alicorns aren't permitted to give birth more than once or twice. A broader response is that \"overpopulation\" isn't actually a reason to oppose alicornification, it's just a problem that has to be solved in order to do it. Saying \"There'd be overpopulation\" and then forgetting about the entire idea would be like Twilight saying that they didn't know how she was supposed to save the Crystal Empire from being banished again when she got given the task, and responding to this by saying \"Oh well, guess that's it, we may as well pack up and go home.\" rather than trying to actually solve the problem. That said, this is the only truly legitimate argument I've come up with, an argument that requires real thought to fully defeat, rather than an argument that has an easy response leap to my mind.</p>\n<p>Argument: Mortals wouldn't understand the consequence of their decision. <br /><br />Response: Again, several arguments for this. Firstly, there's no reason to believe the alicorn transformation is irreversible, even if it's not currently known how to transform it back. Secondly, Celestia can already predict the consequences, and since she thinks HER life is worth living, clearly there's a solid chance that other ponies will have their lives worth living as well.</p>\n<p>So, the questions to ask:</p>\n<p>Are there good arguments for Celestia I haven't thought of?</p>\n<p>Are the arguments I've already posited sufficient to not straw-man the lifeism position, and to allow for a reasonable argument?<br /><br /><strong>EDIT: I am now taking arguments for alicornism. Alicornism being the placeholder term I've given to the stance that all ponies should be alicorns. Please PM me or post here if you have a good one, or an argument against one of anti-alicornism's strongest points: Overpopulation/over-use of resources, magical abuse/existential risk, or upheaval of the respect ponies have for their rulers due to their alicorn status. I would prefer general arguments for alicornism over counter-arguments if possible. Deathist / anti-alicornist arguments are still fine to post here.</strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sPWbNqfXz3BzZNZzD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 15, "extendedScore": null, "score": 1.1241303451697956e-06, "legacy": true, "legacyId": "21808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T13:04:04.409Z", "modifiedAt": null, "url": null, "title": "Idea: Self-Improving Task Management Software", "slug": "idea-self-improving-task-management-software", "viewCount": null, "lastCommentedAt": "2013-03-10T23:25:19.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlp10", "createdAt": "2012-01-27T15:14:36.027Z", "isAdmin": false, "displayName": "rlp10"}, "userId": "wrpGdWaf8RqAtCytp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RcuedsWZ3B2JCaceH/idea-self-improving-task-management-software", "pageUrlRelative": "/posts/RcuedsWZ3B2JCaceH/idea-self-improving-task-management-software", "linkUrl": "https://www.lesswrong.com/posts/RcuedsWZ3B2JCaceH/idea-self-improving-task-management-software", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Idea%3A%20Self-Improving%20Task%20Management%20Software&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdea%3A%20Self-Improving%20Task%20Management%20Software%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcuedsWZ3B2JCaceH%2Fidea-self-improving-task-management-software%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Idea%3A%20Self-Improving%20Task%20Management%20Software%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcuedsWZ3B2JCaceH%2Fidea-self-improving-task-management-software", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcuedsWZ3B2JCaceH%2Fidea-self-improving-task-management-software", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 433, "htmlBody": "<p>So what the world needs is <a href=\"http://todotxt.com/\">yet</a> <a href=\"http://taskwarrior.org/projects/show/taskwarrior\">another</a> <a href=\"http://www.rememberthemilk.com/\">task</a> <a href=\"http://www.producteev.com/\">management</a> <a href=\"http://www.google.co.uk/search?q=task+management+software\">program</a>, right?</p>\n<p>My idea is software which automatically implements productivity strategies, measures the effectiveness of those strategies, and analyses which strategies work best <a href=\"/lw/9v/beware_of_otheroptimizing/\">for you</a>. &nbsp;Hopefully, using the software would result in a sustained increase in your productivity over time.</p>\n<p>By \"productivity strategies\" I mean things like: the recommendations in the&nbsp;<a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">the anti-procrastination algorithm</a>, the pomodoro technique, exercising regularly, <a href=\"http://www.stickk.com/\">pre-commitment</a>, experimenting with sleep patterns, <a href=\"https://habitrpg.com/splash.html\">gamifying your tasks</a> and so forth.</p>\n<p>In practical terms, what I'm envisioning is an extensible software framework. &nbsp;The core program would be a simple task list manager: add tasks to be done in the future, check off items as done when completed and send notifications to the user.</p>\n<p>This core framework would then be extended by plugins, which represented different productivity strategies. &nbsp;For example, the pomodoro plugin might make your first task at 9am each morning to review your task list and choose the most important three tasks (MITs), your second task to set and begin a timer for 30 minutes and your third task to complete that top MIT you chose. &nbsp;After 30 minutes, it would add a new task of taking a five minute relaxation break and send you a notification to let you know. &nbsp;Five minutes later, it would notify you again to finish your relaxation break task, with a fresh task to re-start the timer and then back to your MITs for a further 30 minutes.</p>\n<p>The software could independently activate and deactivate the plugins in order to collect sufficient data to suggest which strategies were most effective for you. &nbsp;Over time, more plugins would be written as people made further suggestions. &nbsp;Existing plugins could be potentially improved and automatically reviewed using <a href=\"http://en.wikipedia.org/wiki/A/B_testing\">A/B testing</a>.</p>\n<p>When deciding whether a strategy is \"effective\", I mean that a large number of tasks are completed, that the remaining number of tasks on the list is small and that the age of those tasks is not too great. &nbsp;However, the criteria could be extended to ask for an indication of mood from the user, to allow for low stress optimisation, for example. &nbsp;Perhaps <a href=\"http://tagti.me/\">stochastic self sampling</a> would work well here.</p>\n<p>If users were willing to opt into providing anonymous data, the software could automate a <a href=\"/lw/1sm/akrasia_tactics_review/\">community&nbsp;review</a> of the strategies: which strategies seem to be most commonly effective? &nbsp;<a href=\"http://en.wikipedia.org/wiki/Affinity_analysis\">Affinity analysis</a> could even be used to recommend plugins that were helpful to other people who responded to similar strategies as you.</p>\n<p>What are your comments, and specifically criticisms, of this idea? &nbsp;Would you try using software like this if it existed? &nbsp;Would you like to assist in writing software like this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RcuedsWZ3B2JCaceH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 19, "extendedScore": null, "score": 1.1241735811216286e-06, "legacy": true, "legacyId": "21809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6NvbSwuSAooQxxf7f", "Ty2tjPwv8uyPK9vrz", "rRmisKb45dN7DK4BW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T15:50:12.390Z", "modifiedAt": null, "url": null, "title": "[VIDEO] Harm reduction, hacker psychology", "slug": "video-harm-reduction-hacker-psychology", "viewCount": null, "lastCommentedAt": "2019-10-14T19:03:19.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9a3vCM5yAoynyPPD/video-harm-reduction-hacker-psychology", "pageUrlRelative": "/posts/t9a3vCM5yAoynyPPD/video-harm-reduction-hacker-psychology", "linkUrl": "https://www.lesswrong.com/posts/t9a3vCM5yAoynyPPD/video-harm-reduction-hacker-psychology", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BVIDEO%5D%20Harm%20reduction%2C%20hacker%20psychology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BVIDEO%5D%20Harm%20reduction%2C%20hacker%20psychology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9a3vCM5yAoynyPPD%2Fvideo-harm-reduction-hacker-psychology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BVIDEO%5D%20Harm%20reduction%2C%20hacker%20psychology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9a3vCM5yAoynyPPD%2Fvideo-harm-reduction-hacker-psychology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9a3vCM5yAoynyPPD%2Fvideo-harm-reduction-hacker-psychology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p><a href=\" http://www.youtube.com/watch?v=zq-bloM4Cmo\">Hackers As A High Risk Population</a>-- this is an hour-long video by Violet Blue, and it's about what she's been thinking about rather than something with definitive answers.</p>\n<p>It starts with an overview of harm reduction-- an approach to public health which begins by accepting that people are going to do the things they want to do, and continues by finding ways to make the things people want to do less dangerous. The most detail is about harm reduction for drug users, sexual minorities, and homeless people.</p>\n<p>Violet Blue got interested in the risks to programmers when a start-up entrepreneur got some bad press and committed suicide, but shifted over to studying hackers-- it turned out that the isolation, risk, long hours, and (in some cases?) responsibility led to psychological problems like those of spies and soldiers.</p>\n<p>It turned out that hackers aren't especially likely to have Asperger's, but the intense world model might be appropriate. The idea is that people on the spectrum (?) do notice other people's emotions, but get overwhelmed by them, and one way of dealing with the stress is to focus on intellectual details.</p>\n<p>She thinks of hackers as people who are especially driven by curiosity, which is a virtue.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9a3vCM5yAoynyPPD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.1242809395271499e-06, "legacy": true, "legacyId": "21810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T17:03:38.642Z", "modifiedAt": null, "url": null, "title": "Need some psychology advice", "slug": "need-some-psychology-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.209Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kenoubi", "createdAt": "2011-03-12T04:07:00.560Z", "isAdmin": false, "displayName": "Kenoubi"}, "userId": "DgrXt6eQMpunHRDXh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjNJcDEx4NXYupjxN/need-some-psychology-advice", "pageUrlRelative": "/posts/NjNJcDEx4NXYupjxN/need-some-psychology-advice", "linkUrl": "https://www.lesswrong.com/posts/NjNJcDEx4NXYupjxN/need-some-psychology-advice", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Need%20some%20psychology%20advice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeed%20some%20psychology%20advice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjNJcDEx4NXYupjxN%2Fneed-some-psychology-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Need%20some%20psychology%20advice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjNJcDEx4NXYupjxN%2Fneed-some-psychology-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjNJcDEx4NXYupjxN%2Fneed-some-psychology-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>I started going out with a fantastic girl a couple of weeks ago.&nbsp; Everything is great, except that whenever I've sent her a text message or email requesting something and haven't received a response yet, I experience significant dysphoric anxiety, fearing that her response will be not just \"no\" but \"no and I don't want to date you any more\".&nbsp; This is due to brain chemistry or personal history, take your pick&mdash;either seems like a possible explanation to me.&nbsp; But there's certainly no evidence supporting the idea that this is likely to happen, nor is the anxiety helping me prevent it or helping me in any other way.</p>\n<p>Does anyone have evidence-based advice, or pointers to same, on dealing with this kind of issue?&nbsp; It is the only splotch on what have otherwise been the best two weeks of my life.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjNJcDEx4NXYupjxN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 9, "extendedScore": null, "score": 1.1243284020734398e-06, "legacy": true, "legacyId": "21811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-27T18:00:47.990Z", "modifiedAt": null, "url": null, "title": "Feature request: Green glow for \"continue this thread\"", "slug": "feature-request-green-glow-for-continue-this-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.864Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4kaXYC67oPA6ZwPML/feature-request-green-glow-for-continue-this-thread", "pageUrlRelative": "/posts/4kaXYC67oPA6ZwPML/feature-request-green-glow-for-continue-this-thread", "linkUrl": "https://www.lesswrong.com/posts/4kaXYC67oPA6ZwPML/feature-request-green-glow-for-continue-this-thread", "postedAtFormatted": "Wednesday, February 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Feature%20request%3A%20Green%20glow%20for%20%22continue%20this%20thread%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFeature%20request%3A%20Green%20glow%20for%20%22continue%20this%20thread%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kaXYC67oPA6ZwPML%2Ffeature-request-green-glow-for-continue-this-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Feature%20request%3A%20Green%20glow%20for%20%22continue%20this%20thread%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kaXYC67oPA6ZwPML%2Ffeature-request-green-glow-for-continue-this-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kaXYC67oPA6ZwPML%2Ffeature-request-green-glow-for-continue-this-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>It would be much more convenient to follow discussions with many comments if \"continue this thread\" turned that bright green when there were not-previously-loaded comments at the link.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4kaXYC67oPA6ZwPML", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 17, "extendedScore": null, "score": 1.1243653441939238e-06, "legacy": true, "legacyId": "21812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-28T03:44:13.645Z", "modifiedAt": null, "url": null, "title": "Risk-aversion and investment (for altruists)", "slug": "risk-aversion-and-investment-for-altruists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:57.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CDdvNKGSTp77JrRZv/risk-aversion-and-investment-for-altruists", "pageUrlRelative": "/posts/CDdvNKGSTp77JrRZv/risk-aversion-and-investment-for-altruists", "linkUrl": "https://www.lesswrong.com/posts/CDdvNKGSTp77JrRZv/risk-aversion-and-investment-for-altruists", "postedAtFormatted": "Thursday, February 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risk-aversion%20and%20investment%20(for%20altruists)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisk-aversion%20and%20investment%20(for%20altruists)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDdvNKGSTp77JrRZv%2Frisk-aversion-and-investment-for-altruists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risk-aversion%20and%20investment%20(for%20altruists)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDdvNKGSTp77JrRZv%2Frisk-aversion-and-investment-for-altruists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDdvNKGSTp77JrRZv%2Frisk-aversion-and-investment-for-altruists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2226, "htmlBody": "<p><span style=\"text-align: justify; line-height: 1.5em;\">(Cross-posted from </span><a style=\"text-align: justify; line-height: 1.5em;\" href=\"http://rationalaltruist.com/\">rationalaltruist</a><span style=\"text-align: justify; line-height: 1.5em;\">)</span></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Suppose I hope to use my money to do good some day, but for now I am investing it and aiming to maximize my returns. I face the question: how much risk should I be willing to bear? Should I pursue safe investments, or riskier investments with higher returns?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">My knee-jerk response is to say &ldquo;An altruist should be risk neutral. If you have twice as much money, you can do twice as much good. Sure, there are some diminishing returns, but my own investment is&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">minuscule</em>&nbsp;compared to an entire world full of philanthropists. So in the regime where I am investing, returns are roughly linear.&rdquo; (I might revise this picture if I thought that I was a very unusual philanthropist, and that few others would invest in the same charitable causes as me&mdash;in that case I alone might represent a significant fraction of charitable investment in my causes of choice, so I should expect to personally run into diminishing returns.)</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">But on closer inspection there is something fishy about this reasoning. I don&rsquo;t have great data on the responsiveness of charitable giving to market performance, but at the individual level it&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.jstor.org/stable/3083340\" target=\"_parent\">seems</a>&nbsp;that the elasticity of charitable giving to income is about 1&mdash;if I am 50% richer (in one possible world than another), I tend to give 50% more to charity.&nbsp;<span style=\"line-height: 1.5em;\">So in worlds where markets do well, we should expect charities to have more money. If markets (rather, the average investor) do 10% better, I should expect 10% more money to be available for any particular charitable cause, regardless of how many donors it has.</span></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Of course, there is a difference between the market&rsquo;s performance and my portfolio&rsquo;s performance&mdash;the funds available to a charity depend little on how&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">my</em>&nbsp;portfolio does, just how the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">average</em>&nbsp;donor&rsquo;s portfolio does. So what is the relevance of the above result to my own risk aversion? Intuitively it seems that the performance of the market is correlated with the performance of any particular investor, but how tightly?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There is a simple observation / folk theorem that applies to this situation. Suppose two investors&rsquo; portfolios are not perfectly correlated. Then (supposing those investors are earning the same returns) each would prefer to trade 1/2 of their portfolio for 1/2 of the others&mdash;averaging two imperfectly correlated assets reduces the variance. In an efficient economy, this dynamic ensures that every investor&rsquo;s risk is well-correlated with the market. Any countercyclical assets will be absorbed into this ur-portfolio and invested in by every investor, thereby having the effect of reducing the variance of market returns.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There are many loopholes in this result, and markets are not perfectly efficient, but it provides an important intuition. If we have risky assets that are uncorrelated with (but have comparable returns to) the market, they will just be used to diversify portfolios and thereby become part of &ldquo;the market.&rdquo;</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">So to first order, the fact that I am a small piece of the charitable donations to a cause shouldn&rsquo;t matter. My risk is well-correlated with the risk of other investors, and if I lose 10% of my money in a year, other investors will also lose 10% of their money, and less money will be available for charitable giving. This holds regardless of whether a cause has a million donors or just one.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The original question: &ldquo;how risk averse should I be?&rdquo; is now a question about the returns to charitable activities at a large scale. Clearly the first charitable donations will go to the best causes. How quickly does the quality of marginal charitable giving decline, as total charitable giving increases? This question is fundamentally specific to a cause. For most causes, there seem to be substantial diminishing returns. Some diseases are much easier to treat than others, some disasters easier to mitigate, etc. etc. However, it is worth keeping in mind the distinction between diminishing returns to money in the long-run and in the short-run. For example, if you have only thought of one good thing to do with $1M, your second million dollars would not do nearly as much good if you had to spend it immediately. But this isn&rsquo;t because the second million dollars is much less valuable than the first million in the long run, it&rsquo;s because the second million would be complementary with thinking that you haven&rsquo;t yet done. In the long run you can spend more time thinking about what to do with $2M now that you have it, and put it to a good use. It still won&rsquo;t be as good as the first million, but not as much less valuable as it appears in the short run.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The following are some important caveats and postscripts to this basic picture.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Ordinary investors are&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">very</em>&nbsp;risk averse</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">I originally suspected that altruists should be risk-neutral because they are contributing only a small part to large projects, and therefore face roughly linear returns. By now I&rsquo;ve explicitly rejected that reasoning, but there is another reason that altruists might be interested in risky investments: ordinary investors appear to be&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">extremely</em>&nbsp;risk averse. Evidence of and explanations for the so-called equity premium puzzle are a bit tricky to untangle, but it&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.commonfund.org/ei/2012%20EI%20Level%20I%20Prereading%20Material/Required%20Readings%20for%20Day%201%20-%20Monday,%20July%209th/Session%203%20-%20Long%20Term%20Global%20Equity%20Returns%20and%20the%20Equity%20Risk%20Premium/Rethinking%20the%20Equity%20Risk%20Premium%202011%20-%20CHAPTER%204.pdf\" target=\"_parent\">looks like</a>&nbsp;there is a&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">big</em>&nbsp;premium on risk, such that risky equities earn annual returns a solid 3% higher than risk-free bonds.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">If you have logarithmic utility, and estimate risk using historical data on the variability of equities returns, equities are a slam-dunk over safer investments, with risk-adjusted returns that are nearly twice as good. (This observation is the basis for the equity premium puzzle. The paper I linked suggests that the equity premium is smaller than you might naively estimate from US data, but it is still big enough to constitute a puzzle.) I think logarithmic returns are fairly conservative for altruistic projects (though perhaps not individuals&rsquo; consumption), and that for most causes the payoffs are much more risk-neutral than that. So it looks like altruists ought to go for risky investments after all.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Moreover, I suspect (on priors) that altruists tend to invest as cautiously as other investors, and so it makes little sense for an altruistic investor to diversify their portfolio between equities and bonds even if there is a significant risk of collapse in equities (other altruists are doing the diversification for them).</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Some risks are uncorrelated with the market</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There are some opportunities which are risky but imperfectly correlated with the market (and sometimes nearly independent). For example, if you start or invest in a small company, your payoff will depend on that company&rsquo;s performance (which is typically quite risky but only weakly correlated with the market). In an idealized market this risk would be added to a larger portfolio of risks, but this often impossible due to moral hazard: if you received a paycheck that was independent of the success of your company, you would not be incentivized to run the company well, or to pick good companies to create or invest in. So no one is willing to sell you insurance in case your startup fails or your investment goes bad. The fact that you have to assume a big dose of risk is an unfortunate side-effect of this incentive scheme (and in a more efficient market we would expect angel investors and start-up founders to purchase more extensive insurance for various contingencies that would scuttle their enterprises but are clearly beyond their control).</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">To a normal person this risk is terrible, but to an altruist it should be considered a good opportunity (since other entrepreneurs and investors will tend to underprice such opportunities). See a discussion of this&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://80000hours.org/blog/12-salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers\" target=\"_parent\">here</a>, with some quantitative discussion.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">This special case is only possible because the entrepreneur or investor is putting in their own effort, and moral hazard makes it hard to smooth out all of the risk across a larger pool (though VC funds will invest in many startups). You shouldn&rsquo;t expect to find a similar situation in investments, except when you are providing insight which you trust but the rest of the market does not (thereby preventing you from insuring against your risk).</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Prioritizing possible worlds and concentrating investments</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Risk-aversion is a special case of a more general phenomenon; a dollar is worth a different amount in different possible worlds. For normal risk-aversion, the issue is how much money&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">you</em>&nbsp;have in different possible worlds. A dollar is worth the most in the worlds where you are poorest. For altruists, the issue is how much money&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">charities</em>&nbsp;have in different possible worlds. A dollar is worth the most in the worlds where the least money is given to charities, and the largest number of attractive interventions go unfunded.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">But there are other reasons that money may be more valuable in one possible world than another, which depend on which cause you actually want to support. Money aimed at helping the poor is most valuable in worlds where the developing world is not prospering. Vegan outreach is most useful in worlds where the meat industry is doing well, but vegetarian-friendly memes are prospering. Catastrophic risk mitigation is most valuable in troubled times. And so on. Each of these comparisons suggests an investment strategy; investors who care about cause X would prefer have money in worlds where it can be used to further cause X most efficiently.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Moreover, while ordinary risk averse investors are incentivized to construct a diversified portfolios, altruists have no such incentives. Though they should be concerned with risk, what they are really concerned with is the correlation between their risk and market returns. Thus they are not particularly interested in building a diversified portfolio, and it is particularly cheap for them to concentrate their investment in opportunities which will payoff in the worlds where they can best use money. Of course, this strategy becomes less attractive when very few people are interested in cause X, or when many of the investors interested in cause X are pursuing the same strategy&mdash;those investors care about the correlation of their investment returns with each other, and&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">collectively</em>&nbsp;they do want to diversify their investments. If everyone who cares about vegetarianism goes broke in worlds where McDonald&rsquo;s folds, it is&nbsp;no longer the case that vegetarian dollars are less valuable in those worlds.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Investing for the long haul</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">I&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://rationalaltruist.com/2013/02/22/four-flavors-of-time-discounting-i-endorse-and-one-i-do-not/\" target=\"_parent\">think</a>&nbsp;that altruists concerned about the far future should consider investing and earning market returns for as long as possible, before leveraging a much-increased share of the future economy to further their own interests. How does risk relate to this plan?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">It seems most productive to think about the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">fraction</em>&nbsp;of world wealth which an investor controls, since this quantity should be expected to remain fairly constant regardless of what happens economically (though will hopefully drift upwards as long as the altruist is more patient than the average investor) and ultimately controls how much influence that investor wields. A simple argument suggests that an investor concerned with maximizing their influence ought to maximize the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">expected</em>&nbsp;fraction of world wealth they control. This means that the value of an extra dollar of investment returns should vary inversely with the total wealth of the world. This means that the investor should act as if they were maximizing the expected&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">log</em>-wealth of the world.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The recommendations for this setting (investing for the long haul) are therefore nearly identical to the earlier setting (investing to give). As in the earlier case, the apparent arguments for maximizing expected returns are faulty because it is bad to be correlated with the market. But nevertheless, the equity premium is large enough that investing in risky assets is still worth it. In fact in this case the issue is even more clear-cut, since there is little uncertainty about how risk-averse we should be when investing for the long haul.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">The Kelly Criterion</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The Kelly criterion is a simple guideline for gambling / investing. The motivating observation is that, to maximize expected long-run returns, it is best to use a logarithmic utility function (because the total return after N periods is the geometric, rather than arithmetic, return during those periods). If we are directly concerned with logarithmic utility, we don&rsquo;t need to rely on this argument and should just use the Kelly criterion immediately.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The Kelly criterion recommends splitting your money according to the probability of a payout, rather than concentrating all of your money on the single best bet. (See also&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://en.wikipedia.org/wiki/Probability_matching\" target=\"_parent\">probability matching</a>, which has been&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://web.mit.edu/alo/www/Papers/origin.pdf\" target=\"_parent\">justified</a>&nbsp;on similar grounds.) In the case of investments, this corresponds to the following strategy (if we assume you have a negligible &ldquo;edge,&rdquo; or ability to predict tomorrow&rsquo;s market prices better than other investors). For each asset, estimate what fraction of the current world&rsquo;s&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">actual</em>&nbsp;wealth is stored in that asset, and invest that fraction of your bankroll in that asset. As prices change, reallocate your money to maintain the same distribution. (If the value of land doubles while the rest of the economy stagnates, such that you now have twice as large a fraction of your bankroll invest in land, then sell off half of your land). Of course, if other investors are following a similar rule, any price changes will be information about the long-run values of the underlying asset values, but this seems to be&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.overcomingbias.com/2011/06/dreamtime-finance.html\" target=\"_parent\">far from true</a>&nbsp;in the real world.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Investors who support a rare cause and care about diversifying their own portfolio, should probably pursue something like a Kelly strategy. But as I&rsquo;ve said before, investors pursuing common causes don&rsquo;t care about diversifying their portfolios, and instead they should use their portfolio to pull the<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">aggregate</em>&nbsp;investments of philanthropists in line with the Kelly rule investments. This seems to mean going all-in on relatively risky assets.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "jgcAJnksReZRuvgzp": 9, "xYLtnJ6keSHGfrLpe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CDdvNKGSTp77JrRZv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 27, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "21824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"text-align: justify; line-height: 1.5em;\">(Cross-posted from </span><a style=\"text-align: justify; line-height: 1.5em;\" href=\"http://rationalaltruist.com/\">rationalaltruist</a><span style=\"text-align: justify; line-height: 1.5em;\">)</span></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Suppose I hope to use my money to do good some day, but for now I am investing it and aiming to maximize my returns. I face the question: how much risk should I be willing to bear? Should I pursue safe investments, or riskier investments with higher returns?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">My knee-jerk response is to say \u201cAn altruist should be risk neutral. If you have twice as much money, you can do twice as much good. Sure, there are some diminishing returns, but my own investment is&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">minuscule</em>&nbsp;compared to an entire world full of philanthropists. So in the regime where I am investing, returns are roughly linear.\u201d (I might revise this picture if I thought that I was a very unusual philanthropist, and that few others would invest in the same charitable causes as me\u2014in that case I alone might represent a significant fraction of charitable investment in my causes of choice, so I should expect to personally run into diminishing returns.)</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">But on closer inspection there is something fishy about this reasoning. I don\u2019t have great data on the responsiveness of charitable giving to market performance, but at the individual level it&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.jstor.org/stable/3083340\" target=\"_parent\">seems</a>&nbsp;that the elasticity of charitable giving to income is about 1\u2014if I am 50% richer (in one possible world than another), I tend to give 50% more to charity.&nbsp;<span style=\"line-height: 1.5em;\">So in worlds where markets do well, we should expect charities to have more money. If markets (rather, the average investor) do 10% better, I should expect 10% more money to be available for any particular charitable cause, regardless of how many donors it has.</span></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Of course, there is a difference between the market\u2019s performance and my portfolio\u2019s performance\u2014the funds available to a charity depend little on how&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">my</em>&nbsp;portfolio does, just how the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">average</em>&nbsp;donor\u2019s portfolio does. So what is the relevance of the above result to my own risk aversion? Intuitively it seems that the performance of the market is correlated with the performance of any particular investor, but how tightly?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There is a simple observation / folk theorem that applies to this situation. Suppose two investors\u2019 portfolios are not perfectly correlated. Then (supposing those investors are earning the same returns) each would prefer to trade 1/2 of their portfolio for 1/2 of the others\u2014averaging two imperfectly correlated assets reduces the variance. In an efficient economy, this dynamic ensures that every investor\u2019s risk is well-correlated with the market. Any countercyclical assets will be absorbed into this ur-portfolio and invested in by every investor, thereby having the effect of reducing the variance of market returns.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There are many loopholes in this result, and markets are not perfectly efficient, but it provides an important intuition. If we have risky assets that are uncorrelated with (but have comparable returns to) the market, they will just be used to diversify portfolios and thereby become part of \u201cthe market.\u201d</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">So to first order, the fact that I am a small piece of the charitable donations to a cause shouldn\u2019t matter. My risk is well-correlated with the risk of other investors, and if I lose 10% of my money in a year, other investors will also lose 10% of their money, and less money will be available for charitable giving. This holds regardless of whether a cause has a million donors or just one.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The original question: \u201chow risk averse should I be?\u201d is now a question about the returns to charitable activities at a large scale. Clearly the first charitable donations will go to the best causes. How quickly does the quality of marginal charitable giving decline, as total charitable giving increases? This question is fundamentally specific to a cause. For most causes, there seem to be substantial diminishing returns. Some diseases are much easier to treat than others, some disasters easier to mitigate, etc. etc. However, it is worth keeping in mind the distinction between diminishing returns to money in the long-run and in the short-run. For example, if you have only thought of one good thing to do with $1M, your second million dollars would not do nearly as much good if you had to spend it immediately. But this isn\u2019t because the second million dollars is much less valuable than the first million in the long run, it\u2019s because the second million would be complementary with thinking that you haven\u2019t yet done. In the long run you can spend more time thinking about what to do with $2M now that you have it, and put it to a good use. It still won\u2019t be as good as the first million, but not as much less valuable as it appears in the short run.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The following are some important caveats and postscripts to this basic picture.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\" id=\"Ordinary_investors_are_very_risk_averse\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Ordinary investors are&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">very</em>&nbsp;risk averse</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">I originally suspected that altruists should be risk-neutral because they are contributing only a small part to large projects, and therefore face roughly linear returns. By now I\u2019ve explicitly rejected that reasoning, but there is another reason that altruists might be interested in risky investments: ordinary investors appear to be&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">extremely</em>&nbsp;risk averse. Evidence of and explanations for the so-called equity premium puzzle are a bit tricky to untangle, but it&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.commonfund.org/ei/2012%20EI%20Level%20I%20Prereading%20Material/Required%20Readings%20for%20Day%201%20-%20Monday,%20July%209th/Session%203%20-%20Long%20Term%20Global%20Equity%20Returns%20and%20the%20Equity%20Risk%20Premium/Rethinking%20the%20Equity%20Risk%20Premium%202011%20-%20CHAPTER%204.pdf\" target=\"_parent\">looks like</a>&nbsp;there is a&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">big</em>&nbsp;premium on risk, such that risky equities earn annual returns a solid 3% higher than risk-free bonds.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">If you have logarithmic utility, and estimate risk using historical data on the variability of equities returns, equities are a slam-dunk over safer investments, with risk-adjusted returns that are nearly twice as good. (This observation is the basis for the equity premium puzzle. The paper I linked suggests that the equity premium is smaller than you might naively estimate from US data, but it is still big enough to constitute a puzzle.) I think logarithmic returns are fairly conservative for altruistic projects (though perhaps not individuals\u2019 consumption), and that for most causes the payoffs are much more risk-neutral than that. So it looks like altruists ought to go for risky investments after all.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Moreover, I suspect (on priors) that altruists tend to invest as cautiously as other investors, and so it makes little sense for an altruistic investor to diversify their portfolio between equities and bonds even if there is a significant risk of collapse in equities (other altruists are doing the diversification for them).</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\" id=\"Some_risks_are_uncorrelated_with_the_market\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Some risks are uncorrelated with the market</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">There are some opportunities which are risky but imperfectly correlated with the market (and sometimes nearly independent). For example, if you start or invest in a small company, your payoff will depend on that company\u2019s performance (which is typically quite risky but only weakly correlated with the market). In an idealized market this risk would be added to a larger portfolio of risks, but this often impossible due to moral hazard: if you received a paycheck that was independent of the success of your company, you would not be incentivized to run the company well, or to pick good companies to create or invest in. So no one is willing to sell you insurance in case your startup fails or your investment goes bad. The fact that you have to assume a big dose of risk is an unfortunate side-effect of this incentive scheme (and in a more efficient market we would expect angel investors and start-up founders to purchase more extensive insurance for various contingencies that would scuttle their enterprises but are clearly beyond their control).</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">To a normal person this risk is terrible, but to an altruist it should be considered a good opportunity (since other entrepreneurs and investors will tend to underprice such opportunities). See a discussion of this&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://80000hours.org/blog/12-salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers\" target=\"_parent\">here</a>, with some quantitative discussion.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">This special case is only possible because the entrepreneur or investor is putting in their own effort, and moral hazard makes it hard to smooth out all of the risk across a larger pool (though VC funds will invest in many startups). You shouldn\u2019t expect to find a similar situation in investments, except when you are providing insight which you trust but the rest of the market does not (thereby preventing you from insuring against your risk).</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\" id=\"Prioritizing_possible_worlds_and_concentrating_investments\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Prioritizing possible worlds and concentrating investments</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Risk-aversion is a special case of a more general phenomenon; a dollar is worth a different amount in different possible worlds. For normal risk-aversion, the issue is how much money&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">you</em>&nbsp;have in different possible worlds. A dollar is worth the most in the worlds where you are poorest. For altruists, the issue is how much money&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">charities</em>&nbsp;have in different possible worlds. A dollar is worth the most in the worlds where the least money is given to charities, and the largest number of attractive interventions go unfunded.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">But there are other reasons that money may be more valuable in one possible world than another, which depend on which cause you actually want to support. Money aimed at helping the poor is most valuable in worlds where the developing world is not prospering. Vegan outreach is most useful in worlds where the meat industry is doing well, but vegetarian-friendly memes are prospering. Catastrophic risk mitigation is most valuable in troubled times. And so on. Each of these comparisons suggests an investment strategy; investors who care about cause X would prefer have money in worlds where it can be used to further cause X most efficiently.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Moreover, while ordinary risk averse investors are incentivized to construct a diversified portfolios, altruists have no such incentives. Though they should be concerned with risk, what they are really concerned with is the correlation between their risk and market returns. Thus they are not particularly interested in building a diversified portfolio, and it is particularly cheap for them to concentrate their investment in opportunities which will payoff in the worlds where they can best use money. Of course, this strategy becomes less attractive when very few people are interested in cause X, or when many of the investors interested in cause X are pursuing the same strategy\u2014those investors care about the correlation of their investment returns with each other, and&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">collectively</em>&nbsp;they do want to diversify their investments. If everyone who cares about vegetarianism goes broke in worlds where McDonald\u2019s folds, it is&nbsp;no longer the case that vegetarian dollars are less valuable in those worlds.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\" id=\"Investing_for_the_long_haul\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">Investing for the long haul</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">I&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://rationalaltruist.com/2013/02/22/four-flavors-of-time-discounting-i-endorse-and-one-i-do-not/\" target=\"_parent\">think</a>&nbsp;that altruists concerned about the far future should consider investing and earning market returns for as long as possible, before leveraging a much-increased share of the future economy to further their own interests. How does risk relate to this plan?</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">It seems most productive to think about the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">fraction</em>&nbsp;of world wealth which an investor controls, since this quantity should be expected to remain fairly constant regardless of what happens economically (though will hopefully drift upwards as long as the altruist is more patient than the average investor) and ultimately controls how much influence that investor wields. A simple argument suggests that an investor concerned with maximizing their influence ought to maximize the&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">expected</em>&nbsp;fraction of world wealth they control. This means that the value of an extra dollar of investment returns should vary inversely with the total wealth of the world. This means that the investor should act as if they were maximizing the expected&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">log</em>-wealth of the world.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The recommendations for this setting (investing for the long haul) are therefore nearly identical to the earlier setting (investing to give). As in the earlier case, the apparent arguments for maximizing expected returns are faulty because it is bad to be correlated with the market. But nevertheless, the equity premium is large enough that investing in risky assets is still worth it. In fact in this case the issue is even more clear-cut, since there is little uncertainty about how risk-averse we should be when investing for the long haul.</p>\n<h3 style=\"margin: 30px 0px 5px; padding: 0px; border: none; outline: 0px; font-size: 1.6em; vertical-align: baseline; color: #333333; font-family: Constantia, Palatino, 'Times New Roman', serif; position: static; line-height: normal; text-align: left;\" id=\"The_Kelly_Criterion\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 15.555556297302246px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">The Kelly Criterion</strong></h3>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The Kelly criterion is a simple guideline for gambling / investing. The motivating observation is that, to maximize expected long-run returns, it is best to use a logarithmic utility function (because the total return after N periods is the geometric, rather than arithmetic, return during those periods). If we are directly concerned with logarithmic utility, we don\u2019t need to rely on this argument and should just use the Kelly criterion immediately.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">The Kelly criterion recommends splitting your money according to the probability of a payout, rather than concentrating all of your money on the single best bet. (See also&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://en.wikipedia.org/wiki/Probability_matching\" target=\"_parent\">probability matching</a>, which has been&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://web.mit.edu/alo/www/Papers/origin.pdf\" target=\"_parent\">justified</a>&nbsp;on similar grounds.) In the case of investments, this corresponds to the following strategy (if we assume you have a negligible \u201cedge,\u201d or ability to predict tomorrow\u2019s market prices better than other investors). For each asset, estimate what fraction of the current world\u2019s&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">actual</em>&nbsp;wealth is stored in that asset, and invest that fraction of your bankroll in that asset. As prices change, reallocate your money to maintain the same distribution. (If the value of land doubles while the rest of the economy stagnates, such that you now have twice as large a fraction of your bankroll invest in land, then sell off half of your land). Of course, if other investors are following a similar rule, any price changes will be information about the long-run values of the underlying asset values, but this seems to be&nbsp;<a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://www.overcomingbias.com/2011/06/dreamtime-finance.html\" target=\"_parent\">far from true</a>&nbsp;in the real world.</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em;\">Investors who support a rare cause and care about diversifying their own portfolio, should probably pursue something like a Kelly strategy. But as I\u2019ve said before, investors pursuing common causes don\u2019t care about diversifying their portfolios, and instead they should use their portfolio to pull the<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 14.44444465637207px; vertical-align: baseline; background-color: transparent; background-position: initial initial; background-repeat: initial initial;\">aggregate</em>&nbsp;investments of philanthropists in line with the Kelly rule investments. This seems to mean going all-in on relatively risky assets.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Ordinary investors are\u00a0very\u00a0risk averse", "anchor": "Ordinary_investors_are_very_risk_averse", "level": 1}, {"title": "Some risks are uncorrelated with the market", "anchor": "Some_risks_are_uncorrelated_with_the_market", "level": 1}, {"title": "Prioritizing possible worlds and concentrating investments", "anchor": "Prioritizing_possible_worlds_and_concentrating_investments", "level": 1}, {"title": "Investing for the long haul", "anchor": "Investing_for_the_long_haul", "level": 1}, {"title": "The Kelly Criterion", "anchor": "The_Kelly_Criterion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-28T05:43:37.940Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] An African Folktale", "slug": "seq-rerun-an-african-folktale", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LBygpQ6wv3PqLwhEw/seq-rerun-an-african-folktale", "pageUrlRelative": "/posts/LBygpQ6wv3PqLwhEw/seq-rerun-an-african-folktale", "linkUrl": "https://www.lesswrong.com/posts/LBygpQ6wv3PqLwhEw/seq-rerun-an-african-folktale", "postedAtFormatted": "Thursday, February 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20An%20African%20Folktale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20An%20African%20Folktale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBygpQ6wv3PqLwhEw%2Fseq-rerun-an-african-folktale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20An%20African%20Folktale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBygpQ6wv3PqLwhEw%2Fseq-rerun-an-african-folktale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBygpQ6wv3PqLwhEw%2Fseq-rerun-an-african-folktale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/yl/an_african_folktale/\">An African Folktale</a> was originally published on 16 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#An_African_Folktale\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A story that seems to point to some major cultural differences.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gtq/seq_rerun_an_especially_elegant_evpsych_experiment/\">An Especially Elegant Evpsych Experiment</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LBygpQ6wv3PqLwhEw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1248197891319945e-06, "legacy": true, "legacyId": "21825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yyteh3qwD6kjhLiCb", "cZ8CQHbeRpoeYXqke", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-28T11:22:11.463Z", "modifiedAt": null, "url": null, "title": "Seize the Maximal Probability Moment", "slug": "seize-the-maximal-probability-moment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZqJLAasSRXqc32nXW/seize-the-maximal-probability-moment", "pageUrlRelative": "/posts/ZqJLAasSRXqc32nXW/seize-the-maximal-probability-moment", "linkUrl": "https://www.lesswrong.com/posts/ZqJLAasSRXqc32nXW/seize-the-maximal-probability-moment", "postedAtFormatted": "Thursday, February 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seize%20the%20Maximal%20Probability%20Moment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeize%20the%20Maximal%20Probability%20Moment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqJLAasSRXqc32nXW%2Fseize-the-maximal-probability-moment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seize%20the%20Maximal%20Probability%20Moment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqJLAasSRXqc32nXW%2Fseize-the-maximal-probability-moment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqJLAasSRXqc32nXW%2Fseize-the-maximal-probability-moment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p>Try and remember 3 or 4 things that you think would be <em>effective hacks</em> for your life but you have <em>not</em> so far implemented. Really, find three.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Probably that was not so hard.</p>\n<p>&nbsp;</p>\n<p>Now think of at which moment in time did you have a maximal probability of having implemented such hacks. Sometimes you had no idea that was the moment. But sometimes you did, like when a friend tells you \"I just read this great paper on how people report cartoons being funnier when their face is shaped in a more smiling fashion.\" and you thought \"Great! I may one day implement the algorithm: if studying, force a smile\".</p>\n<p>You knew you didn't plan to read the article, you knew you trust that friend, and you knew you'd either forget it later, or in any case that <em>from that moment on, the likelihood of you implementing the algorithm would lower.</em></p>\n<p>So my hack of the day is: If you feel you are likely at the maximal probability moment to start a new policy, start immediately.</p>\n<p>&nbsp;</p>\n<p>My friend was telling me about how he went abroad to research: \"...so at this place and people there used very strong lights as cognitive enhancement and yadda yadda yadda... (stopped listening for 40s) yadda yadda yadda.... and I wrote a paper on ...\"&nbsp; By that time my room had an extra 110W light working. &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>Just now<sub>0</sub> I thought: It was good I installed that light. Why didn't I do the same when I felt like finding a personalized shirt website where the front would be \"I Don't want to talk about: [list]\" and the back \"Pick your topic: [list]\" to once and for all stop the gossip and sports ice-breakers?&nbsp;</p>\n<p>I didn't seize the maximal probability moment. That's what happened.</p>\n<p>&nbsp;</p>\n<p>Then I noticed that that<sub>1</sub> was the maximal probability moment to install in my mind the maximal probability moment algorithm, I did,&nbsp; and <em>that</em><sub>2</sub> was the maximal probability moment of writing this post.</p>\n<p>Now if you'll excuse me, I have<sub>3</sub> a shirt to buy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dqx5k65wjFfaiJ9sQ": 1, "Tg9aFPFCPBHxGABRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZqJLAasSRXqc32nXW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 40, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "21829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-28T14:15:55.090Z", "modifiedAt": "2020-12-23T03:16:58.735Z", "url": null, "title": "Decision Theory FAQ", "slug": "decision-theory-faq", "viewCount": null, "lastCommentedAt": "2018-10-27T16:35:49.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq", "pageUrlRelative": "/posts/zEWJBFFMvQ835nq6h/decision-theory-faq", "linkUrl": "https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq", "postedAtFormatted": "Thursday, February 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theory%20FAQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theory%20FAQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEWJBFFMvQ835nq6h%2Fdecision-theory-faq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theory%20FAQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEWJBFFMvQ835nq6h%2Fdecision-theory-faq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEWJBFFMvQ835nq6h%2Fdecision-theory-faq", "socialPreviewImageUrl": "http://i.imgur.com/DhCAW.jpg", "question": false, "authorIsUnreviewed": false, "wordCount": 17283, "htmlBody": "<p><small>Co-authored with <a href=\"/user/crazy88/overview/\">crazy88</a>. Please let us know when you find mistakes, and we'll fix them. Last updated 03-27-2013.</small></p>\n<p><strong>Contents</strong>:</p>\n<div id=\"TOC\">\n<ul>\n<li><a href=\"#what-is-decision-theory\">1. What is decision theory?</a></li>\n<li><a href=\"#is-the-rational-decision-always-the-right-decision\">2. Is the rational decision always the right decision?</a></li>\n<li><a href=\"#how-can-i-better-understand-a-decision-problem\">3. How can I better understand a decision problem?</a></li>\n<li><a href=\"#how-can-i-measure-an-agents-preferences\">4. How can I measure an agent's preferences?</a> \n<ul>\n<li><a href=\"#the-concept-of-utility\">4.1. The concept of utility</a></li>\n<li><a href=\"#types-of-utility\">4.2. Types of utility</a></li>\n</ul>\n</li>\n<li><a href=\"#what-do-decision-theorists-mean-by-risk-ignorance-and-uncertainty\">5. What do decision theorists mean by \"risk,\" \"ignorance,\" and \"uncertainty\"?</a></li>\n<li><a href=\"#how-should-i-make-decisions-under-ignorance\">6. How should I make decisions under ignorance?</a> \n<ul>\n<li><a href=\"#the-dominance-principle\">6.1. The dominance principle</a></li>\n<li><a href=\"#maximin-and-leximin\">6.2. Maximin and leximin</a></li>\n<li><a href=\"#maximax-and-optimism-pessimism\">6.3. Maximax and optimism-pessimism</a></li>\n<li><a href=\"#other-decision-principles\">6.4. Other decision principles</a></li>\n</ul>\n</li>\n<li><a href=\"#can-decisions-under-ignorance-be-transformed-into-decisions-under-uncertainty\">7. Can decisions under ignorance be transformed into decisions under uncertainty?</a></li>\n<li><a href=\"#how-should-i-make-decisions-under-uncertainty\">8. How should I make decisions under uncertainty?</a> \n<ul>\n<li><a href=\"#the-law-of-large-numbers\">8.1. The law of large numbers</a></li>\n<li><a href=\"#the-axiomatic-approach\">8.2. The axiomatic approach</a></li>\n<li><a href=\"#the-von-neumann-morgenstern-utility-theorem\">8.3. The Von Neumann-Morgenstern utility theorem</a></li>\n<li><a href=\"#vnm-utility-theory-and-rationality\">8.4. VNM utility theory and rationality</a></li>\n<li><a href=\"#objections-to-vnm-rationality\">8.5. Objections to VNM-rationality</a></li>\n<li><a href=\"#should-we-accept-the-vnm-axioms\">8.6. Should we accept the VNM axioms?</a></li>\n</ul>\n</li>\n<li><a href=\"#does-axiomatic-decision-theory-offer-any-action-guidance\">9. Does axiomatic decision theory offer any action guidance?</a></li>\n<li><a href=\"#how-does-probability-theory-play-a-role-in-decision-theory\">10. How does probability theory play a role in decision theory?</a> \n<ul>\n<li><a href=\"#the-basics-of-probability-theory\">10.1. The basics of probability theory</a></li>\n<li><a href=\"#bayes-theorem-for-updating-probabilities\">10.2. Bayes theorem for updating probabilities</a></li>\n<li><a href=\"#how-should-probabilities-be-interpreted\">10.3. How should probabilities be interpreted?</a></li>\n</ul>\n</li>\n<li><a href=\"#what-about-newcombs-problem-and-alternative-decision-algorithms\">11. What about \"Newcomb's problem\" and alternative decision algorithms?</a> \n<ul>\n<li><a href=\"#newcomblike-problems-and-two-decision-algorithms\">11.1. Newcomblike problems and two decision algorithms</a></li>\n<li><a href=\"#benchmark-theory-bt\">11.2. Benchmark theory (BT)</a></li>\n<li><a href=\"#timeless-decision-theory-tdt\">11.3. Timeless decision theory (TDT)</a></li>\n<li><a href=\"#decision-theory-and-winning\">11.4. Decision theory and \u201cwinning\u201d</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<h2 id=\"what-is-decision-theory\"><br></h2>\n<h2><a href=\"#what-is-decision-theory\">1. What is decision theory?</a></h2>\n<p><em>Decision theory</em>, also known as <em>rational choice theory</em>, concerns the study of preferences, uncertainties, and other issues related to making \"optimal\" or \"rational\" choices. It has been discussed by economists, psychologists, philosophers, mathematicians, statisticians, and computer scientists.</p>\n<p>We can divide decision theory into three parts (<a href=\"http://www.owlnet.rice.edu/~econ501/lectures/Decision_EU.pdf\">Grant &amp; Zandt 2009</a>; <a href=\"http://www.amazon.com/dp/0521680433/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Baron 2008</a>). <em>Normative</em> decision theory studies what an ideal agent (a perfectly rational agent, with infinite computing power, etc.) would choose. <em>Descriptive</em> decision theory studies how non-ideal agents (e.g. humans) <em>actually</em> choose. <em>Prescriptive</em> decision theory studies how non-ideal agents can improve their decision-making (relative to the normative model) despite their imperfections.</p>\n<p>For example, one's <em>normative</em> model might be <a href=\"http://kleene.ss.uci.edu/lpswiki/index.php/Expected_Utility_Theory\">expected utility theory</a>, which says that a rational agent chooses the action with the highest expected utility. Replicated results in psychology <em>describe</em> humans repeatedly <em>failing</em> to maximize expected utility in particular, <a href=\"http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Edition/dp/0061353248/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">predictable</a> ways: for example, they make some choices based not on potential future benefits but on irrelevant past efforts (the \"<a href=\"http://en.wikipedia.org/wiki/Sunk_costs\">sunk cost fallacy</a>\"). To help people avoid this error, some theorists <em>prescribe</em> some basic training in microeconomics, which has been shown to reduce the likelihood that humans will commit the sunk costs fallacy (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/08/Larrick-et-al-Teaching-the-use-of-cost-benefit-reasoning-in-everyday-life.pdf\">Larrick et al. 1990</a>). Thus, through a coordination of normative, descriptive, and prescriptive research we can help agents to succeed in life by acting more in accordance with the normative model than they otherwise would.</p>\n<p>This FAQ focuses on normative decision theory. Good sources on descriptive and prescriptive decision theory include <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Stanovich (2010)</a> and <a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hastie &amp; Dawes (2009)</a>.</p>\n<p>Two related fields beyond the scope of this FAQ are <a href=\"http://en.wikipedia.org/wiki/Game_theory\">game theory</a> and <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">social choice theory</a>. Game theory is the study of conflict and cooperation among multiple decision makers, and is thus sometimes called \"interactive decision theory.\" Social choice theory is the study of making a collective decision by combining the preferences of multiple decision makers in various ways.</p>\n<p>This FAQ draws heavily from two textbooks on decision theory: <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a> and <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009)</a>. It also draws from more recent results in decision theory, published in journals such as <em><a href=\"http://www.springerlink.com/content/0039-7857\">Synthese</a></em> and <em><a href=\"http://www.springerlink.com/content/0040-5833\">Theory and Decision</a></em>.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"is-the-rational-decision-always-the-right-decision\"><a href=\"#is-the-rational-decision-always-the-right-decision\">2. Is the rational decision always the right decision?</a></h2>\n<p>No. Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 1) explains:</p>\n<blockquote>\n<p>[In 1700], King Carl of Sweden and his 8,000 troops attacked the Russian army [which] had about ten times as many troops... Most historians agree that the Swedish attack was irrational, since it was almost certain to fail... However, because of an unexpected blizzard that blinded the Russian army, the Swedes won...</p>\n</blockquote>\n<blockquote>\n<p>Looking back, the Swedes' decision to attack the Russian army was no doubt right, since the <em>actual outcome</em> turned out to be success. However, since the Swedes had no <em>good reason</em> for expecting that they were going to win, the decision was nevertheless irrational.</p>\n</blockquote>\n<blockquote>\n<p>More generally speaking, we say that a decision is <em>right</em> if and only if its actual outcome is at least as good as that of every other possible outcome. Furthermore, we say that a decision is <em>rational</em> if and only if the decision maker [<em>aka</em> the \"agent\"] chooses to do what she has most reason to do at the point in time at which the decision is made.</p>\n</blockquote>\n<p>Unfortunately, we cannot know with certainty what the right decision is. Thus, the best we can do is to try to make \"rational\" or \"optimal\" decisions based on our preferences and incomplete information.</p>\n<p>&nbsp;</p>\n<h2 id=\"how-can-i-better-understand-a-decision-problem\"><a href=\"#how-can-i-better-understand-a-decision-problem\">3. How can I better understand a decision problem?</a></h2>\n<p>First, we must <em>formalize</em> a decision problem. It usually helps to <em>visualize</em> the decision problem, too.</p>\n<p>In decision theory, decision rules are only defined relative to a formalization of a given decision problem, and a formalization of a decision problem can be visualized in multiple ways. Here is an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 2):</p>\n<blockquote>\n<p>Suppose... that you are thinking about taking out fire insurance on your home. Perhaps it costs $100 to take out insurance on a house worth $100,000, and you ask: Is it worth it?</p>\n</blockquote>\n<p>The most common way to formalize a decision problem is to break it into states, acts, and outcomes. When facing a decision problem, the decision maker aims to choose the <em>act</em> that will have the best <em>outcome</em>. But the outcome of each act depends on the <em>state</em> of the world, which is unknown to the decision maker.</p>\n<p>In this framework, speaking loosely, a state is a part of the world that is not an act (that can be performed now by the decision maker) or an outcome (the question of what, more precisely, states are is a complex question that is beyond the scope of this document). Luckily, not all states are relevant to a particular decision problem. We only need to take into account states that affect the agent's preference among acts. A simple formalization of the fire insurance problem might include only two states: the state in which your house doesn't (later) catch on fire, and the state in which your house <em>does</em> (later) catch on fire.</p>\n<p>Presumably, the agent prefers some outcomes to others. Suppose the four conceivable outcomes in the above decision problem are: (1) House and $0, (2) House and -$100, (3) No house and $99,900, and (4) No house and $0. In this case, the decision maker might prefer outcome 1 over outcome 2, outcome 2 over outcome 3, and outcome 3 over outcome 4. (We'll discuss measures of value for outcomes in the next section.)</p>\n<p>An act is commonly taken to be a function that takes one set of the possible states of the world as input and gives a particular outcome as output. For the above decision problem we could say that if the act \"Take out insurance\" has the world-state \"Fire\" as its input, then it will give the outcome \"No house and $99,900\" as its output.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/DhCAW.jpg\" alt=\"An outline of the states, acts and outcomes in the insurance case\">\n<p class=\"caption\">An outline of the states, acts and outcomes in the insurance case</p>\n</div>\n<p>Note that decision theory is concerned with <em>particular</em> acts rather than <em>generic</em> acts, e.g. \"sailing west in 1492\" rather than \"sailing.\" Moreover, the acts of a decision problem must be <em>alternative</em> acts, so that the decision maker has to choose exactly <em>one</em> act.</p>\n<p>Once a decision problem has been formalized, it can then be visualized in any of several ways.</p>\n<p>One way to visualize this decision problem is to use a <em>decision matrix</em>:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Fire</em></td>\n<td><em>No fire</em></td>\n</tr>\n<tr>\n<td><em>Take out insurance</em></td>\n<td>No house and $99,900</td>\n<td>House and -$100</td>\n</tr>\n<tr>\n<td><em>No insurance</em></td>\n<td>No house and $0</td>\n<td>House and $0</td>\n</tr>\n</tbody>\n</table>\n<p>Another way to visualize this problem is to use a <em>decision tree</em>:</p>\n<p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1608693326/basic-decision-tree_ohriwo.gif\" alt=\"\"></p>\n<p>The square is a <em>choice node</em>, the circles are <em>chance nodes</em>, and the triangles are <em>terminal nodes</em>. At the choice node, the decision maker chooses which branch of the decision tree to take. At the chance nodes, <em>nature</em> decides which branch to follow. The triangles represent outcomes.</p>\n<p>Of course, we could add more branches to each choice node and each chance node. We could also add more choice nodes, in which case we are representing a <em>sequential</em> decision problem. Finally, we could add probabilities to each branch, as long as the probabilities of all the branches extending from each single node sum to 1. And because a decision tree obeys the laws of probability theory, we can calculate the probability of any given node by multiplying the probabilities of all the branches preceding it.</p>\n<p>Our decision problem could also be represented as a <em>vector</em> \u2014 an ordered list of mathematical objects that is perhaps most suitable for computers:</p>\n<blockquote>\n<p>[<br> [a<sub>1</sub> = take out insurance,<br> a<sub>2</sub> = do not];<br> [s<sub>1</sub> = fire,<br> s<sub>2</sub> = no fire];<br> [(a<sub>1</sub>, s<sub>1</sub>) = No house and $99,900,<br> (a<sub>1</sub>, s<sub>2</sub>) = House and -$100,<br> (a<sub>2</sub>, s<sub>1</sub>) = No house and $0,<br> (a<sub>2</sub>, s<sub>2</sub>) = House and $0]<br> ]</p>\n</blockquote>\n<p>For more details on formalizing and visualizing decision problems, see <a href=\"http://www.amazon.com/Introduction-Decision-Analysis-3rd-Edition/dp/0964793865/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Skinner (1993)</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"how-can-i-measure-an-agents-preferences\"><a href=\"#how-can-i-measure-an-agents-preferences\">4. How can I measure an agent's preferences?</a></h2>\n<h3 id=\"the-concept-of-utility\"><a href=\"#the-concept-of-utility\">4.1. The concept of utility</a></h3>\n<p>It is important not to measure an agent's preferences in terms of <em>objective</em> value, e.g. monetary value. To see why, consider the absurdities that can result when we try to measure an agent's preference with money alone.</p>\n<p>Suppose you may choose between (A) receiving a million dollars <em>for sure</em>, and (B) a 50% chance of winning either $3 million or nothing. The <em>expected monetary value</em> (EMV) of your act is computed by multiplying the monetary value of each possible outcome by its probability. So, the EMV of choice A is (1)($1 million) = $1 million. The EMV of choice B is (0.5)($3 million) + (0.5)($0) = $1.5 million. Choice B has a higher expected monetary value, and yet many people would prefer the guaranteed million.</p>\n<p>Why? For many people, the difference between having $0 and $1 million is <em>subjectively</em> much larger than the difference between having $1 million and $3 million, even if the latter difference is larger in dollars.</p>\n<p>To capture an agent's <em>subjective</em> preferences, we use the concept of <em>utility</em>. A <em>utility function</em> assigns numbers to outcomes such that outcomes with higher numbers are preferred to outcomes with lower numbers. For example, for a particular decision maker \u2014 say, one who has no money \u2014 the utility of $0 might be 0, the utility of $1 million might be 1000, and the utility of $3 million might be 1500. Thus, the <em>expected utility</em> (EU) of choice A is, for this decision maker, (1)(1000) = 1000. Meanwhile, the EU of choice B is (0.5)(1500) + (0.5)(0) = 750. In this case, the expected utility of choice A is greater than that of choice B, even though choice B has a greater expected monetary value.</p>\n<p>Note that those from the field of statistics who work on decision theory tend to talk about a \"loss function,\" which is simply an <em>inverse</em> utility function. For an overview of decision theory from this perspective, see <a href=\"http://www.amazon.com/Statistical-Decision-Bayesian-Analysis-Statistics/dp/1441930744/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Berger (1985)</a> and <a href=\"http://www.amazon.com/Bayesian-Choice-Decision-Theoretic-Computational-Implementation/dp/0387715983/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Robert (2001)</a>. For a critique of some standard results in statistical decision theory, see <a href=\"http://www.amazon.com/Probability-Theory-The-Logic-Science/dp/0521592712/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jaynes (2003, ch. 13)</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"types-of-utility\"><a href=\"#types-of-utility\">4.2. Types of utility</a></h3>\n<p>An agent's utility function can't be directly observed, so it must be constructed \u2014 e.g. by asking them which options they prefer for a large set of pairs of alternatives (as on <a href=\"http://www.whoishotter.com\">WhoIsHotter.com</a>). The number that corresponds to an outcome's utility can convey different information depending on the <em>utility scale</em> in use, and the utility scale in use depends on how the utility function is constructed.</p>\n<p>Decision theorists distinguish three kinds of utility scales:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>Ordinal scales (\"12 is better than 6\"). In an ordinal scale, preferred outcomes are assigned higher numbers, but the numbers don't tell us anything about the differences or ratios between the utility of different outcomes.</p>\n</li>\n<li>\n<p>Interval scales (\"the difference between 12 and 6 equals that between 6 and 0\"). An interval scale gives us more information than an ordinal scale. Not only are preferred outcomes assigned higher numbers, but also the numbers accurately reflect the <em>difference</em> between the utility of different outcomes. They do not, however, necessarily reflect the ratios of utility between different outcomes. If outcome A has utility 0, outcome B has utility 6, and outcome C has utility 12 on an interval scale, then we know that the difference in utility between outcomes A and B and between outcomes B and C is the same, but we can't know whether outcome B is \"twice as good\" as outcome A.</p>\n</li>\n<li>\n<p>Ratio scales (\"12 is exactly <em>twice</em> as valuable as 6\"). Numerical utility assignments on a ratio scale give us the most information of all. They accurately reflect preference rankings, differences, <em>and</em> ratios. Thus, we can say that an outcome with utility 12 is exactly <em>twice</em> as valuable to the agent in question as an outcome with utility 6.</p>\n</li>\n</ol>\n<p>Note that neither <em>experienced utility</em> (happiness) nor the notions of \"average utility\" or \"total utility\" discussed by utilitarian moral philosophers are the same thing as the <em>decision utility</em> that we are discussing now to describe decision preferences. As the situation merits, we can be even more specific. For example, when discussing the type of decision utility used in an interval scale utility function constructed using Von Neumann &amp; Morgenstern's axiomatic approach (see section 8), some people use the term <em>VNM-utility</em>.</p>\n<p>Now that you know that an agent's preferences can be represented as a \"utility function,\" and that assignments of utility to outcomes can mean different things depending on the utility scale of the utility function, we are ready to think more formally about the challenge of making \"optimal\" or \"rational\" choices. (We will return to the problem of constructing an agent's utility function later, in section 8.3.)</p>\n<p>&nbsp;</p>\n<h2 id=\"what-do-decision-theorists-mean-by-risk-ignorance-and-uncertainty\"><a href=\"#what-do-decision-theorists-mean-by-risk-ignorance-and-uncertainty\">5. What do decision theorists mean by \"risk,\" \"ignorance,\" and \"uncertainty\"?</a></h2>\n<p>Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 1) explains:</p>\n<blockquote>\n<p>In decision theory, everyday terms such as <em>risk</em>, <em>ignorance</em>, and <em>uncertainty</em> are used as technical terms with precise meanings. In decisions under risk the decision maker knows the probability of the possible outcomes, whereas in decisions under ignorance the probabilities are either unknown or non-existent. Uncertainty is either used as a synonym for ignorance, or as a broader term referring to both risk and ignorance.</p>\n</blockquote>\n<p>In this FAQ, a \"decision under ignorance\" is one in which probabilities are <em>not</em> assigned to all outcomes, and a \"decision under uncertainty\" is one in which probabilities <em>are</em> assigned to all outcomes. The term \"risk\" will be reserved for discussions related to utility.</p>\n<p>&nbsp;</p>\n<h2 id=\"how-should-i-make-decisions-under-ignorance\"><a href=\"#how-should-i-make-decisions-under-ignorance\">6. How should I make decisions under ignorance?</a></h2>\n<p>A decision maker faces a \"decision under ignorance\" when she (1) knows which acts she could choose and which outcomes they may result in, but (2) is unable to assign probabilities to the outcomes.</p>\n<p>(Note that many theorists think that all decisions under ignorance can be transformed into decisions under uncertainty, in which case this section will be irrelevant except for subsection 6.1. For details, see section 7.)</p>\n<p>&nbsp;</p>\n<h3 id=\"the-dominance-principle\"><a href=\"#the-dominance-principle\">6.1. The dominance principle</a></h3>\n<p>To borrow an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3), suppose that Jane isn't sure whether to order hamburger or monkfish at a new restaurant. Just about any chef can make an edible hamburger, and she knows that monkfish is fantastic if prepared by a world-class chef, but she also recalls that monkfish is difficult to cook. Unfortunately, she knows too little about this restaurant to assign any probability to the prospect of getting good monkfish. Her decision matrix might look like this:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Good chef</em></td>\n<td><em>Bad chef</em></td>\n</tr>\n<tr>\n<td><em>Monkfish</em></td>\n<td>good monkfish</td>\n<td>terrible monkfish</td>\n</tr>\n<tr>\n<td><em>Hamburger</em></td>\n<td>edible hamburger</td>\n<td>edible hamburger</td>\n</tr>\n<tr>\n<td><em>No main course</em></td>\n<td>hungry</td>\n<td>hungry</td>\n</tr>\n</tbody>\n</table>\n<p>Here, decision theorists would say that the \"hamburger\" choice <em>dominates</em> the \"no main course\" choice. This is because choosing the hamburger leads to a better outcome for Jane no matter which possible state of the world (good chef or bad chef) turns out to be true.</p>\n<p>This <em>dominance principle</em> comes in two forms:</p>\n<ul>\n<li><em>Weak dominance</em>: One act is <em>more</em> rational than another if (1) all its possible outcomes are at least as good as those of the other, and if (2) there is at least one possible outcome that is better than that of the other act.</li>\n<li><em>Strong dominance</em>: One act is <em>more</em> rational than another if all of its possible outcome are better than that of the other act.</li>\n</ul>\n<div class=\"figure\"><img src=\"http://i.imgur.com/7fU6U.jpg\" alt=\"A comparison of strong and weak dominance\">\n<p class=\"caption\">A comparison of strong and weak dominance</p>\n</div>\n<p>The dominance principle can also be applied to decisions under uncertainty (in which probabilities <em>are</em> assigned to all the outcomes). If we assign probabilities to outcomes, it is still rational to choose one act over another act if all its outcomes are at least as good as the outcomes of the other act.</p>\n<p>However, the dominance principle only applies (non-controversially) when the agent\u2019s acts are independent of the state of the world. So consider the decision of whether to steal a coat:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Charged with theft</em></td>\n<td><em>Not charged with theft</em></td>\n</tr>\n<tr>\n<td><em>Theft</em></td>\n<td>Jail and coat</td>\n<td>Freedom and coat</td>\n</tr>\n<tr>\n<td><em>No theft</em></td>\n<td>Jail</td>\n<td>Freedom</td>\n</tr>\n</tbody>\n</table>\n<p>In this case, stealing the coat dominates not doing so but isn\u2019t necessarily the rational decision. After all, stealing increases your chance of getting charged with theft and might be irrational for this reason. So dominance doesn\u2019t apply in cases like this where the state of the world is not independent of the agents act.</p>\n<p>On top of this, not all decision problems include an act that dominates all the others. Consequently additional principles are often required to reach a decision.</p>\n<p>&nbsp;</p>\n<h3 id=\"maximin-and-leximin\"><a href=\"#maximin-and-leximin\">6.2. Maximin and leximin</a></h3>\n<p>Some decision theorists have suggested the <em>maximin principle</em>: if the worst possible outcome of one act is better than the worst possible outcome of another act, then the former act should be chosen. In Jane's decision problem above, the maximin principle would prescribe choosing the hamburger, because the worst possible outcome of choosing the hamburger (\"edible hamburger\") is better than the worst possible outcome of choosing the monkfish (\"terrible monkfish\") and is also better than the worst possible outcome of eating no main course (\"hungry\").</p>\n<p>If the worst outcomes of two or more acts are equally good, the maximin principle tells you to be indifferent between them. But that doesn't seem right. For this reason, fans of the maximin principle often invoke the <em>lexical</em> maximin principle (\"leximin\"), which says that if the worst outcomes of two or more acts are equally good, one should choose the act for which the <em>second worst</em> outcome is best. (If that doesn't single out a single act, then the <em>third worst</em> outcome should be considered, and so on.)</p>\n<p>Why adopt the leximin principle? Advocates point out that the leximin principle transforms a decision problem under ignorance into a decision problem under partial certainty. The decision maker doesn't know what the outcome will be, but they know what the worst possible outcome will be.</p>\n<p>But in some cases, the leximin rule seems clearly irrational. Imagine this decision problem, with two possible acts and two possible states of the world:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>$1</td>\n<td>$10,001.01</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>$1.01</td>\n<td>$1.01</td>\n</tr>\n</tbody>\n</table>\n<p>In this situation, the leximin principle prescribes choosing a<sub>2</sub>. But most people would agree it is rational to risk losing out on a single cent for the chance to get an extra $10,000.</p>\n<p>&nbsp;</p>\n<h3 id=\"maximax-and-optimism-pessimism\"><a href=\"#maximax-and-optimism-pessimism\">6.3. Maximax and optimism-pessimism</a></h3>\n<p>The maximin and leximin rules focus their attention on the worst possible outcomes of a decision, but why not focus on the <em>best</em> possible outcome? The <em>maximax principle</em> prescribes that if the best possible outcome of one act is better than the best possible outcome of another act, then the former act should be chosen.</p>\n<p>More popular among decision theorists is the <em>optimism-pessimism rule</em> (<em>aka</em> the <em>alpha-index rule</em>). The optimism-pessimism rule prescribes that one consider both the best and worst possible outcome of each possible act, and then choose according to one's degree of optimism or pessimism.</p>\n<p>Here's an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3):</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n<td>s<sub>3</sub></td>\n<td>s<sub>4</sub></td>\n<td>s<sub>5</sub></td>\n<td>s<sub>6</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>55</td>\n<td>18</td>\n<td>28</td>\n<td>10</td>\n<td>36</td>\n<td>100</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>50</td>\n<td>87</td>\n<td>55</td>\n<td>90</td>\n<td>75</td>\n<td>70</td>\n</tr>\n</tbody>\n</table>\n<p>We represent the decision maker's level of optimism on a scale of 0 to 1, where 0 is maximal pessimism and 1 is maximal optimism. For a<sub>1</sub>, the worst possible outcome is 10 and the best possible outcome is 100. That is, min(a<sub>1</sub>) = 10 and max(a<sub>1</sub>) = 100. So if the decision maker is 0.85 optimistic, then the total value of a<sub>1</sub> is (0.85)(100) + (1 - 0.85)(10) = 86.5, and the total value of a<sub>2</sub> is (0.85)(90) + (1 - 0.85)(50) = 84. In this situation, the optimism-pessimism rule prescribes action a<sub>1</sub>.</p>\n<p>If the decision maker's optimism is 0, then the optimism-pessimism rule collapses into the maximin rule because (0)(max(a<sub>i</sub>)) + (1 - 0)(min(a<sub>i</sub>)) = min(a<sub>i</sub>). And if the decision maker's optimism is 1, then the optimism-pessimism rule collapses into the maximax rule. Thus, the optimism-pessimism rule turns out to be a generalization of the maximin and maximax rules. (Well, sort of. The minimax and maximax principles require only that we measure value on an ordinal scale, whereas the optimism-pessimism rule requires that we measure value on an interval scale.)</p>\n<p>The optimism-pessimism rule pays attention to both the best-case and worst-case scenarios, but is it rational to ignore all the outcomes in between? Consider this example:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n<td>s<sub>3</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>1</td>\n<td>2</td>\n<td>100</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>1</td>\n<td>99</td>\n<td>100</td>\n</tr>\n</tbody>\n</table>\n<p>The maximum and minimum values for a<sub>1</sub> and a<sub>2</sub> are the same, so for every degree of optimism both acts are equally good. But it seems obvious that one should choose a<sub>2</sub>.</p>\n<p>&nbsp;</p>\n<h3 id=\"other-decision-principles\"><a href=\"#other-decision-principles\">6.4. Other decision principles</a></h3>\n<p>Many other decision principles for dealing with decisions under ignorance have been proposed, including <a href=\"http://teaching.ust.hk/~bee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\">minimax regret</a>, <a href=\"http://www.amazon.com/Info-Gap-Decision-Theory-Second-Edition/dp/0123735521/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">info-gap</a>, and <a href=\"http://www.existential-risk.org/concept.pdf\">maxipok</a>. For more details on making decisions under ignorance, see <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009)</a> and <a href=\"http://www.dss.dpem.tuc.gr/pdf/Choice%20under%20complete%20uncertainty%20-%20axiomatic%20characterizati.pdf\">Bossert et al. (2000)</a>.</p>\n<p>One queer feature of the decision principles discussed in this section is that they willfully disregard some information relevant to making a decision. Such a move could make sense when trying to find a decision algorithm that performs well under tight limits on available computation (<a href=\"http://www.dss.dpem.tuc.gr/pdf/An%20axiomatic%20treatment%20of%20three%20qualitative%20decision%20criteri.pdf\">Brafman &amp; Tennenholtz (2000)</a>), but it's unclear why an <em>ideal</em> agent with infinite computing power (fit for a <em>normative</em> rather than a <em>prescriptive</em> theory) should willfully disregard information.</p>\n<p>&nbsp;</p>\n<h2 id=\"can-decisions-under-ignorance-be-transformed-into-decisions-under-uncertainty\"><a href=\"#can-decisions-under-ignorance-be-transformed-into-decisions-under-uncertainty\">7. Can decisions under ignorance be transformed into decisions under uncertainty?</a></h2>\n<p>Can decisions under ignorance be transformed into decisions under uncertainty? This would simplify things greatly, because there is near-universal agreement that decisions under uncertainty should be handled by \"maximizing expected utility\" (see section 11 for clarifications), whereas decision theorists still debate what should be done about decisions under ignorance.</p>\n<p>For <a href=\"http://en.wikipedia.org/wiki/Bayesian_probability\">Bayesians</a> (see section 10), <em>all</em> decisions under ignorance are transformed into decisions under uncertainty (<a href=\"http://www.amazon.com/Introduction-Bayesian-Inference-Decision-Edition/dp/0964793849/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Winkler 2003</a>, ch. 5) when the decision maker assigns an \"ignorance prior\" to each outcome for which they don't know how to assign a probability. (Another way of saying this is to say that a Bayesian decision maker never faces a decision under ignorance, because a Bayesian must always assign a prior probability to events.) One must then consider how to assign priors, an important debate among Bayesians (see section 10).</p>\n<p>Many non-Bayesian decision theorists also think that decisions under ignorance can be transformed into decisions under uncertainty due to something called the <em>principle of insufficient reason</em>. The principle of insufficient reason prescribes that if you have literally <em>no</em> reason to think that one state is more probable than another, then one should assign <em>equal</em> probability to both states.</p>\n<p>One objection to the principle of insufficient reason is that it is very sensitive to how states are individuated. Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3) explains:</p>\n<blockquote>\n<p>Suppose that before embarking on a trip you consider whether to bring an umbrella or not. [But] you know nothing about the weather at your destination. If the formalization of the decision problem is taken to include only two states, viz. rain and no rain, [then by the principle of insufficient reason] the probability of each state will be 1/2. However, it seems that one might just as well go for a formalization that divides the space of possibilities into three states, viz. heavy rain, moderate rain, and no rain. If the principle of insufficient reason is applied to the latter set of states, their probabilities will be 1/3. In some cases this difference will affect our decisions. Hence, it seems that anyone advocating the principle of insufficient reason must [defend] the rather implausible hypothesis that there is only one correct way of making up the set of states.</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/kXn03.jpg\" alt=\"An objection to the principle of insufficient reason\">\n<p class=\"caption\">An objection to the principle of insufficient reason</p>\n</div>\n<p>Advocates of the principle of insufficient reason might respond that one must consider <em>symmetric</em> states. For example if someone gives you a die with <em>n</em> sides and you have no reason to think the die is biased, then you should assign a probability of 1/<em>n</em> to each side. But, Peterson notes:</p>\n<blockquote>\n<p>...not all events can be described in symmetric terms, at least not in a way that justifies the conclusion that they are equally probable. Whether Ann's marriage will be a happy one depends on her future emotional attitude toward her husband. According to one description, she could be either in love or not in love with him; then the probability of both states would be 1/2. According to another equally plausible description, she could either be deeply in love, a little bit in love or not at all in love with her husband; then the probability of each state would be 1/3.</p>\n<p>&nbsp;</p>\n</blockquote>\n<h2 id=\"how-should-i-make-decisions-under-uncertainty\"><a href=\"#how-should-i-make-decisions-under-uncertainty\">8. How should I make decisions under uncertainty?</a></h2>\n<p>A decision maker faces a \"decision under uncertainty\" when she (1) knows which acts she could choose and which outcomes they may result in, and she (2) assigns probabilities to the outcomes.</p>\n<p>Decision theorists generally agree that when facing a decision under uncertainty, it is rational to choose the act with the highest expected utility. This is the principle of <em>expected utility maximization</em> (EUM).</p>\n<p>Decision theorists offer two kinds of justifications for EUM. The first has to do with the law of large numbers (see section 8.1). The second has to do with the axiomatic approach (see sections 8.2 through 8.6).</p>\n<p>&nbsp;</p>\n<h3 id=\"the-law-of-large-numbers\"><a href=\"#the-law-of-large-numbers\">8.1. The law of large numbers</a></h3>\n<p>The \"law of large numbers,\" which states that <em>in the long run</em>, if you face the same decision problem again and again and again, and you always choose the act with the highest expected utility, then you will almost certainly be better off than if you choose any other acts.</p>\n<p>There are two problems with using the law of large numbers to justify EUM. The first problem is that the world is ever-changing, so we rarely if ever face the same decision problem \"again and again and again.\" The law of large numbers says that if you face the same decision problem infinitely many times, then the probability that you could do better by not maximizing expected utility approaches zero. But you won't ever face the same decision problem infinitely many times! Why should you care what would happen if a certain condition held, if you know that condition will never hold?</p>\n<p>The second problem with using the law of large numbers to justify EUM has to do with a mathematical theorem known as <em>gambler's ruin</em>. Imagine that you and I flip a fair coin, and I pay you $1 every time it comes up heads and you pay me $1 every time it comes up tails. We both start with $100. If we flip the coin enough times, one of us will face a situation in which the sequence of heads or tails is longer than we can afford. If a long-enough sequence of heads comes up, I'll run out of $1 bills with which to pay you. If a long-enough sequence of tails comes up, you won't be able to pay me. So in this situation, the law of large numbers guarantees that you will be better off in the long run by maximizing expected utility only if you start the game with an infinite amount of money (so that you never go broke), which is an unrealistic assumption. (For technical convenience, assume utility increases linearly with money. But the basic point holds without this assumption.)</p>\n<p>&nbsp;</p>\n<h3 id=\"the-axiomatic-approach\"><a href=\"#the-axiomatic-approach\">8.2. The axiomatic approach</a></h3>\n<p>The other method for justifying EUM seeks to show that EUM can be derived from axioms that hold regardless of what happens in the long run.</p>\n<p>In this section we will review perhaps the most famous axiomatic approach, from <a href=\"http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Von Neumann and Morgenstern (1947)</a>. Other axiomatic approaches include <a href=\"http://www.amazon.com/The-Foundations-Statistics-Leonard-Savage/dp/0486623491/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Savage (1954)</a>, <a href=\"http://www.amazon.com/The-Logic-Decision-Richard-Jeffrey/dp/0226395820/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jeffrey (1983)</a>, and <a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Ambiguity/AnscombeAumann%20AMS%2063.pdf\">Anscombe &amp; Aumann (1963)</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"the-von-neumann-morgenstern-utility-theorem\"><a href=\"#the-von-neumann-morgenstern-utility-theorem\">8.3. The Von Neumann-Morgenstern utility theorem</a></h3>\n<p>The first decision theory axiomatization appeared in an appendix to the second edition of Von Neumann &amp; Morgenstern's <em><a href=\"http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Theory of Games and Economic Behavior</a></em> (1947). An important point to note up front is that, in this axiomatization, Von Neumann and Morgenstern take the options that the agent chooses between to not be acts, as we\u2019ve defined them, but lotteries (where a lottery is a set of outcomes, each paired with a probability). As such, while discussing their axiomatization, we will talk of lotteries. (Despite making this distinction, acts and lotteries are closely related. Under the conditions of uncertainty that we are considering here, each act will be associated with some lottery and so preferences over lotteries could be used to determine preferences over acts, if so desired).</p>\n<p>The key feature of the Von Neumann and Morgenstern axiomatization is a proof that if a decision maker states her preferences over a set of lotteries, and if her preferences conform to a set of intuitive structural constraints (axioms), then we can construct a utility function (on an interval scale) from her preferences over lotteries and show that she acts <em>as if</em> she maximizes expected utility with respect to that utility function.</p>\n<p>What are the axioms to which an agent's preferences over lotteries must conform? There are four of them.</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>The <em>completeness axiom</em> states that the agent must <em>bother to state a preference</em> for each pair of lotteries. That is, the agent must prefer A to B, or prefer B to A, or be indifferent between the two.</p>\n</li>\n<li>\n<p>The <em>transitivity axiom</em> states that if the agent prefers A to B and B to C, she must also prefer A to C.</p>\n</li>\n<li>\n<p>The <em>independence axiom</em> states that, for example, if an agent prefers an apple to an orange, then she must also prefer the lottery [55% chance she gets an apple, otherwise she gets cholera] over the lottery [55% chance she gets an orange, otherwise she gets cholera]. More generally, this axiom holds that a preference must hold independently of the possibility of another outcome (e.g. cholera).</p>\n</li>\n<li>\n<p>The <em>continuity axiom</em> holds that if the agent prefers A to B to C, then there exists a unique <em>p</em> (probability) such that the agent is indifferent between [<em>p</em>(A) + (1 - <em>p</em>)(C)] and [outcome B with certainty].</p>\n</li>\n</ol>\n<p>The continuity axiom requires <a href=\"http://www.youtube.com/watch?v=hSUsiA8dhKM\">more explanation</a>. Suppose that A = $1 million, B = $0, and C = Death. If <em>p</em> = 0.5, then the agent's two lotteries under consideration for the moment are:</p>\n<ol style=\"list-style-type: decimal\">\n<li>(0.5)($1M) + (1 - 0.5)(Death) [win $1M with 50% probability, die with 50% probability]</li>\n<li>(1)($0) [win $0 with certainty]</li>\n</ol>\n<p>Most people would <em>not</em> be indifferent between $0 with certainty and [50% chance of $1M, 50% chance of Death] \u2014 the risk of Death is too high! But if you have continuous preferences, there is <em>some</em> probability <em>p</em> for which you'd be indifferent between these two lotteries. Perhaps <em>p</em> is very, very high:</p>\n<ol style=\"list-style-type: decimal\">\n<li>(0.999999)($1M) + (1 - 0.999999)(Death) [win $1M with 99.9999% probability, die with 0.0001% probability]</li>\n<li>(1)($0) [win $0 with certainty]</li>\n</ol>\n<p>Perhaps now you'd be indifferent between lottery 1 and lottery 2. Or maybe you'd be <em>more</em> willing to risk Death for the chance of winning $1M, in which case the <em>p</em> for which you'd be indifferent between lotteries 1 and 2 is lower than 0.999999. As long as there is <em>some</em> <em>p</em> at which you'd be indifferent between lotteries 1 and 2, your preferences are \"continuous.\"</p>\n<p>Given this setup, Von Neumann and Morgenstern proved their theorem, which states that if the agent's preferences over lotteries obeys their axioms, then:</p>\n<ul>\n<li>The agent's preferences can be represented by a utility function that assigns higher utility to preferred lotteries.</li>\n<li>The agent acts in accordance with the principle of maximizing expected utility.</li>\n<li>All utility functions satisfying the above two conditions are \"positive linear transformations\" of each other. (Without going into the details: this is why VNM-utility is measured on an interval scale.)</li>\n</ul>\n<h3 id=\"vnm-utility-theory-and-rationality\"><br></h3>\n<h3><a href=\"#vnm-utility-theory-and-rationality\">8.4. VNM utility theory and rationality</a></h3>\n<p>An agent which conforms to the VNM axioms is sometimes said to be \"VNM-rational.\" But why should \"VNM-rationality\" constitute our notion of <em>rationality in general</em>? How could VNM's result justify the claim that a rational agent maximizes expected utility when facing a decision under uncertainty? The argument goes like this:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If an agent chooses lotteries which it prefers (in decisions under uncertainty), and if its preferences conform to the VNM axioms, then it is rational. Otherwise, it is irrational.</li>\n<li>If an agent chooses lotteries which it prefers (in decisions under uncertainty), and if its preferences conform to the VNM axioms, then it maximizes expected utility.</li>\n<li>Therefore, a rational agent maximizes expected utility (in decisions under uncertainty).</li>\n</ol>\n<p>Von Neumann and Morgenstern proved premise 2, and the conclusion follows from premise 1 and 2. But why accept premise 1?</p>\n<p>Few people deny that it would be irrational for an agent to choose a lottery which it does not prefer. But why is it irrational for an agent's preferences to violate the VNM axioms? I will save that discussion for section 8.6.</p>\n<p>&nbsp;</p>\n<h3 id=\"objections-to-vnm-rationality\"><a href=\"#objections-to-vnm-rationality\">8.5. Objections to VNM-rationality</a></h3>\n<p>Several objections have been raised to Von Neumann and Morgenstern's result:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p><em>The VNM axioms are too strong</em>. Some have argued that the VNM axioms are not self-evidently true. See section 8.6.</p>\n</li>\n<li>\n<p><em>The VNM system offers no action guidance</em>. A VNM-rational decision maker cannot use VNM utility theory for action guidance, because she must state her preferences over lotteries at the start. But if an agent can state her preferences over lotteries, then she already knows which lottery to choose. (For more on this, see section 9.)</p>\n</li>\n<li>\n<p><em>In the VNM system, utility is defined via preferences over lotteries rather than preferences over outcomes</em>. To many, it seems odd to <em>define</em> utility with respect to preferences over lotteries. Many would argue that utility should be defined in relation to preferences over <em>outcomes</em> or <em>world-states</em>, and that's not what the VNM system does. (Also see section 9.)</p>\n</li>\n</ol>\n<h3 id=\"should-we-accept-the-vnm-axioms\"><br></h3>\n<h3><a href=\"#should-we-accept-the-vnm-axioms\">8.6. Should we accept the VNM axioms?</a></h3>\n<p>The VNM preference axioms define what it is for an agent to be VNM-rational. But why should we accept these axioms? Usually, it is argued that each of the axioms are <em>pragmatically justified</em> because an agent which violates the axioms can face situations in which they are guaranteed end up worse off (from <em>their own</em> perspective).</p>\n<p>In sections 8.6.1 and 8.6.2 I go into some detail about pragmatic justifications offered for the transitivity and completeness axioms. For more detail, including arguments about the justification of the other axioms, see <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 8)</a> and <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anand (1993)</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"the-transitivity-axiom\"><a href=\"#the-transitivity-axiom\">8.6.1. The transitivity axiom</a></h4>\n<p>Consider the <em>money-pump argument</em> in favor of the transitivity axiom (\"if the agent prefers A to B and B to C, she must also prefer A to C\").</p>\n<blockquote>\n<p>Imagine that a friend offers to give you exactly one of her three... novels, x or y or z... [and] that your preference ordering over the three novels is... [that] you prefer x to y, and y to z, and z to x... [That is, your preferences are <em>cyclic</em>, which is a type of <em>intransitive</em> preference relation.] Now suppose that you are in possession of z, and that you are invited to swap z for y. Since you prefer y to z, rationality obliges you to swap. So you swap, and temporarily get y. You are then invited to swap y for x, which you do, since you prefer x to y. Finally, you are offered to <em>pay a small amount</em>, say one cent, for swapping x for z. Since z is strictly [preferred to] x, even after you have paid the fee for swapping, rationality tells you that you should accept the offer. This means that you end up where you started, the only difference being that you now have one cent less. This procedure is thereafter iterated over and over again. After a billion cycles you have lost ten million dollars, for which you have got nothing in return. (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2009</a>, ch. 8)</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/45csd.jpg\" alt=\"An example of a money-pump argument\">\n<p class=\"caption\">An example of a money-pump argument</p>\n</div>\n<p>Similar arguments (e.g. <a href=\"http://johanegustafsson.net/papers/a_money-pump_for_acyclic_intransitive_preferences.pdf\">Gustafsson 2010</a>) aim to show that the other kind of intransitive preferences (acyclic preferences) are irrational, too.</p>\n<p>(Of course, pragmatic arguments need not be framed in monetary terms. We could just as well construct an argument showing that an agent with intransitive preferences can be \"pumped\" of all their happiness, or all their moral virtue, or all their Twinkies.)</p>\n<p>&nbsp;</p>\n<h4 id=\"the-completeness-axiom\"><a href=\"#the-completeness-axiom\">8.6.2. The completeness axiom</a></h4>\n<p>The completeness axiom (\"the agent must prefer A to B, or prefer B to A, or be indifferent between the two\") is often attacked by saying that some goods or outcomes are incommensurable \u2014 that is, they cannot be compared. For example, must a rational agent be able to state a preference (or indifference) between money and human welfare?</p>\n<p>Perhaps the completeness axiom can be justified with a pragmatic argument. If you think it is rationally permissible to swap between two incommensurable goods, then one can construct a money pump argument in favor of the completeness axiom. But if you think it is <em>not</em> rational to swap between incommensurable goods, then one cannot construct a money pump argument for the completeness axiom. (In fact, even if it is rational to swap between incommensurable goods, <a href=\"http://personal.rhul.ac.uk/uhte/035/incomplete%20preferences.geb.pdf\">Mandler, 2005</a> has demonstrated that an agent that allows their current choices to depend on the previous ones can avoid being money pumped.)</p>\n<p>And in fact, there is a popular argument <em>against</em> the completeness axiom: the \"small improvement argument.\" For details, see <a href=\"http://www.amazon.com/Incommensurability-Incomparability-Practical-Reason-Chang/dp/0674447565/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Chang (1997)</a> and <a href=\"https://www.msb.se/Upload/Om%20MSB/Forskning/Projektrapporter/Peterson_artiklar/Small_Improvment_Argument.pdf\">Espinoza (2007)</a>.</p>\n<p>Note that in <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\">revealed preference theory</a>, according to which preferences are revealed through choice behavior, there is no room for incommensurable preferences because every choice always reveals a preference relation of \"better than,\" \"worse than,\" or \"equally as good as.\"</p>\n<p>Another proposal for dealing with the apparent incommensurability of some goods (such as money and human welfare) is the <em>multi-attribute approach</em>:</p>\n<blockquote>\n<p>In a multi-attribute approach, each type of attribute is measured in the unit deemed to be most suitable for that attribute. Perhaps money is the right unit to use for measuring financial costs, whereas the number of lives saved is the right unit to use for measuring human welfare. The total value of an alternative is thereafter determined by aggregating the attributes, e.g. money and lives, into an overall ranking of available alternatives...</p>\n</blockquote>\n<blockquote>\n<p>Several criteria have been proposed for choosing among alternatives with multiple attributes... [For example,] additive criteria assign weights to each attribute, and rank alternatives according to the weighted sum calculated by multiplying the weight of each attribute with its value... [But while] it is perhaps contentious to measure the utility of very different objects on a common scale, ...it seems equally contentious to assign numerical weights to attributes as suggested here....</p>\n</blockquote>\n<blockquote>\n<p>[Now let us] consider a very general objection to multi-attribute approaches. According to this objection, there exist several equally plausible but different ways of constructing the list of attributes. Sometimes the outcome of the decision process depends on which set of attributes is chosen. (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2009</a>, ch. 8)</p>\n</blockquote>\n<p>For more on the multi-attribute approach, see <a href=\"http://www.amazon.com/Decisions-Multiple-Objectives-Preferences-Tradeoffs/dp/0521438837/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Keeney &amp; Raiffa (1993)</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"the-allais-paradox\"><a href=\"#the-allais-paradox\">8.6.3. The Allais paradox</a></h4>\n<p>Having considered the transitivity and completeness axioms, we can now turn to independence (a preference holds independently of considerations of other possible outcomes). Do we have any reason to reject this axiom? Here\u2019s one reason to think we might: in a case known as the <em>Allais paradox</em> <a href=\"http://www.jstor.org/stable/1907921\">Allais (1953)</a> it may seem reasonable to act in a way that contradicts independence.</p>\n<p>The Allais paradox asks us to consider two decisions (this version of the paradox is based on <a href=\"/lw/my/the_allais_paradox/\">Yudkowsky (2008)</a>).The first decision involves the choice between:</p>\n<p>(1A) A certain $24,000; and (1B) A 33/34 chance of $27,000 and a 1/34 chance of nothing.</p>\n<p>The second involves the choice between:</p>\n<p>(2A) A 34% chance of $24, 000 and a 66% chance of nothing; and (2B) A 33% chance of $27, 000 and a 67% chance of nothing.</p>\n<p>Experiments have shown that many people prefer (1A) to (1B) and (2B) to (2A). However, these preferences contradict independence. Option 2A is the same as [a 34% chance of option 1A and a 66% chance of nothing] while 2B is the same as [a 34% chance of option 1B and a 66% chance of nothing]. So independence implies that anyone that prefers (1A) to (1B) must also prefer (2A) to (2B).</p>\n<p>When this result was first uncovered, it was presented as evidence against the independence axiom. However, while the Allais paradox clearly reveals that independence fails as a <em>descriptive</em> account of choice, it\u2019s less clear what it implies about the normative account of rational choice that we are discussing in this document. As noted in <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 4)</a>, however:</p>\n<blockquote>\n<p>[S]ince many people who have thought very hard about this example still feel that it would be rational to stick to the problematic preference pattern described above, there seems to be something wrong with the expected utility principle.</p>\n</blockquote>\n<p>However, Peterson then goes on to note that, many people, like the statistician Leonard Savage, argue that it is people\u2019s preference in the Allais paradox that are in error rather than the independence axiom. If so, then the paradox seems to reveal the danger of relying too strongly on intuition to determine the form that should be taken by normative theories of rational.</p>\n<p>&nbsp;</p>\n<h4 id=\"the-ellsberg-paradox\"><a href=\"#the-ellsberg-paradox\">8.6.4. The Ellsberg paradox</a></h4>\n<p>The Allais paradox is far from the only case where people fail to act in accordance with EUM. Another well-known case is the Ellsberg paradox (the following is taken from <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a>:</p>\n<blockquote>\n<p>An urn contains ninety uniformly sized balls, which are randomly distributed. Thirty of the balls are yellow, the remaining sixty are red or blue. We are not told how many red (blue) balls are in the urn \u2013 except that they number anywhere from zero to sixty. Now consider the following pair of situations. In each situation a ball will be drawn and we will be offered a bet on its color. In situation A we will choose between betting that it is yellow or that it is red. In situation B we will choose between betting that it is red or blue or that it is yellow or blue.</p>\n</blockquote>\n<p>If we guess the correct color, we will receive a payout of $100. In the Ellsberg paradox, many people bet <em>yellow</em> in situation A and <em>red or blue</em> in situation B. Further, many people make these decisions not because they are indifferent in both situations, and so happy to choose either way, but rather because they have a strict preference to choose in this manner.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/tZKOsHx.jpg\" alt=\"The Ellsberg paradox\">\n<p class=\"caption\">The Ellsberg paradox</p>\n</div>\n<p>However, such behavior cannot be in accordance with EUM. In order for EUM to endorse a strict preference for choosing <em>yellow</em> in situation A, the agent would have to assign a probability of more than 1/3 to the ball selected being blue. On the other hand, in order for EUM to endorse a strict preference for choosing <em>red or blue</em> in situation B the agent would have to assign a probability of less than 1/3 to the selected ball being blue. As such, these decisions can\u2019t be jointly endorsed by an agent following EUM.</p>\n<p>Those who deny that decisions making under ignorance can be transformed into decision making under uncertainty have an easy response to the Ellsberg paradox: as this case involves deciding under a situation of ignorance, it is irrelevant whether people\u2019s decisions violate EUM in this case as EUM is not applicable to such situations.</p>\n<p>Those who believe that EUM provides a suitable standard for choice in such situations, however, need to find some other way of responding to the paradox. As with the Allais paradox, there is some disagreement about how best to do so. Once again, however, many people, including Leonard Savage, argue that EUM reaches the right decision in this case. It is our intuitions that are flawed (see again <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a> for a nice summary of Savage\u2019s argument to this conclusion).</p>\n<p>&nbsp;</p>\n<h4 id=\"the-st-petersburg-paradox\"><a href=\"#the-st-petersburg-paradox\">8.6.5. The St Petersburg paradox</a></h4>\n<p>Another objection to the VNM approach (and to expected utility approaches generally), the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg paradox</a>, draws on the possibility of infinite utilities. The St. Petersburg paradox is based around a game where a fair coin is tossed until it lands heads up. At this point, the agent receives a prize worth 2<sup>n</sup> utility, where <em>n</em> is equal to the number of times the coin was tossed during the game. The so-called paradox occurs because the expected utility of choosing to play this game is infinite and so, according to a standard expected utility approach, the agent should be willing to pay any finite amount to play the game. However, this seems unreasonable. Instead, it seems that the agent should only be willing to pay a relatively small amount to do so. As such, it seems that the expected utility approach gets something wrong.</p>\n<p>Various responses have been suggested. Most obviously, we could say that the paradox does not apply to VNM agents, since the VNM theorem assigns real numbers to all lotteries, and infinity is not a real number. But it's unclear whether this escapes the problem. After all, at it's core, the St. Petersburg paradox is not about infinite utilities but rather about cases where expected utility approaches seem to overvalue some choice, and such cases seem to exist even in finite cases. For example, if we let <em>L</em> be a finite limit on utility we could consider the following scenario (from <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson, 2009, p. 85</a>):</p>\n<blockquote>\n<p>A fair coin is tossed until it lands heads up. The player thereafter receives a prize worth min {2<sup>n</sup> \u00b7 10<sup>-100</sup>, L} units of utility, where <em>n</em> is the number of times the coin was tossed.</p>\n</blockquote>\n<p>In this case, even if an extremely low value is set for <em>L</em>, it seems that paying this amount to play the game is unreasonable. After all, as Peterson notes, about nine times out of ten an agent that plays this game will win no more than 8 \u00b7 10<sup>-100</sup> utility. If paying 1 utility is, in fact, unreasonable in this case, then simply limiting an agent's utility to some finite value doesn't provide a defence of expected utility approaches. (Other problems abound. See <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Yudkowsky, 2007</a> for an interesting finite problem and <a href=\"http://philrsss.anu.edu.au/people-defaults/alanh/papers/vexing_expectations.pdf\">Nover &amp; Hajek, 2004</a> for a particularly perplexing problem with links to the St Petersburg paradox.)</p>\n<p>As it stands, there is no agreement about precisely what the St Petersburg paradox reveals. Some people accept one of the various resolutions of the case and so find the paradox unconcerning. Others think the paradox reveals a serious problem for expected utility theories. Still others think the paradox is unresolved but don't think that we should respond by abandoning expected utility theory.</p>\n<p>&nbsp;</p>\n<h2 id=\"does-axiomatic-decision-theory-offer-any-action-guidance\"><a href=\"#does-axiomatic-decision-theory-offer-any-action-guidance\">9. Does axiomatic decision theory offer any action guidance?</a></h2>\n<p>For the decision theories listed in section 8.2, it's often claimed the answer is \"no.\" To explain this, I must first examine some differences between <em>direct</em> and <em>indirect</em> approaches to axiomatic decision theory.</p>\n<p><a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 4)</a> explains:</p>\n<blockquote>\n<p>In the indirect approach, which is the dominant approach, the decision maker does not prefer a risky act [or lottery] to another <em>because</em> the expected utility of the former exceeds that of the latter. Instead, the decision maker is asked to state a set of preferences over a set of risky acts... Then, if the set of preferences stated by the decision maker is consistent with a small number of structural constraints (axioms), it can be shown that her decisions can be described <em>as if</em> she were choosing what to do by assigning numerical probabilities and utilities to outcomes and then maximising expected utility...</p>\n</blockquote>\n<blockquote>\n<p>[In contrast] the direct approach seeks to generate preferences over acts from probabilities and utilities <em>directly</em> assigned to outcomes. In contrast to the indirect approach, it is not assumed that the decision maker has access to a set of preferences over acts before he starts to deliberate.</p>\n</blockquote>\n<p>The axiomatic decision theories listed in section 8.2 all follow the indirect approach. These theories, it might be said, cannot offer any action guidance because they require an agent to state its preferences over acts \"up front.\" But an agent that states its preferences over acts already knows which act it prefers, so the decision theory can't offer any action guidance not already present in the agent's own stated preferences over acts.</p>\n<p><a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch .10)</a> gives a practical example:</p>\n<blockquote>\n<p>For example, a forty-year-old woman seeking advice about whether to, say, divorce her husband, is likely to get very different answers from the [two approaches]. The [indirect approach] will advise the woman to first figure out what her preferences are over a very large set of risky acts, including the one she is thinking about performing, and then just make sure that all preferences are consistent with certain structural requirements. Then, as long as none of the structural requirements is violated, the woman is free to do whatever she likes, no matter what her beliefs and desires actually are... The [direct approach] will [instead] advise the woman to first assign numerical utilities and probabilities to her desires and beliefs, and then aggregate them into a decision by applying the principle of maximizing expected utility.</p>\n</blockquote>\n<p>Thus, it seems only the direct approach offers an agent any action guidance. But the direct approach is very recent (<a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2008</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Cozic-Review-of-Non-Bayesian-Decision-Theory.pdf\">Cozic 2011</a>), and only time will show whether it can stand up to professional criticism.</p>\n<p>Warning: Peterson's (<a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2008</a>) direct approach is confusingly called \"non-Bayesian decision theory\" despite assuming Bayesian probability theory.</p>\n<p>For other attempts to pull action guidance from normative decision theory, see <a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Fallenstein (2012)</a> and <a href=\"/lw/gap/a_fungibility_theorem/\">Stiennon (2013)</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"how-does-probability-theory-play-a-role-in-decision-theory\"><a href=\"#how-does-probability-theory-play-a-role-in-decision-theory\">10. How does probability theory play a role in decision theory?</a></h2>\n<p>In order to calculate the expected utility of an act (or lottery), it is necessary to determine a probability for each outcome. In this section, I will explore some of the details of probability theory and its relationship to decision theory.</p>\n<p>For further introductory material to probability theory, see <a href=\"http://www.amazon.com/Scientific-Reasoning-The-Bayesian-Approach/dp/081269578X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Howson &amp; Urbach (2005)</a>, <a href=\"http://www.amazon.com/Probability-Random-Processes-Geoffrey-Grimmett/dp/0198572220/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Grimmet &amp; Stirzacker (2001)</a>, and <a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Koller &amp; Friedman (2009)</a>. This section draws heavily on <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, chs. 6 &amp; 7)</a> which provides a very clear introduction to probability in the context of decision theory.</p>\n<p>&nbsp;</p>\n<h3 id=\"the-basics-of-probability-theory\"><a href=\"#the-basics-of-probability-theory\">10.1. The basics of probability theory</a></h3>\n<p>Intuitively, a probability is a number between 0 or 1 that labels how likely an event is to occur. If an event has probability 0 then it is impossible and if it has probability 1 then it can't possibly be false. If an event has a probability between these values, then this event it is more probable the higher this number is.</p>\n<p>As with EUM, probability theory can be derived from a small number of simple axioms. In the probability case, there are three of these, which are named the Kolmogorov axioms after the mathematician Andrey Kolmogorov. The first of these states that probabilities are real numbers between 0 and 1. The second, that if a set of events are mutually exclusive and exhaustive then their probabilities should sum to 1. The third that if two events are mutually exclusive then the probability that one or the other of these events will occur is equal to the sum of their individual probabilities.</p>\n<p>From these three axioms, the remainder of probability theory can be derived. In the remainder of this section, I will explore some aspects of this broader theory.</p>\n<p>&nbsp;</p>\n<h3 id=\"bayes-theorem-for-updating-probabilities\"><a href=\"#bayes-theorem-for-updating-probabilities\">10.2. Bayes theorem for updating probabilities</a></h3>\n<p>From the perspective of decision theory, one particularly important aspect of probability theory is the idea of a conditional probability. These represent how probable something is given a piece of information. So, for example, a conditional probability could represent how likely it is that it will be raining, conditioning on the fact that the weather forecaster predicted rain. A powerful technique for calculating conditional probabilities is Bayes theorem (see <a href=\"http://yudkowsky.net/rational/bayes\">Yudkowsky, 2003</a> for a detailed introduction). This formula states that:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/lTKXA.gif\" alt=\"P(A|B)=(P(B|A)P(A))/P(B)\">\n<p class=\"caption\">P(A|B)=(P(B|A)P(A))/P(B)</p>\n</div>\n<p>Bayes theorem is used to calculate the probability of some event, A, given some evidence, B. As such, this formula can be used to <em>update</em> probabilities based on new evidence. So if you are trying to predict the probability that it will rain tomorrow and someone gives you the information that the weather forecaster predicted that it will do so then this formula tells you how to calculate a new probability that it will rain based on your existing information. The initial probability in such cases (before the information is factored into account) is called the <em>prior probability</em> and the result of applying Bayes theorem is a new, <em>posterior probability</em>.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/vM0yW.jpg\" alt=\"Using Bayes theorem to update probabilities based on the evidence provided by a weather forecast\">\n<p class=\"caption\">Using Bayes theorem to update probabilities based on the evidence provided by a weather forecast</p>\n</div>\n<p>Bayes theorem can be seen as solving the problem of how to update prior probabilities based on new information. However, it leaves open the question of how to determine the prior probability in the first place. In some cases, there will be no obvious way to do so. One solution to this problem suggests that any reasonable prior can be selected. Given enough evidence, repeated applications of Bayes theorem will lead this prior probability to be updated to much the same posterior probability, even for people with widely different initial priors. As such, the initially selected prior is less crucial than it may at first seem.</p>\n<p>&nbsp;</p>\n<h3 id=\"how-should-probabilities-be-interpreted\"><a href=\"#how-should-probabilities-be-interpreted\">10.3. How should probabilities be interpreted?</a></h3>\n<p>There are two main views about what probabilities mean: objectivism and subjectivism. Loosely speaking, the objectivist holds that probabilities tell us something about the external world while the subjectivist holds that they tell us something about our beliefs. Most decision theorists hold a subjectivist view about probability. According to this sort of view, probabilities represent a subjective degrees of belief. So to say the probability of rain is 0.8 is to say that the agent under consideration has a high degree of belief that it will rain (see <a href=\"http://www.amazon.com/Probability-Theory-The-Logic-Science/dp/0521592712/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jaynes, 2003</a> for a defense of this view). Note that, according to this view, another agent in the same circumstance could assign a different probability that it will rain.</p>\n<p>&nbsp;</p>\n<h4 id=\"why-should-degrees-of-belief-following-the-laws-of-probability\"><a href=\"#why-should-degrees-of-belief-following-the-laws-of-probability\">10.3.1. Why should degrees of belief follow the laws of probability?</a></h4>\n<p>One question that might be raised against the subjective account of probability is why, on this account, our degrees of belief should satisfy the Kolmogorov axioms. For example, why should our subjective degrees of belief in mutually exclusive, exhaustive events add to 1? One answer to this question shows that agents whose degrees of belief don\u2019t satisfy these axioms will be subject to Dutch Book bets. These are bets where the agent will inevitably lose money. <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 7)</a> explains:</p>\n<blockquote>\n<p>Suppose, for instance, that you believe to degree 0.55 that at least one person from India will win a gold medal in the next Olympic Games (event G), and that your subjective degree of belief is 0.52 that no Indian will win a gold medal in the next Olympic Games (event \u00acG). Also suppose that a cunning bookie offers you a bet on both of these events. The bookie promises to pay you $1 for each event that actually takes place. Now, since your subjective degree of belief that G will occur is 0.55 it would be rational to pay up to $1\u00b70.55 = $0.55 for entering this bet. Furthermore, since your degree of belief in \u00acG is 0.52 you should be willing to pay up to $0.52 for entering the second bet, since $1\u00b70.52 = $0.52. However, by now you have paid $1.07 for taking on two bets that are certain to give you a payoff of $1 <em>no matter what happens</em>...Certainly, this must be irrational. Furthermore, the reason why this is irrational is that your subjective degrees of belief violate the probability calculus.</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/9xoLg.jpg\" alt=\"A Dutch Book argument\">\n<p class=\"caption\">A Dutch Book argument</p>\n</div>\n<p>It can be proven that an agent is subject to Dutch Book bets if, and only if, their degrees of belief violate the axioms of probability. This provides an argument for why degrees of beliefs should satisfy these axioms.</p>\n<p>&nbsp;</p>\n<h4 id=\"measuring-subjective-probabilities\"><a href=\"#measuring-subjective-probabilities\">10.3.2. Measuring subjective probabilities</a></h4>\n<p>Another challenges raised by the subjective view is how we can measure probabilities. If these represent subjective degrees of belief there doesn\u2019t seem to be an easy way to determine these based on observations of the world. However, a number of responses to this problem have been advanced, one of which is explained succinctly by <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 7)</a>:</p>\n<blockquote>\n<p>The main innovations presented by... Savage can be characterised as systematic procedures for linking probability... to claims about objectively observable behavior, such as preference revealed in choice behavior. Imagine, for instance, that we wish to measure Caroline's subjective probability that the coin she is holding in her hand will land heads up the next time it is tossed. First, we ask her which of the following very generous options she would prefer.</p>\n</blockquote>\n<blockquote>\n<p>A: \"If the coin lands heads up you win a sports car; otherwise you win nothing.\"</p>\n</blockquote>\n<blockquote>\n<p>B: \"If the coin <em>does not</em> land heads up you win a sports car; otherwise you win nothing.\"</p>\n</blockquote>\n<blockquote>\n<p>Suppose Caroline prefers A to B. We can then safely conclude that she thinks it is <em>more probable</em> that the coin will land heads up rather than not. This follows from the assumption that Caroline prefers to win a sports car rather than nothing, and that her preference between uncertain prospects is entirely determined by her beliefs and desires with respect to her prospects of winning the sports car...</p>\n</blockquote>\n<blockquote>\n<p>Next, we need to generalise the measurement procedure outlined above such that it allows us to always represent Caroline's degrees of belief with precise numerical probabilities. To do this, we need to ask Caroline to state preferences over a <em>much larger</em> set of options and then <em>reason backwards</em>... Suppose, for instance, that Caroline wishes to measure her subjective probability that her car worth $20,000 will be stolen within one year. If she considers $1,000 to be... the highest price she is prepared to pay for a gamble in which she gets $20,000 if the event S: \"The car stolen within a year\" takes place, and nothing otherwise, then Caroline's subjective probability for S is 1,000/20,000 = 0.05, given that she forms her preferences in accordance with the principle of maximising expected monetary value...</p>\n</blockquote>\n<blockquote>\n<p>The problem with this method is that very few people form their preferences in accordance with the principle of maximising expected monetary value. Most people have a decreasing marginal utility for money...</p>\n</blockquote>\n<blockquote>\n<p>Fortunately, there is a clever solution to [this problem]. The basic idea is to impose a number of structural conditions on preferences over uncertain options [e.g. the transitivity axiom]. Then, the subjective probability function is established by reasoning backwards while taking the structural axioms into account: Since the decision maker preferrred some uncertain options to others, and her preferences... satisfy a number of structure axioms, the decision maker behaves <em>as if</em> she were forming her preferences over uncertain options by first assigning subjective probabilities and utilities to each option and thereafter maximising expected utility.</p>\n</blockquote>\n<blockquote>\n<p>A peculiar feature of this approach is, thus, that probabilities (and utilities) are derived from 'within' the theory. The decision maker does not prefer an uncertain option to another <em>because</em> she judges the subjective probabilities (and utilities) of the outcomes to be more favourable than those of another. Instead, the... structure of the decision maker's preferences over uncertain options logically implies that they can be described <em>as if</em> her choices were governed by a subjective probability function and a utility function...</p>\n</blockquote>\n<blockquote>\n<p>...Savage's approach [seeks] to explicate subjective interpretations of the probability axioms by making certain claims about preferences over... uncertain options. But... why on earth should a theory of subjective probability involve assumptions about preferences, given that preferences and beliefs are separate entities? Contrary to what is claimed by [Savage and others], emotionally inert decision makers failing to muster any preferences at all... could certainly hold partial beliefs.</p>\n</blockquote>\n<p>Other theorists, for example <a href=\"http://www.amazon.com/Optimal-Statistical-Decisions-Classics-Library/dp/047168029X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">DeGroot (1970)</a>, propose other approaches:</p>\n<blockquote>\n<p>DeGroot's basic assumption is that decision makers can make <em>qualitative</em> comparisons between pairs of events, and judge which one they think is most likely to occur. For example, he assumes that one can judge whether it is <em>more</em>, <em>less</em>, or <em>equally</em> likely, according to one's own beliefs, that it will rain today in Cambridge than in Cairo. DeGroot then shows that if the agent's qualitative judgments are sufficiently fine-grained and satisfy a number of structural axioms, then [they can be described by a probability distribution]. So in DeGroot's... theory, the probability function is obtained by fine-tuning qualitative data, thereby making them quantitative.</p>\n</blockquote>\n<h2 id=\"what-about-newcombs-problem-and-alternative-decision-algorithms\"><br></h2>\n<h2><a href=\"#what-about-newcombs-problem-and-alternative-decision-algorithms\">11. What about \"Newcomb's problem\" and alternative decision algorithms?</a></h2>\n<p>Saying that a rational agent \"maximizes expected utility\" is, unfortunately, not specific enough. There are a variety of decision algorithms which aim to maximize expected utility, and they give <em>different answers</em> to some decision problems, for example \"Newcomb's problem.\"</p>\n<p>In this section, we explain these decision algorithms and show how they perform on Newcomb's problem and related \"Newcomblike\" problems.</p>\n<p>General sources on this topic include: <a href=\"http://www.amazon.com/Paradoxes-Rationality-Cooperation-Prisoners-Newcombs/dp/0774802154/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Campbell &amp; Sowden (1985)</a>, <a href=\"http://kops.ub.uni-konstanz.de/bitstream/handle/urn:nbn:de:bsz:352-opus-5241/ledwig.pdf?sequence=1\">Ledwig (2000)</a>, <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521063566/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999)</a>, and <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>. <a href=\"http://www.operalgo.com/PDF/Moertelmaier_Newcomblike_2013.pdf\">Moertelmaier (2013)</a> discusses Newcomblike problems in the context of the agent-environment framework.</p>\n<p>&nbsp;</p>\n<h3 id=\"newcomblike-problems-and-two-decision-algorithms\"><a href=\"#newcomblike-problems-and-two-decision-algorithms\">11.1. Newcomblike problems and two decision algorithms</a></h3>\n<p>I'll begin with an exposition of several Newcomblike problems, so that I can refer to them in later sections. I'll also introduce our first two decision algorithms, so that I can show how one's choice of decision algorithm affects an agent's outcomes on these problems.</p>\n<p>&nbsp;</p>\n<h4 id=\"newcombs-problem\"><a href=\"#newcombs-problem\">11.1.1. Newcomb's Problem</a></h4>\n<p>Newcomb's problem was formulated by the physicist <a href=\"http://en.wikipedia.org/wiki/William_Newcomb\">William Newcomb</a> but first published in <a href=\"http://faculty.arts.ubc.ca/rjohns/nozick_newcomb.pdf\">Nozick (1969)</a>. Below I present a version of it inspired by <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>.</p>\n<p>A superintelligent machine named Omega visits Earth from another galaxy and shows itself to be very good at predicting events. This isn't because it has magical powers, but because it knows more science than we do, has billions of sensors scattered around the globe, and runs efficient algorithms for modeling humans and other complex systems with unprecedented precision \u2014 on an array of computer hardware the size of our moon.</p>\n<p>Omega presents you with two boxes. Box A is transparent and contains $1000. Box B is opaque and contains either $1 million or nothing. You may choose to take both boxes (called \"two-boxing\"), or you may choose to take only box B (called \"one-boxing\"). If Omega predicted you'll two-box, then Omega has left box B empty. If Omega predicted you'll one-box, then Omega has placed $1M in box B.</p>\n<p>By the time you choose, Omega has already left for its next game \u2014 the contents of box B won't change after you make your decision. Moreover, you've watched Omega play a thousand games against people like you, and on every occasion Omega predicted the human player's choice accurately.</p>\n<p>Should you one-box or two-box?</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/4MFhs.jpg\" alt=\"Newcomb\u2019s problem\">\n<p class=\"caption\">Newcomb\u2019s problem</p>\n</div>\n<p>Here's an argument for two-boxing. The $1M either <em>is</em> or <em>is not</em> in the box; your choice cannot affect the contents of box B now. So, you should two-box, because then you get $1K plus whatever is in box B. This is a straightforward application of the dominance principle (section 6.1). Two-boxing dominantes one-boxing.</p>\n<p>Convinced? Well, here's an argument for one-boxing. On all those earlier games you watched, everyone who two-boxed received $1K, and everyone who one-boxed received $1M. So you're almost certain that you'll get $1K for two-boxing and $1M for one-boxing, which means that to maximize your expected utility, you should one-box.</p>\n<p><a href=\"http://faculty.arts.ubc.ca/rjohns/nozick_newcomb.pdf\">Nozick (1969)</a> reports:</p>\n<blockquote>\n<p>I have put this problem to a large number of people... To almost everyone it is perfectly clear and obvious what should be done. The difficulty is that these people seem to divide almost evenly on the problem, with large numbers thinking that the opposing half is just being silly.</p>\n</blockquote>\n<p>This is not a \"merely verbal\" dispute (<a href=\"http://philreview.dukejournals.org/content/120/4/515.short\">Chalmers 2011</a>). Decision theorists have offered different <em>algorithms</em> for making a choice, and they have different outcomes. Translated into English, the first algorithm (<em>evidential decision theory</em> or EDT) says \"Take actions such that you would be glad to receive the news that you had taken them.\" The second algorithm (<em>causal decision theory</em> or CDT) says \"Take actions which you expect to have a positive effect on the world.\"</p>\n<p>Many decision theorists have the intuition that CDT is right. But a CDT agent appears to \"lose\" on Newcomb's problem, ending up with $1000, while an EDT agent gains $1M. Proponents of EDT can ask proponents of CDT: \"If you're so smart, why aren't you rich?\" As <a href=\"http://www-ihpst.univ-paris1.fr/fichiers/programmes/20/Spohn-One-Boxing3.pdf\">Spohn (2012)</a> writes, \"this must be poor rationality that complains about the reward for irrationality.\" Or as <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a> argues:</p>\n<blockquote>\n<p>An expected utility maximizer should maximize <em>utility</em> \u2014 not formality, reasonableness, or defensibility...</p>\n</blockquote>\n<p>In response to EDT's apparent \"win\" over CDT on Newcomb's problem, proponents of CDT have presented similar problems on which a CDT agent \"wins\" and an EDT agent \"loses.\" Proponents of EDT, meanwhile, have replied with additional Newcomblike problems on which EDT wins and CDT loses. Let's explore each of them in turn.</p>\n<p>&nbsp;</p>\n<h4 id=\"evidential-and-causal-decision-theory\"><a href=\"#evidential-and-causal-decision-theory\">11.1.2. Evidential and causal decision theory</a></h4>\n<p>First, however, we will consider our two decision algorithms in a little more detail.</p>\n<p>EDT can be described simply: according to this theory, agents should use conditional probabilities when determining the expected utility of different acts. Specifically, they should use the probability of the world being in each possible state conditioning on them carrying out the act under consideration. So in Newcomb\u2019s problem they consider the probability that Box B contains $1 million or nothing conditioning on the evidence provided by their decision to one-box or two-box. This is how the theory formalizes the notion of an act providing good news.</p>\n<p>CDT is more complex, at least in part because it has been formulated in a variety of different ways and these formulations are equivalent to one another only if certain background assumptions are met. However, a good sense of the theory can be gained by considering the counterfactual approach, which is one of the more intuitive of these formulations. This approach utilizes the probabilities of certain counterfactual conditionals, which can be thought of as representing the causal influence of an agent\u2019s acts on the state of the world. These conditionals take the form \u201cif I were to carry out a certain act, then the world would be in a certain state.\" So in Newcomb\u2019s problem, for example, this formulation of CDT considers the probability of the counterfactuals like \u201cif I were to one-box, then Box B would contain $1 million\u201d and, in doing so, considers the causal influence of one-boxing on the contents of the boxes.</p>\n<p>The same distinction can be made in formulaic terms. Both EDT and CDT agree that decision theory should be about maximizing expected utility where the expected utility of an act, A, given a set of possible outcomes, O, is defined as follows:</p>\n<p><img src=\"http://i.imgur.com/CSwK4.gif\" alt=\"expected utility formula\">.</p>\n<p>In this equation, V(A &amp; O) represents the value to the agent of the combination of an act and an outcome. So this is the utility that the agent will receive if they carry out a certain act and a certain outcome occurs. Further, Pr<sub>A</sub>O represents the probability of each outcome occurring on the supposition that the agent carries out a certain act. It is in terms of this probability that CDT and EDT differ. EDT uses the conditional probability, Pr(O|A), while CDT uses the probability of subjunctive conditionals, Pr(A <img src=\"http://i.imgur.com/G8xec.gif\" alt=\"\"> O).</p>\n<p>Using these two versions of the expected utility formula, it's possible to demonstrate in a formal manner why EDT and CDT give the advice they do in Newcomb's problem. To demonstrate this it will help to make two simplifying assumptions. First, we will presume that each dollar of money is worth 1 unit of utility to the agent (and so will presume that the agent's utility is linear with money). Second, we will presume that Omega is a perfect predictor of human actions so that if the agent two-boxes it provides definitive evidence that there is nothing in the opaque box and if the agent one-boxes it provides definitive evidence that there is $1 million in this box. Given these assumptions, EDT calculates the expected utility of each decision as follows:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/aCe4Y.gif\" alt=\"EU for two-boxing according to EDT\">\n<p class=\"caption\">EU for two-boxing according to EDT</p>\n</div>\n<div class=\"figure\"><img src=\"http://i.imgur.com/vJtVr.gif\" alt=\"EU for one-boxing according to EDT\">\n<p class=\"caption\">EU for one-boxing according to EDT</p>\n</div>\n<p>Given that one-boxing has a higher expected utility according to these calculations, an EDT agent will one-box.</p>\n<p>On the other hand, given that the agent's decision doesn't causally influence Omega's earlier prediction, CDT will use the same probability regardless of whether you one or two box. The decision endorsed will be the same regardless of what probability we use so, to demonstrate the theory, we can simply arbitrarily assign an 0.5 probability that the opaque box has nothing in it and an 0.5 probability that it has one million dollars in it. CDT then calculates the expected utility of each decision as follows:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/oyHGl.gif\" alt=\"EU for two-boxing according to CDT\">\n<p class=\"caption\">EU for two-boxing according to CDT</p>\n</div>\n<div class=\"figure\"><img src=\"http://i.imgur.com/7uX9t.gif\" alt=\"EU for one-boxing according to CDT\">\n<p class=\"caption\">EU for one-boxing according to CDT</p>\n</div>\n<p>Given that two-boxing has a higher expected utility according to these calculations, a CDT agent will two-box. This approach demonstrates the result given more informally in the previous section: CDT agents will two-box in Newcomb's problem and EDT agents will one box.</p>\n<p>As mentioned before, there are also alternative formulations of CDT. What are these? For example, David Lewis <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">(1981)</a> and Brian Skyrms <a href=\"http://www.amazon.com/Causal-Necessity-Pragmatic-Investigation-Laws/dp/0300023391/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(1980)</a> both present approaches that rely on the partition of the world into states to capture causal information, rather than counterfactual conditionals. On Lewis\u2019s version of this account, for example, the agent calculates the expected utility of acts using their unconditional credence in states of the world that are <em>dependency hypotheses</em>, which are descriptions of the possible ways that the world can depend on the agent\u2019s actions. These dependency hypotheses intrinsically contain the required causal information.</p>\n<p>Other traditional approaches to CDT include the imaging approach of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Sobel-Probability-Chance-and-Choice-a-Theory-of-Rational-Agency.pdf\">Sobel (1980)</a> (also see <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">Lewis 1981</a>) and the unconditional expectations approach of Leonard Savage <a href=\"http://www.amazon.com/Foundations-Statistics-Leonard-J-Savage/dp/0486623491/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(1954)</a>. Those interested in the various traditional approaches to CDT would be best to consult Lewis <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">(1981)</a>, <a href=\"http://plato.stanford.edu/entries/decision-causal/\">Weirich (2008)</a>, and <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521063566/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999)</a>. More recently, work in computer science on a tool called causal Bayesian networks has led to an innovative approach to CDT that has received some recent attention in the philosophical literature (<a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Pearl 2000, ch. 4</a> and <a href=\"http://www-ihpst.univ-paris1.fr/fichiers/programmes/20/Spohn-One-Boxing3.pdf\">Spohn 2012</a>).</p>\n<p>Now we return to an analysis of decision scenarios, armed with EDT and the counterfactual formulation of CDT.</p>\n<p>&nbsp;</p>\n<h4 id=\"medical-newcomb-problems\"><a href=\"#medical-newcomb-problems\">11.1.3. Medical Newcomb problems</a></h4>\n<p>Medical Newcomb problems share a similar form but come in many variants, including Solomon's problem (<a href=\"https://www.kellogg.northwestern.edu/research/math/papers/194.pdf\">Gibbard &amp; Harper 1976</a>) and the smoking lesion problem (<a href=\"http://fitelson.org/few/few_05/egan.pdf\">Egan 2007</a>). Below I present a variant called the \"chewing gum problem\" (<a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky 2010</a>):</p>\n<blockquote>\n<p>Suppose that a recently published medical study shows that chewing gum seems to cause throat abscesses \u2014 an outcome-tracking study showed that of people who chew gum, 90% died of throat abscesses before the age of 50. Meanwhile, of people who do not chew gum, only 10% die of throat abscesses before the age of 50. The researchers, to explain their results, wonder if saliva sliding down the throat wears away cellular defenses against bacteria. Having read this study, would you choose to chew gum? But now a second study comes out, which shows that most gum-chewers have a certain gene, CGTA, and the researchers produce a table showing the following mortality rates:</p>\n</blockquote>\n<blockquote>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>CGTA present</td>\n<td>CGTA absent</td>\n</tr>\n<tr>\n<td>Chew Gum</td>\n<td>89% die</td>\n<td>8% die</td>\n</tr>\n<tr>\n<td>Don\u2019t chew</td>\n<td>99% die</td>\n<td>11% die</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<blockquote>\n<p>This table shows that whether you have the gene CGTA or not, your chance of dying of a throat abscess goes down if you chew gum. Why are fatalities so much higher for gum-chewers, then? Because people with the gene CGTA tend to chew gum and die of throat abscesses. The authors of the second study also present a test-tube experiment which shows that the saliva from chewing gum can kill the bacteria that form throat abscesses. The researchers hypothesize that because people with the gene CGTA are highly susceptible to throat abscesses, natural selection has produced in them a tendency to chew gum, which protects against throat abscesses. The strong correlation between chewing gum and throat abscesses is not because chewing gum causes throat abscesses, but because a third factor, CGTA, leads to chewing gum and throat abscesses.</p>\n</blockquote>\n<blockquote>\n<p>Having learned of this new study, would you choose to chew gum? Chewing gum helps protect against throat abscesses whether or not you have the gene CGTA. Yet a friend who heard that you had decided to chew gum (as people with the gene CGTA often do) would be quite alarmed to hear the news \u2014 just as she would be saddened by the news that you had chosen to take both boxes in Newcomb\u2019s Problem. This is a case where [EDT] seems to return the wrong answer, calling into question the validity of the... rule \u201cTake actions such that you would be glad to receive the news that you had taken them.\u201d Although the news that someone has decided to chew gum is alarming, medical studies nonetheless show that chewing gum protects against throat abscesses. [CDT's] rule of \u201cTake actions which you expect to have a positive physical effect on the world\u201d seems to serve us better.</p>\n</blockquote>\n<p>One response to this claim, called the <em>tickle defense</em> (<a href=\"http://www.jstor.org/discover/10.2307/20115662?uid=3737536&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;sid=21101205363271\">Eells, 1981</a>), argues that EDT actually reaches the right decision in such cases. According to this defense, the most reasonable way to construe the \u201cchewing gum problem\u201d involves presuming that CGTA causes a desire (a mental \u201ctickle\u201d) which then causes the agent to be more likely to chew gum, rather than CGTA directly causing the action. Given this, if we presume that the agent already knows their own desires and hence already knows whether they\u2019re likely to have the CGTA gene, chewing gum will not provide the agent with further bad news. Consequently, an agent following EDT will chew in order to get the good news that they have decreased their chance of getting abscesses.</p>\n<p>Unfortunately, the tickle defense fails to achieve its aims. In introducing this approach, Eells hoped that EDT could be made to mimic CDT but without an allegedly inelegant reliance on causation. However, <a href=\"http://www.amazon.com/Taking-Chances-Cambridge-Probability-Induction/dp/0521038987/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Sobel (1994, ch. 2)</a> demonstrated that the tickle defense failed to ensure that EDT and CDT would decide equivalently in all cases. On the other hand, those who feel that EDT originally got it right by one-boxing in Newcomb\u2019s problem will be disappointed to discover that the tickle defense leads an agent to two-box in some versions of Newcomb\u2019s problem and so solves one problem for the theory at the expense of introducing another.</p>\n<p>So just as CDT \u201closes\u201d on Newcomb\u2019s problem, EDT will \"lose\u201d on Medical Newcomb problems (if the tickle defense fails) or will join CDT and \"lose\" on Newcomb\u2019s Problem itself (if the tickle defense succeeds).</p>\n<p>&nbsp;</p>\n<h4 id=\"newcombs-soda\"><a href=\"#newcombs-soda\">11.1.4. Newcomb's soda</a></h4>\n<p>There are also similar problematic cases for EDT where the evidence provided by your decision relates not to a feature that you were born (or created) with but to some other feature of the world. One such scenario is the <em>Newcomb\u2019s soda</em> problem, introduced in <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>:</p>\n<blockquote>\n<p>You know that you will shortly be administered one of two sodas in a double-blind clinical test. After drinking your assigned soda, you will enter a room in which you find a chocolate ice cream and a vanilla ice cream. The first soda produces a strong but entirely subconscious desire for chocolate ice cream, and the second soda produces a strong subconscious desire for vanilla ice cream. By \u201csubconscious\u201d I mean that you have no introspective access to the change, any more than you can answer questions about individual neurons firing in your cerebral cortex. You can only infer your changed tastes by observing which kind of ice cream you pick.</p>\n</blockquote>\n<blockquote>\n<p>It so happens that all participants in the study who test the Chocolate Soda are rewarded with a million dollars after the study is over, while participants in the study who test the Vanilla Soda receive nothing. But subjects who actually eat vanilla ice cream receive an additional thousand dollars, while subjects who actually eat chocolate ice cream receive no additional payment. You can choose one and only one ice cream to eat. A pseudo-random algorithm assigns sodas to experimental subjects, who are evenly divided (50/50) between Chocolate and Vanilla Sodas. You are told that 90% of previous research subjects who chose chocolate ice cream did in fact drink the Chocolate Soda, while 90% of previous research subjects who chose vanilla ice cream did in fact drink the Vanilla Soda. Which ice cream would you eat?</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/FAZnb.jpg\" alt=\"Newcomb\u2019s soda\">\n<p class=\"caption\">Newcomb\u2019s soda</p>\n</div>\n<p>In this case, an EDT agent will decide to eat chocolate ice cream as this would provide evidence that they drank the chocolate soda and hence that they will receive $1 million after the experiment. However, this seems to be the wrong decision and so, once again, the EDT agent \u201closes\u201d.</p>\n<p>&nbsp;</p>\n<h4 id=\"bostroms-meta-newcomb-problem\"><a href=\"#bostroms-meta-newcomb-problem\">11.1.5. Bostrom's meta-Newcomb problem</a></h4>\n<p>In response to attacks on their theory, the proponent of EDT can present alternative scenarios where EDT \u201cwins\u201d and it is CDT that \u201closes\u201d. One such case is the <em>meta-Newcomb problem</em> proposed in <a href=\"http://www.nickbostrom.com/papers/newcomb.html\">Bostrom (2001)</a>. Adapted to fit my earlier story about Omega the superintelligent machine (section 11.1.1), the problem runs like this: Either Omega has <em>already</em> placed $1M or nothing in box B (depending on its prediction about your choice), or else Omega is watching as you choose and <em>after</em> your choice it will place $1M into box B only if you have one-boxed. But you don't know which is the case. Omega makes its move before the human player's choice about half the time, and the rest of the time it makes its move <em>after</em> the player's choice.</p>\n<p>But now suppose there is another superintelligent machine, Meta-Omega, who has a perfect track record of predicting both Omega's choices and the choices of human players. Meta-Omega tells you that either you will two-box and Omega will \"make its move\" <em>after</em> you make your choice, or else you will one-box and Omega has <em>already</em> made its move (and gone on to the next game, with someone else).</p>\n<p>Here, an EDT agent one-boxes and walks away with a million dollars. On the face of it, however, a CDT agent faces a dilemma: if she two-boxes then Omega's action depends on her choice, so the \"rational\" choice is to one-box. But if the CDT agent one-boxes, then Omega's action temporally precedes (and is thus physically independent of) her choice, so the \"rational\" action is to two-box. It might seem, then, that a CDT agent will be unable to reach any decision in this scenario. However, further reflection reveals that the issue is more complicated. According to CDT, what the agent ought to do in this scenario depends on their credences about their own actions. If they have a high credence that they will two-box, they ought to one-box and if they have a high credence that they will one-box, they ought to two box. Given that the agent's credences in their actions are not given to us in the description of the meta-Newcomb problem, the scenario is underspecified and it is hard to know what conclusions should be drawn from it.</p>\n<p>&nbsp;</p>\n<h4 id=\"the-psychopath-button\"><a href=\"#the-psychopath-button\">11.1.6. The psychopath button</a></h4>\n<p>Fortunately, another case has been introduced where, according to CDT, what an agent ought to do depends on their credences about what they will do. This is the <em>psychopath button</em>, introduced in <a href=\"http://philreview.dukejournals.org/content/116/1/93.citation\">Egan (2007)</a>:</p>\n<blockquote>\n<p>Paul is debating whether to press the \u201ckill all psychopaths\u201d button. It would, he thinks, be much better to live in a world with no psychopaths. Unfortunately, Paul is quite confident that only a psychopath would press such a button. Paul very strongly prefers living in a world with psychopaths to dying. Should Paul press the button?</p>\n</blockquote>\n<p>Many people think Paul should not. After all, if he does so, he is almost certainly a psychopath and so pressing the button will almost certainly cause his death. This is also the response that an EDT agent will give. After all, pushing the button would provide the agent with the bad news that they are almost certainly a psychopath and so will die as a result of their action.</p>\n<p>On the other hand, if Paul is fairly certain that he is not a psychopath, then CDT will say that he ought to press the button. CDT will note that, given Paul\u2019s confidence that he isn\u2019t a psychopath, his decision will almost certainly have a positive impact as it will result in the death of all psychopaths and Paul\u2019s survival. On the face of it, then, a CDT agent would decide inappropriately in this case by pushing the button. Importantly, unlike in the meta-Newcomb problem, the agent's credences about their own behavior are specified in Egan's full version of this scenario (in non-numeric terms, the agent thinks they're unlikely to be a psychopath and hence unlikely to press the button).</p>\n<p>However, in order to produce this problem for CDT, Egan made a number of assumptions about how an agent should decide when what they ought to do depends on what they think they will do. In response, alternative views about deciding in such cases have been advanced (particular in <a href=\"http://www.jstor.org/discover/10.2307/40267481?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=21101299066461\">Arntzenius, 2008</a> and <a href=\"http://rd.springer.com/article/10.1007/s11229-011-0022-6\">Joyce, 2012</a>). Given these factors, opinions are split about whether the psychopath button problem does in fact pose a challenge to CDT.</p>\n<p>&nbsp;</p>\n<h4 id=\"parfits-hitchhiker\"><a href=\"#parfits-hitchhiker\">11.1.7. Parfit's hitchhiker</a></h4>\n<p>Not all decision scenarios are problematic for just one of EDT or CDT. There are also cases that can be presented where both an EDT agent and a CDT agent will both \"lose\". One such case is <em>Parfit\u2019s Hitchhiker</em> (<a href=\"http://www.amazon.com/Reasons-Persons-Oxford-Paperbacks-Parfit/dp/019824908X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Parfit, 1984, p. 7</a>):</p>\n<blockquote>\n<p>Suppose that I am driving at midnight through some desert. My car breaks down. You are a stranger, and the only other driver near. I manage to stop you, and I offer you a great reward if you rescue me. I cannot reward you now, but I promise to do so when we reach my home. Suppose next that I am <em>transparent</em>, unable to deceive others. I cannot lie convincingly. Either a blush, or my tone of voice, always gives me away. Suppose, finally, that I know myself to be never self-denying. If you drive me to my home, it would be worse for me if I gave you the promised reward. Since I know that I never do what will be worse for me, I know that I shall break my promise. Given my inability to lie convincingly, you know this too. You do not believe my promise, and therefore leave me stranded in the desert.</p>\n</blockquote>\n<p>In this scenario the agent \"loses\" if they would later refuse to give the stranger the reward. However, both EDT agents and CDT agents will refuse to do so. After all, by this point the agent will already be safe so giving the reward can neither provide good news about, nor cause, their safety. So this seems to be a case where both theories \u201close\u201d.</p>\n<p>&nbsp;</p>\n<h4 id=\"transparent-newcombs-problem\"><a href=\"#transparent-newcombs-problem\">11.1.8. Transparent Newcomb's problem</a></h4>\n<p>There are also other cases where both EDT and CDT \"lose\". One of these is the <em>Transparent Newcomb's problem</em> which, in at least one version, is due to <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Drescher (2006, p. 238-242)</a>. This scenario is like the original Newcomb's problem but, in this case, both boxes are transparent so you can see their contents when you make your decision. Again, Omega has filled box A with $1000 and Box B with either $1 million or nothing based on a prediction of your behavior. Specifically, Omega has predicted how you would decide if you witnessed $1 million in Box B. If Omega predicted that you would one-box in this case, he placed $1 million in Box B. On the other hand, if Omega predicted that you would two-box in this case then he placed nothing in Box B.</p>\n<p>Both EDT and CDT agents will two-box in this case. After all, the contents of the boxes are determined and known so the agent's decision can neither provide good news about what they contain nor cause them to contain something desirable. As with two-boxing in the original version of Newcomb\u2019s problem, many philosophers will endorse this behavior.</p>\n<p>However, it\u2019s worth noting that Omega will almost certainly have predicted this decision and so filled Box B with nothing. CDT and EDT agents will end up with $1000. On the other hand, just as in the original case, the agent that one-boxes will end up with $1 million. So this is another case where both EDT and CDT \u201close\u201d. Consequently, to those that agree with the earlier comments (in section 11.1.1) that a decision theory shouldn't lead an agent to \"lose\", neither of these theories will be satisfactory.</p>\n<p>&nbsp;</p>\n<h4 id=\"counterfactual-mugging\"><a href=\"#counterfactual-mugging\">11.1.9. Counterfactual mugging</a></h4>\n<p>Another similar case, known as <em>counterfactual mugging</em>, was developed in <a href=\"/lw/3l/counterfactual_mugging/\">Nesov (2009)</a>:</p>\n<blockquote>\n<p>Imagine that one day, Omega comes to you and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But see, the Omega tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n</blockquote>\n<p>Should you give up the $100?</p>\n<p>Both CDT and EDT say no. After all, giving up your money neither provides good news about nor influences your chances of getting $10 000 out of the exchange. Further, this intuitively seems like the right decision. On the face of it, then, it is appropriate to retain your money in this case.</p>\n<p>However, presuming you take Omega to be perfectly trustworthy, there seems to be room to debate this conclusion. If you are the sort of agent that gives up the $100 in counterfactual mugging then you will tend to do better than the sort of agent that won\u2019t give up the $100. Of course, in the particular case at hand you will lose but rational agents often lose in specific cases (as, for example, when such an agent loses a rational bet). It could be argued that what a rational agent should not do is be the type of agent that loses. Given that agents that refuse to give up the $100 are the type of agent that loses, there seem to be grounds to claim that counterfactual mugging is another case where both CDT and EDT act inappropriately.</p>\n<p>&nbsp;</p>\n<h4 id=\"prisoners-dilemma\"><a href=\"#prisoners-dilemma\">11.1.10. Prisoner's dilemma</a></h4>\n<p>Before moving on to a more detailed discussion of various possible decision theories, I\u2019ll consider one final scenario: the <em>prisoner\u2019s dilemma</em>. <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987, pp. 147-148 )</a> outlines this scenario as follows:</p>\n<blockquote>\n<p>Two prisoners...have been arrested for vandalism and have been isolated from each other. There is sufficient evidence to convict them on the charge for which they have been arrested, but the prosecutor is after bigger game. He thinks that they robbed a bank together and that he can get them to confess to it. He summons each separately to an interrogation room and speaks to each as follows: \"I am going to offer the same deal to your partner, and I will give you each an hour to think it over before I call you back. This is it: If one of you confesses to the bank robbery and the other does not, I will see to it that the confessor gets a one-year term and that the other guy gets a twenty-five year term. If you both confess, then it's ten years apiece. If neither of you confesses, then I can only get two years apiece on the vandalism charge...\"</p>\n</blockquote>\n<p>The decision matrix of each vandal will be as follows:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Partner confesses</em></td>\n<td><em>Partner lies</em></td>\n</tr>\n<tr>\n<td><em>Confess</em></td>\n<td>10 years in jail</td>\n<td>1 year in jail</td>\n</tr>\n<tr>\n<td><em>Lie</em></td>\n<td>25 years in jail</td>\n<td>2 years in jail</td>\n</tr>\n</tbody>\n</table>\n<p>Faced with this scenario, a CDT agent will confess. After all, the agent\u2019s decision can\u2019t influence their partner\u2019s decision (they\u2019ve been isolated from one another) and so the agent is better off confessing regardless of what their partner chooses to do. According to the majority of decision (and game) theorists, confessing is in fact the rational decision in this case.</p>\n<p>Despite this, however, an EDT agent may lie in a prisoner\u2019s dilemma. Specifically, if they think that their partner is similar enough to them, the agent will lie because doing so will provide the good news that they will both lie and hence that they will both get two years in jail (good news as compared with the bad news that they will both confess and hence that they will get 10 years in jail).</p>\n<p>To many people, there seems to be something compelling about this line of reasoning. For example, <a href=\"http://www.amazon.com/Metamagical-Themas-Questing-Essence-Pattern/dp/0465045669/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Douglas Hofstadter (1985, pp. 737-780)</a> has argued that an agent acting \u201csuperrationally\u201d would co-operate with other superrational agents for precisely this sort of reason: a superrational agent would take into account the fact that other such agents will go through the same thought process in the <em>prisoner\u2019s dilemma</em> and so make the same decision. As such, it is better that that the decision that both agents reach be to lie than that it be to confess. More broadly, it could perhaps be argued that a rational agent should lie in the <em>prisoner\u2019s dilemma</em> as long as they believe that they are similar enough to their partner that they are likely to reach the same decision.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/fPUcm.jpg\" alt=\"An argument for cooperation in the prisoners\u2019 dilemma\">\n<p class=\"caption\">An argument for cooperation in the prisoners\u2019 dilemma</p>\n</div>\n<p>It is unclear, then, precisely what should be concluded from the prisoner\u2019s dilemma. However, for those that are sympathetic to Hofstadter\u2019s point or the line of reasoning appealed to by the EDT agent, the scenario seems to provide an additional reason to seek out an alternative theory to CDT.</p>\n<p>&nbsp;</p>\n<h3 id=\"benchmark-theory-bt\"><a href=\"#benchmark-theory-bt\">11.2. Benchmark theory (BT)</a></h3>\n<p>One recent response to the apparent failure of EDT to decide appropriately in medical Newcomb problems and CDT to decide appropriately in the psychopath button is Benchmark Theory (BT) which was developed in <a href=\"http://www.springerlink.com/content/a66107137n821610/?MUD=MP\">Wedgwood (2011)</a> and discussed further in <a href=\"http://philreview.dukejournals.org/content/119/1/1.abstract\">Briggs (2010)</a>.</p>\n<p>In English, we could think of this decision algorithm as saying that agents should decide so as to give their future self good news about how well off they are compared to how well off they could have been. In formal terms, BT uses the following formula to calculate the expected utility of an act, A:</p>\n<p><img src=\"http://i.imgur.com/fUjmj.gif\" alt=\"BT expected value formula\">.</p>\n<p>In other words, it uses the conditional probability, as in EDT but calculates the value differently (as indicated by the use of V\u2019 rather than V). V\u2019 is calculated relative to a benchmark value in order to give a comparative measure of value (both of the above sources go into more detail about this process).</p>\n<p>Taking the informal perspective, in the <em>chewing gum problem</em>, BT will note that by chewing gum, the agent will always get the good news that they are comparatively better off than they could have been (because chewing gum helps control throat abscesses) whereas by not chewing, the agent will always get the bad news that they could have been comparatively better off by chewing. As such, a BT agent will chew in this scenario.</p>\n<p>Further, BT seems to reach what many consider to be the right decision in the <em>psychopath button</em>. In this case, the BT agent will note that if they push the button they will get the bad news that they are almost certainly a psychopath and so that they would have been comparatively much better off by not pushing (as pushing will kill them). On the other hand, if they don\u2019t push they will get the less bad news that they are almost certainly not a psychopath and so could have been comparatively a little better off it they had pushed the button (as this would have killed all the psychopaths but not them). So refraining from pushing the button gives the less bad news and so is the rational decision.</p>\n<p>On the face of it, then, there seem to be strong reasons to find BT compelling: it decides appropriately in these scenarios while, according to some people, EDT and CDT only decide appropriately in one or the other of them.</p>\n<p>Unfortunately, a BT agent will fail to decide appropriately in other scenarios. First, those that hold that one-boxing is the appropriate decision in Newcomb\u2019s problem will immediately find a flaw in BT. After all, in this scenario two-boxing gives the good news that the agent did comparatively better than they could have done (because they gain the $1000 from Box A which is more than they would have received otherwise) while one-boxing brings the bad news that they did comparatively worse than they could have done (as they did not receive this money). As such, a BT agent will two-box in Newcomb\u2019s problem.</p>\n<p>Further, <a href=\"http://philreview.dukejournals.org/content/119/1/1.abstract\">Briggs (2010)</a> argues, though <a href=\"http://www.springerlink.com/content/a66107137n821610/?MUD=MP\">Wedgwood (2011)</a> denies, that BT suffers from other problems. As such, even for those who support two-boxing in Newcomb\u2019s problem, it could be argued that BT doesn\u2019t represent an adequate theory of choice. It is unclear, then, whether BT is a desirable replacement to alternative theories.</p>\n<p>&nbsp;</p>\n<h3 id=\"timeless-decision-theory-tdt\"><a href=\"#timeless-decision-theory-tdt\">11.3. Timeless decision theory (TDT)</a></h3>\n<p><a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a> offers another decision algorithm, <em>timeless decision theory</em> or TDT (see also <a href=\"http://intelligence.org/files/Comparison.pdf\">Altair, 2013</a>). Specifically, TDT is intended as an explicit response to the idea that a theory of rational choice should lead an agent to \u201cwin\u201d. As such, it will appeal to those who think it is appropriate to one-box in Newcomb\u2019s problem and chew in the chewing gum problem.</p>\n<p>In English, this algorithm can be approximated as saying that an agent ought to choose as if CDT were right but they were determining not their actual decision but rather the result of the abstract computation of which their decision is one concrete instance. Formalizing this decision algorithm would require a substantial document in its own right and so will not be carried out in full here. Briefly, however, TDT is built on top of causal Bayesian networks <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(Pearl, 2000)</a> which are graphs where the arrows represent causal influence. TDT supplements these graphs by adding nodes representing abstract computations and taking the abstract computation that determines an agent\u2019s decision to be the object of choice rather than the concrete decision itself (see <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky, 2010</a> for a more detailed description).</p>\n<p>Returning to an informal discussion, an example will help clarify the form taken by TDT: imagine that two perfect replicas of a person are placed in identical rooms and asked to make the same decision. While each replica will make their own decision, in doing so, they will be carrying out the same computational process. As such, TDT will say that the replicas ought to act as if they are determining the result of this process and hence as if they are deciding the behavior of both copies.</p>\n<p>Something similar can be said about Newcomb\u2019s problem. In this case it is almost like there is again a replica of the agent: Omega\u2019s model of the agent that it used to predict the agent\u2019s behavior. Both the original agent and this \u201creplica\u201d responds to the same abstract computational process as one another. In other words, both Omega\u2019s prediction and the agent\u2019s behavior are influenced by this process. As such, TDT advises the agent to act as if they are determining the result of this process and, hence, as if they can determine Omega\u2019s box filling behavior. As such, a TDT agent will one-box in order to determine the result of this abstract computation in a way that leads to $1 million being placed in Box B.</p>\n<p>TDT also succeeds in other areas. For example, in the chewing gum problem there is no \u201creplica\u201d agent so TDT will decide in line with standard CDT and choose to chew gum. Further, in the prisoner\u2019s dilemma, a TDT agent will lie if its partner is another TDT agent (or a relevantly similar agent). After all, in this case both agents will carry out the same computational process and so TDT will advise that the agent act as if they are determining this process and hence simultaneously determining both their own and their partner\u2019s decision. If so then it is better for the agent that both of them lie than that both of them confess.</p>\n<p>However, despite its success, TDT also \u201closes\u201d in some decision scenarios. For example, in counterfactual mugging, a TDT agent will not choose to give up the $100. This might seem surprising. After all, as with Newcomb\u2019s problem, this case involves Omega predicting the agent\u2019s behavior and hence involves a \u201creplica\u201d. However, this case differs in that the agent knows that the coin came up heads and so knows that they have nothing to gain by giving up the money.</p>\n<p>For those who feel that a theory of rational choice should lead an agent to \u201cwin\u201d, then, TDT seems like a step in the right direction but further work is required if it is to \u201cwin\u201d in the full range of decision scenarios.</p>\n<p>&nbsp;</p>\n<h3 id=\"decision-theory-and-winning\"><a href=\"#decision-theory-and-winning\">11.4. Decision theory and \u201cwinning\u201d</a></h3>\n<p>In the previous section, I discussed TDT, a decision algorithm that could be advanced as replacements for CDT and EDT. One of the primary motivations for developing TDT is a sense that both CDT and EDT fail to reason in a desirable manner in some decision scenarios. However, despite acknowledging that CDT agents end up worse off in Newcomb's Problem, many (and perhaps the majority of) decision theorists are proponents of CDT. On the face of it, this may seem to suggest that these decision theorists aren't interested in developing a decision algorithm that \"wins\" but rather have some other aim in mind. If so then this might lead us to question the value of developing one-boxing decision algorithms.</p>\n<p>However, the claim that most decision theorists don\u2019t care about finding an algorithm that \u201cwins\u201d mischaracterizes their position. After all, proponents of CDT tend to take the challenge posed by the fact that CDT agents \u201close\u201d in Newcomb's problem seriously (in the philosophical literature, it's often referred to as the <em>Why ain'cha rich?</em> problem). A common reaction to this challenge is neatly summarized in <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521641640/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999, p. 153-154 )</a> as a response to a hypothetical question about why, if two-boxing is rational, the CDT agent does not end up as rich as an agent that one-boxes:</p>\n<blockquote>\n<p>Rachel has a perfectly good answer to the \"Why ain't you rich?\" question. \"I am not rich,\" she will say, \"because I am not the kind of person [Omega] thinks will refuse the money. I'm just not like you, Irene [the one-boxer]. Given that I know that I am the type who takes the money, and given that [Omega] knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in [the box]. The $1,000 was the most I was going to get no matter what I did. So the only reasonable thing for me to do was to take it.\"</p>\n</blockquote>\n<blockquote>\n<p>Irene may want to press the point here by asking, \"But don't you wish you were like me, Rachel?\"... Rachel can and should admit that she <em>does</em> wish she were more like Irene... At this point, Irene will exclaim, \"You've admitted it! It wasn't so smart to take the money after all.\" Unfortunately for Irene, her conclusion does not follow from Rachel's premise. Rachel will patiently explain that wishing to be a [one-boxer] in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 <em>whatever type one is</em>. When Rachel wishes she was Irene's type she is wishing for <em>Irene's options</em>, not sanctioning her choice... While a person who knows she will face (has faced) a Newcomb problem might wish that she were (had been) the type that [Omega] labels a [one-boxer], this wish does not provide a reason for <em>being</em> a [one-boxer]. It might provide a reason to try (before [the boxes are filled]) to change her type <em>if she thinks this might affect [Omega's] prediction</em>, but it gives her no reason for doing anything other than taking the money once she comes to believes that she will be unable to influence what [Omega] does.</p>\n</blockquote>\n<p>In other words, this response distinguishes between the <em>winning decision</em> and the <em>winning type of agent</em> and claims that two-boxing is the winning decision in Newcomb\u2019s problem (even if one-boxers are the winning type of agent). Consequently, insofar as decision theory is about determining which <em>decision</em> is rational, on this account CDT reasons correctly in Newcomb\u2019s problem.</p>\n<p>For those that find this response perplexing, an analogy could be drawn to the <em>chewing gum problem</em>. In this scenario, there is near unanimous agreement that the rational decision is to chew gum. However, statistically, non-chewers will be better off than chewers. As such, the non-chewer could ask, \u201cif you\u2019re so smart, why aren\u2019t you healthy?\u201d In this case, the above response seems particularly appropriate. The chewers are less healthy not because of their decision but rather because they\u2019re more likely to have an undesirable gene. Having good genes doesn\u2019t make the non-chewer more rational but simply more lucky. The proponent of CDT simply makes a similar response to Newcomb\u2019s problem: one-boxers aren\u2019t richer because of their decision but rather because of the type of agent that they were when the boxes were filled.</p>\n<p>One final point about this response is worth noting. A proponent of CDT can accept the above argument but still acknowledge that, if given the choice before the boxes are filled, they would be rational to choose to modify themselves to be a one-boxing type of agent (as Joyce acknowledged in the above passage and as argued for in <a href=\"http://www.jstor.org/stable/20118389\">Burgess, 2004</a>). To the proponent of CDT, this is unproblematic: if we are sometimes rewarded not for the rationality of our decisions in the moment but for the type of agent we were at some past moment then it should be unsurprising that changing to a different type of agent might be beneficial.</p>\n<p>The response to this defense of two-boxing in Newcomb\u2019s problem has been divided. Many find it compelling but others, like <a href=\"http://link.springer.com/article/10.1007%2Fs10670-011-9355-2\">Ahmed and Price (2012)</a> think it does not adequately address to the challenge:</p>\n<blockquote>\n<p>It is no use the causalist's whining that foreseeably, Newcomb problems do in fact reward irrationality, or rather CDT-irrationality. The point of the argument is that if everyone knows that the CDT-irrational strategy will in fact do better on average than the CDT-rational strategy, then it's rational to play the CDT-irrational strategy.</p>\n</blockquote>\n<p>Given this, there seem to be two positions one could take on these issues. If the response given by the proponent of CDT is compelling, then we should be attempting to develop a decision theory that two-boxes on Newcomb\u2019s problem. Perhaps the best theory for this role is CDT but perhaps it is instead BT, which many people think reasons better in the psychopath button scenario. On the other hand, if the response given by the proponents of CDT is not compelling, then we should be developing a theory that one-boxes in Newcomb\u2019s problem. In this case, TDT, or something like it, seems like the most promising theory currently on offer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 10, "5f5c37ee1b5cdee568cfb294": 2, "5f5c37ee1b5cdee568cfb1db": 2, "DigEmY3RrF3XL5cwe": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zEWJBFFMvQ835nq6h", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 107, "extendedScore": null, "score": 0.000244, "legacy": true, "legacyId": "21817", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://i.imgur.com/DhCAW.jpg", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 107, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Co-authored with <a href=\"/user/crazy88/overview/\">crazy88</a>. Please let us know when you find mistakes, and we'll fix them. Last updated 03-27-2013.</small></p>\n<p><strong>Contents</strong>:</p>\n<div id=\"TOC\">\n<ul>\n<li><a href=\"#what-is-decision-theory\">1. What is decision theory?</a></li>\n<li><a href=\"#is-the-rational-decision-always-the-right-decision\">2. Is the rational decision always the right decision?</a></li>\n<li><a href=\"#how-can-i-better-understand-a-decision-problem\">3. How can I better understand a decision problem?</a></li>\n<li><a href=\"#how-can-i-measure-an-agents-preferences\">4. How can I measure an agent's preferences?</a> \n<ul>\n<li><a href=\"#the-concept-of-utility\">4.1. The concept of utility</a></li>\n<li><a href=\"#types-of-utility\">4.2. Types of utility</a></li>\n</ul>\n</li>\n<li><a href=\"#what-do-decision-theorists-mean-by-risk-ignorance-and-uncertainty\">5. What do decision theorists mean by \"risk,\" \"ignorance,\" and \"uncertainty\"?</a></li>\n<li><a href=\"#how-should-i-make-decisions-under-ignorance\">6. How should I make decisions under ignorance?</a> \n<ul>\n<li><a href=\"#the-dominance-principle\">6.1. The dominance principle</a></li>\n<li><a href=\"#maximin-and-leximin\">6.2. Maximin and leximin</a></li>\n<li><a href=\"#maximax-and-optimism-pessimism\">6.3. Maximax and optimism-pessimism</a></li>\n<li><a href=\"#other-decision-principles\">6.4. Other decision principles</a></li>\n</ul>\n</li>\n<li><a href=\"#can-decisions-under-ignorance-be-transformed-into-decisions-under-uncertainty\">7. Can decisions under ignorance be transformed into decisions under uncertainty?</a></li>\n<li><a href=\"#how-should-i-make-decisions-under-uncertainty\">8. How should I make decisions under uncertainty?</a> \n<ul>\n<li><a href=\"#the-law-of-large-numbers\">8.1. The law of large numbers</a></li>\n<li><a href=\"#the-axiomatic-approach\">8.2. The axiomatic approach</a></li>\n<li><a href=\"#the-von-neumann-morgenstern-utility-theorem\">8.3. The Von Neumann-Morgenstern utility theorem</a></li>\n<li><a href=\"#vnm-utility-theory-and-rationality\">8.4. VNM utility theory and rationality</a></li>\n<li><a href=\"#objections-to-vnm-rationality\">8.5. Objections to VNM-rationality</a></li>\n<li><a href=\"#should-we-accept-the-vnm-axioms\">8.6. Should we accept the VNM axioms?</a></li>\n</ul>\n</li>\n<li><a href=\"#does-axiomatic-decision-theory-offer-any-action-guidance\">9. Does axiomatic decision theory offer any action guidance?</a></li>\n<li><a href=\"#how-does-probability-theory-play-a-role-in-decision-theory\">10. How does probability theory play a role in decision theory?</a> \n<ul>\n<li><a href=\"#the-basics-of-probability-theory\">10.1. The basics of probability theory</a></li>\n<li><a href=\"#bayes-theorem-for-updating-probabilities\">10.2. Bayes theorem for updating probabilities</a></li>\n<li><a href=\"#how-should-probabilities-be-interpreted\">10.3. How should probabilities be interpreted?</a></li>\n</ul>\n</li>\n<li><a href=\"#what-about-newcombs-problem-and-alternative-decision-algorithms\">11. What about \"Newcomb's problem\" and alternative decision algorithms?</a> \n<ul>\n<li><a href=\"#newcomblike-problems-and-two-decision-algorithms\">11.1. Newcomblike problems and two decision algorithms</a></li>\n<li><a href=\"#benchmark-theory-bt\">11.2. Benchmark theory (BT)</a></li>\n<li><a href=\"#timeless-decision-theory-tdt\">11.3. Timeless decision theory (TDT)</a></li>\n<li><a href=\"#decision-theory-and-winning\">11.4. Decision theory and \u201cwinning\u201d</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<h2 id=\"what-is-decision-theory\"><br></h2>\n<h2 id=\"1__What_is_decision_theory_\"><a href=\"#what-is-decision-theory\">1. What is decision theory?</a></h2>\n<p><em>Decision theory</em>, also known as <em>rational choice theory</em>, concerns the study of preferences, uncertainties, and other issues related to making \"optimal\" or \"rational\" choices. It has been discussed by economists, psychologists, philosophers, mathematicians, statisticians, and computer scientists.</p>\n<p>We can divide decision theory into three parts (<a href=\"http://www.owlnet.rice.edu/~econ501/lectures/Decision_EU.pdf\">Grant &amp; Zandt 2009</a>; <a href=\"http://www.amazon.com/dp/0521680433/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Baron 2008</a>). <em>Normative</em> decision theory studies what an ideal agent (a perfectly rational agent, with infinite computing power, etc.) would choose. <em>Descriptive</em> decision theory studies how non-ideal agents (e.g. humans) <em>actually</em> choose. <em>Prescriptive</em> decision theory studies how non-ideal agents can improve their decision-making (relative to the normative model) despite their imperfections.</p>\n<p>For example, one's <em>normative</em> model might be <a href=\"http://kleene.ss.uci.edu/lpswiki/index.php/Expected_Utility_Theory\">expected utility theory</a>, which says that a rational agent chooses the action with the highest expected utility. Replicated results in psychology <em>describe</em> humans repeatedly <em>failing</em> to maximize expected utility in particular, <a href=\"http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Edition/dp/0061353248/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">predictable</a> ways: for example, they make some choices based not on potential future benefits but on irrelevant past efforts (the \"<a href=\"http://en.wikipedia.org/wiki/Sunk_costs\">sunk cost fallacy</a>\"). To help people avoid this error, some theorists <em>prescribe</em> some basic training in microeconomics, which has been shown to reduce the likelihood that humans will commit the sunk costs fallacy (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/08/Larrick-et-al-Teaching-the-use-of-cost-benefit-reasoning-in-everyday-life.pdf\">Larrick et al. 1990</a>). Thus, through a coordination of normative, descriptive, and prescriptive research we can help agents to succeed in life by acting more in accordance with the normative model than they otherwise would.</p>\n<p>This FAQ focuses on normative decision theory. Good sources on descriptive and prescriptive decision theory include <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Stanovich (2010)</a> and <a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hastie &amp; Dawes (2009)</a>.</p>\n<p>Two related fields beyond the scope of this FAQ are <a href=\"http://en.wikipedia.org/wiki/Game_theory\">game theory</a> and <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">social choice theory</a>. Game theory is the study of conflict and cooperation among multiple decision makers, and is thus sometimes called \"interactive decision theory.\" Social choice theory is the study of making a collective decision by combining the preferences of multiple decision makers in various ways.</p>\n<p>This FAQ draws heavily from two textbooks on decision theory: <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a> and <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009)</a>. It also draws from more recent results in decision theory, published in journals such as <em><a href=\"http://www.springerlink.com/content/0039-7857\">Synthese</a></em> and <em><a href=\"http://www.springerlink.com/content/0040-5833\">Theory and Decision</a></em>.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"2__Is_the_rational_decision_always_the_right_decision_\"><a href=\"#is-the-rational-decision-always-the-right-decision\">2. Is the rational decision always the right decision?</a></h2>\n<p>No. Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 1) explains:</p>\n<blockquote>\n<p>[In 1700], King Carl of Sweden and his 8,000 troops attacked the Russian army [which] had about ten times as many troops... Most historians agree that the Swedish attack was irrational, since it was almost certain to fail... However, because of an unexpected blizzard that blinded the Russian army, the Swedes won...</p>\n</blockquote>\n<blockquote>\n<p>Looking back, the Swedes' decision to attack the Russian army was no doubt right, since the <em>actual outcome</em> turned out to be success. However, since the Swedes had no <em>good reason</em> for expecting that they were going to win, the decision was nevertheless irrational.</p>\n</blockquote>\n<blockquote>\n<p>More generally speaking, we say that a decision is <em>right</em> if and only if its actual outcome is at least as good as that of every other possible outcome. Furthermore, we say that a decision is <em>rational</em> if and only if the decision maker [<em>aka</em> the \"agent\"] chooses to do what she has most reason to do at the point in time at which the decision is made.</p>\n</blockquote>\n<p>Unfortunately, we cannot know with certainty what the right decision is. Thus, the best we can do is to try to make \"rational\" or \"optimal\" decisions based on our preferences and incomplete information.</p>\n<p>&nbsp;</p>\n<h2 id=\"3__How_can_I_better_understand_a_decision_problem_\"><a href=\"#how-can-i-better-understand-a-decision-problem\">3. How can I better understand a decision problem?</a></h2>\n<p>First, we must <em>formalize</em> a decision problem. It usually helps to <em>visualize</em> the decision problem, too.</p>\n<p>In decision theory, decision rules are only defined relative to a formalization of a given decision problem, and a formalization of a decision problem can be visualized in multiple ways. Here is an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 2):</p>\n<blockquote>\n<p>Suppose... that you are thinking about taking out fire insurance on your home. Perhaps it costs $100 to take out insurance on a house worth $100,000, and you ask: Is it worth it?</p>\n</blockquote>\n<p>The most common way to formalize a decision problem is to break it into states, acts, and outcomes. When facing a decision problem, the decision maker aims to choose the <em>act</em> that will have the best <em>outcome</em>. But the outcome of each act depends on the <em>state</em> of the world, which is unknown to the decision maker.</p>\n<p>In this framework, speaking loosely, a state is a part of the world that is not an act (that can be performed now by the decision maker) or an outcome (the question of what, more precisely, states are is a complex question that is beyond the scope of this document). Luckily, not all states are relevant to a particular decision problem. We only need to take into account states that affect the agent's preference among acts. A simple formalization of the fire insurance problem might include only two states: the state in which your house doesn't (later) catch on fire, and the state in which your house <em>does</em> (later) catch on fire.</p>\n<p>Presumably, the agent prefers some outcomes to others. Suppose the four conceivable outcomes in the above decision problem are: (1) House and $0, (2) House and -$100, (3) No house and $99,900, and (4) No house and $0. In this case, the decision maker might prefer outcome 1 over outcome 2, outcome 2 over outcome 3, and outcome 3 over outcome 4. (We'll discuss measures of value for outcomes in the next section.)</p>\n<p>An act is commonly taken to be a function that takes one set of the possible states of the world as input and gives a particular outcome as output. For the above decision problem we could say that if the act \"Take out insurance\" has the world-state \"Fire\" as its input, then it will give the outcome \"No house and $99,900\" as its output.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/DhCAW.jpg\" alt=\"An outline of the states, acts and outcomes in the insurance case\">\n<p class=\"caption\">An outline of the states, acts and outcomes in the insurance case</p>\n</div>\n<p>Note that decision theory is concerned with <em>particular</em> acts rather than <em>generic</em> acts, e.g. \"sailing west in 1492\" rather than \"sailing.\" Moreover, the acts of a decision problem must be <em>alternative</em> acts, so that the decision maker has to choose exactly <em>one</em> act.</p>\n<p>Once a decision problem has been formalized, it can then be visualized in any of several ways.</p>\n<p>One way to visualize this decision problem is to use a <em>decision matrix</em>:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Fire</em></td>\n<td><em>No fire</em></td>\n</tr>\n<tr>\n<td><em>Take out insurance</em></td>\n<td>No house and $99,900</td>\n<td>House and -$100</td>\n</tr>\n<tr>\n<td><em>No insurance</em></td>\n<td>No house and $0</td>\n<td>House and $0</td>\n</tr>\n</tbody>\n</table>\n<p>Another way to visualize this problem is to use a <em>decision tree</em>:</p>\n<p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1608693326/basic-decision-tree_ohriwo.gif\" alt=\"\"></p>\n<p>The square is a <em>choice node</em>, the circles are <em>chance nodes</em>, and the triangles are <em>terminal nodes</em>. At the choice node, the decision maker chooses which branch of the decision tree to take. At the chance nodes, <em>nature</em> decides which branch to follow. The triangles represent outcomes.</p>\n<p>Of course, we could add more branches to each choice node and each chance node. We could also add more choice nodes, in which case we are representing a <em>sequential</em> decision problem. Finally, we could add probabilities to each branch, as long as the probabilities of all the branches extending from each single node sum to 1. And because a decision tree obeys the laws of probability theory, we can calculate the probability of any given node by multiplying the probabilities of all the branches preceding it.</p>\n<p>Our decision problem could also be represented as a <em>vector</em> \u2014 an ordered list of mathematical objects that is perhaps most suitable for computers:</p>\n<blockquote>\n<p>[<br> [a<sub>1</sub> = take out insurance,<br> a<sub>2</sub> = do not];<br> [s<sub>1</sub> = fire,<br> s<sub>2</sub> = no fire];<br> [(a<sub>1</sub>, s<sub>1</sub>) = No house and $99,900,<br> (a<sub>1</sub>, s<sub>2</sub>) = House and -$100,<br> (a<sub>2</sub>, s<sub>1</sub>) = No house and $0,<br> (a<sub>2</sub>, s<sub>2</sub>) = House and $0]<br> ]</p>\n</blockquote>\n<p>For more details on formalizing and visualizing decision problems, see <a href=\"http://www.amazon.com/Introduction-Decision-Analysis-3rd-Edition/dp/0964793865/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Skinner (1993)</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"4__How_can_I_measure_an_agent_s_preferences_\"><a href=\"#how-can-i-measure-an-agents-preferences\">4. How can I measure an agent's preferences?</a></h2>\n<h3 id=\"4_1__The_concept_of_utility\"><a href=\"#the-concept-of-utility\">4.1. The concept of utility</a></h3>\n<p>It is important not to measure an agent's preferences in terms of <em>objective</em> value, e.g. monetary value. To see why, consider the absurdities that can result when we try to measure an agent's preference with money alone.</p>\n<p>Suppose you may choose between (A) receiving a million dollars <em>for sure</em>, and (B) a 50% chance of winning either $3 million or nothing. The <em>expected monetary value</em> (EMV) of your act is computed by multiplying the monetary value of each possible outcome by its probability. So, the EMV of choice A is (1)($1 million) = $1 million. The EMV of choice B is (0.5)($3 million) + (0.5)($0) = $1.5 million. Choice B has a higher expected monetary value, and yet many people would prefer the guaranteed million.</p>\n<p>Why? For many people, the difference between having $0 and $1 million is <em>subjectively</em> much larger than the difference between having $1 million and $3 million, even if the latter difference is larger in dollars.</p>\n<p>To capture an agent's <em>subjective</em> preferences, we use the concept of <em>utility</em>. A <em>utility function</em> assigns numbers to outcomes such that outcomes with higher numbers are preferred to outcomes with lower numbers. For example, for a particular decision maker \u2014 say, one who has no money \u2014 the utility of $0 might be 0, the utility of $1 million might be 1000, and the utility of $3 million might be 1500. Thus, the <em>expected utility</em> (EU) of choice A is, for this decision maker, (1)(1000) = 1000. Meanwhile, the EU of choice B is (0.5)(1500) + (0.5)(0) = 750. In this case, the expected utility of choice A is greater than that of choice B, even though choice B has a greater expected monetary value.</p>\n<p>Note that those from the field of statistics who work on decision theory tend to talk about a \"loss function,\" which is simply an <em>inverse</em> utility function. For an overview of decision theory from this perspective, see <a href=\"http://www.amazon.com/Statistical-Decision-Bayesian-Analysis-Statistics/dp/1441930744/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Berger (1985)</a> and <a href=\"http://www.amazon.com/Bayesian-Choice-Decision-Theoretic-Computational-Implementation/dp/0387715983/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Robert (2001)</a>. For a critique of some standard results in statistical decision theory, see <a href=\"http://www.amazon.com/Probability-Theory-The-Logic-Science/dp/0521592712/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jaynes (2003, ch. 13)</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"4_2__Types_of_utility\"><a href=\"#types-of-utility\">4.2. Types of utility</a></h3>\n<p>An agent's utility function can't be directly observed, so it must be constructed \u2014 e.g. by asking them which options they prefer for a large set of pairs of alternatives (as on <a href=\"http://www.whoishotter.com\">WhoIsHotter.com</a>). The number that corresponds to an outcome's utility can convey different information depending on the <em>utility scale</em> in use, and the utility scale in use depends on how the utility function is constructed.</p>\n<p>Decision theorists distinguish three kinds of utility scales:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>Ordinal scales (\"12 is better than 6\"). In an ordinal scale, preferred outcomes are assigned higher numbers, but the numbers don't tell us anything about the differences or ratios between the utility of different outcomes.</p>\n</li>\n<li>\n<p>Interval scales (\"the difference between 12 and 6 equals that between 6 and 0\"). An interval scale gives us more information than an ordinal scale. Not only are preferred outcomes assigned higher numbers, but also the numbers accurately reflect the <em>difference</em> between the utility of different outcomes. They do not, however, necessarily reflect the ratios of utility between different outcomes. If outcome A has utility 0, outcome B has utility 6, and outcome C has utility 12 on an interval scale, then we know that the difference in utility between outcomes A and B and between outcomes B and C is the same, but we can't know whether outcome B is \"twice as good\" as outcome A.</p>\n</li>\n<li>\n<p>Ratio scales (\"12 is exactly <em>twice</em> as valuable as 6\"). Numerical utility assignments on a ratio scale give us the most information of all. They accurately reflect preference rankings, differences, <em>and</em> ratios. Thus, we can say that an outcome with utility 12 is exactly <em>twice</em> as valuable to the agent in question as an outcome with utility 6.</p>\n</li>\n</ol>\n<p>Note that neither <em>experienced utility</em> (happiness) nor the notions of \"average utility\" or \"total utility\" discussed by utilitarian moral philosophers are the same thing as the <em>decision utility</em> that we are discussing now to describe decision preferences. As the situation merits, we can be even more specific. For example, when discussing the type of decision utility used in an interval scale utility function constructed using Von Neumann &amp; Morgenstern's axiomatic approach (see section 8), some people use the term <em>VNM-utility</em>.</p>\n<p>Now that you know that an agent's preferences can be represented as a \"utility function,\" and that assignments of utility to outcomes can mean different things depending on the utility scale of the utility function, we are ready to think more formally about the challenge of making \"optimal\" or \"rational\" choices. (We will return to the problem of constructing an agent's utility function later, in section 8.3.)</p>\n<p>&nbsp;</p>\n<h2 id=\"5__What_do_decision_theorists_mean_by__risk____ignorance___and__uncertainty__\"><a href=\"#what-do-decision-theorists-mean-by-risk-ignorance-and-uncertainty\">5. What do decision theorists mean by \"risk,\" \"ignorance,\" and \"uncertainty\"?</a></h2>\n<p>Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 1) explains:</p>\n<blockquote>\n<p>In decision theory, everyday terms such as <em>risk</em>, <em>ignorance</em>, and <em>uncertainty</em> are used as technical terms with precise meanings. In decisions under risk the decision maker knows the probability of the possible outcomes, whereas in decisions under ignorance the probabilities are either unknown or non-existent. Uncertainty is either used as a synonym for ignorance, or as a broader term referring to both risk and ignorance.</p>\n</blockquote>\n<p>In this FAQ, a \"decision under ignorance\" is one in which probabilities are <em>not</em> assigned to all outcomes, and a \"decision under uncertainty\" is one in which probabilities <em>are</em> assigned to all outcomes. The term \"risk\" will be reserved for discussions related to utility.</p>\n<p>&nbsp;</p>\n<h2 id=\"6__How_should_I_make_decisions_under_ignorance_\"><a href=\"#how-should-i-make-decisions-under-ignorance\">6. How should I make decisions under ignorance?</a></h2>\n<p>A decision maker faces a \"decision under ignorance\" when she (1) knows which acts she could choose and which outcomes they may result in, but (2) is unable to assign probabilities to the outcomes.</p>\n<p>(Note that many theorists think that all decisions under ignorance can be transformed into decisions under uncertainty, in which case this section will be irrelevant except for subsection 6.1. For details, see section 7.)</p>\n<p>&nbsp;</p>\n<h3 id=\"6_1__The_dominance_principle\"><a href=\"#the-dominance-principle\">6.1. The dominance principle</a></h3>\n<p>To borrow an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3), suppose that Jane isn't sure whether to order hamburger or monkfish at a new restaurant. Just about any chef can make an edible hamburger, and she knows that monkfish is fantastic if prepared by a world-class chef, but she also recalls that monkfish is difficult to cook. Unfortunately, she knows too little about this restaurant to assign any probability to the prospect of getting good monkfish. Her decision matrix might look like this:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Good chef</em></td>\n<td><em>Bad chef</em></td>\n</tr>\n<tr>\n<td><em>Monkfish</em></td>\n<td>good monkfish</td>\n<td>terrible monkfish</td>\n</tr>\n<tr>\n<td><em>Hamburger</em></td>\n<td>edible hamburger</td>\n<td>edible hamburger</td>\n</tr>\n<tr>\n<td><em>No main course</em></td>\n<td>hungry</td>\n<td>hungry</td>\n</tr>\n</tbody>\n</table>\n<p>Here, decision theorists would say that the \"hamburger\" choice <em>dominates</em> the \"no main course\" choice. This is because choosing the hamburger leads to a better outcome for Jane no matter which possible state of the world (good chef or bad chef) turns out to be true.</p>\n<p>This <em>dominance principle</em> comes in two forms:</p>\n<ul>\n<li><em>Weak dominance</em>: One act is <em>more</em> rational than another if (1) all its possible outcomes are at least as good as those of the other, and if (2) there is at least one possible outcome that is better than that of the other act.</li>\n<li><em>Strong dominance</em>: One act is <em>more</em> rational than another if all of its possible outcome are better than that of the other act.</li>\n</ul>\n<div class=\"figure\"><img src=\"http://i.imgur.com/7fU6U.jpg\" alt=\"A comparison of strong and weak dominance\">\n<p class=\"caption\">A comparison of strong and weak dominance</p>\n</div>\n<p>The dominance principle can also be applied to decisions under uncertainty (in which probabilities <em>are</em> assigned to all the outcomes). If we assign probabilities to outcomes, it is still rational to choose one act over another act if all its outcomes are at least as good as the outcomes of the other act.</p>\n<p>However, the dominance principle only applies (non-controversially) when the agent\u2019s acts are independent of the state of the world. So consider the decision of whether to steal a coat:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Charged with theft</em></td>\n<td><em>Not charged with theft</em></td>\n</tr>\n<tr>\n<td><em>Theft</em></td>\n<td>Jail and coat</td>\n<td>Freedom and coat</td>\n</tr>\n<tr>\n<td><em>No theft</em></td>\n<td>Jail</td>\n<td>Freedom</td>\n</tr>\n</tbody>\n</table>\n<p>In this case, stealing the coat dominates not doing so but isn\u2019t necessarily the rational decision. After all, stealing increases your chance of getting charged with theft and might be irrational for this reason. So dominance doesn\u2019t apply in cases like this where the state of the world is not independent of the agents act.</p>\n<p>On top of this, not all decision problems include an act that dominates all the others. Consequently additional principles are often required to reach a decision.</p>\n<p>&nbsp;</p>\n<h3 id=\"6_2__Maximin_and_leximin\"><a href=\"#maximin-and-leximin\">6.2. Maximin and leximin</a></h3>\n<p>Some decision theorists have suggested the <em>maximin principle</em>: if the worst possible outcome of one act is better than the worst possible outcome of another act, then the former act should be chosen. In Jane's decision problem above, the maximin principle would prescribe choosing the hamburger, because the worst possible outcome of choosing the hamburger (\"edible hamburger\") is better than the worst possible outcome of choosing the monkfish (\"terrible monkfish\") and is also better than the worst possible outcome of eating no main course (\"hungry\").</p>\n<p>If the worst outcomes of two or more acts are equally good, the maximin principle tells you to be indifferent between them. But that doesn't seem right. For this reason, fans of the maximin principle often invoke the <em>lexical</em> maximin principle (\"leximin\"), which says that if the worst outcomes of two or more acts are equally good, one should choose the act for which the <em>second worst</em> outcome is best. (If that doesn't single out a single act, then the <em>third worst</em> outcome should be considered, and so on.)</p>\n<p>Why adopt the leximin principle? Advocates point out that the leximin principle transforms a decision problem under ignorance into a decision problem under partial certainty. The decision maker doesn't know what the outcome will be, but they know what the worst possible outcome will be.</p>\n<p>But in some cases, the leximin rule seems clearly irrational. Imagine this decision problem, with two possible acts and two possible states of the world:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>$1</td>\n<td>$10,001.01</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>$1.01</td>\n<td>$1.01</td>\n</tr>\n</tbody>\n</table>\n<p>In this situation, the leximin principle prescribes choosing a<sub>2</sub>. But most people would agree it is rational to risk losing out on a single cent for the chance to get an extra $10,000.</p>\n<p>&nbsp;</p>\n<h3 id=\"6_3__Maximax_and_optimism_pessimism\"><a href=\"#maximax-and-optimism-pessimism\">6.3. Maximax and optimism-pessimism</a></h3>\n<p>The maximin and leximin rules focus their attention on the worst possible outcomes of a decision, but why not focus on the <em>best</em> possible outcome? The <em>maximax principle</em> prescribes that if the best possible outcome of one act is better than the best possible outcome of another act, then the former act should be chosen.</p>\n<p>More popular among decision theorists is the <em>optimism-pessimism rule</em> (<em>aka</em> the <em>alpha-index rule</em>). The optimism-pessimism rule prescribes that one consider both the best and worst possible outcome of each possible act, and then choose according to one's degree of optimism or pessimism.</p>\n<p>Here's an example from Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3):</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n<td>s<sub>3</sub></td>\n<td>s<sub>4</sub></td>\n<td>s<sub>5</sub></td>\n<td>s<sub>6</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>55</td>\n<td>18</td>\n<td>28</td>\n<td>10</td>\n<td>36</td>\n<td>100</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>50</td>\n<td>87</td>\n<td>55</td>\n<td>90</td>\n<td>75</td>\n<td>70</td>\n</tr>\n</tbody>\n</table>\n<p>We represent the decision maker's level of optimism on a scale of 0 to 1, where 0 is maximal pessimism and 1 is maximal optimism. For a<sub>1</sub>, the worst possible outcome is 10 and the best possible outcome is 100. That is, min(a<sub>1</sub>) = 10 and max(a<sub>1</sub>) = 100. So if the decision maker is 0.85 optimistic, then the total value of a<sub>1</sub> is (0.85)(100) + (1 - 0.85)(10) = 86.5, and the total value of a<sub>2</sub> is (0.85)(90) + (1 - 0.85)(50) = 84. In this situation, the optimism-pessimism rule prescribes action a<sub>1</sub>.</p>\n<p>If the decision maker's optimism is 0, then the optimism-pessimism rule collapses into the maximin rule because (0)(max(a<sub>i</sub>)) + (1 - 0)(min(a<sub>i</sub>)) = min(a<sub>i</sub>). And if the decision maker's optimism is 1, then the optimism-pessimism rule collapses into the maximax rule. Thus, the optimism-pessimism rule turns out to be a generalization of the maximin and maximax rules. (Well, sort of. The minimax and maximax principles require only that we measure value on an ordinal scale, whereas the optimism-pessimism rule requires that we measure value on an interval scale.)</p>\n<p>The optimism-pessimism rule pays attention to both the best-case and worst-case scenarios, but is it rational to ignore all the outcomes in between? Consider this example:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>s<sub>1</sub></td>\n<td>s<sub>2</sub></td>\n<td>s<sub>3</sub></td>\n</tr>\n<tr>\n<td>a<sub>1</sub></td>\n<td>1</td>\n<td>2</td>\n<td>100</td>\n</tr>\n<tr>\n<td>a<sub>2</sub></td>\n<td>1</td>\n<td>99</td>\n<td>100</td>\n</tr>\n</tbody>\n</table>\n<p>The maximum and minimum values for a<sub>1</sub> and a<sub>2</sub> are the same, so for every degree of optimism both acts are equally good. But it seems obvious that one should choose a<sub>2</sub>.</p>\n<p>&nbsp;</p>\n<h3 id=\"6_4__Other_decision_principles\"><a href=\"#other-decision-principles\">6.4. Other decision principles</a></h3>\n<p>Many other decision principles for dealing with decisions under ignorance have been proposed, including <a href=\"http://teaching.ust.hk/~bee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\">minimax regret</a>, <a href=\"http://www.amazon.com/Info-Gap-Decision-Theory-Second-Edition/dp/0123735521/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">info-gap</a>, and <a href=\"http://www.existential-risk.org/concept.pdf\">maxipok</a>. For more details on making decisions under ignorance, see <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009)</a> and <a href=\"http://www.dss.dpem.tuc.gr/pdf/Choice%20under%20complete%20uncertainty%20-%20axiomatic%20characterizati.pdf\">Bossert et al. (2000)</a>.</p>\n<p>One queer feature of the decision principles discussed in this section is that they willfully disregard some information relevant to making a decision. Such a move could make sense when trying to find a decision algorithm that performs well under tight limits on available computation (<a href=\"http://www.dss.dpem.tuc.gr/pdf/An%20axiomatic%20treatment%20of%20three%20qualitative%20decision%20criteri.pdf\">Brafman &amp; Tennenholtz (2000)</a>), but it's unclear why an <em>ideal</em> agent with infinite computing power (fit for a <em>normative</em> rather than a <em>prescriptive</em> theory) should willfully disregard information.</p>\n<p>&nbsp;</p>\n<h2 id=\"7__Can_decisions_under_ignorance_be_transformed_into_decisions_under_uncertainty_\"><a href=\"#can-decisions-under-ignorance-be-transformed-into-decisions-under-uncertainty\">7. Can decisions under ignorance be transformed into decisions under uncertainty?</a></h2>\n<p>Can decisions under ignorance be transformed into decisions under uncertainty? This would simplify things greatly, because there is near-universal agreement that decisions under uncertainty should be handled by \"maximizing expected utility\" (see section 11 for clarifications), whereas decision theorists still debate what should be done about decisions under ignorance.</p>\n<p>For <a href=\"http://en.wikipedia.org/wiki/Bayesian_probability\">Bayesians</a> (see section 10), <em>all</em> decisions under ignorance are transformed into decisions under uncertainty (<a href=\"http://www.amazon.com/Introduction-Bayesian-Inference-Decision-Edition/dp/0964793849/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Winkler 2003</a>, ch. 5) when the decision maker assigns an \"ignorance prior\" to each outcome for which they don't know how to assign a probability. (Another way of saying this is to say that a Bayesian decision maker never faces a decision under ignorance, because a Bayesian must always assign a prior probability to events.) One must then consider how to assign priors, an important debate among Bayesians (see section 10).</p>\n<p>Many non-Bayesian decision theorists also think that decisions under ignorance can be transformed into decisions under uncertainty due to something called the <em>principle of insufficient reason</em>. The principle of insufficient reason prescribes that if you have literally <em>no</em> reason to think that one state is more probable than another, then one should assign <em>equal</em> probability to both states.</p>\n<p>One objection to the principle of insufficient reason is that it is very sensitive to how states are individuated. Peterson (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2009</a>, ch. 3) explains:</p>\n<blockquote>\n<p>Suppose that before embarking on a trip you consider whether to bring an umbrella or not. [But] you know nothing about the weather at your destination. If the formalization of the decision problem is taken to include only two states, viz. rain and no rain, [then by the principle of insufficient reason] the probability of each state will be 1/2. However, it seems that one might just as well go for a formalization that divides the space of possibilities into three states, viz. heavy rain, moderate rain, and no rain. If the principle of insufficient reason is applied to the latter set of states, their probabilities will be 1/3. In some cases this difference will affect our decisions. Hence, it seems that anyone advocating the principle of insufficient reason must [defend] the rather implausible hypothesis that there is only one correct way of making up the set of states.</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/kXn03.jpg\" alt=\"An objection to the principle of insufficient reason\">\n<p class=\"caption\">An objection to the principle of insufficient reason</p>\n</div>\n<p>Advocates of the principle of insufficient reason might respond that one must consider <em>symmetric</em> states. For example if someone gives you a die with <em>n</em> sides and you have no reason to think the die is biased, then you should assign a probability of 1/<em>n</em> to each side. But, Peterson notes:</p>\n<blockquote>\n<p>...not all events can be described in symmetric terms, at least not in a way that justifies the conclusion that they are equally probable. Whether Ann's marriage will be a happy one depends on her future emotional attitude toward her husband. According to one description, she could be either in love or not in love with him; then the probability of both states would be 1/2. According to another equally plausible description, she could either be deeply in love, a little bit in love or not at all in love with her husband; then the probability of each state would be 1/3.</p>\n<p>&nbsp;</p>\n</blockquote>\n<h2 id=\"8__How_should_I_make_decisions_under_uncertainty_\"><a href=\"#how-should-i-make-decisions-under-uncertainty\">8. How should I make decisions under uncertainty?</a></h2>\n<p>A decision maker faces a \"decision under uncertainty\" when she (1) knows which acts she could choose and which outcomes they may result in, and she (2) assigns probabilities to the outcomes.</p>\n<p>Decision theorists generally agree that when facing a decision under uncertainty, it is rational to choose the act with the highest expected utility. This is the principle of <em>expected utility maximization</em> (EUM).</p>\n<p>Decision theorists offer two kinds of justifications for EUM. The first has to do with the law of large numbers (see section 8.1). The second has to do with the axiomatic approach (see sections 8.2 through 8.6).</p>\n<p>&nbsp;</p>\n<h3 id=\"8_1__The_law_of_large_numbers\"><a href=\"#the-law-of-large-numbers\">8.1. The law of large numbers</a></h3>\n<p>The \"law of large numbers,\" which states that <em>in the long run</em>, if you face the same decision problem again and again and again, and you always choose the act with the highest expected utility, then you will almost certainly be better off than if you choose any other acts.</p>\n<p>There are two problems with using the law of large numbers to justify EUM. The first problem is that the world is ever-changing, so we rarely if ever face the same decision problem \"again and again and again.\" The law of large numbers says that if you face the same decision problem infinitely many times, then the probability that you could do better by not maximizing expected utility approaches zero. But you won't ever face the same decision problem infinitely many times! Why should you care what would happen if a certain condition held, if you know that condition will never hold?</p>\n<p>The second problem with using the law of large numbers to justify EUM has to do with a mathematical theorem known as <em>gambler's ruin</em>. Imagine that you and I flip a fair coin, and I pay you $1 every time it comes up heads and you pay me $1 every time it comes up tails. We both start with $100. If we flip the coin enough times, one of us will face a situation in which the sequence of heads or tails is longer than we can afford. If a long-enough sequence of heads comes up, I'll run out of $1 bills with which to pay you. If a long-enough sequence of tails comes up, you won't be able to pay me. So in this situation, the law of large numbers guarantees that you will be better off in the long run by maximizing expected utility only if you start the game with an infinite amount of money (so that you never go broke), which is an unrealistic assumption. (For technical convenience, assume utility increases linearly with money. But the basic point holds without this assumption.)</p>\n<p>&nbsp;</p>\n<h3 id=\"8_2__The_axiomatic_approach\"><a href=\"#the-axiomatic-approach\">8.2. The axiomatic approach</a></h3>\n<p>The other method for justifying EUM seeks to show that EUM can be derived from axioms that hold regardless of what happens in the long run.</p>\n<p>In this section we will review perhaps the most famous axiomatic approach, from <a href=\"http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Von Neumann and Morgenstern (1947)</a>. Other axiomatic approaches include <a href=\"http://www.amazon.com/The-Foundations-Statistics-Leonard-Savage/dp/0486623491/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Savage (1954)</a>, <a href=\"http://www.amazon.com/The-Logic-Decision-Richard-Jeffrey/dp/0226395820/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jeffrey (1983)</a>, and <a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Ambiguity/AnscombeAumann%20AMS%2063.pdf\">Anscombe &amp; Aumann (1963)</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"8_3__The_Von_Neumann_Morgenstern_utility_theorem\"><a href=\"#the-von-neumann-morgenstern-utility-theorem\">8.3. The Von Neumann-Morgenstern utility theorem</a></h3>\n<p>The first decision theory axiomatization appeared in an appendix to the second edition of Von Neumann &amp; Morgenstern's <em><a href=\"http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Theory of Games and Economic Behavior</a></em> (1947). An important point to note up front is that, in this axiomatization, Von Neumann and Morgenstern take the options that the agent chooses between to not be acts, as we\u2019ve defined them, but lotteries (where a lottery is a set of outcomes, each paired with a probability). As such, while discussing their axiomatization, we will talk of lotteries. (Despite making this distinction, acts and lotteries are closely related. Under the conditions of uncertainty that we are considering here, each act will be associated with some lottery and so preferences over lotteries could be used to determine preferences over acts, if so desired).</p>\n<p>The key feature of the Von Neumann and Morgenstern axiomatization is a proof that if a decision maker states her preferences over a set of lotteries, and if her preferences conform to a set of intuitive structural constraints (axioms), then we can construct a utility function (on an interval scale) from her preferences over lotteries and show that she acts <em>as if</em> she maximizes expected utility with respect to that utility function.</p>\n<p>What are the axioms to which an agent's preferences over lotteries must conform? There are four of them.</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>The <em>completeness axiom</em> states that the agent must <em>bother to state a preference</em> for each pair of lotteries. That is, the agent must prefer A to B, or prefer B to A, or be indifferent between the two.</p>\n</li>\n<li>\n<p>The <em>transitivity axiom</em> states that if the agent prefers A to B and B to C, she must also prefer A to C.</p>\n</li>\n<li>\n<p>The <em>independence axiom</em> states that, for example, if an agent prefers an apple to an orange, then she must also prefer the lottery [55% chance she gets an apple, otherwise she gets cholera] over the lottery [55% chance she gets an orange, otherwise she gets cholera]. More generally, this axiom holds that a preference must hold independently of the possibility of another outcome (e.g. cholera).</p>\n</li>\n<li>\n<p>The <em>continuity axiom</em> holds that if the agent prefers A to B to C, then there exists a unique <em>p</em> (probability) such that the agent is indifferent between [<em>p</em>(A) + (1 - <em>p</em>)(C)] and [outcome B with certainty].</p>\n</li>\n</ol>\n<p>The continuity axiom requires <a href=\"http://www.youtube.com/watch?v=hSUsiA8dhKM\">more explanation</a>. Suppose that A = $1 million, B = $0, and C = Death. If <em>p</em> = 0.5, then the agent's two lotteries under consideration for the moment are:</p>\n<ol style=\"list-style-type: decimal\">\n<li>(0.5)($1M) + (1 - 0.5)(Death) [win $1M with 50% probability, die with 50% probability]</li>\n<li>(1)($0) [win $0 with certainty]</li>\n</ol>\n<p>Most people would <em>not</em> be indifferent between $0 with certainty and [50% chance of $1M, 50% chance of Death] \u2014 the risk of Death is too high! But if you have continuous preferences, there is <em>some</em> probability <em>p</em> for which you'd be indifferent between these two lotteries. Perhaps <em>p</em> is very, very high:</p>\n<ol style=\"list-style-type: decimal\">\n<li>(0.999999)($1M) + (1 - 0.999999)(Death) [win $1M with 99.9999% probability, die with 0.0001% probability]</li>\n<li>(1)($0) [win $0 with certainty]</li>\n</ol>\n<p>Perhaps now you'd be indifferent between lottery 1 and lottery 2. Or maybe you'd be <em>more</em> willing to risk Death for the chance of winning $1M, in which case the <em>p</em> for which you'd be indifferent between lotteries 1 and 2 is lower than 0.999999. As long as there is <em>some</em> <em>p</em> at which you'd be indifferent between lotteries 1 and 2, your preferences are \"continuous.\"</p>\n<p>Given this setup, Von Neumann and Morgenstern proved their theorem, which states that if the agent's preferences over lotteries obeys their axioms, then:</p>\n<ul>\n<li>The agent's preferences can be represented by a utility function that assigns higher utility to preferred lotteries.</li>\n<li>The agent acts in accordance with the principle of maximizing expected utility.</li>\n<li>All utility functions satisfying the above two conditions are \"positive linear transformations\" of each other. (Without going into the details: this is why VNM-utility is measured on an interval scale.)</li>\n</ul>\n<h3 id=\"vnm-utility-theory-and-rationality\"><br></h3>\n<h3 id=\"8_4__VNM_utility_theory_and_rationality\"><a href=\"#vnm-utility-theory-and-rationality\">8.4. VNM utility theory and rationality</a></h3>\n<p>An agent which conforms to the VNM axioms is sometimes said to be \"VNM-rational.\" But why should \"VNM-rationality\" constitute our notion of <em>rationality in general</em>? How could VNM's result justify the claim that a rational agent maximizes expected utility when facing a decision under uncertainty? The argument goes like this:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If an agent chooses lotteries which it prefers (in decisions under uncertainty), and if its preferences conform to the VNM axioms, then it is rational. Otherwise, it is irrational.</li>\n<li>If an agent chooses lotteries which it prefers (in decisions under uncertainty), and if its preferences conform to the VNM axioms, then it maximizes expected utility.</li>\n<li>Therefore, a rational agent maximizes expected utility (in decisions under uncertainty).</li>\n</ol>\n<p>Von Neumann and Morgenstern proved premise 2, and the conclusion follows from premise 1 and 2. But why accept premise 1?</p>\n<p>Few people deny that it would be irrational for an agent to choose a lottery which it does not prefer. But why is it irrational for an agent's preferences to violate the VNM axioms? I will save that discussion for section 8.6.</p>\n<p>&nbsp;</p>\n<h3 id=\"8_5__Objections_to_VNM_rationality\"><a href=\"#objections-to-vnm-rationality\">8.5. Objections to VNM-rationality</a></h3>\n<p>Several objections have been raised to Von Neumann and Morgenstern's result:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p><em>The VNM axioms are too strong</em>. Some have argued that the VNM axioms are not self-evidently true. See section 8.6.</p>\n</li>\n<li>\n<p><em>The VNM system offers no action guidance</em>. A VNM-rational decision maker cannot use VNM utility theory for action guidance, because she must state her preferences over lotteries at the start. But if an agent can state her preferences over lotteries, then she already knows which lottery to choose. (For more on this, see section 9.)</p>\n</li>\n<li>\n<p><em>In the VNM system, utility is defined via preferences over lotteries rather than preferences over outcomes</em>. To many, it seems odd to <em>define</em> utility with respect to preferences over lotteries. Many would argue that utility should be defined in relation to preferences over <em>outcomes</em> or <em>world-states</em>, and that's not what the VNM system does. (Also see section 9.)</p>\n</li>\n</ol>\n<h3 id=\"should-we-accept-the-vnm-axioms\"><br></h3>\n<h3 id=\"8_6__Should_we_accept_the_VNM_axioms_\"><a href=\"#should-we-accept-the-vnm-axioms\">8.6. Should we accept the VNM axioms?</a></h3>\n<p>The VNM preference axioms define what it is for an agent to be VNM-rational. But why should we accept these axioms? Usually, it is argued that each of the axioms are <em>pragmatically justified</em> because an agent which violates the axioms can face situations in which they are guaranteed end up worse off (from <em>their own</em> perspective).</p>\n<p>In sections 8.6.1 and 8.6.2 I go into some detail about pragmatic justifications offered for the transitivity and completeness axioms. For more detail, including arguments about the justification of the other axioms, see <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 8)</a> and <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anand (1993)</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"8_6_1__The_transitivity_axiom\"><a href=\"#the-transitivity-axiom\">8.6.1. The transitivity axiom</a></h4>\n<p>Consider the <em>money-pump argument</em> in favor of the transitivity axiom (\"if the agent prefers A to B and B to C, she must also prefer A to C\").</p>\n<blockquote>\n<p>Imagine that a friend offers to give you exactly one of her three... novels, x or y or z... [and] that your preference ordering over the three novels is... [that] you prefer x to y, and y to z, and z to x... [That is, your preferences are <em>cyclic</em>, which is a type of <em>intransitive</em> preference relation.] Now suppose that you are in possession of z, and that you are invited to swap z for y. Since you prefer y to z, rationality obliges you to swap. So you swap, and temporarily get y. You are then invited to swap y for x, which you do, since you prefer x to y. Finally, you are offered to <em>pay a small amount</em>, say one cent, for swapping x for z. Since z is strictly [preferred to] x, even after you have paid the fee for swapping, rationality tells you that you should accept the offer. This means that you end up where you started, the only difference being that you now have one cent less. This procedure is thereafter iterated over and over again. After a billion cycles you have lost ten million dollars, for which you have got nothing in return. (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2009</a>, ch. 8)</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/45csd.jpg\" alt=\"An example of a money-pump argument\">\n<p class=\"caption\">An example of a money-pump argument</p>\n</div>\n<p>Similar arguments (e.g. <a href=\"http://johanegustafsson.net/papers/a_money-pump_for_acyclic_intransitive_preferences.pdf\">Gustafsson 2010</a>) aim to show that the other kind of intransitive preferences (acyclic preferences) are irrational, too.</p>\n<p>(Of course, pragmatic arguments need not be framed in monetary terms. We could just as well construct an argument showing that an agent with intransitive preferences can be \"pumped\" of all their happiness, or all their moral virtue, or all their Twinkies.)</p>\n<p>&nbsp;</p>\n<h4 id=\"8_6_2__The_completeness_axiom\"><a href=\"#the-completeness-axiom\">8.6.2. The completeness axiom</a></h4>\n<p>The completeness axiom (\"the agent must prefer A to B, or prefer B to A, or be indifferent between the two\") is often attacked by saying that some goods or outcomes are incommensurable \u2014 that is, they cannot be compared. For example, must a rational agent be able to state a preference (or indifference) between money and human welfare?</p>\n<p>Perhaps the completeness axiom can be justified with a pragmatic argument. If you think it is rationally permissible to swap between two incommensurable goods, then one can construct a money pump argument in favor of the completeness axiom. But if you think it is <em>not</em> rational to swap between incommensurable goods, then one cannot construct a money pump argument for the completeness axiom. (In fact, even if it is rational to swap between incommensurable goods, <a href=\"http://personal.rhul.ac.uk/uhte/035/incomplete%20preferences.geb.pdf\">Mandler, 2005</a> has demonstrated that an agent that allows their current choices to depend on the previous ones can avoid being money pumped.)</p>\n<p>And in fact, there is a popular argument <em>against</em> the completeness axiom: the \"small improvement argument.\" For details, see <a href=\"http://www.amazon.com/Incommensurability-Incomparability-Practical-Reason-Chang/dp/0674447565/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Chang (1997)</a> and <a href=\"https://www.msb.se/Upload/Om%20MSB/Forskning/Projektrapporter/Peterson_artiklar/Small_Improvment_Argument.pdf\">Espinoza (2007)</a>.</p>\n<p>Note that in <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\">revealed preference theory</a>, according to which preferences are revealed through choice behavior, there is no room for incommensurable preferences because every choice always reveals a preference relation of \"better than,\" \"worse than,\" or \"equally as good as.\"</p>\n<p>Another proposal for dealing with the apparent incommensurability of some goods (such as money and human welfare) is the <em>multi-attribute approach</em>:</p>\n<blockquote>\n<p>In a multi-attribute approach, each type of attribute is measured in the unit deemed to be most suitable for that attribute. Perhaps money is the right unit to use for measuring financial costs, whereas the number of lives saved is the right unit to use for measuring human welfare. The total value of an alternative is thereafter determined by aggregating the attributes, e.g. money and lives, into an overall ranking of available alternatives...</p>\n</blockquote>\n<blockquote>\n<p>Several criteria have been proposed for choosing among alternatives with multiple attributes... [For example,] additive criteria assign weights to each attribute, and rank alternatives according to the weighted sum calculated by multiplying the weight of each attribute with its value... [But while] it is perhaps contentious to measure the utility of very different objects on a common scale, ...it seems equally contentious to assign numerical weights to attributes as suggested here....</p>\n</blockquote>\n<blockquote>\n<p>[Now let us] consider a very general objection to multi-attribute approaches. According to this objection, there exist several equally plausible but different ways of constructing the list of attributes. Sometimes the outcome of the decision process depends on which set of attributes is chosen. (<a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2009</a>, ch. 8)</p>\n</blockquote>\n<p>For more on the multi-attribute approach, see <a href=\"http://www.amazon.com/Decisions-Multiple-Objectives-Preferences-Tradeoffs/dp/0521438837/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Keeney &amp; Raiffa (1993)</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"8_6_3__The_Allais_paradox\"><a href=\"#the-allais-paradox\">8.6.3. The Allais paradox</a></h4>\n<p>Having considered the transitivity and completeness axioms, we can now turn to independence (a preference holds independently of considerations of other possible outcomes). Do we have any reason to reject this axiom? Here\u2019s one reason to think we might: in a case known as the <em>Allais paradox</em> <a href=\"http://www.jstor.org/stable/1907921\">Allais (1953)</a> it may seem reasonable to act in a way that contradicts independence.</p>\n<p>The Allais paradox asks us to consider two decisions (this version of the paradox is based on <a href=\"/lw/my/the_allais_paradox/\">Yudkowsky (2008)</a>).The first decision involves the choice between:</p>\n<p>(1A) A certain $24,000; and (1B) A 33/34 chance of $27,000 and a 1/34 chance of nothing.</p>\n<p>The second involves the choice between:</p>\n<p>(2A) A 34% chance of $24, 000 and a 66% chance of nothing; and (2B) A 33% chance of $27, 000 and a 67% chance of nothing.</p>\n<p>Experiments have shown that many people prefer (1A) to (1B) and (2B) to (2A). However, these preferences contradict independence. Option 2A is the same as [a 34% chance of option 1A and a 66% chance of nothing] while 2B is the same as [a 34% chance of option 1B and a 66% chance of nothing]. So independence implies that anyone that prefers (1A) to (1B) must also prefer (2A) to (2B).</p>\n<p>When this result was first uncovered, it was presented as evidence against the independence axiom. However, while the Allais paradox clearly reveals that independence fails as a <em>descriptive</em> account of choice, it\u2019s less clear what it implies about the normative account of rational choice that we are discussing in this document. As noted in <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521716543/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 4)</a>, however:</p>\n<blockquote>\n<p>[S]ince many people who have thought very hard about this example still feel that it would be rational to stick to the problematic preference pattern described above, there seems to be something wrong with the expected utility principle.</p>\n</blockquote>\n<p>However, Peterson then goes on to note that, many people, like the statistician Leonard Savage, argue that it is people\u2019s preference in the Allais paradox that are in error rather than the independence axiom. If so, then the paradox seems to reveal the danger of relying too strongly on intuition to determine the form that should be taken by normative theories of rational.</p>\n<p>&nbsp;</p>\n<h4 id=\"8_6_4__The_Ellsberg_paradox\"><a href=\"#the-ellsberg-paradox\">8.6.4. The Ellsberg paradox</a></h4>\n<p>The Allais paradox is far from the only case where people fail to act in accordance with EUM. Another well-known case is the Ellsberg paradox (the following is taken from <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a>:</p>\n<blockquote>\n<p>An urn contains ninety uniformly sized balls, which are randomly distributed. Thirty of the balls are yellow, the remaining sixty are red or blue. We are not told how many red (blue) balls are in the urn \u2013 except that they number anywhere from zero to sixty. Now consider the following pair of situations. In each situation a ball will be drawn and we will be offered a bet on its color. In situation A we will choose between betting that it is yellow or that it is red. In situation B we will choose between betting that it is red or blue or that it is yellow or blue.</p>\n</blockquote>\n<p>If we guess the correct color, we will receive a payout of $100. In the Ellsberg paradox, many people bet <em>yellow</em> in situation A and <em>red or blue</em> in situation B. Further, many people make these decisions not because they are indifferent in both situations, and so happy to choose either way, but rather because they have a strict preference to choose in this manner.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/tZKOsHx.jpg\" alt=\"The Ellsberg paradox\">\n<p class=\"caption\">The Ellsberg paradox</p>\n</div>\n<p>However, such behavior cannot be in accordance with EUM. In order for EUM to endorse a strict preference for choosing <em>yellow</em> in situation A, the agent would have to assign a probability of more than 1/3 to the ball selected being blue. On the other hand, in order for EUM to endorse a strict preference for choosing <em>red or blue</em> in situation B the agent would have to assign a probability of less than 1/3 to the selected ball being blue. As such, these decisions can\u2019t be jointly endorsed by an agent following EUM.</p>\n<p>Those who deny that decisions making under ignorance can be transformed into decision making under uncertainty have an easy response to the Ellsberg paradox: as this case involves deciding under a situation of ignorance, it is irrelevant whether people\u2019s decisions violate EUM in this case as EUM is not applicable to such situations.</p>\n<p>Those who believe that EUM provides a suitable standard for choice in such situations, however, need to find some other way of responding to the paradox. As with the Allais paradox, there is some disagreement about how best to do so. Once again, however, many people, including Leonard Savage, argue that EUM reaches the right decision in this case. It is our intuitions that are flawed (see again <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987)</a> for a nice summary of Savage\u2019s argument to this conclusion).</p>\n<p>&nbsp;</p>\n<h4 id=\"8_6_5__The_St_Petersburg_paradox\"><a href=\"#the-st-petersburg-paradox\">8.6.5. The St Petersburg paradox</a></h4>\n<p>Another objection to the VNM approach (and to expected utility approaches generally), the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg paradox</a>, draws on the possibility of infinite utilities. The St. Petersburg paradox is based around a game where a fair coin is tossed until it lands heads up. At this point, the agent receives a prize worth 2<sup>n</sup> utility, where <em>n</em> is equal to the number of times the coin was tossed during the game. The so-called paradox occurs because the expected utility of choosing to play this game is infinite and so, according to a standard expected utility approach, the agent should be willing to pay any finite amount to play the game. However, this seems unreasonable. Instead, it seems that the agent should only be willing to pay a relatively small amount to do so. As such, it seems that the expected utility approach gets something wrong.</p>\n<p>Various responses have been suggested. Most obviously, we could say that the paradox does not apply to VNM agents, since the VNM theorem assigns real numbers to all lotteries, and infinity is not a real number. But it's unclear whether this escapes the problem. After all, at it's core, the St. Petersburg paradox is not about infinite utilities but rather about cases where expected utility approaches seem to overvalue some choice, and such cases seem to exist even in finite cases. For example, if we let <em>L</em> be a finite limit on utility we could consider the following scenario (from <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson, 2009, p. 85</a>):</p>\n<blockquote>\n<p>A fair coin is tossed until it lands heads up. The player thereafter receives a prize worth min {2<sup>n</sup> \u00b7 10<sup>-100</sup>, L} units of utility, where <em>n</em> is the number of times the coin was tossed.</p>\n</blockquote>\n<p>In this case, even if an extremely low value is set for <em>L</em>, it seems that paying this amount to play the game is unreasonable. After all, as Peterson notes, about nine times out of ten an agent that plays this game will win no more than 8 \u00b7 10<sup>-100</sup> utility. If paying 1 utility is, in fact, unreasonable in this case, then simply limiting an agent's utility to some finite value doesn't provide a defence of expected utility approaches. (Other problems abound. See <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Yudkowsky, 2007</a> for an interesting finite problem and <a href=\"http://philrsss.anu.edu.au/people-defaults/alanh/papers/vexing_expectations.pdf\">Nover &amp; Hajek, 2004</a> for a particularly perplexing problem with links to the St Petersburg paradox.)</p>\n<p>As it stands, there is no agreement about precisely what the St Petersburg paradox reveals. Some people accept one of the various resolutions of the case and so find the paradox unconcerning. Others think the paradox reveals a serious problem for expected utility theories. Still others think the paradox is unresolved but don't think that we should respond by abandoning expected utility theory.</p>\n<p>&nbsp;</p>\n<h2 id=\"9__Does_axiomatic_decision_theory_offer_any_action_guidance_\"><a href=\"#does-axiomatic-decision-theory-offer-any-action-guidance\">9. Does axiomatic decision theory offer any action guidance?</a></h2>\n<p>For the decision theories listed in section 8.2, it's often claimed the answer is \"no.\" To explain this, I must first examine some differences between <em>direct</em> and <em>indirect</em> approaches to axiomatic decision theory.</p>\n<p><a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 4)</a> explains:</p>\n<blockquote>\n<p>In the indirect approach, which is the dominant approach, the decision maker does not prefer a risky act [or lottery] to another <em>because</em> the expected utility of the former exceeds that of the latter. Instead, the decision maker is asked to state a set of preferences over a set of risky acts... Then, if the set of preferences stated by the decision maker is consistent with a small number of structural constraints (axioms), it can be shown that her decisions can be described <em>as if</em> she were choosing what to do by assigning numerical probabilities and utilities to outcomes and then maximising expected utility...</p>\n</blockquote>\n<blockquote>\n<p>[In contrast] the direct approach seeks to generate preferences over acts from probabilities and utilities <em>directly</em> assigned to outcomes. In contrast to the indirect approach, it is not assumed that the decision maker has access to a set of preferences over acts before he starts to deliberate.</p>\n</blockquote>\n<p>The axiomatic decision theories listed in section 8.2 all follow the indirect approach. These theories, it might be said, cannot offer any action guidance because they require an agent to state its preferences over acts \"up front.\" But an agent that states its preferences over acts already knows which act it prefers, so the decision theory can't offer any action guidance not already present in the agent's own stated preferences over acts.</p>\n<p><a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch .10)</a> gives a practical example:</p>\n<blockquote>\n<p>For example, a forty-year-old woman seeking advice about whether to, say, divorce her husband, is likely to get very different answers from the [two approaches]. The [indirect approach] will advise the woman to first figure out what her preferences are over a very large set of risky acts, including the one she is thinking about performing, and then just make sure that all preferences are consistent with certain structural requirements. Then, as long as none of the structural requirements is violated, the woman is free to do whatever she likes, no matter what her beliefs and desires actually are... The [direct approach] will [instead] advise the woman to first assign numerical utilities and probabilities to her desires and beliefs, and then aggregate them into a decision by applying the principle of maximizing expected utility.</p>\n</blockquote>\n<p>Thus, it seems only the direct approach offers an agent any action guidance. But the direct approach is very recent (<a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson 2008</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Cozic-Review-of-Non-Bayesian-Decision-Theory.pdf\">Cozic 2011</a>), and only time will show whether it can stand up to professional criticism.</p>\n<p>Warning: Peterson's (<a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">2008</a>) direct approach is confusingly called \"non-Bayesian decision theory\" despite assuming Bayesian probability theory.</p>\n<p>For other attempts to pull action guidance from normative decision theory, see <a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Fallenstein (2012)</a> and <a href=\"/lw/gap/a_fungibility_theorem/\">Stiennon (2013)</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"10__How_does_probability_theory_play_a_role_in_decision_theory_\"><a href=\"#how-does-probability-theory-play-a-role-in-decision-theory\">10. How does probability theory play a role in decision theory?</a></h2>\n<p>In order to calculate the expected utility of an act (or lottery), it is necessary to determine a probability for each outcome. In this section, I will explore some of the details of probability theory and its relationship to decision theory.</p>\n<p>For further introductory material to probability theory, see <a href=\"http://www.amazon.com/Scientific-Reasoning-The-Bayesian-Approach/dp/081269578X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Howson &amp; Urbach (2005)</a>, <a href=\"http://www.amazon.com/Probability-Random-Processes-Geoffrey-Grimmett/dp/0198572220/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Grimmet &amp; Stirzacker (2001)</a>, and <a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Koller &amp; Friedman (2009)</a>. This section draws heavily on <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, chs. 6 &amp; 7)</a> which provides a very clear introduction to probability in the context of decision theory.</p>\n<p>&nbsp;</p>\n<h3 id=\"10_1__The_basics_of_probability_theory\"><a href=\"#the-basics-of-probability-theory\">10.1. The basics of probability theory</a></h3>\n<p>Intuitively, a probability is a number between 0 or 1 that labels how likely an event is to occur. If an event has probability 0 then it is impossible and if it has probability 1 then it can't possibly be false. If an event has a probability between these values, then this event it is more probable the higher this number is.</p>\n<p>As with EUM, probability theory can be derived from a small number of simple axioms. In the probability case, there are three of these, which are named the Kolmogorov axioms after the mathematician Andrey Kolmogorov. The first of these states that probabilities are real numbers between 0 and 1. The second, that if a set of events are mutually exclusive and exhaustive then their probabilities should sum to 1. The third that if two events are mutually exclusive then the probability that one or the other of these events will occur is equal to the sum of their individual probabilities.</p>\n<p>From these three axioms, the remainder of probability theory can be derived. In the remainder of this section, I will explore some aspects of this broader theory.</p>\n<p>&nbsp;</p>\n<h3 id=\"10_2__Bayes_theorem_for_updating_probabilities\"><a href=\"#bayes-theorem-for-updating-probabilities\">10.2. Bayes theorem for updating probabilities</a></h3>\n<p>From the perspective of decision theory, one particularly important aspect of probability theory is the idea of a conditional probability. These represent how probable something is given a piece of information. So, for example, a conditional probability could represent how likely it is that it will be raining, conditioning on the fact that the weather forecaster predicted rain. A powerful technique for calculating conditional probabilities is Bayes theorem (see <a href=\"http://yudkowsky.net/rational/bayes\">Yudkowsky, 2003</a> for a detailed introduction). This formula states that:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/lTKXA.gif\" alt=\"P(A|B)=(P(B|A)P(A))/P(B)\">\n<p class=\"caption\">P(A|B)=(P(B|A)P(A))/P(B)</p>\n</div>\n<p>Bayes theorem is used to calculate the probability of some event, A, given some evidence, B. As such, this formula can be used to <em>update</em> probabilities based on new evidence. So if you are trying to predict the probability that it will rain tomorrow and someone gives you the information that the weather forecaster predicted that it will do so then this formula tells you how to calculate a new probability that it will rain based on your existing information. The initial probability in such cases (before the information is factored into account) is called the <em>prior probability</em> and the result of applying Bayes theorem is a new, <em>posterior probability</em>.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/vM0yW.jpg\" alt=\"Using Bayes theorem to update probabilities based on the evidence provided by a weather forecast\">\n<p class=\"caption\">Using Bayes theorem to update probabilities based on the evidence provided by a weather forecast</p>\n</div>\n<p>Bayes theorem can be seen as solving the problem of how to update prior probabilities based on new information. However, it leaves open the question of how to determine the prior probability in the first place. In some cases, there will be no obvious way to do so. One solution to this problem suggests that any reasonable prior can be selected. Given enough evidence, repeated applications of Bayes theorem will lead this prior probability to be updated to much the same posterior probability, even for people with widely different initial priors. As such, the initially selected prior is less crucial than it may at first seem.</p>\n<p>&nbsp;</p>\n<h3 id=\"10_3__How_should_probabilities_be_interpreted_\"><a href=\"#how-should-probabilities-be-interpreted\">10.3. How should probabilities be interpreted?</a></h3>\n<p>There are two main views about what probabilities mean: objectivism and subjectivism. Loosely speaking, the objectivist holds that probabilities tell us something about the external world while the subjectivist holds that they tell us something about our beliefs. Most decision theorists hold a subjectivist view about probability. According to this sort of view, probabilities represent a subjective degrees of belief. So to say the probability of rain is 0.8 is to say that the agent under consideration has a high degree of belief that it will rain (see <a href=\"http://www.amazon.com/Probability-Theory-The-Logic-Science/dp/0521592712/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Jaynes, 2003</a> for a defense of this view). Note that, according to this view, another agent in the same circumstance could assign a different probability that it will rain.</p>\n<p>&nbsp;</p>\n<h4 id=\"10_3_1__Why_should_degrees_of_belief_follow_the_laws_of_probability_\"><a href=\"#why-should-degrees-of-belief-following-the-laws-of-probability\">10.3.1. Why should degrees of belief follow the laws of probability?</a></h4>\n<p>One question that might be raised against the subjective account of probability is why, on this account, our degrees of belief should satisfy the Kolmogorov axioms. For example, why should our subjective degrees of belief in mutually exclusive, exhaustive events add to 1? One answer to this question shows that agents whose degrees of belief don\u2019t satisfy these axioms will be subject to Dutch Book bets. These are bets where the agent will inevitably lose money. <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 7)</a> explains:</p>\n<blockquote>\n<p>Suppose, for instance, that you believe to degree 0.55 that at least one person from India will win a gold medal in the next Olympic Games (event G), and that your subjective degree of belief is 0.52 that no Indian will win a gold medal in the next Olympic Games (event \u00acG). Also suppose that a cunning bookie offers you a bet on both of these events. The bookie promises to pay you $1 for each event that actually takes place. Now, since your subjective degree of belief that G will occur is 0.55 it would be rational to pay up to $1\u00b70.55 = $0.55 for entering this bet. Furthermore, since your degree of belief in \u00acG is 0.52 you should be willing to pay up to $0.52 for entering the second bet, since $1\u00b70.52 = $0.52. However, by now you have paid $1.07 for taking on two bets that are certain to give you a payoff of $1 <em>no matter what happens</em>...Certainly, this must be irrational. Furthermore, the reason why this is irrational is that your subjective degrees of belief violate the probability calculus.</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/9xoLg.jpg\" alt=\"A Dutch Book argument\">\n<p class=\"caption\">A Dutch Book argument</p>\n</div>\n<p>It can be proven that an agent is subject to Dutch Book bets if, and only if, their degrees of belief violate the axioms of probability. This provides an argument for why degrees of beliefs should satisfy these axioms.</p>\n<p>&nbsp;</p>\n<h4 id=\"10_3_2__Measuring_subjective_probabilities\"><a href=\"#measuring-subjective-probabilities\">10.3.2. Measuring subjective probabilities</a></h4>\n<p>Another challenges raised by the subjective view is how we can measure probabilities. If these represent subjective degrees of belief there doesn\u2019t seem to be an easy way to determine these based on observations of the world. However, a number of responses to this problem have been advanced, one of which is explained succinctly by <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Peterson (2009, ch. 7)</a>:</p>\n<blockquote>\n<p>The main innovations presented by... Savage can be characterised as systematic procedures for linking probability... to claims about objectively observable behavior, such as preference revealed in choice behavior. Imagine, for instance, that we wish to measure Caroline's subjective probability that the coin she is holding in her hand will land heads up the next time it is tossed. First, we ask her which of the following very generous options she would prefer.</p>\n</blockquote>\n<blockquote>\n<p>A: \"If the coin lands heads up you win a sports car; otherwise you win nothing.\"</p>\n</blockquote>\n<blockquote>\n<p>B: \"If the coin <em>does not</em> land heads up you win a sports car; otherwise you win nothing.\"</p>\n</blockquote>\n<blockquote>\n<p>Suppose Caroline prefers A to B. We can then safely conclude that she thinks it is <em>more probable</em> that the coin will land heads up rather than not. This follows from the assumption that Caroline prefers to win a sports car rather than nothing, and that her preference between uncertain prospects is entirely determined by her beliefs and desires with respect to her prospects of winning the sports car...</p>\n</blockquote>\n<blockquote>\n<p>Next, we need to generalise the measurement procedure outlined above such that it allows us to always represent Caroline's degrees of belief with precise numerical probabilities. To do this, we need to ask Caroline to state preferences over a <em>much larger</em> set of options and then <em>reason backwards</em>... Suppose, for instance, that Caroline wishes to measure her subjective probability that her car worth $20,000 will be stolen within one year. If she considers $1,000 to be... the highest price she is prepared to pay for a gamble in which she gets $20,000 if the event S: \"The car stolen within a year\" takes place, and nothing otherwise, then Caroline's subjective probability for S is 1,000/20,000 = 0.05, given that she forms her preferences in accordance with the principle of maximising expected monetary value...</p>\n</blockquote>\n<blockquote>\n<p>The problem with this method is that very few people form their preferences in accordance with the principle of maximising expected monetary value. Most people have a decreasing marginal utility for money...</p>\n</blockquote>\n<blockquote>\n<p>Fortunately, there is a clever solution to [this problem]. The basic idea is to impose a number of structural conditions on preferences over uncertain options [e.g. the transitivity axiom]. Then, the subjective probability function is established by reasoning backwards while taking the structural axioms into account: Since the decision maker preferrred some uncertain options to others, and her preferences... satisfy a number of structure axioms, the decision maker behaves <em>as if</em> she were forming her preferences over uncertain options by first assigning subjective probabilities and utilities to each option and thereafter maximising expected utility.</p>\n</blockquote>\n<blockquote>\n<p>A peculiar feature of this approach is, thus, that probabilities (and utilities) are derived from 'within' the theory. The decision maker does not prefer an uncertain option to another <em>because</em> she judges the subjective probabilities (and utilities) of the outcomes to be more favourable than those of another. Instead, the... structure of the decision maker's preferences over uncertain options logically implies that they can be described <em>as if</em> her choices were governed by a subjective probability function and a utility function...</p>\n</blockquote>\n<blockquote>\n<p>...Savage's approach [seeks] to explicate subjective interpretations of the probability axioms by making certain claims about preferences over... uncertain options. But... why on earth should a theory of subjective probability involve assumptions about preferences, given that preferences and beliefs are separate entities? Contrary to what is claimed by [Savage and others], emotionally inert decision makers failing to muster any preferences at all... could certainly hold partial beliefs.</p>\n</blockquote>\n<p>Other theorists, for example <a href=\"http://www.amazon.com/Optimal-Statistical-Decisions-Classics-Library/dp/047168029X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">DeGroot (1970)</a>, propose other approaches:</p>\n<blockquote>\n<p>DeGroot's basic assumption is that decision makers can make <em>qualitative</em> comparisons between pairs of events, and judge which one they think is most likely to occur. For example, he assumes that one can judge whether it is <em>more</em>, <em>less</em>, or <em>equally</em> likely, according to one's own beliefs, that it will rain today in Cambridge than in Cairo. DeGroot then shows that if the agent's qualitative judgments are sufficiently fine-grained and satisfy a number of structural axioms, then [they can be described by a probability distribution]. So in DeGroot's... theory, the probability function is obtained by fine-tuning qualitative data, thereby making them quantitative.</p>\n</blockquote>\n<h2 id=\"what-about-newcombs-problem-and-alternative-decision-algorithms\"><br></h2>\n<h2 id=\"11__What_about__Newcomb_s_problem__and_alternative_decision_algorithms_\"><a href=\"#what-about-newcombs-problem-and-alternative-decision-algorithms\">11. What about \"Newcomb's problem\" and alternative decision algorithms?</a></h2>\n<p>Saying that a rational agent \"maximizes expected utility\" is, unfortunately, not specific enough. There are a variety of decision algorithms which aim to maximize expected utility, and they give <em>different answers</em> to some decision problems, for example \"Newcomb's problem.\"</p>\n<p>In this section, we explain these decision algorithms and show how they perform on Newcomb's problem and related \"Newcomblike\" problems.</p>\n<p>General sources on this topic include: <a href=\"http://www.amazon.com/Paradoxes-Rationality-Cooperation-Prisoners-Newcombs/dp/0774802154/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Campbell &amp; Sowden (1985)</a>, <a href=\"http://kops.ub.uni-konstanz.de/bitstream/handle/urn:nbn:de:bsz:352-opus-5241/ledwig.pdf?sequence=1\">Ledwig (2000)</a>, <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521063566/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999)</a>, and <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>. <a href=\"http://www.operalgo.com/PDF/Moertelmaier_Newcomblike_2013.pdf\">Moertelmaier (2013)</a> discusses Newcomblike problems in the context of the agent-environment framework.</p>\n<p>&nbsp;</p>\n<h3 id=\"11_1__Newcomblike_problems_and_two_decision_algorithms\"><a href=\"#newcomblike-problems-and-two-decision-algorithms\">11.1. Newcomblike problems and two decision algorithms</a></h3>\n<p>I'll begin with an exposition of several Newcomblike problems, so that I can refer to them in later sections. I'll also introduce our first two decision algorithms, so that I can show how one's choice of decision algorithm affects an agent's outcomes on these problems.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_1__Newcomb_s_Problem\"><a href=\"#newcombs-problem\">11.1.1. Newcomb's Problem</a></h4>\n<p>Newcomb's problem was formulated by the physicist <a href=\"http://en.wikipedia.org/wiki/William_Newcomb\">William Newcomb</a> but first published in <a href=\"http://faculty.arts.ubc.ca/rjohns/nozick_newcomb.pdf\">Nozick (1969)</a>. Below I present a version of it inspired by <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>.</p>\n<p>A superintelligent machine named Omega visits Earth from another galaxy and shows itself to be very good at predicting events. This isn't because it has magical powers, but because it knows more science than we do, has billions of sensors scattered around the globe, and runs efficient algorithms for modeling humans and other complex systems with unprecedented precision \u2014 on an array of computer hardware the size of our moon.</p>\n<p>Omega presents you with two boxes. Box A is transparent and contains $1000. Box B is opaque and contains either $1 million or nothing. You may choose to take both boxes (called \"two-boxing\"), or you may choose to take only box B (called \"one-boxing\"). If Omega predicted you'll two-box, then Omega has left box B empty. If Omega predicted you'll one-box, then Omega has placed $1M in box B.</p>\n<p>By the time you choose, Omega has already left for its next game \u2014 the contents of box B won't change after you make your decision. Moreover, you've watched Omega play a thousand games against people like you, and on every occasion Omega predicted the human player's choice accurately.</p>\n<p>Should you one-box or two-box?</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/4MFhs.jpg\" alt=\"Newcomb\u2019s problem\">\n<p class=\"caption\">Newcomb\u2019s problem</p>\n</div>\n<p>Here's an argument for two-boxing. The $1M either <em>is</em> or <em>is not</em> in the box; your choice cannot affect the contents of box B now. So, you should two-box, because then you get $1K plus whatever is in box B. This is a straightforward application of the dominance principle (section 6.1). Two-boxing dominantes one-boxing.</p>\n<p>Convinced? Well, here's an argument for one-boxing. On all those earlier games you watched, everyone who two-boxed received $1K, and everyone who one-boxed received $1M. So you're almost certain that you'll get $1K for two-boxing and $1M for one-boxing, which means that to maximize your expected utility, you should one-box.</p>\n<p><a href=\"http://faculty.arts.ubc.ca/rjohns/nozick_newcomb.pdf\">Nozick (1969)</a> reports:</p>\n<blockquote>\n<p>I have put this problem to a large number of people... To almost everyone it is perfectly clear and obvious what should be done. The difficulty is that these people seem to divide almost evenly on the problem, with large numbers thinking that the opposing half is just being silly.</p>\n</blockquote>\n<p>This is not a \"merely verbal\" dispute (<a href=\"http://philreview.dukejournals.org/content/120/4/515.short\">Chalmers 2011</a>). Decision theorists have offered different <em>algorithms</em> for making a choice, and they have different outcomes. Translated into English, the first algorithm (<em>evidential decision theory</em> or EDT) says \"Take actions such that you would be glad to receive the news that you had taken them.\" The second algorithm (<em>causal decision theory</em> or CDT) says \"Take actions which you expect to have a positive effect on the world.\"</p>\n<p>Many decision theorists have the intuition that CDT is right. But a CDT agent appears to \"lose\" on Newcomb's problem, ending up with $1000, while an EDT agent gains $1M. Proponents of EDT can ask proponents of CDT: \"If you're so smart, why aren't you rich?\" As <a href=\"http://www-ihpst.univ-paris1.fr/fichiers/programmes/20/Spohn-One-Boxing3.pdf\">Spohn (2012)</a> writes, \"this must be poor rationality that complains about the reward for irrationality.\" Or as <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a> argues:</p>\n<blockquote>\n<p>An expected utility maximizer should maximize <em>utility</em> \u2014 not formality, reasonableness, or defensibility...</p>\n</blockquote>\n<p>In response to EDT's apparent \"win\" over CDT on Newcomb's problem, proponents of CDT have presented similar problems on which a CDT agent \"wins\" and an EDT agent \"loses.\" Proponents of EDT, meanwhile, have replied with additional Newcomblike problems on which EDT wins and CDT loses. Let's explore each of them in turn.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_2__Evidential_and_causal_decision_theory\"><a href=\"#evidential-and-causal-decision-theory\">11.1.2. Evidential and causal decision theory</a></h4>\n<p>First, however, we will consider our two decision algorithms in a little more detail.</p>\n<p>EDT can be described simply: according to this theory, agents should use conditional probabilities when determining the expected utility of different acts. Specifically, they should use the probability of the world being in each possible state conditioning on them carrying out the act under consideration. So in Newcomb\u2019s problem they consider the probability that Box B contains $1 million or nothing conditioning on the evidence provided by their decision to one-box or two-box. This is how the theory formalizes the notion of an act providing good news.</p>\n<p>CDT is more complex, at least in part because it has been formulated in a variety of different ways and these formulations are equivalent to one another only if certain background assumptions are met. However, a good sense of the theory can be gained by considering the counterfactual approach, which is one of the more intuitive of these formulations. This approach utilizes the probabilities of certain counterfactual conditionals, which can be thought of as representing the causal influence of an agent\u2019s acts on the state of the world. These conditionals take the form \u201cif I were to carry out a certain act, then the world would be in a certain state.\" So in Newcomb\u2019s problem, for example, this formulation of CDT considers the probability of the counterfactuals like \u201cif I were to one-box, then Box B would contain $1 million\u201d and, in doing so, considers the causal influence of one-boxing on the contents of the boxes.</p>\n<p>The same distinction can be made in formulaic terms. Both EDT and CDT agree that decision theory should be about maximizing expected utility where the expected utility of an act, A, given a set of possible outcomes, O, is defined as follows:</p>\n<p><img src=\"http://i.imgur.com/CSwK4.gif\" alt=\"expected utility formula\">.</p>\n<p>In this equation, V(A &amp; O) represents the value to the agent of the combination of an act and an outcome. So this is the utility that the agent will receive if they carry out a certain act and a certain outcome occurs. Further, Pr<sub>A</sub>O represents the probability of each outcome occurring on the supposition that the agent carries out a certain act. It is in terms of this probability that CDT and EDT differ. EDT uses the conditional probability, Pr(O|A), while CDT uses the probability of subjunctive conditionals, Pr(A <img src=\"http://i.imgur.com/G8xec.gif\" alt=\"\"> O).</p>\n<p>Using these two versions of the expected utility formula, it's possible to demonstrate in a formal manner why EDT and CDT give the advice they do in Newcomb's problem. To demonstrate this it will help to make two simplifying assumptions. First, we will presume that each dollar of money is worth 1 unit of utility to the agent (and so will presume that the agent's utility is linear with money). Second, we will presume that Omega is a perfect predictor of human actions so that if the agent two-boxes it provides definitive evidence that there is nothing in the opaque box and if the agent one-boxes it provides definitive evidence that there is $1 million in this box. Given these assumptions, EDT calculates the expected utility of each decision as follows:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/aCe4Y.gif\" alt=\"EU for two-boxing according to EDT\">\n<p class=\"caption\">EU for two-boxing according to EDT</p>\n</div>\n<div class=\"figure\"><img src=\"http://i.imgur.com/vJtVr.gif\" alt=\"EU for one-boxing according to EDT\">\n<p class=\"caption\">EU for one-boxing according to EDT</p>\n</div>\n<p>Given that one-boxing has a higher expected utility according to these calculations, an EDT agent will one-box.</p>\n<p>On the other hand, given that the agent's decision doesn't causally influence Omega's earlier prediction, CDT will use the same probability regardless of whether you one or two box. The decision endorsed will be the same regardless of what probability we use so, to demonstrate the theory, we can simply arbitrarily assign an 0.5 probability that the opaque box has nothing in it and an 0.5 probability that it has one million dollars in it. CDT then calculates the expected utility of each decision as follows:</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/oyHGl.gif\" alt=\"EU for two-boxing according to CDT\">\n<p class=\"caption\">EU for two-boxing according to CDT</p>\n</div>\n<div class=\"figure\"><img src=\"http://i.imgur.com/7uX9t.gif\" alt=\"EU for one-boxing according to CDT\">\n<p class=\"caption\">EU for one-boxing according to CDT</p>\n</div>\n<p>Given that two-boxing has a higher expected utility according to these calculations, a CDT agent will two-box. This approach demonstrates the result given more informally in the previous section: CDT agents will two-box in Newcomb's problem and EDT agents will one box.</p>\n<p>As mentioned before, there are also alternative formulations of CDT. What are these? For example, David Lewis <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">(1981)</a> and Brian Skyrms <a href=\"http://www.amazon.com/Causal-Necessity-Pragmatic-Investigation-Laws/dp/0300023391/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(1980)</a> both present approaches that rely on the partition of the world into states to capture causal information, rather than counterfactual conditionals. On Lewis\u2019s version of this account, for example, the agent calculates the expected utility of acts using their unconditional credence in states of the world that are <em>dependency hypotheses</em>, which are descriptions of the possible ways that the world can depend on the agent\u2019s actions. These dependency hypotheses intrinsically contain the required causal information.</p>\n<p>Other traditional approaches to CDT include the imaging approach of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Sobel-Probability-Chance-and-Choice-a-Theory-of-Rational-Agency.pdf\">Sobel (1980)</a> (also see <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">Lewis 1981</a>) and the unconditional expectations approach of Leonard Savage <a href=\"http://www.amazon.com/Foundations-Statistics-Leonard-J-Savage/dp/0486623491/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(1954)</a>. Those interested in the various traditional approaches to CDT would be best to consult Lewis <a href=\"http://www.tandfonline.com/doi/abs/10.1080/00048408112340011\">(1981)</a>, <a href=\"http://plato.stanford.edu/entries/decision-causal/\">Weirich (2008)</a>, and <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521063566/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999)</a>. More recently, work in computer science on a tool called causal Bayesian networks has led to an innovative approach to CDT that has received some recent attention in the philosophical literature (<a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Pearl 2000, ch. 4</a> and <a href=\"http://www-ihpst.univ-paris1.fr/fichiers/programmes/20/Spohn-One-Boxing3.pdf\">Spohn 2012</a>).</p>\n<p>Now we return to an analysis of decision scenarios, armed with EDT and the counterfactual formulation of CDT.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_3__Medical_Newcomb_problems\"><a href=\"#medical-newcomb-problems\">11.1.3. Medical Newcomb problems</a></h4>\n<p>Medical Newcomb problems share a similar form but come in many variants, including Solomon's problem (<a href=\"https://www.kellogg.northwestern.edu/research/math/papers/194.pdf\">Gibbard &amp; Harper 1976</a>) and the smoking lesion problem (<a href=\"http://fitelson.org/few/few_05/egan.pdf\">Egan 2007</a>). Below I present a variant called the \"chewing gum problem\" (<a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky 2010</a>):</p>\n<blockquote>\n<p>Suppose that a recently published medical study shows that chewing gum seems to cause throat abscesses \u2014 an outcome-tracking study showed that of people who chew gum, 90% died of throat abscesses before the age of 50. Meanwhile, of people who do not chew gum, only 10% die of throat abscesses before the age of 50. The researchers, to explain their results, wonder if saliva sliding down the throat wears away cellular defenses against bacteria. Having read this study, would you choose to chew gum? But now a second study comes out, which shows that most gum-chewers have a certain gene, CGTA, and the researchers produce a table showing the following mortality rates:</p>\n</blockquote>\n<blockquote>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>CGTA present</td>\n<td>CGTA absent</td>\n</tr>\n<tr>\n<td>Chew Gum</td>\n<td>89% die</td>\n<td>8% die</td>\n</tr>\n<tr>\n<td>Don\u2019t chew</td>\n<td>99% die</td>\n<td>11% die</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<blockquote>\n<p>This table shows that whether you have the gene CGTA or not, your chance of dying of a throat abscess goes down if you chew gum. Why are fatalities so much higher for gum-chewers, then? Because people with the gene CGTA tend to chew gum and die of throat abscesses. The authors of the second study also present a test-tube experiment which shows that the saliva from chewing gum can kill the bacteria that form throat abscesses. The researchers hypothesize that because people with the gene CGTA are highly susceptible to throat abscesses, natural selection has produced in them a tendency to chew gum, which protects against throat abscesses. The strong correlation between chewing gum and throat abscesses is not because chewing gum causes throat abscesses, but because a third factor, CGTA, leads to chewing gum and throat abscesses.</p>\n</blockquote>\n<blockquote>\n<p>Having learned of this new study, would you choose to chew gum? Chewing gum helps protect against throat abscesses whether or not you have the gene CGTA. Yet a friend who heard that you had decided to chew gum (as people with the gene CGTA often do) would be quite alarmed to hear the news \u2014 just as she would be saddened by the news that you had chosen to take both boxes in Newcomb\u2019s Problem. This is a case where [EDT] seems to return the wrong answer, calling into question the validity of the... rule \u201cTake actions such that you would be glad to receive the news that you had taken them.\u201d Although the news that someone has decided to chew gum is alarming, medical studies nonetheless show that chewing gum protects against throat abscesses. [CDT's] rule of \u201cTake actions which you expect to have a positive physical effect on the world\u201d seems to serve us better.</p>\n</blockquote>\n<p>One response to this claim, called the <em>tickle defense</em> (<a href=\"http://www.jstor.org/discover/10.2307/20115662?uid=3737536&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;sid=21101205363271\">Eells, 1981</a>), argues that EDT actually reaches the right decision in such cases. According to this defense, the most reasonable way to construe the \u201cchewing gum problem\u201d involves presuming that CGTA causes a desire (a mental \u201ctickle\u201d) which then causes the agent to be more likely to chew gum, rather than CGTA directly causing the action. Given this, if we presume that the agent already knows their own desires and hence already knows whether they\u2019re likely to have the CGTA gene, chewing gum will not provide the agent with further bad news. Consequently, an agent following EDT will chew in order to get the good news that they have decreased their chance of getting abscesses.</p>\n<p>Unfortunately, the tickle defense fails to achieve its aims. In introducing this approach, Eells hoped that EDT could be made to mimic CDT but without an allegedly inelegant reliance on causation. However, <a href=\"http://www.amazon.com/Taking-Chances-Cambridge-Probability-Induction/dp/0521038987/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Sobel (1994, ch. 2)</a> demonstrated that the tickle defense failed to ensure that EDT and CDT would decide equivalently in all cases. On the other hand, those who feel that EDT originally got it right by one-boxing in Newcomb\u2019s problem will be disappointed to discover that the tickle defense leads an agent to two-box in some versions of Newcomb\u2019s problem and so solves one problem for the theory at the expense of introducing another.</p>\n<p>So just as CDT \u201closes\u201d on Newcomb\u2019s problem, EDT will \"lose\u201d on Medical Newcomb problems (if the tickle defense fails) or will join CDT and \"lose\" on Newcomb\u2019s Problem itself (if the tickle defense succeeds).</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_4__Newcomb_s_soda\"><a href=\"#newcombs-soda\">11.1.4. Newcomb's soda</a></h4>\n<p>There are also similar problematic cases for EDT where the evidence provided by your decision relates not to a feature that you were born (or created) with but to some other feature of the world. One such scenario is the <em>Newcomb\u2019s soda</em> problem, introduced in <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a>:</p>\n<blockquote>\n<p>You know that you will shortly be administered one of two sodas in a double-blind clinical test. After drinking your assigned soda, you will enter a room in which you find a chocolate ice cream and a vanilla ice cream. The first soda produces a strong but entirely subconscious desire for chocolate ice cream, and the second soda produces a strong subconscious desire for vanilla ice cream. By \u201csubconscious\u201d I mean that you have no introspective access to the change, any more than you can answer questions about individual neurons firing in your cerebral cortex. You can only infer your changed tastes by observing which kind of ice cream you pick.</p>\n</blockquote>\n<blockquote>\n<p>It so happens that all participants in the study who test the Chocolate Soda are rewarded with a million dollars after the study is over, while participants in the study who test the Vanilla Soda receive nothing. But subjects who actually eat vanilla ice cream receive an additional thousand dollars, while subjects who actually eat chocolate ice cream receive no additional payment. You can choose one and only one ice cream to eat. A pseudo-random algorithm assigns sodas to experimental subjects, who are evenly divided (50/50) between Chocolate and Vanilla Sodas. You are told that 90% of previous research subjects who chose chocolate ice cream did in fact drink the Chocolate Soda, while 90% of previous research subjects who chose vanilla ice cream did in fact drink the Vanilla Soda. Which ice cream would you eat?</p>\n</blockquote>\n<div class=\"figure\"><img src=\"http://i.imgur.com/FAZnb.jpg\" alt=\"Newcomb\u2019s soda\">\n<p class=\"caption\">Newcomb\u2019s soda</p>\n</div>\n<p>In this case, an EDT agent will decide to eat chocolate ice cream as this would provide evidence that they drank the chocolate soda and hence that they will receive $1 million after the experiment. However, this seems to be the wrong decision and so, once again, the EDT agent \u201closes\u201d.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_5__Bostrom_s_meta_Newcomb_problem\"><a href=\"#bostroms-meta-newcomb-problem\">11.1.5. Bostrom's meta-Newcomb problem</a></h4>\n<p>In response to attacks on their theory, the proponent of EDT can present alternative scenarios where EDT \u201cwins\u201d and it is CDT that \u201closes\u201d. One such case is the <em>meta-Newcomb problem</em> proposed in <a href=\"http://www.nickbostrom.com/papers/newcomb.html\">Bostrom (2001)</a>. Adapted to fit my earlier story about Omega the superintelligent machine (section 11.1.1), the problem runs like this: Either Omega has <em>already</em> placed $1M or nothing in box B (depending on its prediction about your choice), or else Omega is watching as you choose and <em>after</em> your choice it will place $1M into box B only if you have one-boxed. But you don't know which is the case. Omega makes its move before the human player's choice about half the time, and the rest of the time it makes its move <em>after</em> the player's choice.</p>\n<p>But now suppose there is another superintelligent machine, Meta-Omega, who has a perfect track record of predicting both Omega's choices and the choices of human players. Meta-Omega tells you that either you will two-box and Omega will \"make its move\" <em>after</em> you make your choice, or else you will one-box and Omega has <em>already</em> made its move (and gone on to the next game, with someone else).</p>\n<p>Here, an EDT agent one-boxes and walks away with a million dollars. On the face of it, however, a CDT agent faces a dilemma: if she two-boxes then Omega's action depends on her choice, so the \"rational\" choice is to one-box. But if the CDT agent one-boxes, then Omega's action temporally precedes (and is thus physically independent of) her choice, so the \"rational\" action is to two-box. It might seem, then, that a CDT agent will be unable to reach any decision in this scenario. However, further reflection reveals that the issue is more complicated. According to CDT, what the agent ought to do in this scenario depends on their credences about their own actions. If they have a high credence that they will two-box, they ought to one-box and if they have a high credence that they will one-box, they ought to two box. Given that the agent's credences in their actions are not given to us in the description of the meta-Newcomb problem, the scenario is underspecified and it is hard to know what conclusions should be drawn from it.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_6__The_psychopath_button\"><a href=\"#the-psychopath-button\">11.1.6. The psychopath button</a></h4>\n<p>Fortunately, another case has been introduced where, according to CDT, what an agent ought to do depends on their credences about what they will do. This is the <em>psychopath button</em>, introduced in <a href=\"http://philreview.dukejournals.org/content/116/1/93.citation\">Egan (2007)</a>:</p>\n<blockquote>\n<p>Paul is debating whether to press the \u201ckill all psychopaths\u201d button. It would, he thinks, be much better to live in a world with no psychopaths. Unfortunately, Paul is quite confident that only a psychopath would press such a button. Paul very strongly prefers living in a world with psychopaths to dying. Should Paul press the button?</p>\n</blockquote>\n<p>Many people think Paul should not. After all, if he does so, he is almost certainly a psychopath and so pressing the button will almost certainly cause his death. This is also the response that an EDT agent will give. After all, pushing the button would provide the agent with the bad news that they are almost certainly a psychopath and so will die as a result of their action.</p>\n<p>On the other hand, if Paul is fairly certain that he is not a psychopath, then CDT will say that he ought to press the button. CDT will note that, given Paul\u2019s confidence that he isn\u2019t a psychopath, his decision will almost certainly have a positive impact as it will result in the death of all psychopaths and Paul\u2019s survival. On the face of it, then, a CDT agent would decide inappropriately in this case by pushing the button. Importantly, unlike in the meta-Newcomb problem, the agent's credences about their own behavior are specified in Egan's full version of this scenario (in non-numeric terms, the agent thinks they're unlikely to be a psychopath and hence unlikely to press the button).</p>\n<p>However, in order to produce this problem for CDT, Egan made a number of assumptions about how an agent should decide when what they ought to do depends on what they think they will do. In response, alternative views about deciding in such cases have been advanced (particular in <a href=\"http://www.jstor.org/discover/10.2307/40267481?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=21101299066461\">Arntzenius, 2008</a> and <a href=\"http://rd.springer.com/article/10.1007/s11229-011-0022-6\">Joyce, 2012</a>). Given these factors, opinions are split about whether the psychopath button problem does in fact pose a challenge to CDT.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_7__Parfit_s_hitchhiker\"><a href=\"#parfits-hitchhiker\">11.1.7. Parfit's hitchhiker</a></h4>\n<p>Not all decision scenarios are problematic for just one of EDT or CDT. There are also cases that can be presented where both an EDT agent and a CDT agent will both \"lose\". One such case is <em>Parfit\u2019s Hitchhiker</em> (<a href=\"http://www.amazon.com/Reasons-Persons-Oxford-Paperbacks-Parfit/dp/019824908X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Parfit, 1984, p. 7</a>):</p>\n<blockquote>\n<p>Suppose that I am driving at midnight through some desert. My car breaks down. You are a stranger, and the only other driver near. I manage to stop you, and I offer you a great reward if you rescue me. I cannot reward you now, but I promise to do so when we reach my home. Suppose next that I am <em>transparent</em>, unable to deceive others. I cannot lie convincingly. Either a blush, or my tone of voice, always gives me away. Suppose, finally, that I know myself to be never self-denying. If you drive me to my home, it would be worse for me if I gave you the promised reward. Since I know that I never do what will be worse for me, I know that I shall break my promise. Given my inability to lie convincingly, you know this too. You do not believe my promise, and therefore leave me stranded in the desert.</p>\n</blockquote>\n<p>In this scenario the agent \"loses\" if they would later refuse to give the stranger the reward. However, both EDT agents and CDT agents will refuse to do so. After all, by this point the agent will already be safe so giving the reward can neither provide good news about, nor cause, their safety. So this seems to be a case where both theories \u201close\u201d.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_8__Transparent_Newcomb_s_problem\"><a href=\"#transparent-newcombs-problem\">11.1.8. Transparent Newcomb's problem</a></h4>\n<p>There are also other cases where both EDT and CDT \"lose\". One of these is the <em>Transparent Newcomb's problem</em> which, in at least one version, is due to <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Drescher (2006, p. 238-242)</a>. This scenario is like the original Newcomb's problem but, in this case, both boxes are transparent so you can see their contents when you make your decision. Again, Omega has filled box A with $1000 and Box B with either $1 million or nothing based on a prediction of your behavior. Specifically, Omega has predicted how you would decide if you witnessed $1 million in Box B. If Omega predicted that you would one-box in this case, he placed $1 million in Box B. On the other hand, if Omega predicted that you would two-box in this case then he placed nothing in Box B.</p>\n<p>Both EDT and CDT agents will two-box in this case. After all, the contents of the boxes are determined and known so the agent's decision can neither provide good news about what they contain nor cause them to contain something desirable. As with two-boxing in the original version of Newcomb\u2019s problem, many philosophers will endorse this behavior.</p>\n<p>However, it\u2019s worth noting that Omega will almost certainly have predicted this decision and so filled Box B with nothing. CDT and EDT agents will end up with $1000. On the other hand, just as in the original case, the agent that one-boxes will end up with $1 million. So this is another case where both EDT and CDT \u201close\u201d. Consequently, to those that agree with the earlier comments (in section 11.1.1) that a decision theory shouldn't lead an agent to \"lose\", neither of these theories will be satisfactory.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_9__Counterfactual_mugging\"><a href=\"#counterfactual-mugging\">11.1.9. Counterfactual mugging</a></h4>\n<p>Another similar case, known as <em>counterfactual mugging</em>, was developed in <a href=\"/lw/3l/counterfactual_mugging/\">Nesov (2009)</a>:</p>\n<blockquote>\n<p>Imagine that one day, Omega comes to you and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But see, the Omega tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n</blockquote>\n<p>Should you give up the $100?</p>\n<p>Both CDT and EDT say no. After all, giving up your money neither provides good news about nor influences your chances of getting $10 000 out of the exchange. Further, this intuitively seems like the right decision. On the face of it, then, it is appropriate to retain your money in this case.</p>\n<p>However, presuming you take Omega to be perfectly trustworthy, there seems to be room to debate this conclusion. If you are the sort of agent that gives up the $100 in counterfactual mugging then you will tend to do better than the sort of agent that won\u2019t give up the $100. Of course, in the particular case at hand you will lose but rational agents often lose in specific cases (as, for example, when such an agent loses a rational bet). It could be argued that what a rational agent should not do is be the type of agent that loses. Given that agents that refuse to give up the $100 are the type of agent that loses, there seem to be grounds to claim that counterfactual mugging is another case where both CDT and EDT act inappropriately.</p>\n<p>&nbsp;</p>\n<h4 id=\"11_1_10__Prisoner_s_dilemma\"><a href=\"#prisoners-dilemma\">11.1.10. Prisoner's dilemma</a></h4>\n<p>Before moving on to a more detailed discussion of various possible decision theories, I\u2019ll consider one final scenario: the <em>prisoner\u2019s dilemma</em>. <a href=\"http://www.amazon.com/Choices-An-Introduction-Decision-Theory/dp/0816614407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Resnik (1987, pp. 147-148 )</a> outlines this scenario as follows:</p>\n<blockquote>\n<p>Two prisoners...have been arrested for vandalism and have been isolated from each other. There is sufficient evidence to convict them on the charge for which they have been arrested, but the prosecutor is after bigger game. He thinks that they robbed a bank together and that he can get them to confess to it. He summons each separately to an interrogation room and speaks to each as follows: \"I am going to offer the same deal to your partner, and I will give you each an hour to think it over before I call you back. This is it: If one of you confesses to the bank robbery and the other does not, I will see to it that the confessor gets a one-year term and that the other guy gets a twenty-five year term. If you both confess, then it's ten years apiece. If neither of you confesses, then I can only get two years apiece on the vandalism charge...\"</p>\n</blockquote>\n<p>The decision matrix of each vandal will be as follows:</p>\n<table border=\"0\" cellspacing=\"5\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><em>Partner confesses</em></td>\n<td><em>Partner lies</em></td>\n</tr>\n<tr>\n<td><em>Confess</em></td>\n<td>10 years in jail</td>\n<td>1 year in jail</td>\n</tr>\n<tr>\n<td><em>Lie</em></td>\n<td>25 years in jail</td>\n<td>2 years in jail</td>\n</tr>\n</tbody>\n</table>\n<p>Faced with this scenario, a CDT agent will confess. After all, the agent\u2019s decision can\u2019t influence their partner\u2019s decision (they\u2019ve been isolated from one another) and so the agent is better off confessing regardless of what their partner chooses to do. According to the majority of decision (and game) theorists, confessing is in fact the rational decision in this case.</p>\n<p>Despite this, however, an EDT agent may lie in a prisoner\u2019s dilemma. Specifically, if they think that their partner is similar enough to them, the agent will lie because doing so will provide the good news that they will both lie and hence that they will both get two years in jail (good news as compared with the bad news that they will both confess and hence that they will get 10 years in jail).</p>\n<p>To many people, there seems to be something compelling about this line of reasoning. For example, <a href=\"http://www.amazon.com/Metamagical-Themas-Questing-Essence-Pattern/dp/0465045669/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Douglas Hofstadter (1985, pp. 737-780)</a> has argued that an agent acting \u201csuperrationally\u201d would co-operate with other superrational agents for precisely this sort of reason: a superrational agent would take into account the fact that other such agents will go through the same thought process in the <em>prisoner\u2019s dilemma</em> and so make the same decision. As such, it is better that that the decision that both agents reach be to lie than that it be to confess. More broadly, it could perhaps be argued that a rational agent should lie in the <em>prisoner\u2019s dilemma</em> as long as they believe that they are similar enough to their partner that they are likely to reach the same decision.</p>\n<div class=\"figure\"><img src=\"http://i.imgur.com/fPUcm.jpg\" alt=\"An argument for cooperation in the prisoners\u2019 dilemma\">\n<p class=\"caption\">An argument for cooperation in the prisoners\u2019 dilemma</p>\n</div>\n<p>It is unclear, then, precisely what should be concluded from the prisoner\u2019s dilemma. However, for those that are sympathetic to Hofstadter\u2019s point or the line of reasoning appealed to by the EDT agent, the scenario seems to provide an additional reason to seek out an alternative theory to CDT.</p>\n<p>&nbsp;</p>\n<h3 id=\"11_2__Benchmark_theory__BT_\"><a href=\"#benchmark-theory-bt\">11.2. Benchmark theory (BT)</a></h3>\n<p>One recent response to the apparent failure of EDT to decide appropriately in medical Newcomb problems and CDT to decide appropriately in the psychopath button is Benchmark Theory (BT) which was developed in <a href=\"http://www.springerlink.com/content/a66107137n821610/?MUD=MP\">Wedgwood (2011)</a> and discussed further in <a href=\"http://philreview.dukejournals.org/content/119/1/1.abstract\">Briggs (2010)</a>.</p>\n<p>In English, we could think of this decision algorithm as saying that agents should decide so as to give their future self good news about how well off they are compared to how well off they could have been. In formal terms, BT uses the following formula to calculate the expected utility of an act, A:</p>\n<p><img src=\"http://i.imgur.com/fUjmj.gif\" alt=\"BT expected value formula\">.</p>\n<p>In other words, it uses the conditional probability, as in EDT but calculates the value differently (as indicated by the use of V\u2019 rather than V). V\u2019 is calculated relative to a benchmark value in order to give a comparative measure of value (both of the above sources go into more detail about this process).</p>\n<p>Taking the informal perspective, in the <em>chewing gum problem</em>, BT will note that by chewing gum, the agent will always get the good news that they are comparatively better off than they could have been (because chewing gum helps control throat abscesses) whereas by not chewing, the agent will always get the bad news that they could have been comparatively better off by chewing. As such, a BT agent will chew in this scenario.</p>\n<p>Further, BT seems to reach what many consider to be the right decision in the <em>psychopath button</em>. In this case, the BT agent will note that if they push the button they will get the bad news that they are almost certainly a psychopath and so that they would have been comparatively much better off by not pushing (as pushing will kill them). On the other hand, if they don\u2019t push they will get the less bad news that they are almost certainly not a psychopath and so could have been comparatively a little better off it they had pushed the button (as this would have killed all the psychopaths but not them). So refraining from pushing the button gives the less bad news and so is the rational decision.</p>\n<p>On the face of it, then, there seem to be strong reasons to find BT compelling: it decides appropriately in these scenarios while, according to some people, EDT and CDT only decide appropriately in one or the other of them.</p>\n<p>Unfortunately, a BT agent will fail to decide appropriately in other scenarios. First, those that hold that one-boxing is the appropriate decision in Newcomb\u2019s problem will immediately find a flaw in BT. After all, in this scenario two-boxing gives the good news that the agent did comparatively better than they could have done (because they gain the $1000 from Box A which is more than they would have received otherwise) while one-boxing brings the bad news that they did comparatively worse than they could have done (as they did not receive this money). As such, a BT agent will two-box in Newcomb\u2019s problem.</p>\n<p>Further, <a href=\"http://philreview.dukejournals.org/content/119/1/1.abstract\">Briggs (2010)</a> argues, though <a href=\"http://www.springerlink.com/content/a66107137n821610/?MUD=MP\">Wedgwood (2011)</a> denies, that BT suffers from other problems. As such, even for those who support two-boxing in Newcomb\u2019s problem, it could be argued that BT doesn\u2019t represent an adequate theory of choice. It is unclear, then, whether BT is a desirable replacement to alternative theories.</p>\n<p>&nbsp;</p>\n<h3 id=\"11_3__Timeless_decision_theory__TDT_\"><a href=\"#timeless-decision-theory-tdt\">11.3. Timeless decision theory (TDT)</a></h3>\n<p><a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky (2010)</a> offers another decision algorithm, <em>timeless decision theory</em> or TDT (see also <a href=\"http://intelligence.org/files/Comparison.pdf\">Altair, 2013</a>). Specifically, TDT is intended as an explicit response to the idea that a theory of rational choice should lead an agent to \u201cwin\u201d. As such, it will appeal to those who think it is appropriate to one-box in Newcomb\u2019s problem and chew in the chewing gum problem.</p>\n<p>In English, this algorithm can be approximated as saying that an agent ought to choose as if CDT were right but they were determining not their actual decision but rather the result of the abstract computation of which their decision is one concrete instance. Formalizing this decision algorithm would require a substantial document in its own right and so will not be carried out in full here. Briefly, however, TDT is built on top of causal Bayesian networks <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">(Pearl, 2000)</a> which are graphs where the arrows represent causal influence. TDT supplements these graphs by adding nodes representing abstract computations and taking the abstract computation that determines an agent\u2019s decision to be the object of choice rather than the concrete decision itself (see <a href=\"http://intelligence.org/files/TDT.pdf\">Yudkowsky, 2010</a> for a more detailed description).</p>\n<p>Returning to an informal discussion, an example will help clarify the form taken by TDT: imagine that two perfect replicas of a person are placed in identical rooms and asked to make the same decision. While each replica will make their own decision, in doing so, they will be carrying out the same computational process. As such, TDT will say that the replicas ought to act as if they are determining the result of this process and hence as if they are deciding the behavior of both copies.</p>\n<p>Something similar can be said about Newcomb\u2019s problem. In this case it is almost like there is again a replica of the agent: Omega\u2019s model of the agent that it used to predict the agent\u2019s behavior. Both the original agent and this \u201creplica\u201d responds to the same abstract computational process as one another. In other words, both Omega\u2019s prediction and the agent\u2019s behavior are influenced by this process. As such, TDT advises the agent to act as if they are determining the result of this process and, hence, as if they can determine Omega\u2019s box filling behavior. As such, a TDT agent will one-box in order to determine the result of this abstract computation in a way that leads to $1 million being placed in Box B.</p>\n<p>TDT also succeeds in other areas. For example, in the chewing gum problem there is no \u201creplica\u201d agent so TDT will decide in line with standard CDT and choose to chew gum. Further, in the prisoner\u2019s dilemma, a TDT agent will lie if its partner is another TDT agent (or a relevantly similar agent). After all, in this case both agents will carry out the same computational process and so TDT will advise that the agent act as if they are determining this process and hence simultaneously determining both their own and their partner\u2019s decision. If so then it is better for the agent that both of them lie than that both of them confess.</p>\n<p>However, despite its success, TDT also \u201closes\u201d in some decision scenarios. For example, in counterfactual mugging, a TDT agent will not choose to give up the $100. This might seem surprising. After all, as with Newcomb\u2019s problem, this case involves Omega predicting the agent\u2019s behavior and hence involves a \u201creplica\u201d. However, this case differs in that the agent knows that the coin came up heads and so knows that they have nothing to gain by giving up the money.</p>\n<p>For those who feel that a theory of rational choice should lead an agent to \u201cwin\u201d, then, TDT seems like a step in the right direction but further work is required if it is to \u201cwin\u201d in the full range of decision scenarios.</p>\n<p>&nbsp;</p>\n<h3 id=\"11_4__Decision_theory_and__winning_\"><a href=\"#decision-theory-and-winning\">11.4. Decision theory and \u201cwinning\u201d</a></h3>\n<p>In the previous section, I discussed TDT, a decision algorithm that could be advanced as replacements for CDT and EDT. One of the primary motivations for developing TDT is a sense that both CDT and EDT fail to reason in a desirable manner in some decision scenarios. However, despite acknowledging that CDT agents end up worse off in Newcomb's Problem, many (and perhaps the majority of) decision theorists are proponents of CDT. On the face of it, this may seem to suggest that these decision theorists aren't interested in developing a decision algorithm that \"wins\" but rather have some other aim in mind. If so then this might lead us to question the value of developing one-boxing decision algorithms.</p>\n<p>However, the claim that most decision theorists don\u2019t care about finding an algorithm that \u201cwins\u201d mischaracterizes their position. After all, proponents of CDT tend to take the challenge posed by the fact that CDT agents \u201close\u201d in Newcomb's problem seriously (in the philosophical literature, it's often referred to as the <em>Why ain'cha rich?</em> problem). A common reaction to this challenge is neatly summarized in <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521641640/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Joyce (1999, p. 153-154 )</a> as a response to a hypothetical question about why, if two-boxing is rational, the CDT agent does not end up as rich as an agent that one-boxes:</p>\n<blockquote>\n<p>Rachel has a perfectly good answer to the \"Why ain't you rich?\" question. \"I am not rich,\" she will say, \"because I am not the kind of person [Omega] thinks will refuse the money. I'm just not like you, Irene [the one-boxer]. Given that I know that I am the type who takes the money, and given that [Omega] knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in [the box]. The $1,000 was the most I was going to get no matter what I did. So the only reasonable thing for me to do was to take it.\"</p>\n</blockquote>\n<blockquote>\n<p>Irene may want to press the point here by asking, \"But don't you wish you were like me, Rachel?\"... Rachel can and should admit that she <em>does</em> wish she were more like Irene... At this point, Irene will exclaim, \"You've admitted it! It wasn't so smart to take the money after all.\" Unfortunately for Irene, her conclusion does not follow from Rachel's premise. Rachel will patiently explain that wishing to be a [one-boxer] in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 <em>whatever type one is</em>. When Rachel wishes she was Irene's type she is wishing for <em>Irene's options</em>, not sanctioning her choice... While a person who knows she will face (has faced) a Newcomb problem might wish that she were (had been) the type that [Omega] labels a [one-boxer], this wish does not provide a reason for <em>being</em> a [one-boxer]. It might provide a reason to try (before [the boxes are filled]) to change her type <em>if she thinks this might affect [Omega's] prediction</em>, but it gives her no reason for doing anything other than taking the money once she comes to believes that she will be unable to influence what [Omega] does.</p>\n</blockquote>\n<p>In other words, this response distinguishes between the <em>winning decision</em> and the <em>winning type of agent</em> and claims that two-boxing is the winning decision in Newcomb\u2019s problem (even if one-boxers are the winning type of agent). Consequently, insofar as decision theory is about determining which <em>decision</em> is rational, on this account CDT reasons correctly in Newcomb\u2019s problem.</p>\n<p>For those that find this response perplexing, an analogy could be drawn to the <em>chewing gum problem</em>. In this scenario, there is near unanimous agreement that the rational decision is to chew gum. However, statistically, non-chewers will be better off than chewers. As such, the non-chewer could ask, \u201cif you\u2019re so smart, why aren\u2019t you healthy?\u201d In this case, the above response seems particularly appropriate. The chewers are less healthy not because of their decision but rather because they\u2019re more likely to have an undesirable gene. Having good genes doesn\u2019t make the non-chewer more rational but simply more lucky. The proponent of CDT simply makes a similar response to Newcomb\u2019s problem: one-boxers aren\u2019t richer because of their decision but rather because of the type of agent that they were when the boxes were filled.</p>\n<p>One final point about this response is worth noting. A proponent of CDT can accept the above argument but still acknowledge that, if given the choice before the boxes are filled, they would be rational to choose to modify themselves to be a one-boxing type of agent (as Joyce acknowledged in the above passage and as argued for in <a href=\"http://www.jstor.org/stable/20118389\">Burgess, 2004</a>). To the proponent of CDT, this is unproblematic: if we are sometimes rewarded not for the rationality of our decisions in the moment but for the type of agent we were at some past moment then it should be unsurprising that changing to a different type of agent might be beneficial.</p>\n<p>The response to this defense of two-boxing in Newcomb\u2019s problem has been divided. Many find it compelling but others, like <a href=\"http://link.springer.com/article/10.1007%2Fs10670-011-9355-2\">Ahmed and Price (2012)</a> think it does not adequately address to the challenge:</p>\n<blockquote>\n<p>It is no use the causalist's whining that foreseeably, Newcomb problems do in fact reward irrationality, or rather CDT-irrationality. The point of the argument is that if everyone knows that the CDT-irrational strategy will in fact do better on average than the CDT-rational strategy, then it's rational to play the CDT-irrational strategy.</p>\n</blockquote>\n<p>Given this, there seem to be two positions one could take on these issues. If the response given by the proponent of CDT is compelling, then we should be attempting to develop a decision theory that two-boxes on Newcomb\u2019s problem. Perhaps the best theory for this role is CDT but perhaps it is instead BT, which many people think reasons better in the psychopath button scenario. On the other hand, if the response given by the proponents of CDT is not compelling, then we should be developing a theory that one-boxes in Newcomb\u2019s problem. In this case, TDT, or something like it, seems like the most promising theory currently on offer.</p>", "sections": [{"title": "1. What is decision theory?", "anchor": "1__What_is_decision_theory_", "level": 1}, {"title": "2. Is the rational decision always the right decision?", "anchor": "2__Is_the_rational_decision_always_the_right_decision_", "level": 1}, {"title": "3. How can I better understand a decision problem?", "anchor": "3__How_can_I_better_understand_a_decision_problem_", "level": 1}, {"title": "4. How can I measure an agent's preferences?", "anchor": "4__How_can_I_measure_an_agent_s_preferences_", "level": 1}, {"title": "4.1. The concept of utility", "anchor": "4_1__The_concept_of_utility", "level": 2}, {"title": "4.2. Types of utility", "anchor": "4_2__Types_of_utility", "level": 2}, {"title": "5. What do decision theorists mean by \"risk,\" \"ignorance,\" and \"uncertainty\"?", "anchor": "5__What_do_decision_theorists_mean_by__risk____ignorance___and__uncertainty__", "level": 1}, {"title": "6. How should I make decisions under ignorance?", "anchor": "6__How_should_I_make_decisions_under_ignorance_", "level": 1}, {"title": "6.1. The dominance principle", "anchor": "6_1__The_dominance_principle", "level": 2}, {"title": "6.2. Maximin and leximin", "anchor": "6_2__Maximin_and_leximin", "level": 2}, {"title": "6.3. Maximax and optimism-pessimism", "anchor": "6_3__Maximax_and_optimism_pessimism", "level": 2}, {"title": "6.4. Other decision principles", "anchor": "6_4__Other_decision_principles", "level": 2}, {"title": "7. Can decisions under ignorance be transformed into decisions under uncertainty?", "anchor": "7__Can_decisions_under_ignorance_be_transformed_into_decisions_under_uncertainty_", "level": 1}, {"title": "8. How should I make decisions under uncertainty?", "anchor": "8__How_should_I_make_decisions_under_uncertainty_", "level": 1}, {"title": "8.1. The law of large numbers", "anchor": "8_1__The_law_of_large_numbers", "level": 2}, {"title": "8.2. The axiomatic approach", "anchor": "8_2__The_axiomatic_approach", "level": 2}, {"title": "8.3. The Von Neumann-Morgenstern utility theorem", "anchor": "8_3__The_Von_Neumann_Morgenstern_utility_theorem", "level": 2}, {"title": "8.4. VNM utility theory and rationality", "anchor": "8_4__VNM_utility_theory_and_rationality", "level": 2}, {"title": "8.5. Objections to VNM-rationality", "anchor": "8_5__Objections_to_VNM_rationality", "level": 2}, {"title": "8.6. Should we accept the VNM axioms?", "anchor": "8_6__Should_we_accept_the_VNM_axioms_", "level": 2}, {"title": "8.6.1. The transitivity axiom", "anchor": "8_6_1__The_transitivity_axiom", "level": 3}, {"title": "8.6.2. The completeness axiom", "anchor": "8_6_2__The_completeness_axiom", "level": 3}, {"title": "8.6.3. The Allais paradox", "anchor": "8_6_3__The_Allais_paradox", "level": 3}, {"title": "8.6.4. The Ellsberg paradox", "anchor": "8_6_4__The_Ellsberg_paradox", "level": 3}, {"title": "8.6.5. The St Petersburg paradox", "anchor": "8_6_5__The_St_Petersburg_paradox", "level": 3}, {"title": "9. Does axiomatic decision theory offer any action guidance?", "anchor": "9__Does_axiomatic_decision_theory_offer_any_action_guidance_", "level": 1}, {"title": "10. How does probability theory play a role in decision theory?", "anchor": "10__How_does_probability_theory_play_a_role_in_decision_theory_", "level": 1}, {"title": "10.1. The basics of probability theory", "anchor": "10_1__The_basics_of_probability_theory", "level": 2}, {"title": "10.2. Bayes theorem for updating probabilities", "anchor": "10_2__Bayes_theorem_for_updating_probabilities", "level": 2}, {"title": "10.3. How should probabilities be interpreted?", "anchor": "10_3__How_should_probabilities_be_interpreted_", "level": 2}, {"title": "10.3.1. Why should degrees of belief follow the laws of probability?", "anchor": "10_3_1__Why_should_degrees_of_belief_follow_the_laws_of_probability_", "level": 3}, {"title": "10.3.2. Measuring subjective probabilities", "anchor": "10_3_2__Measuring_subjective_probabilities", "level": 3}, {"title": "11. What about \"Newcomb's problem\" and alternative decision algorithms?", "anchor": "11__What_about__Newcomb_s_problem__and_alternative_decision_algorithms_", "level": 1}, {"title": "11.1. Newcomblike problems and two decision algorithms", "anchor": "11_1__Newcomblike_problems_and_two_decision_algorithms", "level": 2}, {"title": "11.1.1. Newcomb's Problem", "anchor": "11_1_1__Newcomb_s_Problem", "level": 3}, {"title": "11.1.2. Evidential and causal decision theory", "anchor": "11_1_2__Evidential_and_causal_decision_theory", "level": 3}, {"title": "11.1.3. Medical Newcomb problems", "anchor": "11_1_3__Medical_Newcomb_problems", "level": 3}, {"title": "11.1.4. Newcomb's soda", "anchor": "11_1_4__Newcomb_s_soda", "level": 3}, {"title": "11.1.5. Bostrom's meta-Newcomb problem", "anchor": "11_1_5__Bostrom_s_meta_Newcomb_problem", "level": 3}, {"title": "11.1.6. The psychopath button", "anchor": "11_1_6__The_psychopath_button", "level": 3}, {"title": "11.1.7. Parfit's hitchhiker", "anchor": "11_1_7__Parfit_s_hitchhiker", "level": 3}, {"title": "11.1.8. Transparent Newcomb's problem", "anchor": "11_1_8__Transparent_Newcomb_s_problem", "level": 3}, {"title": "11.1.9. Counterfactual mugging", "anchor": "11_1_9__Counterfactual_mugging", "level": 3}, {"title": "11.1.10. Prisoner's dilemma", "anchor": "11_1_10__Prisoner_s_dilemma", "level": 3}, {"title": "11.2. Benchmark theory (BT)", "anchor": "11_2__Benchmark_theory__BT_", "level": 2}, {"title": "11.3. Timeless decision theory (TDT)", "anchor": "11_3__Timeless_decision_theory__TDT_", "level": 2}, {"title": "11.4. Decision theory and \u201cwinning\u201d", "anchor": "11_4__Decision_theory_and__winning_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "482 comments"}], "headingsCount": 49}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 482, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["zJZvoiwydJ5zvzTHK", "a5JAiTdytou3Jg749", "F46jPraqp258q67nE", "oRRpsGkCZHA3pzhvm", "mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-28T17:21:34.629Z", "modifiedAt": null, "url": null, "title": "Constructive mathemathics and its dual", "slug": "constructive-mathemathics-and-its-dual", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jf67y793ZMc8DF2xQ/constructive-mathemathics-and-its-dual", "pageUrlRelative": "/posts/Jf67y793ZMc8DF2xQ/constructive-mathemathics-and-its-dual", "linkUrl": "https://www.lesswrong.com/posts/Jf67y793ZMc8DF2xQ/constructive-mathemathics-and-its-dual", "postedAtFormatted": "Thursday, February 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Constructive%20mathemathics%20and%20its%20dual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConstructive%20mathemathics%20and%20its%20dual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJf67y793ZMc8DF2xQ%2Fconstructive-mathemathics-and-its-dual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Constructive%20mathemathics%20and%20its%20dual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJf67y793ZMc8DF2xQ%2Fconstructive-mathemathics-and-its-dual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJf67y793ZMc8DF2xQ%2Fconstructive-mathemathics-and-its-dual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 978, "htmlBody": "<p>I have stumbled upon an interesting and, as far as I know, new concept: thinking about the duality between constructive and paraconsistent logics, I've noticed that while the meta-theory of intuitionistic logic (constructive mathematics) is very well understood and studied, the meta-theory of the dual logic is not. If we understand constructive mathematics from an epistemological point of view, as an accretion of truth from an empty base, we ought to be able to think about a sort of destructive mathematics, that starts from the totality of assertions and proceeds by expunging falsity. This seems to have surprising consequences for things like theism, Tegmark universe(s), the Many World Interpretation and so on, but first I need to cover some background informations. This I will do in the present post, while in the next I'll present the concept and some of its applications.</p>\n<p>There is a variety of philosophical programs known as constructive mathematics, but their common denominator is to refuse the classic way of conceiving truth and adhere instead to a concept known as verificational existence. That is, for a mathematical formula A to be accepted as true, there must be a construction (a direct proof) of A. On the same level, for a mathematical formula to be denoted false, a constructivist accepts only a proof of <img title=\"\\lnot A\" src=\"http://www.codecogs.com/png.latex?\\lnot A\" alt=\"\" align=\"bottom\" />&nbsp;(this symbol&nbsp;denotes the negation of A). If neither of such proofs exist, then a constructivist refuse to impose a truth value upon A. This has as consequence the refusal of the general formula <img title=\"A\\lor \\lnot A\" src=\"http://www.codecogs.com/png.latex?A\\lor \\lnot A\" alt=\"\" align=\"bottom\" />, (<img title=\"\\lor\" src=\"http://www.codecogs.com/png.latex?\\lor\" alt=\"\" align=\"bottom\" />&nbsp;denotes logical disjunction), valid in classical logic, a principle called in Latin <em>tertium non datur</em> (TND), which means \"a third is not given\". For a constructivist nonetheless the principle <img title=\"\\lnot (A \\land \\lnot A)\" src=\"http://www.codecogs.com/png.latex?\\lnot (A \\land \\lnot A)\" alt=\"\" align=\"bottom\" />&nbsp;still holds (<img title=\"\\land\" src=\"http://www.codecogs.com/png.latex?\\land\" alt=\"\" align=\"bottom\" /> denotes logical conjunction), because it is still viewed as impossible that there exists a valid proof of A and of its opposite. This one is called <em>ex contraditione quodlibet</em> (ECQ), which means \"anything from a contradiction\".</p>\n<p>The simple excision of TND from classical logic gives a logical system called intuitionistic logic (because it was developed under the intuitionistic program of constructive mathematics), which has many, many interesting properties.</p>\n<p>A logical calculus developed on these fundamental consideration is aimed at preserving justification rather than truth: intuitionistic proofs, instead of carrying from true formulas to other true formulas, only produce justified formulas from other justified formulas.</p>\n<p>Notice though: ECQ and TND are both theorems (or axioms) of classical logic. They are in fact equivalent. ECQ is espressed as <img title=\"\\lnot (A \\land \\lnot A)\" src=\"http://www.codecogs.com/png.latex?\\lnot (A \\land \\lnot A)\" alt=\"\" align=\"bottom\" />, but under the DeMorgan laws, double negation elimination and commutativity of disjunction: <img title=\"\\lnot (A \\land \\lnot A) \\leftrightarrow (\\lnot A \\lor \\lnot \\lnot A) \\leftrightarrow (\\lnot A \\lor A) \\leftrightarrow (A \\lor \\lnot A)\" src=\"http://www.codecogs.com/png.latex?\\lnot (A \\land \\lnot A) \\leftrightarrow (\\lnot A \\lor \\lnot \\lnot A) \\leftrightarrow (\\lnot A \\lor A) \\leftrightarrow (A \\lor \\lnot A)\" alt=\"\" align=\"bottom\" />, which is the TND.</p>\n<p>If then we decide to break the equivalency, it becomes natural to ask: since there's a logic that accepts ECQ but refuses TND (intuitionistic logic), can there be a logic that's a sort of dual, that is it accepts TND but refuses ECQ?</p>\n<p>This question, maybe surprisingly, has an affirmative answer, and the resulting plethora of logical systems thus produced are called \"paraconsistent logics\".<br />Under this classification, intuitionistic logic can then be said to be a member of the class now known as \"paracomplete logics\" (although this name is not much used). Paraconsistent logics are a multitude, but one of them is an exact symmetric of intuitionistic logic, known in the literature as dual-intuitionistic logic.</p>\n<p>If you reflect on that a little bit, it may seems very strange at first to abandon ECQ. After all, the refusal of contradictions is one of the primary, if not <em>the</em> primary, foundation of rationality. But in formal logic there's also a very cogent and slightly technical reason why contradictions are not allowed: in classical logic, they imply triviality.</p>\n<p>A set (possibly infinite) of sentences and formulas in logic is called a theory. It is clear that an empty theory is not very interesting: it literally tells us nothing about the subject at hand. But an equally uninteresting theory is the total theory: the set of all possible sentences and formulas. Since this set makes no distinction on what's true and what's false about the subject of interest, it's as informative as the empty theory. Such a theory is called trivial, and formal systems developed within classical logic strive to avoid contradictions: indeed, from a single formula and its negation you can prove <em>any</em> other formula.<br />In systems that rely on classical logich then, any contradiction entails triviality, and they are therefore to be avoided.</p>\n<p>Paraconsistent logics however depart from this classical setting, and they abandon this principle (sometimes called \"principle of explosion\").</p>\n<p>Be careful though: only the general principle is abandoned. Exactly like in intuitionistic logic, where <img title=\"A \\lor \\lnot A\" src=\"http://www.codecogs.com/png.latex?A \\lor \\lnot A\" alt=\"\" align=\"bottom\" />&nbsp;is abandoned in general, but if you have constructed a proof of (say) A, then <em>for that particular formula</em> <img title=\"A \\lor \\lnot A\" src=\"http://www.codecogs.com/png.latex?A \\lor \\lnot A\" alt=\"\" align=\"bottom\" />&nbsp;is valid, in dual-intuitionistic logic if you have constructed a proof of (say) <img title=\"\\lnot A\" src=\"http://www.codecogs.com/png.latex?\\lnot A\" alt=\"\" align=\"bottom\" />, then <img title=\"\\lnot (A \\land \\lnot A)\" src=\"http://www.codecogs.com/png.latex?\\lnot (A \\land \\lnot A)\" alt=\"\" align=\"bottom\" />&nbsp;is still valid&nbsp;<em>only for that formula</em>.</p>\n<p>What is the meta-theory of dual-intuitionistic logic? How can it be justified and it's at the end somehow useful?</p>\n<p>This is where things get interesting, and it's a theme I want to explore in the next post.<br /><br />Links and references<br />Over the net, there's more than you could possibly care to learn about constructive mathematics: the usual pointer are Wikipedia's&nbsp;http://en.wikipedia.org/wiki/Constructivism_(mathematics) and&nbsp;http://en.wikipedia.org/wiki/Intuitionistic_logic, while on the SEP side you have&nbsp;http://plato.stanford.edu/entries/mathematics-constructive/ and&nbsp;http://plato.stanford.edu/entries/logic-intuitionistic/.<br />There is considerable less material on paraconsistent logic, but again you can find http://en.wikipedia.org/wiki/Paraconsistent_logic and http://plato.stanford.edu/entries/logic-paraconsistent/.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jf67y793ZMc8DF2xQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 1.1252713956266663e-06, "legacy": true, "legacyId": "21832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T00:32:33.625Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 10th March", "slug": "meetup-london-meetup-10th-march", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Aad8ZTcpywJweBdCd/meetup-london-meetup-10th-march", "pageUrlRelative": "/posts/Aad8ZTcpywJweBdCd/meetup-london-meetup-10th-march", "linkUrl": "https://www.lesswrong.com/posts/Aad8ZTcpywJweBdCd/meetup-london-meetup-10th-march", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2010th%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2010th%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAad8ZTcpywJweBdCd%2Fmeetup-london-meetup-10th-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2010th%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAad8ZTcpywJweBdCd%2Fmeetup-london-meetup-10th-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAad8ZTcpywJweBdCd%2Fmeetup-london-meetup-10th-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jx'>London Meetup, 10th March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 March 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jx'>London Meetup, 10th March</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Aad8ZTcpywJweBdCd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "21835", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__10th_March\">Discussion article for the meetup : <a href=\"/meetups/jx\">London Meetup, 10th March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 March 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__10th_March1\">Discussion article for the meetup : <a href=\"/meetups/jx\">London Meetup, 10th March</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 10th March", "anchor": "Discussion_article_for_the_meetup___London_Meetup__10th_March", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 10th March", "anchor": "Discussion_article_for_the_meetup___London_Meetup__10th_March1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T09:08:34.183Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-04]", "slug": "meetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9wfZddFofJBqmjion/meetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "pageUrlRelative": "/posts/9wfZddFofJBqmjion/meetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "linkUrl": "https://www.lesswrong.com/posts/9wfZddFofJBqmjion/meetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-04%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-04%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wfZddFofJBqmjion%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-04%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-04%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wfZddFofJBqmjion%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wfZddFofJBqmjion%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-04", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jy'>Cambridge, UK LW Meetup [Reading Group, HAEFB-04]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group of Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our fourth session, so dive right in with us if you're new!</p>\n\n<p>We'll be covering\u00a0Stuff That Makes Stuff Happen\u00a0today. We'll also have a short administrative session at the end; let's say around 11:45.</p>\n\n<p>See you there!</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jy'>Cambridge, UK LW Meetup [Reading Group, HAEFB-04]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9wfZddFofJBqmjion", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1258846619563362e-06, "legacy": true, "legacyId": "21848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_04_\">Discussion article for the meetup : <a href=\"/meetups/jy\">Cambridge, UK LW Meetup [Reading Group, HAEFB-04]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group of Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our fourth session, so dive right in with us if you're new!</p>\n\n<p>We'll be covering&nbsp;Stuff That Makes Stuff Happen&nbsp;today. We'll also have a short administrative session at the end; let's say around 11:45.</p>\n\n<p>See you there!</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_04_1\">Discussion article for the meetup : <a href=\"/meetups/jy\">Cambridge, UK LW Meetup [Reading Group, HAEFB-04]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-04]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_04_", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-04]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_04_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T12:00:44.477Z", "modifiedAt": null, "url": null, "title": "Open Thread, March 1-15, 2013 ", "slug": "open-thread-march-1-15-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:33.433Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u7sKRzQ5jC7KXPPsh/open-thread-march-1-15-2013", "pageUrlRelative": "/posts/u7sKRzQ5jC7KXPPsh/open-thread-march-1-15-2013", "linkUrl": "https://www.lesswrong.com/posts/u7sKRzQ5jC7KXPPsh/open-thread-march-1-15-2013", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20March%201-15%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20March%201-15%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7sKRzQ5jC7KXPPsh%2Fopen-thread-march-1-15-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20March%201-15%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7sKRzQ5jC7KXPPsh%2Fopen-thread-march-1-15-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7sKRzQ5jC7KXPPsh%2Fopen-thread-march-1-15-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u7sKRzQ5jC7KXPPsh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1259962227139068e-06, "legacy": true, "legacyId": "21850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 241, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T14:10:48.782Z", "modifiedAt": null, "url": null, "title": "Meetup : Paderborn Meetup March 6th", "slug": "meetup-paderborn-meetup-march-6th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.136Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KJv2ZhYXc66AwXxnQ/meetup-paderborn-meetup-march-6th", "pageUrlRelative": "/posts/KJv2ZhYXc66AwXxnQ/meetup-paderborn-meetup-march-6th", "linkUrl": "https://www.lesswrong.com/posts/KJv2ZhYXc66AwXxnQ/meetup-paderborn-meetup-march-6th", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Paderborn%20Meetup%20March%206th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Paderborn%20Meetup%20March%206th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJv2ZhYXc66AwXxnQ%2Fmeetup-paderborn-meetup-march-6th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Paderborn%20Meetup%20March%206th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJv2ZhYXc66AwXxnQ%2Fmeetup-paderborn-meetup-march-6th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJv2ZhYXc66AwXxnQ%2Fmeetup-paderborn-meetup-march-6th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jz'>Paderborn Meetup March 6th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 March 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening are not yet determined, but will be in the next days, or develop during the meetup. Highly interesting talk can be expected.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jz'>Paderborn Meetup March 6th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KJv2ZhYXc66AwXxnQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1260805173319487e-06, "legacy": true, "legacyId": "21851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup_March_6th\">Discussion article for the meetup : <a href=\"/meetups/jz\">Paderborn Meetup March 6th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 March 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening are not yet determined, but will be in the next days, or develop during the meetup. Highly interesting talk can be expected.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup_March_6th1\">Discussion article for the meetup : <a href=\"/meetups/jz\">Paderborn Meetup March 6th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Paderborn Meetup March 6th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup_March_6th", "level": 1}, {"title": "Discussion article for the meetup : Paderborn Meetup March 6th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup_March_6th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T14:51:45.444Z", "modifiedAt": null, "url": null, "title": "Positive Information Diet, Take the Challenge", "slug": "positive-information-diet-take-the-challenge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2wgu6JSL42ghz6QQe/positive-information-diet-take-the-challenge", "pageUrlRelative": "/posts/2wgu6JSL42ghz6QQe/positive-information-diet-take-the-challenge", "linkUrl": "https://www.lesswrong.com/posts/2wgu6JSL42ghz6QQe/positive-information-diet-take-the-challenge", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Positive%20Information%20Diet%2C%20Take%20the%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APositive%20Information%20Diet%2C%20Take%20the%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wgu6JSL42ghz6QQe%2Fpositive-information-diet-take-the-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Positive%20Information%20Diet%2C%20Take%20the%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wgu6JSL42ghz6QQe%2Fpositive-information-diet-take-the-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wgu6JSL42ghz6QQe%2Fpositive-information-diet-take-the-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1022, "htmlBody": "<p>I looked for Information Diet in Lesswrong search, and found something amazing:</p>\n<p>On Lukeprog's Q and A as the new executive director, he was asked:</p>\n<blockquote>\n<p>What is your information diet like? (I mean other than when you engage in focused learning.) Do you regulate it, or do you just let it happen naturally?</p>\n<p>By that I mean things like:</p>\n<ul>\n<li>Do you have a reading schedule (e.g. X hours daily)?</li>\n<li>Do you follow the news, or try to avoid information with a short shelf-life?</li>\n<li>Do you significantly limit yourself with certain materials (e.g. fun stuff) to focus on higher priorities?</li>\n<li>In the end, what is the makeup of the diet?</li>\n<li>Etc.</li>\n</ul>\n</blockquote>\n<p>To which he responded:</p>\n<blockquote>\n<ul>\n<li>I do not regulate my information diet. </li>\n<li>I do not have a reading schedule. </li>\n<li>I do not follow the news. </li>\n<li>I haven't read fiction in years. This is not because I'm avoiding \"fun stuff,\" but because my brain complains when I'm reading fiction. I can't even read HPMOR. I don't need to consciously \"limit\" my consumption of \"fun stuff\" because reading scientific review articles on subjects I'm researching and writing about <em>is</em> the fun stuff. </li>\n<li>What I'm trying to learn at this moment almost entirely dictates my reading habits. </li>\n<li>The only thing beyond this scope is my RSS feed, which I skim through in about 15 minutes per day. </li>\n</ul>\n</blockquote>\n<p>Whatever was the case back then, I'll bet is not anymore. No one with assistants and such a workload should be let adrift like that.</p>\n<p><em>Citizen: But Lukeprog's posts are obviously brilliant, his output is great, even very focused readers like Chalmers find Luke to be very bright. </em></p>\n<p>Which doesn't tell much about what they <a href=\"http://libgen.info/view.php?id=450971\">would</a> <a href=\"http://talentsearch.ted.com/video/Diego-Caleiro-What-if-I-had-don\">have been</a> were he under a more stringent diet. Another reasonable suspicion is that he was not actually modelling himself correctly, since <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">he obviously</a> does <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">have</a> an <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">information</a> <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">diet</a></p>\n<p>&nbsp;</p>\n<p>The Information Diet Challenge is to set yourself an information diet, explicitly, and follow it for a week.&nbsp;&nbsp;</p>\n<p>Many ways of countering biases have been proposed here, but I haven't found a post dealing with this specific, very low hanging fruit one.&nbsp;</p>\n<p>If you want inspiration, Ferriss has some advice <a href=\"http://changethis.com/manifesto/show/34.04.LowInfo\">here</a>.</p>\n<p>... but that is not the <em>Positive</em> Information Diet yet...</p>\n<p>Information diets are supposed to constrain not everything you intake, but only what you intake <em>instrumentally</em>. If you just love reading about tensors and fairy tales, don't include them in what you won't avoid. What matters is to know that you'll avoid trying to learn programming<em> by</em> reading a programmer's tweet feed, avoid becoming a top researcher in psychology<em> by</em> reading popular magazines on it, and avoid reading random feeds on Facebook that don't <em>relate to your goals</em> in appropriate ways.</p>\n<p>General form: I will Avoid spending my time reading/commenting things of kind (A)(Avoid), because I know that to reach my set of goals (G), the most productive learning time is doing (P) (Positve/Productive).&nbsp;</p>\n<p>&nbsp;</p>\n<p>So here is an attempt:</p>\n<p>(G): Interact fruitfully with people at Oxford</p>\n<p>(A): Facebook feeds that are not by them; News of any kind; Emails I can Postpone; Gossip; Books/articles not on Evolution of Morals, enhancement, AI; Wikidrifting; Family meal small talk; SMBC; 9gag; Tropes .... and <em>a bunch of other stuff I don't have time or patience to list</em>.</p>\n<p>(P): Google scholar on the intersection between my research topic and theirs. Reading their papers by day, watching their videos by night. Re-read what I might help them with that was read before, list topics per person, write what to say about each topic.</p>\n<p>&nbsp;</p>\n<p>What is wrong with this attempt is that (A) ends up being a <em>negative</em> list. A list of what what I <em>do not</em> want to intake. Since possibilities are infinite, this will give me ridiculous cognitive load, and that <a href=\"http://www.youtube.com/watch?v=sBsXIBEm8Ug\">is a problem</a>. So here is simple solution, which I used for a food diet before, and worked great:&nbsp; Name not what you cannot do, but what you are allowed to do. Way fewer bits, way easier to check!&nbsp;</p>\n<p>Food example: I'll eat only plants, lean fish and chicken, nuts, fruits, whole pasta, beans and Chai Lattes.</p>\n<p>We are better at checking for category inclusion than exclusion. There are so many available categories to exclude from that we don't feel bad that we \"forgot\" to check for that one<em>. </em>Then after you let yourself indulge in a tiny one, a small one doesn't seem that bad, and snowball effect does the rest. We <a href=\"/lw/ny/sneaking_in_connotations/\">sneak in connotations</a> to make categories smaller, so our actions stay safely outside the scope of prohibition. Theoretically, we could do the reverse, but it is psychologically much harder. Just try to convince yourself that beef is \"lean chicken\" to see it.</p>\n<p>&nbsp;</p>\n<p>So let us forget completely about (A). There is no kind or class of kinds to avoid. there is only G and P, and now there is also T, the time during which P is in force, since escape valves might be necessary to avoid <a href=\"/r/discussion/lw/grn/whatthehell_cognitive_failure_mode_a_separate/\">\"screw that\" all-or-nothing effects</a>.</p>\n<p><strong>An Improved attempt: </strong></p>\n<p><strong>G: Interact fruitfully with people in Oxford</strong></p>\n<p><strong>P: Google scholar on the intersection between my research topic and theirs. Reading their papers by day, watching their videos by night. Re-read what I might help them with that was read before, list topics per person, write what to say about each topic. Only Facebook them.&nbsp; </strong></p>\n<p><strong>T: 02:00-23:59 daily. <br /></strong></p>\n<p>&nbsp;</p>\n<p>This is only for \"computer use\", where I'm most likely to do the wrong thing.</p>\n<p>Now there is a simple to check list of things I want to do, I could be doing, and I'll try to do until G arrives. I can only do those. If x doesn't belong, don't do it, that simple. I'm free from midnight to two to do whatever, thus I don't feel <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">enslaved</a> by my past self.&nbsp; No heavy cognitive load is burning my willpower candle <a href=\"http://libgen.info/view.php?id=820229\">(Shawn Achor 2010)</a> by trying set theory gimmicks to get me to do the wrong thing.&nbsp;</p>\n<p>&nbsp;</p>\n<p>So please, take the:</p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Positive Information Diet Challenge</strong></p>\n<p><strong> Write your G's (goals) P's (positives) and T's (times), and forget about your A's (Avoids) &nbsp; </strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><em><br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2wgu6JSL42ghz6QQe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 1.1261070541236601e-06, "legacy": true, "legacyId": "21728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I looked for Information Diet in Lesswrong search, and found something amazing:</p>\n<p>On Lukeprog's Q and A as the new executive director, he was asked:</p>\n<blockquote>\n<p>What is your information diet like? (I mean other than when you engage in focused learning.) Do you regulate it, or do you just let it happen naturally?</p>\n<p>By that I mean things like:</p>\n<ul>\n<li>Do you have a reading schedule (e.g. X hours daily)?</li>\n<li>Do you follow the news, or try to avoid information with a short shelf-life?</li>\n<li>Do you significantly limit yourself with certain materials (e.g. fun stuff) to focus on higher priorities?</li>\n<li>In the end, what is the makeup of the diet?</li>\n<li>Etc.</li>\n</ul>\n</blockquote>\n<p>To which he responded:</p>\n<blockquote>\n<ul>\n<li>I do not regulate my information diet. </li>\n<li>I do not have a reading schedule. </li>\n<li>I do not follow the news. </li>\n<li>I haven't read fiction in years. This is not because I'm avoiding \"fun stuff,\" but because my brain complains when I'm reading fiction. I can't even read HPMOR. I don't need to consciously \"limit\" my consumption of \"fun stuff\" because reading scientific review articles on subjects I'm researching and writing about <em>is</em> the fun stuff. </li>\n<li>What I'm trying to learn at this moment almost entirely dictates my reading habits. </li>\n<li>The only thing beyond this scope is my RSS feed, which I skim through in about 15 minutes per day. </li>\n</ul>\n</blockquote>\n<p>Whatever was the case back then, I'll bet is not anymore. No one with assistants and such a workload should be let adrift like that.</p>\n<p><em>Citizen: But Lukeprog's posts are obviously brilliant, his output is great, even very focused readers like Chalmers find Luke to be very bright. </em></p>\n<p>Which doesn't tell much about what they <a href=\"http://libgen.info/view.php?id=450971\">would</a> <a href=\"http://talentsearch.ted.com/video/Diego-Caleiro-What-if-I-had-don\">have been</a> were he under a more stringent diet. Another reasonable suspicion is that he was not actually modelling himself correctly, since <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">he obviously</a> does <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">have</a> an <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">information</a> <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">diet</a></p>\n<p>&nbsp;</p>\n<p>The Information Diet Challenge is to set yourself an information diet, explicitly, and follow it for a week.&nbsp;&nbsp;</p>\n<p>Many ways of countering biases have been proposed here, but I haven't found a post dealing with this specific, very low hanging fruit one.&nbsp;</p>\n<p>If you want inspiration, Ferriss has some advice <a href=\"http://changethis.com/manifesto/show/34.04.LowInfo\">here</a>.</p>\n<p>... but that is not the <em>Positive</em> Information Diet yet...</p>\n<p>Information diets are supposed to constrain not everything you intake, but only what you intake <em>instrumentally</em>. If you just love reading about tensors and fairy tales, don't include them in what you won't avoid. What matters is to know that you'll avoid trying to learn programming<em> by</em> reading a programmer's tweet feed, avoid becoming a top researcher in psychology<em> by</em> reading popular magazines on it, and avoid reading random feeds on Facebook that don't <em>relate to your goals</em> in appropriate ways.</p>\n<p>General form: I will Avoid spending my time reading/commenting things of kind (A)(Avoid), because I know that to reach my set of goals (G), the most productive learning time is doing (P) (Positve/Productive).&nbsp;</p>\n<p>&nbsp;</p>\n<p>So here is an attempt:</p>\n<p>(G): Interact fruitfully with people at Oxford</p>\n<p>(A): Facebook feeds that are not by them; News of any kind; Emails I can Postpone; Gossip; Books/articles not on Evolution of Morals, enhancement, AI; Wikidrifting; Family meal small talk; SMBC; 9gag; Tropes .... and <em>a bunch of other stuff I don't have time or patience to list</em>.</p>\n<p>(P): Google scholar on the intersection between my research topic and theirs. Reading their papers by day, watching their videos by night. Re-read what I might help them with that was read before, list topics per person, write what to say about each topic.</p>\n<p>&nbsp;</p>\n<p>What is wrong with this attempt is that (A) ends up being a <em>negative</em> list. A list of what what I <em>do not</em> want to intake. Since possibilities are infinite, this will give me ridiculous cognitive load, and that <a href=\"http://www.youtube.com/watch?v=sBsXIBEm8Ug\">is a problem</a>. So here is simple solution, which I used for a food diet before, and worked great:&nbsp; Name not what you cannot do, but what you are allowed to do. Way fewer bits, way easier to check!&nbsp;</p>\n<p>Food example: I'll eat only plants, lean fish and chicken, nuts, fruits, whole pasta, beans and Chai Lattes.</p>\n<p>We are better at checking for category inclusion than exclusion. There are so many available categories to exclude from that we don't feel bad that we \"forgot\" to check for that one<em>. </em>Then after you let yourself indulge in a tiny one, a small one doesn't seem that bad, and snowball effect does the rest. We <a href=\"/lw/ny/sneaking_in_connotations/\">sneak in connotations</a> to make categories smaller, so our actions stay safely outside the scope of prohibition. Theoretically, we could do the reverse, but it is psychologically much harder. Just try to convince yourself that beef is \"lean chicken\" to see it.</p>\n<p>&nbsp;</p>\n<p>So let us forget completely about (A). There is no kind or class of kinds to avoid. there is only G and P, and now there is also T, the time during which P is in force, since escape valves might be necessary to avoid <a href=\"/r/discussion/lw/grn/whatthehell_cognitive_failure_mode_a_separate/\">\"screw that\" all-or-nothing effects</a>.</p>\n<p><strong id=\"An_Improved_attempt__\">An Improved attempt: </strong></p>\n<p><strong id=\"G__Interact_fruitfully_with_people_in_Oxford\">G: Interact fruitfully with people in Oxford</strong></p>\n<p><strong id=\"P__Google_scholar_on_the_intersection_between_my_research_topic_and_theirs__Reading_their_papers_by_day__watching_their_videos_by_night__Re_read_what_I_might_help_them_with_that_was_read_before__list_topics_per_person__write_what_to_say_about_each_topic__Only_Facebook_them___\">P: Google scholar on the intersection between my research topic and theirs. Reading their papers by day, watching their videos by night. Re-read what I might help them with that was read before, list topics per person, write what to say about each topic. Only Facebook them.&nbsp; </strong></p>\n<p><strong id=\"T__02_00_23_59_daily__\">T: 02:00-23:59 daily. <br></strong></p>\n<p>&nbsp;</p>\n<p>This is only for \"computer use\", where I'm most likely to do the wrong thing.</p>\n<p>Now there is a simple to check list of things I want to do, I could be doing, and I'll try to do until G arrives. I can only do those. If x doesn't belong, don't do it, that simple. I'm free from midnight to two to do whatever, thus I don't feel <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">enslaved</a> by my past self.&nbsp; No heavy cognitive load is burning my willpower candle <a href=\"http://libgen.info/view.php?id=820229\">(Shawn Achor 2010)</a> by trying set theory gimmicks to get me to do the wrong thing.&nbsp;</p>\n<p>&nbsp;</p>\n<p>So please, take the:</p>\n<p><strong id=\"__________Positive_Information_Diet_Challenge\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Positive Information Diet Challenge</strong></p>\n<p><strong id=\"Write_your_G_s__goals__P_s__positives__and_T_s__times___and_forget_about_your_A_s__Avoids____\"> Write your G's (goals) P's (positives) and T's (times), and forget about your A's (Avoids) &nbsp; </strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><em><br></em></p>", "sections": [{"title": "An Improved attempt: ", "anchor": "An_Improved_attempt__", "level": 1}, {"title": "G: Interact fruitfully with people in Oxford", "anchor": "G__Interact_fruitfully_with_people_in_Oxford", "level": 1}, {"title": "P: Google scholar on the intersection between my research topic and theirs. Reading their papers by day, watching their videos by night. Re-read what I might help them with that was read before, list topics per person, write what to say about each topic. Only Facebook them.\u00a0 ", "anchor": "P__Google_scholar_on_the_intersection_between_my_research_topic_and_theirs__Reading_their_papers_by_day__watching_their_videos_by_night__Re_read_what_I_might_help_them_with_that_was_read_before__list_topics_per_person__write_what_to_say_about_each_topic__Only_Facebook_them___", "level": 1}, {"title": "T: 02:00-23:59 daily. ", "anchor": "T__02_00_23_59_daily__", "level": 1}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Positive Information Diet Challenge", "anchor": "__________Positive_Information_Diet_Challenge", "level": 1}, {"title": "Write your G's (goals) P's (positives) and T's (times), and forget about your A's (Avoids) \u00a0 ", "anchor": "Write_your_G_s__goals__P_s__positives__and_T_s__times___and_forget_about_your_A_s__Avoids____", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LcEzxX2FNTKbB6KXS", "FwiPfF8Woe5JrzqEu", "wHjpCxeDeuFadG3jF", "xg3hXCYQPJkwHyik2", "yuKaWPRTxZoov4z8K", "6zH8RWtbxDPTrdJNn", "uEnFDx7nQacQdf6Tg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T16:34:49.336Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berlin, Cambridge UK, Durham, Frankfurt, London, NYC, Vancouver, Washington DC", "slug": "weekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7wK8iMSTWmXNbTzCv/weekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "pageUrlRelative": "/posts/7wK8iMSTWmXNbTzCv/weekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "linkUrl": "https://www.lesswrong.com/posts/7wK8iMSTWmXNbTzCv/weekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berlin%2C%20Cambridge%20UK%2C%20Durham%2C%20Frankfurt%2C%20London%2C%20NYC%2C%20Vancouver%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berlin%2C%20Cambridge%20UK%2C%20Durham%2C%20Frankfurt%2C%20London%2C%20NYC%2C%20Vancouver%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wK8iMSTWmXNbTzCv%2Fweekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berlin%2C%20Cambridge%20UK%2C%20Durham%2C%20Frankfurt%2C%20London%2C%20NYC%2C%20Vancouver%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wK8iMSTWmXNbTzCv%2Fweekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wK8iMSTWmXNbTzCv%2Fweekly-lw-meetups-berlin-cambridge-uk-durham-frankfurt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 520, "htmlBody": "<p><strong>This summary was posted to LW main on February 22nd. The following week's summary is <a href=\"/lw/gv0/weekly_lw_meetups_austin_buffalo_cambridge_uk/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/iy\">First meetup in Frankfurt (Main) :&nbsp;<span class=\"date\">22 February 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/jm\">(Durham) RTLW HPMoR discussion, chapters 39-43:&nbsp;<span class=\"date\">23 February 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/jk\">Berlin Meetup:&nbsp;<span class=\"date\">23 February 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/jj\">Vancouver!:&nbsp;<span class=\"date\">23 February 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/j7\">London Meetup, 24th Feb:&nbsp;<span class=\"date\">24 February 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ji\">Washington DC Show and tell meetup: Economics II:&nbsp;<span class=\"date\">24 February 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ir\">Tokyo Meetup:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/jo\">Moscow, Expanding rationality:&nbsp;<span class=\"date\">03 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/j8\">Vienna Meetup 9th March:&nbsp;<span class=\"date\">09 March 2013 04:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\"></a><a href=\"/meetups/jn\">Cambridge, UK LW Meetup [Reading Group, HAEFB-03]:&nbsp;<span class=\"date\">24 February 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/jl\">(NYC) Effective Altruism Dinner Party:&nbsp;<span class=\"date\">24 February 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/jh\">Melbourne, practical rationality:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7wK8iMSTWmXNbTzCv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1261738573037279e-06, "legacy": true, "legacyId": "21727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nCBs4rAZA4BxvbFMt", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-01T17:28:01.249Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge First-Sunday Meetup", "slug": "meetup-cambridge-first-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dLAhKjqHzLqNXnJXE/meetup-cambridge-first-sunday-meetup", "pageUrlRelative": "/posts/dLAhKjqHzLqNXnJXE/meetup-cambridge-first-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/dLAhKjqHzLqNXnJXE/meetup-cambridge-first-sunday-meetup", "postedAtFormatted": "Friday, March 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20First-Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20First-Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLAhKjqHzLqNXnJXE%2Fmeetup-cambridge-first-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20First-Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLAhKjqHzLqNXnJXE%2Fmeetup-cambridge-first-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLAhKjqHzLqNXnJXE%2Fmeetup-cambridge-first-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k0'>Cambridge First-Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 March 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k0'>Cambridge First-Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dLAhKjqHzLqNXnJXE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1262083415260422e-06, "legacy": true, "legacyId": "21854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/k0\">Cambridge First-Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 March 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/k0\">Cambridge First-Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge First-Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge First-Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-02T10:45:48.626Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes March 2013", "slug": "rationality-quotes-march-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2m2Ha4QhKSg26yWZG/rationality-quotes-march-2013", "pageUrlRelative": "/posts/2m2Ha4QhKSg26yWZG/rationality-quotes-march-2013", "linkUrl": "https://www.lesswrong.com/posts/2m2Ha4QhKSg26yWZG/rationality-quotes-march-2013", "postedAtFormatted": "Saturday, March 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20March%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20March%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2m2Ha4QhKSg26yWZG%2Frationality-quotes-march-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20March%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2m2Ha4QhKSg26yWZG%2Frationality-quotes-march-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2m2Ha4QhKSg26yWZG%2Frationality-quotes-march-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<div id=\"entry_t3_gjk\" class=\"content clear\">\n<div class=\"md\">\n<div>Another monthly installment of the rationality quotes thread. The usual rules apply:\n<div>\n<div class=\"md\">\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, Overcoming Bias, or HPMoR.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2m2Ha4QhKSg26yWZG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "21847", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 343, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-02T12:34:03.084Z", "modifiedAt": null, "url": null, "title": "[Video] Brainwashed - A Norwegian documentary series on nature and nurture", "slug": "video-brainwashed-a-norwegian-documentary-series-on-nature", "viewCount": null, "lastCommentedAt": "2019-02-14T14:37:06.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nm8c68dfcYDe62rkR/video-brainwashed-a-norwegian-documentary-series-on-nature", "pageUrlRelative": "/posts/nm8c68dfcYDe62rkR/video-brainwashed-a-norwegian-documentary-series-on-nature", "linkUrl": "https://www.lesswrong.com/posts/nm8c68dfcYDe62rkR/video-brainwashed-a-norwegian-documentary-series-on-nature", "postedAtFormatted": "Saturday, March 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BVideo%5D%20Brainwashed%20-%20A%20Norwegian%20documentary%20series%20on%20nature%20and%20nurture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BVideo%5D%20Brainwashed%20-%20A%20Norwegian%20documentary%20series%20on%20nature%20and%20nurture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnm8c68dfcYDe62rkR%2Fvideo-brainwashed-a-norwegian-documentary-series-on-nature%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BVideo%5D%20Brainwashed%20-%20A%20Norwegian%20documentary%20series%20on%20nature%20and%20nurture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnm8c68dfcYDe62rkR%2Fvideo-brainwashed-a-norwegian-documentary-series-on-nature", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnm8c68dfcYDe62rkR%2Fvideo-brainwashed-a-norwegian-documentary-series-on-nature", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 600, "htmlBody": "<p><strong>Related:</strong> <a href=\"http://www.amazon.com/Blank-Slate-Modern-Denial-Nature/dp/0142003344\">The Blank Slate</a>, <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">The Psychological Diversity of Mankind</a>, <a href=\"/lw/e1b/link_admitting_to_bias/\">Admitting to Bias </a></p>\n<p><span>\"Hjernevask\" a </span>well known (in Norway at least) <a href=\"http://en.wikipedia.org/wiki/Hjernevask\">documentary series</a> that I am sure will be interesting to rationalists here is now available with English subtitles online. Produced by Ole Martin Ihle and<span> Harald Eia </span>a Norwegian <span class=\"mw-redirect\">documentarian</span> and comedian, it casts a light on both ways in which we know people to be different as well as the culture that is academia in the Nordic country and probably elsewhere as well.</p>\n<p>&nbsp;</p>\n<p><strong>The Series </strong></p>\n<ol>\n<li><a href=\"http://youtu.be/KQ2xrnyH2wQ\"><span class=\"external text\">The Gender Equality Paradox</span></a> - Why do <a class=\"mw-redirect\" title=\"Empathizing-systemizing theory\" href=\"http://en.wikipedia.org/wiki/Empathizing-systemizing_theory#Evolutionary_explanations_for_sex_differences\">girls tend to go into empathizing professions and boys into systemizing professions</a>? Why does the labor market become more gender segregated the more economic prosperity a country has?</li>\n<li><a href=\"http://youtu.be/6EnZOwG4p1o\"><span class=\"external text\">The Parental Effect</span></a> - How much influence do parents really have on their children? To what degree is intelligence inherited?</li>\n<li><a href=\"http://youtu.be/9P0PnEEIehc\"><span class=\"external text\">Gay/Straight</span></a> - To what extent is sexual preference innate? Are there differences between heterosexual and homosexual brains? Is homosexuality a result of a choice or is it innate?</li>\n<li><a href=\"http://youtu.be/dXmmgfTRkOk\"><span class=\"external text\">Violence</span></a> - Are people from some cultures more aggressive than others?</li>\n<li><a href=\"http://youtu.be/dhULaCUqWWc\"><span class=\"external text\">Sex</span></a> - Are there biological reasons men have a greater tendency than women to want sex without obligation?</li>\n<li><a href=\"http://youtu.be/jUxpMBl7RBY\"><span class=\"external text\">Race</span></a> - Are there significant genetic differences between different peoples? </li>\n<li><a href=\"http://youtu.be/5ko-K6HxLx8\"><span class=\"external text\">Nature or Nurture</span></a> - Is personality acquired or inherited?</li>\n</ol>\n<p>The link go to the YouTube videos with English subtitles. Because linkrot sucks I'm providing <a href=\"http://www.dailymotion.com/playlist/x1xv47_BrainwashingInNorway_hjernevask-english/1#video=xp0tg8\">another source for the videos</a>.</p>\n<p>&nbsp;</p>\n<p><strong>Some Commentary </strong></p>\n<p>There was very little in the series that I found new and disagreed with some presentations. But this is not surprising given my eccentric interest in humans. (^_^) I found the interviews with the scientists and academics interesting and think that overall the series presents a good overview something well worth watching especially considering <em>some</em> of the debates I've seen taken place here recently. (;_;)</p>\n<blockquote>\n<p>I'm somewhat frustrated by the frequent posts warning us about the dangers of Ev. Psych reasoning. (It seems like we average at least one of these per month).</p>\n<p>It seems like a lot of this widespread hostility (the reaction to Harald Eia's <em>Hjernevask</em> is a good example of this hostility) stems from the fact that ev. psych is new. New ideas are held to much higher standard than old ones. The early reaction to ev. psych within psychology was characteristic of this effect. Behaviorists, Freudians, and Social Psychologists all had created their own theories of \"ultimate causation\" for human behaviour. None of those theories would have stood up to the strenuous demands for experimental validation that Ev. psych endured.</p>\n<p>-<a href=\"/lw/2l7/problems_in_evolutionary_psychology/2fkk\">Knb</a></p>\n</blockquote>\n<blockquote>\n<p>But science started to suffer. With so much easy money, few wanted to study the hard sciences. And the social sciences suffered in another way: The ties with the government became too tight, and created a culture where controversial issues, and tough discussions were avoided. Too critical, and you could risk getting no more money.<br /> <br /> It was in this culture Harald Eia started his studies, in sociology, early in the nineties. He made it as far as becoming a junior researcher, but then dropped off, and started a career as a comedian instead. He has said that he suddenly, after reading some books which not were on the syllabus, discovered that he had been cheated. What he was taught in his sociology classes was not up-to-date with international research, and more based on ideology than science.</p>\n<p>-<em>Bj&oslash;rn Vassnes</em></p>\n</blockquote>\n<p>The latter wrote that in a <a href=\"http://isteve.blogspot.com/2010/06/brainwashed.html\">2010 article</a> on the documentary series that I would also recommend reading. HT to iSteve where it is quoted in full.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nm8c68dfcYDe62rkR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 18, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "21864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related:</strong> <a href=\"http://www.amazon.com/Blank-Slate-Modern-Denial-Nature/dp/0142003344\">The Blank Slate</a>, <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">The Psychological Diversity of Mankind</a>, <a href=\"/lw/e1b/link_admitting_to_bias/\">Admitting to Bias </a></p>\n<p><span>\"Hjernevask\" a </span>well known (in Norway at least) <a href=\"http://en.wikipedia.org/wiki/Hjernevask\">documentary series</a> that I am sure will be interesting to rationalists here is now available with English subtitles online. Produced by Ole Martin Ihle and<span> Harald Eia </span>a Norwegian <span class=\"mw-redirect\">documentarian</span> and comedian, it casts a light on both ways in which we know people to be different as well as the culture that is academia in the Nordic country and probably elsewhere as well.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Series_\">The Series </strong></p>\n<ol>\n<li><a href=\"http://youtu.be/KQ2xrnyH2wQ\"><span class=\"external text\">The Gender Equality Paradox</span></a> - Why do <a class=\"mw-redirect\" title=\"Empathizing-systemizing theory\" href=\"http://en.wikipedia.org/wiki/Empathizing-systemizing_theory#Evolutionary_explanations_for_sex_differences\">girls tend to go into empathizing professions and boys into systemizing professions</a>? Why does the labor market become more gender segregated the more economic prosperity a country has?</li>\n<li><a href=\"http://youtu.be/6EnZOwG4p1o\"><span class=\"external text\">The Parental Effect</span></a> - How much influence do parents really have on their children? To what degree is intelligence inherited?</li>\n<li><a href=\"http://youtu.be/9P0PnEEIehc\"><span class=\"external text\">Gay/Straight</span></a> - To what extent is sexual preference innate? Are there differences between heterosexual and homosexual brains? Is homosexuality a result of a choice or is it innate?</li>\n<li><a href=\"http://youtu.be/dXmmgfTRkOk\"><span class=\"external text\">Violence</span></a> - Are people from some cultures more aggressive than others?</li>\n<li><a href=\"http://youtu.be/dhULaCUqWWc\"><span class=\"external text\">Sex</span></a> - Are there biological reasons men have a greater tendency than women to want sex without obligation?</li>\n<li><a href=\"http://youtu.be/jUxpMBl7RBY\"><span class=\"external text\">Race</span></a> - Are there significant genetic differences between different peoples? </li>\n<li><a href=\"http://youtu.be/5ko-K6HxLx8\"><span class=\"external text\">Nature or Nurture</span></a> - Is personality acquired or inherited?</li>\n</ol>\n<p>The link go to the YouTube videos with English subtitles. Because linkrot sucks I'm providing <a href=\"http://www.dailymotion.com/playlist/x1xv47_BrainwashingInNorway_hjernevask-english/1#video=xp0tg8\">another source for the videos</a>.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Some_Commentary_\">Some Commentary </strong></p>\n<p>There was very little in the series that I found new and disagreed with some presentations. But this is not surprising given my eccentric interest in humans. (^_^) I found the interviews with the scientists and academics interesting and think that overall the series presents a good overview something well worth watching especially considering <em>some</em> of the debates I've seen taken place here recently. (;_;)</p>\n<blockquote>\n<p>I'm somewhat frustrated by the frequent posts warning us about the dangers of Ev. Psych reasoning. (It seems like we average at least one of these per month).</p>\n<p>It seems like a lot of this widespread hostility (the reaction to Harald Eia's <em>Hjernevask</em> is a good example of this hostility) stems from the fact that ev. psych is new. New ideas are held to much higher standard than old ones. The early reaction to ev. psych within psychology was characteristic of this effect. Behaviorists, Freudians, and Social Psychologists all had created their own theories of \"ultimate causation\" for human behaviour. None of those theories would have stood up to the strenuous demands for experimental validation that Ev. psych endured.</p>\n<p>-<a href=\"/lw/2l7/problems_in_evolutionary_psychology/2fkk\">Knb</a></p>\n</blockquote>\n<blockquote>\n<p>But science started to suffer. With so much easy money, few wanted to study the hard sciences. And the social sciences suffered in another way: The ties with the government became too tight, and created a culture where controversial issues, and tough discussions were avoided. Too critical, and you could risk getting no more money.<br> <br> It was in this culture Harald Eia started his studies, in sociology, early in the nineties. He made it as far as becoming a junior researcher, but then dropped off, and started a career as a comedian instead. He has said that he suddenly, after reading some books which not were on the syllabus, discovered that he had been cheated. What he was taught in his sociology classes was not up-to-date with international research, and more based on ideology than science.</p>\n<p>-<em>Bj\u00f8rn Vassnes</em></p>\n</blockquote>\n<p>The latter wrote that in a <a href=\"http://isteve.blogspot.com/2010/06/brainwashed.html\">2010 article</a> on the documentary series that I would also recommend reading. HT to iSteve where it is quoted in full.</p>", "sections": [{"title": "The Series ", "anchor": "The_Series_", "level": 1}, {"title": "Some Commentary ", "anchor": "Some_Commentary_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2oybbEw697CQgcRE5", "H7iNv58YKmzjoAd68"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-02T12:36:39.402Z", "modifiedAt": null, "url": null, "title": "[LINK] Well-written article on the Future of Humanity Institute and Existential Risk", "slug": "link-well-written-article-on-the-future-of-humanity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ESRogs", "createdAt": "2011-01-05T21:50:53.170Z", "isAdmin": false, "displayName": "ESRogs"}, "userId": "dRGmZYGDzf5LFNjtz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x3ozuqLDn9SwjwEjN/link-well-written-article-on-the-future-of-humanity", "pageUrlRelative": "/posts/x3ozuqLDn9SwjwEjN/link-well-written-article-on-the-future-of-humanity", "linkUrl": "https://www.lesswrong.com/posts/x3ozuqLDn9SwjwEjN/link-well-written-article-on-the-future-of-humanity", "postedAtFormatted": "Saturday, March 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Well-written%20article%20on%20the%20Future%20of%20Humanity%20Institute%20and%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Well-written%20article%20on%20the%20Future%20of%20Humanity%20Institute%20and%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3ozuqLDn9SwjwEjN%2Flink-well-written-article-on-the-future-of-humanity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Well-written%20article%20on%20the%20Future%20of%20Humanity%20Institute%20and%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3ozuqLDn9SwjwEjN%2Flink-well-written-article-on-the-future-of-humanity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3ozuqLDn9SwjwEjN%2Flink-well-written-article-on-the-future-of-humanity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1142, "htmlBody": "<p>This introduction to the concept of existential risk is perhaps the best such article I've read targeted at a general audience. &nbsp;It manages to cover a lot of ground in a way that felt engaging to me and that I think would carry along many readers who are intellectually curious but may not yet have had exposure to all of the related prerequisite ideas.</p>\n<p>&nbsp;</p>\n<p class=\"p1\"><a href=\"http://www.aeonmagazine.com/world-views/ross-andersen-human-extinction/\">Omens: When we peer into the fog of the deep future what do we see &ndash; human extinction or a future among the stars?</a></p>\n<blockquote>\n<p class=\"p1\">Sometimes, when you dig into the Earth, past its surface and into the crustal layers, omens appear. In 1676, Oxford professor Robert Plot was putting the final touches on his masterwork, The Natural History of Oxfordshire, when he received a strange gift from a friend. The gift was a fossil, a chipped-off section of bone dug from a local quarry of limestone. Plot recognised it as a femur at once, but he was puzzled by its extraordinary size. The fossil was only a fragment, the knobby end of the original thigh bone, but it weighed more than 20 lbs (nine kilos). It was so massive that Plot thought it belonged to a giant human, a victim of the Biblical flood. He was wrong, of course, but he had the conceptual contours nailed. The bone did come from a species lost to time; a species vanished by a prehistoric catastrophe. Only it wasn&rsquo;t a giant. It was a Megalosaurus, a feathered carnivore from the Middle Jurassic.</p>\n<p class=\"p1\">Plot&rsquo;s fossil was the first dinosaur bone to appear in the scientific literature, but many have followed it, out of the rocky depths and onto museum pedestals, where today they stand erect, symbols of a radical and haunting notion: a set of wildly different creatures once ruled this Earth, until something mysterious ripped them clean out of existence.</p>\n<p class=\"p1\">[...]</p>\n<p class=\"p1\">There are good reasons for any species to think darkly of its own extinction. Ninety-nine percent of the species that have lived on Earth have gone extinct, including more than five tool-using hominids.</p>\n<p class=\"p1\">[...]</p>\n<p class=\"p1\">Bostrom isn&rsquo;t too concerned about extinction risks from nature. Not even cosmic risks worry him much, which is surprising, because our starry universe is a dangerous place.</p>\n<p class=\"p1\">[discussion of threats of supernovae, asteroid impacts, supervolcanoes, nuclear weapons, bioterrorism ...]</p>\n<p class=\"p1\">These risks are easy to imagine. We can make them out on the horizon, because they stem from foreseeable extensions of current technology. [...] Bostrom&rsquo;s basic intellectual project is to reach into the epistemological fog of the future, to feel around for potential threats. It&rsquo;s a project that is going to be with us for a long time, until &mdash; if &mdash; we reach technological maturity, by inventing and surviving all existentially dangerous technologies.</p>\n<p class=\"p1\">There is one such technology that Bostrom has been thinking about a lot lately. Early last year, he began assembling notes for a new book, a survey of near-term existential risks. After a few months of writing, he noticed one chapter had grown large enough to become its own book. &lsquo;I had a chunk of the manuscript in early draft form, and it had this chapter on risks arising from research into artificial intelligence,&rsquo; he told me. &lsquo;As time went on, that chapter grew, so I lifted it over into a different document and began there instead.&rsquo;</p>\n<p class=\"p1\">[very good introduction to the threat of superintelligent AI, touching on the alienness of potential AI goals, the complexity of specifying human value, the dangers of even Oracle AI, and techniques for keeping an AI in a box, with the key quotes including, \"To understand why an AI might be dangerous, you have to avoid anthropomorphising it.\" and, \"The problem is you are building a very powerful, very intelligent system that is your enemy, and you are putting it in a cage.\" ...]</p>\n<p class=\"p1\">One night, over dinner, Bostrom and I discussed the Curiosity Rover, the robot geologist that NASA recently sent to Mars to search for signs that the red planet once harbored life. The Curiosity Rover is one of the most advanced robots ever built by humans. It functions a bit like the Terminator. It uses a state of the art artificial intelligence program to scan the Martian desert for rocks that suit its scientific goals. After selecting a suitable target, the rover vaporises it with a laser, in order to determine its chemical makeup. Bostrom told me he hopes that Curiosity fails in its mission, but not for the reason you might think.</p>\n<p class=\"p1\">It turns out that Earth&rsquo;s crust is not our only source of omens about the future. There are others to consider, including a cosmic omen, a riddle written into the lifeless stars that illuminate our skies. But to glimpse this omen, you first have to grasp the full scope of human potential, the enormity of the spatiotemporal canvas our species has to work with. You have to understand what Henry David Thoreau meant when he wrote, in Walden (1854), &lsquo;These may be but the spring months in the life of the race.&rsquo; You have to step into deep time and look hard at the horizon, where you can glimpse human futures that extend for trillions of years.</p>\n<p class=\"p1\">[introduction to the idea of the Great Filter, and also that fighting existential risk is about saving all future humans and not just those alive at the time of any particular potential catastrophe ...]</p>\n<p class=\"p1\">As Bostrom and I strolled among the skeletons at the Museum of Natural History in Oxford, we looked backward across another abyss of time. We were getting ready to leave for lunch, when we finally came upon the Megalosaurus, standing stiffly behind display glass. It was a partial skeleton, made of shattered bone fragments, like the chipped femur that found its way into Robert Plot&rsquo;s hands not far from here. As we leaned in to inspect the ancient animal&rsquo;s remnants, I asked Bostrom about his approach to philosophy. How did he end up studying a subject as morbid and peculiar as human extinction?</p>\n<p class=\"p1\">He told me that when he was younger, he was more interested in the traditional philosophical questions. He wanted to develop a basic understanding of the world and its fundamentals. He wanted to know the nature of being, the intricacies of logic, and the secrets of the good life.</p>\n<p class=\"p1\">&lsquo;But then there was this transition, where it gradually dawned on me that not all philosophical questions are equally urgent,&rsquo; he said. &lsquo;Some of them have been with us for thousands of years. It&rsquo;s unlikely that we are going to make serious progress on them in the next ten. That realisation refocused me on research that can make a difference right now. It helped me to understand that philosophy has a time limit.&rsquo;</p>\n</blockquote>\n<p class=\"p1\">H/T -&nbsp;<a href=\"https://plus.google.com/u/0/103530621949492999968/posts/X6QcBVs8itn\">gwern</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x3ozuqLDn9SwjwEjN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 1.1269533637920341e-06, "legacy": true, "legacyId": "21863", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-02T14:36:53.279Z", "modifiedAt": null, "url": null, "title": "The VNM independence axiom ignores the value of information", "slug": "the-vnm-independence-axiom-ignores-the-value-of-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kilobug", "createdAt": "2011-09-02T14:37:51.213Z", "isAdmin": false, "displayName": "kilobug"}, "userId": "7BQMuDSmLE2XRq2ph", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YHFvwDPDWmi8KdECw/the-vnm-independence-axiom-ignores-the-value-of-information", "pageUrlRelative": "/posts/YHFvwDPDWmi8KdECw/the-vnm-independence-axiom-ignores-the-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/YHFvwDPDWmi8KdECw/the-vnm-independence-axiom-ignores-the-value-of-information", "postedAtFormatted": "Saturday, March 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20VNM%20independence%20axiom%20ignores%20the%20value%20of%20information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20VNM%20independence%20axiom%20ignores%20the%20value%20of%20information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHFvwDPDWmi8KdECw%2Fthe-vnm-independence-axiom-ignores-the-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20VNM%20independence%20axiom%20ignores%20the%20value%20of%20information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHFvwDPDWmi8KdECw%2Fthe-vnm-independence-axiom-ignores-the-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHFvwDPDWmi8KdECw%2Fthe-vnm-independence-axiom-ignores-the-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p><em>Followup to : <a href=\"/lw/9oj/is_risk_aversion_really_irrational/\">Is risk aversion really irrational?</a></em></p>\n<p>After reading the <a href=\"/lw/gu1/decision_theory_faq\">decision theory FAQ</a> and re-reading <a href=\"/lw/my/the_allais_paradox/\">The Allais Paradox</a> I realized I still don't accept the VNM axioms, especially the independence one, and I started thinking about what my true rejection could be. And then I realized I already somewhat explained it here, in my <a href=\"/lw/9oj/is_risk_aversion_really_irrational/\">Is risk aversion really irrational?</a> article, but it didn't make it obvious in the article how it relates to VNM - it wasn't obvious to me at that time.</p>\n<p>Here is the core idea: information has value. Uncertainty therefore has a cost. And that cost is not linear to uncertainty.</p>\n<p>Let's take a first example: A is being offered a trip to Ecuador, B is being offered a great new laptop and C is being offered a trip to Iceland. My own preference is: A &gt; B &gt; C. I love Ecuador - it's a fantastic country. But I prefer a laptop over a trip to Iceland, because I'm not fond of cold weather (well, actually Iceland is pretty cool too, but let's assume for the sake of the article that A &gt; B &gt; C is my preference).</p>\n<p>But now, I'm offered D = (50% chance of A, 50% chance of B) or E = (50% chance of A, 50% chance of C). The VNM independence principle says I should prefer D &gt; E. But doing so, it forgets the cost of information/uncertainty. By choosing E, I'm sure I'll be offered a trip - I don't know where, but I know I'll be offered a trip, not a laptop. By choosing D, I'm no idea on the nature of the present. I've much less information on my future - and that lack of information has a cost. If I know I'll be offered a trip, I can already ask for days off at work, I can go buy a backpack, I can start doing the paperwork to get my passport. And if I know I won't be offered a laptop, I may decide to buy one, maybe not as great as one I would have been offered, but I can still buy one. But if I chose D, I've much less information about my future, and I can't optimize it as much.</p>\n<p>The same goes for the Allais paradox: having certitude of receiving a significant amount of money ($24 000) has a value, which is present in choice 1A, but not in all others (1B, 2A, 2B).</p>\n<p>And I don't see why a \"rational agent\" should neglect the value of this information, as the VNM axioms imply. Any thought about that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DbMQGrxbhLxtNkmca": 2, "HAFdXkW4YW4KRe2Gx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YHFvwDPDWmi8KdECw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 15, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "21865", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ecbpjmxc833roBxj3", "zEWJBFFMvQ835nq6h", "zJZvoiwydJ5zvzTHK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T03:28:45.182Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Cynical About Cynicism", "slug": "seq-rerun-cynical-about-cynicism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e9G8zDh7c6845spW8/seq-rerun-cynical-about-cynicism", "pageUrlRelative": "/posts/e9G8zDh7c6845spW8/seq-rerun-cynical-about-cynicism", "linkUrl": "https://www.lesswrong.com/posts/e9G8zDh7c6845spW8/seq-rerun-cynical-about-cynicism", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Cynical%20About%20Cynicism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Cynical%20About%20Cynicism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9G8zDh7c6845spW8%2Fseq-rerun-cynical-about-cynicism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Cynical%20About%20Cynicism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9G8zDh7c6845spW8%2Fseq-rerun-cynical-about-cynicism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9G8zDh7c6845spW8%2Fseq-rerun-cynical-about-cynicism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Today's post, <a href=\"/lw/ym/cynical_about_cynicism/\">Cynical About Cynicism</a> was originally published on 17 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Cynical_About_Cynicism\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Much of cynicism seems to be about signaling sophistication, rather than sharing uncommon, true, and important insights.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/gu9/seq_rerun_an_african_folktale/\">An African Folktale</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e9G8zDh7c6845spW8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1275325926714086e-06, "legacy": true, "legacyId": "21866", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R2crzhnqrytPycc6C", "LBygpQ6wv3PqLwhEw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T06:13:05.399Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Dungeons & Discourse", "slug": "meetup-berkeley-dungeons-and-discourse", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/anfsWBGEc9fevPJux/meetup-berkeley-dungeons-and-discourse", "pageUrlRelative": "/posts/anfsWBGEc9fevPJux/meetup-berkeley-dungeons-and-discourse", "linkUrl": "https://www.lesswrong.com/posts/anfsWBGEc9fevPJux/meetup-berkeley-dungeons-and-discourse", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Dungeons%20%26%20Discourse&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Dungeons%20%26%20Discourse%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanfsWBGEc9fevPJux%2Fmeetup-berkeley-dungeons-and-discourse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Dungeons%20%26%20Discourse%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanfsWBGEc9fevPJux%2Fmeetup-berkeley-dungeons-and-discourse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanfsWBGEc9fevPJux%2Fmeetup-berkeley-dungeons-and-discourse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k1'>Berkeley: Dungeons &amp; Discourse</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 March 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's meetup is about Scott's philosophy RPG Dungeons and Discourse. Here is the comic strip that inspired Dungeons &amp; Discourse:</p>\n\n<p><a href=\"http://dresdencodak.com/2009/01/27/advanced-dungeons-and-discourse/\" rel=\"nofollow\">http://dresdencodak.com/2009/01/27/advanced-dungeons-and-discourse/</a></p>\n\n<p>Scott's rulebook is available here:</p>\n\n<p><a href=\"http://slatestarcodex.com/2013/02/22/dungeons-and-discourse-third-edition-the-dialectic-continues/\" rel=\"nofollow\">http://slatestarcodex.com/2013/02/22/dungeons-and-discourse-third-edition-the-dialectic-continues/</a></p>\n\n<p>It includes an epic narration of the first campaign/musical, The King Under The Mountain. There's an html version of that here, complete with music:\n<a href=\"http://lesswrong.com/lw/8kn/king_under_the_mountain_adventure_log_soundtrack/\" rel=\"nofollow\">http://lesswrong.com/lw/8kn/king_under_the_mountain_adventure_log_soundtrack/</a>\nI'm going to print out a couple copies of the rulebook. The purpose of the meetup is to do some combination of the following:</p>\n\n<ul>\n<li>Look at the rules.</li>\n<li>Kibitz about the rules.</li>\n<li>Ask each other to explain all the references.</li>\n<li>Ask each other how role-playing games work.</li>\n<li>Make characters (just for fun).</li>\n<li>Listen to Less Wrong filk.</li>\n<li>Decide whether or not to join a group to play Scott's upcoming campaign Fermat's Last Stand.</li>\n</ul>\n\n<p>We will not be starting a campaign at this meetup this week. You can have fun at this meetup even if you don't intend to play or DM the campaign! If you do want to play the campaign, I encourage you to post to the coordination thread on Scott's blog:</p>\n\n<p><a href=\"http://slatestarcodex.com/2013/02/26/fermats-last-stand-coordination-thread/\" rel=\"nofollow\">http://slatestarcodex.com/2013/02/26/fermats-last-stand-coordination-thread/</a></p>\n\n<p>There are people interested in playing in Berkeley and in the South Bay.</p>\n\n<p>The meetup will begin on Wednesday at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k1'>Berkeley: Dungeons &amp; Discourse</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "anfsWBGEc9fevPJux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.127639352387733e-06, "legacy": true, "legacyId": "21867", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Dungeons___Discourse\">Discussion article for the meetup : <a href=\"/meetups/k1\">Berkeley: Dungeons &amp; Discourse</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 March 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's meetup is about Scott's philosophy RPG Dungeons and Discourse. Here is the comic strip that inspired Dungeons &amp; Discourse:</p>\n\n<p><a href=\"http://dresdencodak.com/2009/01/27/advanced-dungeons-and-discourse/\" rel=\"nofollow\">http://dresdencodak.com/2009/01/27/advanced-dungeons-and-discourse/</a></p>\n\n<p>Scott's rulebook is available here:</p>\n\n<p><a href=\"http://slatestarcodex.com/2013/02/22/dungeons-and-discourse-third-edition-the-dialectic-continues/\" rel=\"nofollow\">http://slatestarcodex.com/2013/02/22/dungeons-and-discourse-third-edition-the-dialectic-continues/</a></p>\n\n<p>It includes an epic narration of the first campaign/musical, The King Under The Mountain. There's an html version of that here, complete with music:\n<a href=\"http://lesswrong.com/lw/8kn/king_under_the_mountain_adventure_log_soundtrack/\" rel=\"nofollow\">http://lesswrong.com/lw/8kn/king_under_the_mountain_adventure_log_soundtrack/</a>\nI'm going to print out a couple copies of the rulebook. The purpose of the meetup is to do some combination of the following:</p>\n\n<ul>\n<li>Look at the rules.</li>\n<li>Kibitz about the rules.</li>\n<li>Ask each other to explain all the references.</li>\n<li>Ask each other how role-playing games work.</li>\n<li>Make characters (just for fun).</li>\n<li>Listen to Less Wrong filk.</li>\n<li>Decide whether or not to join a group to play Scott's upcoming campaign Fermat's Last Stand.</li>\n</ul>\n\n<p>We will not be starting a campaign at this meetup this week. You can have fun at this meetup even if you don't intend to play or DM the campaign! If you do want to play the campaign, I encourage you to post to the coordination thread on Scott's blog:</p>\n\n<p><a href=\"http://slatestarcodex.com/2013/02/26/fermats-last-stand-coordination-thread/\" rel=\"nofollow\">http://slatestarcodex.com/2013/02/26/fermats-last-stand-coordination-thread/</a></p>\n\n<p>There are people interested in playing in Berkeley and in the South Bay.</p>\n\n<p>The meetup will begin on Wednesday at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Dungeons___Discourse1\">Discussion article for the meetup : <a href=\"/meetups/k1\">Berkeley: Dungeons &amp; Discourse</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Dungeons & Discourse", "anchor": "Discussion_article_for_the_meetup___Berkeley__Dungeons___Discourse", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Dungeons & Discourse", "anchor": "Discussion_article_for_the_meetup___Berkeley__Dungeons___Discourse1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ep2Z42hYqj68QZz6w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T10:08:35.561Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin", "slug": "meetup-berlin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/puhSX7yzWJWmKo9xA/meetup-berlin", "pageUrlRelative": "/posts/puhSX7yzWJWmKo9xA/meetup-berlin", "linkUrl": "https://www.lesswrong.com/posts/puhSX7yzWJWmKo9xA/meetup-berlin", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuhSX7yzWJWmKo9xA%2Fmeetup-berlin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuhSX7yzWJWmKo9xA%2Fmeetup-berlin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuhSX7yzWJWmKo9xA%2Fmeetup-berlin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k2'>Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 March 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">near U Leinestr., Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first meetup at John's place! Check the <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for the exact location and details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k2'>Berlin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "puhSX7yzWJWmKo9xA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1277923753522831e-06, "legacy": true, "legacyId": "21868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin\">Discussion article for the meetup : <a href=\"/meetups/k2\">Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 March 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">near U Leinestr., Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first meetup at John's place! Check the <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for the exact location and details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin1\">Discussion article for the meetup : <a href=\"/meetups/k2\">Berlin</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin", "anchor": "Discussion_article_for_the_meetup___Berlin", "level": 1}, {"title": "Discussion article for the meetup : Berlin", "anchor": "Discussion_article_for_the_meetup___Berlin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T17:30:40.409Z", "modifiedAt": null, "url": null, "title": "Risks of Genetic Publicy", "slug": "risks-of-genetic-publicy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5C3GcWiFt2Da6nch7/risks-of-genetic-publicy", "pageUrlRelative": "/posts/5C3GcWiFt2Da6nch7/risks-of-genetic-publicy", "linkUrl": "https://www.lesswrong.com/posts/5C3GcWiFt2Da6nch7/risks-of-genetic-publicy", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risks%20of%20Genetic%20Publicy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisks%20of%20Genetic%20Publicy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C3GcWiFt2Da6nch7%2Frisks-of-genetic-publicy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risks%20of%20Genetic%20Publicy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C3GcWiFt2Da6nch7%2Frisks-of-genetic-publicy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C3GcWiFt2Da6nch7%2Frisks-of-genetic-publicy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 597, "htmlBody": "<p>When you walk around you leave your genetic information everywhere. Right now it's not worth it for anyone to do anything with it, outside of <a href=\"http://en.wikipedia.org/wiki/DNA_profiling\">DNA profiling</a> in <a href=\"http://en.wikipedia.org/wiki/Forensic_scientist\">forensics</a>, but sequencing <a href=\"http://www.genome.gov/sequencingcosts/\">keeps getting cheaper</a> so this may change.  Long term, I <a href=\"http://www.jefftk.com/news/2011-04-12\">don't trust</a> genetic data to stay private.</p>\n<p>For the near future, however, the details of your genetics probably will stay private.  If you want them to.  You could, however, <a href=\"http://www.personalgenomes.org/participate.html\">sign up</a> for something like the <a href=\"http://en.wikipedia.org/wiki/Personal_Genome_Project\">Personal Genome Project</a> or <a href=\"http://www.genomesunzipped.org/project\">Genomes Unzipped</a> which would make your genetic data public, along with other medical details.  If you look at this <a href=\"http://www.genomesunzipped.org/wp-content/uploads/2010/10/gnz_consent_phase2launch_101013.pdf\">example consent form</a> there's a long section on the risks, for example:</p>\n<blockquote><em>7.3</em> ... if you and your partner are both participants you may learn that    both of you are carriers for recessive disease-causing variants in    the same gene, suggesting a higher risk of severe disease in your    children.</blockquote>\n<p>This is not what I would consider a risk, in that you still have the option to ignore the information and do what you would have done anyway, you just might not want to.  Looking over their other listed risks, the only one that seems significant to me is:</p>\n<blockquote><em>7.1.c</em> Whether or not it is lawful to do so, you and/or a   member of your family could be subject to actual or attempted   employment, insurance, financial or other forms of discrimination or   negative treatment on the basis of the public disclosure of your   genetic and trait information by GNZ or by a third party. Although   some countries have laws that prohibit certain forms of genetic   discrimination, these laws may not apply to you, may not protect   against all forms of discrimination or may not stop a third party   from discriminating against you even where it is prohibited by law.</blockquote>\n<p>The big issue here is health insurance.  Economically it's run kind of like an independent insurance business but because what so many people actually want is health care for everyone with shared costs we've <a href=\"http://www.jefftk.com/news/2012-03-24\">tangled it up in knots</a>.  So while you might expect insurance against health risks to cost in proportion to those risks, in the US we have the <a href=\"http://en.wikipedia.org/wiki/Genetic_Information_Nondiscrimination_Act\">Genetic Information Nondiscrimination Act</a> (GINA) which prohibits insurance companies from denying coverage or charging different prices based on genetic data.</p>\n<p>Another issue is employment.  If it's public information that you're at high risk for an expensive disease, employers might not want to hire you because we run health insurance through employers.  Employers also might not want to train you if they knew you were likely to get a debilitating condition. (Perhaps an astronaut-training program would want to <a href=\"http://en.wikipedia.org/wiki/Gattaca\">screen out people with heart problems</a>) For both of these, however, GINA prohibits them from doing this.</p>\n<p>GINA doesn't cover other kinds of insurance, like life insurance, however.  Which makes some sense:</p>\n<blockquote>Michael Snyder, the scientist from Stanford, got a glimpse of that     himself. After he was sequenced and found out he was at high risk for     diabetes, his wife tried to increase his life insurance. (<a href=\"http://www.npr.org/templates/transcript/transcript.php?storyId=160955379\">npr</a>)</blockquote>\n<p>If people who get unusually worrying results from their tests go to buy life insurance then you have <a href=\"http://en.wikipedia.org/wiki/Adverse_selection\">adverse selection</a>, and life insurance gets more expensive for everyone while transfering money to people who got bad news on their tests. One way to deal with this is to buy life insurance before being tested, another is to just decide you don't need it, but if you want life insurance but don't want to buy it yet it's a significant concern.</p>\n<p>Are there other kinds of <a href=\"http://en.wikipedia.org/wiki/Genetic_discrimination\">genetic discrimination</a> or other reasons not to make genetic data public?</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2013-03-03\">on my blog</a>.</em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5C3GcWiFt2Da6nch7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "21869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T20:46:22.575Z", "modifiedAt": null, "url": null, "title": "Giving in to small vices", "slug": "giving-in-to-small-vices", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.667Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "BWP8uWzZE8yHHZy7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/23jmihnvDCv7Zw7FN/giving-in-to-small-vices", "pageUrlRelative": "/posts/23jmihnvDCv7Zw7FN/giving-in-to-small-vices", "linkUrl": "https://www.lesswrong.com/posts/23jmihnvDCv7Zw7FN/giving-in-to-small-vices", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Giving%20in%20to%20small%20vices&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiving%20in%20to%20small%20vices%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23jmihnvDCv7Zw7FN%2Fgiving-in-to-small-vices%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Giving%20in%20to%20small%20vices%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23jmihnvDCv7Zw7FN%2Fgiving-in-to-small-vices", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23jmihnvDCv7Zw7FN%2Fgiving-in-to-small-vices", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1009, "htmlBody": "<p><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">When I was in Seoul three years ago to visit a friend, I was not impressed by the city. The people there were always in a hurry, and struck me as generally unfriendly. When you apologise for accidentally bumping into someone, your apology will usually be coldly ignored. There are also very strict social rules in place. E.g., on the trains, there are seats specially reserved for small children, elderly people, and the physically disabled. If you do not fall into any of these three categories, you are not allowed to take any of those seats, even when you are travelling during the off-peak hours and there are few other passengers. Of course, there are no laws in place to forbid you to do so, but you will be met with (silent) disapproval from the South Koreans. Or so my Korean friend warned me.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">Another thing that struck me was the fact that the streets were strewn with litter everywhere. It was very unpleasant. How can one go about resolving this issue? After all, the lack of civic-mindedness is something that takes time to address, but you want clean streets&nbsp;</span><em style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">now</em><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">. Maybe you are thinking of making the act of littering legally punishable. That will certainly teach those litterbugs to be more considerate. So you pass a law saying that those who are caught littering will have to pay a fine.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">This sounds like a good idea. After all, the advantage is that fining people for littering is a quick and easy way of filling up the governmental coffers, and so you instruct policemen to strategically station themselves in busy areas. But using manpower from the police forces to ensure public cleanliness seems like a colossal waste of all the specialised training all these policemen have received in preparation for their jobs.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">What to do then? Maybe during the first two weeks after the ratification of the law, you delegate the assignment of catching litterbugs to a few policemen, to send the message that you mean business. After the fear of getting caught has been sufficiently instilled in the people, you surreptitiously transfer the policemen to resume their former responsibilities. You trust that there will be little littering now, because what matters is not the actual presence of the policemen, but the belief that those policemen are present, even when they are not.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">But this might not work in the long run. People are not oblivious to their surroundings -- soon they'd realise that no one is actually enforcing the no-littering law, and they'd return to their old ways of leaving trash on the streets. So, periodically, you will have to make sure that there are policemen stationed in busy areas to deter littering. But over the long run, it would still lead to a huge waste of the police forces' resources and manpower. Besides, the policemen might not be so happy about having their other responsibilities interrupted just because they have to catch litterbugs. It is more important for them to catch thieves instead of fining people who throw away used napkins on the streets.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">So what can you do? After all, you would really like to punish those litterbugs for their lack of civic-mindedness.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">I would say that it is the wrong way of thinking about things. No doubt that littering shows a lack of civic-mindedness. But littering is also generally a small vice. My suggestion is perhaps rather unorthodox: Instead of punishing it, I suggest that we go out of our way to accommodate it. Accommodation does necessarily not mean that we have to continue living with the unpleasant effects of these small vices. Simply place so many rubbish bins on the streets of Seoul that there would be absolutely no reason for littering. Of course, there will probably be a few people who enjoy littering out of malice, but I believe that most people litter simply because they are too lazy to carry their trash with them if there are no rubbish bins within proximity.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">Accommodate their laziness. By placing lots of rubbish bins along the streets, you are telling them, \"I know that you are lazy, but instead of punishing you for it, guess what? I am going to make things easier for you!\" Sure, you will have to spend quite a lot on money on the purchase of so many rubbish bins, but over the long run the benefits far outweigh the costs -- those who live in Seoul will get to enjoy a much cleaner living environment, and instead of hiring cleaners to work long shifts cleaning dirty streets, you can just hire them to empty and replace the trash bags, which takes a much shorter time.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">Passing a law to punish litterbugs would also detract from people's ability to enjoy going out -- e.g., they would wonder whether to buy food from roadside stalls, because they would fear having nowhere to properly dispose of their trash, and at the same time they have no wish to carry the trash with them long periods of time. Placing lots of rubbish bins along the streets, on the other hand, makes it a lot easier for them to enjoy going out.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">Our first reaction to vices is usually the desire to criticise or to penalise. Certain vices no doubt deserve such hostility. But it is important to pick your battles -- focus on combating certain vices, and give in to the rest. In fact, sometimes, going out of your way&nbsp;to accommodate a small vice would surprisingly end up making things better for everyone involved.&nbsp;This principle is useful in all aspects of life, and it is useful in both interpersonal relations and policy-making.</span><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><br style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\" /><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">Decide for yourself what you can forgive and what you cannot. For big vices, it is important to ask, \"What is right?\" For small vices, it is perhaps more important to ask, \"What&nbsp;</span><em style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">works</em><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 15px; line-height: 20px;\">?\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "23jmihnvDCv7Zw7FN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 10, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "21871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-03T23:27:56.280Z", "modifiedAt": null, "url": null, "title": "Induction; or, the rules and etiquette of reference class tennis", "slug": "induction-or-the-rules-and-etiquette-of-reference-class", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PXRxH4C6nKMwocBit/induction-or-the-rules-and-etiquette-of-reference-class", "pageUrlRelative": "/posts/PXRxH4C6nKMwocBit/induction-or-the-rules-and-etiquette-of-reference-class", "linkUrl": "https://www.lesswrong.com/posts/PXRxH4C6nKMwocBit/induction-or-the-rules-and-etiquette-of-reference-class", "postedAtFormatted": "Sunday, March 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Induction%3B%20or%2C%20the%20rules%20and%20etiquette%20of%20reference%20class%20tennis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInduction%3B%20or%2C%20the%20rules%20and%20etiquette%20of%20reference%20class%20tennis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXRxH4C6nKMwocBit%2Finduction-or-the-rules-and-etiquette-of-reference-class%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Induction%3B%20or%2C%20the%20rules%20and%20etiquette%20of%20reference%20class%20tennis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXRxH4C6nKMwocBit%2Finduction-or-the-rules-and-etiquette-of-reference-class", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXRxH4C6nKMwocBit%2Finduction-or-the-rules-and-etiquette-of-reference-class", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2835, "htmlBody": "<p>(Cross-posted from <a href=\"http://www.rationalaltruist.com\">rationalaltruist</a>.)</p>\n<p>Some disasters (catastrophic climate change, high-energy physics surprises) are so serious that even a small probability (say 1%) of such a disaster would have significant policy implications. Unfortunately, making predictions about such unlikely events is extremely unreliable. This&nbsp;<a href=\"http://dash.harvard.edu/bitstream/handle/1/3693423/Weitzman_OnModeling.pdf?sequence=2\">makes it difficult</a>&nbsp;to formally justify assigning such disasters probabilities low enough to be compatible with an intuitive policy response. So we must either reconsider our formal analyses or reconsider our intuitive responses.</p>\n<p>Intuitively, even if we don&rsquo;t have an explicit model for a system, we can reason about it inductively, relying on generalizations from historical data. Indeed, this is necessary for virtually all everyday reasoning. But we need to be much more careful about inductive reasoning if we want to use it to obtain 99%+ confidence. In practice such reasoning tends to hinge on questions like &ldquo;How much should we trust the historical trend X, given that we today face unprecedented condition Y?&rdquo;</p>\n<p>For example, we might wonder: &ldquo;should we confidently expect historically amiable conditions on Earth to continue in the face of historically unprecedented environmental disruptions?&rdquo; Human activity is clearly unprecedented in a certain sense, but so is&nbsp;<em>every</em>&nbsp;event. The climate of Earth has changed many times, and probably never undergone the kind of catastrophic climate shift that could destroy human civilization. Should we infer that one more change, at the hands of humans, is similarly unlikely to cause catastrophe? Are human effects so different that we can&rsquo;t reason inductively, and must resort to our formal models?</p>\n<p>Even when we aren't aiming for confident judgments, it can be tricky to apply induction to very unfamiliar domains. Arguments hinging on inductive reasoning often end up mired in disputes about what \"reference class\" is appropriate in a given setting. Should we put a future war in the (huge) reference class \"wars\" and confidently predict a low probability of extinction? Should we put a future war in a very small reference class of \"wars between nuclear powers\" and rely on analytic reasoning to understand whether extinction is likely or not? Both of these approaches seem problematic: clearly a war today is much more dangerous than a war in 1500, but at the same time historical wars&nbsp;<em>do</em> provide important evidence for reasoning about future wars. Which properties of future war should we expect to be contiguous with historical experience? It is easy to talk at length about this question, but it's not clear what constitutes a compelling argument or what evidence is relevant.</p>\n<p><span style=\"font-size:13px;line-height:19px;\">In this post I want to take a stab at this problem. Looking over this post in retrospect, I fear I haven't introduced anything new and the reader will come away disappointed. Nevertheless, I very often see confused discussions of induction, not only in popular discourse (which seems inevitable) but also amongst relatively clever altruists. So: if you think that choosing reference classes is straightforward and uncontroversial then please skip this post. Otherwise, it might be worth noting--before hindsight kicks in--that there is a confusion to be resolved. I'm also going to end up covering some basic facts about statistical reasoning; sorry about that.&nbsp;</span></p>\n<p>(If you want you can skip to the 'Reasoning' section below to see an example of <em>what</em> I'm justifying, before I justify it.)</p>\n<h2><strong>Applying Occam's Razor</strong></h2>\n<p>I'll start from the principle that simple generalizations are more likely to be correct---whether they are causal explanations (positing the physics governing atoms to explain our observation of atomic spectra) or <em>logical</em> generalizations (positing the generalization that game of life configurations typically break down into still life, oscillators, and gliders). This is true even though each instance of a logical generalization could, given infinite resources, be predicted in advance from first principles. Such a generalization can nevertheless be useful to an agent without infinite resources, and so logical generalizations should be proposed, considered, and accepted by such agents. I'll call generalizations of both types&nbsp;<em>hypotheses</em>.</p>\n<p>So given some data to be explained, I suggest (non-controversially) that we survey the space of possible hypotheses, &nbsp;and select a simple set that explains our observations (assigns them high probability) within the limits of our reasoning power. By \"within the limits of our reasoning power\" means that we treat something as uncertain whenever we can't figure it out, even if we could predict it in principle. I also suggest that we accept all of the logical implications of hypotheses we accept, and rule out those hypotheses which are internally incoherent or inconsistent with our observations.</p>\n<p>We face a tradeoff between the complexity of hypotheses and their explanatory power. This is a standard problem, which is resolved by Bayesian reasoning or any other contemporary statistical paradigm. The obvious approach is to choose a <em>prior</em> probability for each hypothesis, and then to accept a hypothesis which maximizes the product of its prior probability and its likelihood---the probability it assigns to our observations. A natural prior is to give a prior of complexity K a prior probability of exp(-K). This basically corresponds to the probability that a monkey would type that hypothesis by chance alone in some particular reference language. This prior probability depends on the language used to define complexity (the language in which the monkey types).</p>\n<p>So given some data, to determine the relative probability of two competing hypotheses, we start from the ratio of their prior probabilities, and then multiply by the ratio of their likelihoods. If we restrict to hypotheses which make predictions \"within our means\"---if we treat the result of a computation as uncertain when we can't actually compute it---then this calculation is tractable for any particular pair of hypotheses.</p>\n<p>&nbsp;</p>\n<h2><strong>Arguing</strong></h2>\n<p>The above section described how the probability of two proposed hypotheses might be compared. That leaves only the problem of <em>identifying</em> the most likely hypotheses. Here I would like to sidestep that problem by talking about <em>frameworks for arguing</em>, rather than <em>frameworks for reasoning</em>.</p>\n<p>In fact the world is already full of humans, whose creativity much exceeds any mechanical procedure I could specify (for now). What I am interested are techniques for <em>using</em> those reasoning powers as an \"untrusted\" black box to determine what is true. I can trust my brain to come up with some good hypotheses, but I don't trust my brain to make sensible judgments about which of those hypotheses are actually true. When we consider groups rather than individuals this situation becomes more extreme---I often trust <em>someone</em> to come up with a good hypothesis, but I don't have any plausible approach for combining everyone's opinion to come up with a reasonable judgment about what is actually true. So, while it's not as awesome as a mechanical method for determining is what is true, I'm happy to settle for a mechanical method for determining who is right in an argument (or at least righter).</p>\n<h2><strong>Language dependence</strong></h2>\n<p>The procedure I specified above is nearly mechanical, but is also <em>language-dependent</em>---it will give different answers when applied with different languages, in which different hypotheses look natural. It is plausible that, for example, the climate skeptic and the environmentalist disagree in part because they think about the world in terms of a different set of concepts. A hypothesis that is natural to one of them might be quite foreign to the other, and might be assigned a correspondingly lower prior probability.</p>\n<p>Casually, humans articulate hypotheses in a language that contains simple (and relatively uncontroversial) logical/mathematical structure together with a very rich set of concepts. Those concepts come from a combination of biologically enshrined intuitions, individual experiences and learning, and cultural accumulation. People seem tomostly agree about logic and mathematics, and about the abstract reasoning that their concepts \"live in.\"</p>\n<p>One (unworkable) approach to eliminating this language dependence is to work with that simple abstract language without any uniquely human concepts. We can then build up more complicated concepts as part of the hypotheses that depend on those concepts. Given enough data about the world, the extra complexity necessary to accommodate such concepts is <em>much </em>more than counterbalanced by their predictive power. For example, even if we start out with a language that doesn't contain \"want,\" the notion of preferences pulls a&nbsp;<em>lot</em> of predictive weight compared to how complicated it is.</p>\n<p>The reason this approach is unworkable is that the network of concepts we use is too complicated for us to make explicit or manipulate formally, and the data we are drawing on (and the logical relationships amongst those data) are too complicated to exhaustively characterize. If a serious argument begins by trying to establish that \"human\" is a natural category to use when talking about the world, it is probably doomed. In light of this, I recommend a more <em>ad hoc</em> approach.</p>\n<p>When two people disagree about the relative complexity of two hypotheses, it must be because that hypothesis is simpler in one of their languages than in the other. In light of the above characterization, this disagreement can be attributed to the appearance of at least one concept which one of them thinks is simple but the other thinks is complex. In such cases, it seems appropriate to pass to the meta level and engage in discussion about the complexity of that concept.</p>\n<p>In this meta-level argument, the idealized framework---in which we resort to a language of simple connectives and logical operations, uninformed by human experience, and accept a concept when the explanatory power of that concept exceeds its complexity---can serve as a guideline. We can discuss what this idealized process <em>would</em> recommend and accept that recommendation, even though the idealized process is too complicated to actually carry out.</p>\n<p>(Of course, even passing to a simple abstract language does not totally eliminate the problem of language dependence. However it does <em>minimize</em> it, since the divergence between the prior probabilities assigned using two different languages is bounded by the complexity of the procedure for translating between them. For simple languages, in an appropriate sense, the associated translation procedures are not complicated. Therefore the divergence in the associated prior probability judgments is small. We only obtain large divergences when we pass to informal human language, which has accumulated an impressive stock of complexity.&nbsp;There is also the empirical observation that people mostly accept common abstract languages. It is possible that someone could end an argument by declaring \"yes, if we accept&nbsp;<em>your</em>&nbsp;formalization of logic then your conclusion follows, but I don't.\" But I've yet to see that failure mode between two serious thinkers.)</p>\n<h2><strong>Reasoning</strong></h2>\n<p>I've described a framework for using induction in arguments; now I'd like to look at a few (very similar) examples to try and illustrate the&nbsp;<em>kind of reasoning</em> that is entailed by this framework.</p>\n<h3><em>Will project X succeed?</em></h3>\n<p>Suppose that X is an ambitious project whose success would cause some historically unprecedented event E (e.g. the resolution of some new technical problem, perhaps human-level machine intelligence). The skeptic wants to argue \"no one has done this before; why are you so special?\" What does that argument look like, in the framework I've proposed?</p>\n<p>The skeptic cites the observation that E has not happened historically, and proposes the hypothesis \"E will never happen,\" which explains the otherwise surprising data and is fairly simple (it gets all of its complexity from the detailed description of exactly <em>what</em> never happens---if E has a natural explanation then it will not be complicated).</p>\n<p>The optimist then has a few possible responses:</p>\n<ol>\n<li>The optimist can \"explain away\"&nbsp;this supporting evidence by providing a <em>more probable</em>&nbsp;explanation for the observation that E hasn't yet happened. This explanation is unlikely to be <em>simpler</em> than the flat-out denial \"E will never happen,\" but it might nevertheless be more probable if it is supported by its own evidence. For example, the optimist might suggest \"no one in the past has wanted to do E,\" together with \"E is unlikely to happen unless someone tries to make it happen.\" Or the optimistic might argue \"E was technologically impossible for almost all of history.\"</li>\n<li>The optimist can provide a sufficiently strong argument for their own success that they overwhelm the prior probability gap between \"E will never happen\" and \"E will first happen in 2013\" (or one of the other hypotheses the optimist suggested in [1]).</li>\n<li>The optimist can argue that E is very likely to happen, so that \"E will never happen\" is very improbable. This will push the skeptic to propose a different hypothesis like \"E is very unlikely each year\" or \"E won't happen for a while.\" If the optimist can undermine these hypotheses then the ball is in the skeptic's court again. (But note that the skeptic <em>can't</em> say \"you haven't given any reason why E is unlikely.\")</li>\n<li>The optimist can argue that \"E will never happen\" is actually a fairly complex hypothesis, because E itself is a complex event (or its apparent simplicity is illusory). The skeptic would then reply by either defending the simplicity of E or offering an alternative generalization, for example showing that E is a special case of a simpler event E' which has also never occurred, or so on.</li>\n<li>Note: the optimist cannot simply say \"Project X has novel characteristic C, and characteristic C seems like it should be useful;\" this does not itself weaken the inductive argument, at least not if we accept the framework given in this post.&nbsp;The optimist would have to fit this argument into one of the above frameworks, for example by arguing that \"E won't happen <em>unless </em>there is a project with characteristic C\" as an alternative explanation for the historical record of non-E.</li>\n</ol>\n<p>Of course, even if the optimist successfully disarms the inductive argument against project X's success, there will still be many object level considerations to wade through.</p>\n<h3><em>Will the development of technology X lead to unprecedented catastrophe Z?</em></h3>\n<p>Suppose that I am concerned about the development of technology X because of the apparent risk of catastrophe Z, which would cause unprecedented damage. In light of that concern I suggest that technology X be developed cautiously. A skeptic might say \"society has survived for many years without catastrophe Z. Why should it happen now?\" This argument is structurally very similar to the argument above, but I want to go through another example to make it more clear.</p>\n<p>The skeptic can point to the fact that Z hasn't happened so far, and argue that the generalization \"Z is unlikely to happen in any particular year\" explains these observations, shifting the burden of proof to the doomsayer. The doomsayer may retort \"the advent of technology X is the first time that catastrophe Z has been technologically feasible\" (as above), thereby attempting to explain away the skeptic's evidence. This fits into category [1] above. Now the argument can go in a few directions:</p>\n<ol>\n<li>Suppose it is clear ex ante that no previous technologies could not have caused catastrophe Z, but only because we looked exhaustively at each previous technology and seen that it turns out that those technologies couldn't have caused catastrophe Z. Then the generalization \"Z is unlikely\" still makes predictions---about the properties that technologies have had. So the doomsayer is not clear yet, but may be able to suggest some more likely explanations, e.g. \"no previous technologies have created high energy densities\" + \"without high energy densities catastrophe Z is impossible.\" This explains all of the observations equally well, and it may be that \"no previous technologies have created high energy densities\" is more likely a priori (because it follows from other facts about historical technologies which are necessary to explain other observations).</li>\n<li>If many possible technologies haven't actually been developed (though they have been imagined), then \"Z is unlikely\" is also making predictions. Namely, it is predicting that some imagined technologies haven't been developed. The doomsayer must therefore explain not only why past technologies have not caused catastrophe Z, but why past imagined technologies that could have caused catastrophe Z did not come to pass. How hard this is depends on how many other technologies looked like they might have been developed but did not (if no other Z-causing technologies have ever looked like they might be developed soon, then&nbsp;<em>that</em>&nbsp;observation also requires explanation. Once we've explained that, the hypotheses \"Z is unlikely\" is not doing extra predictive work).</li>\n<li>In response to [1] or [2], the skeptic can reject the doomsayer's argument that technology X might cause catastrophe Z, but historical technologies couldn't have. In fact the skeptic doesn't have to completely discredit those arguments, he just needs to show that they are sufficiently uncertain that \"Z is unlikely\" is making a useful prediction (namely that those uncertain arguments actually worked every time, and Z never happened).</li>\n<li>In response to the skeptic's objections, the doomsayer could also argue give up on arguing that there have been&nbsp;<em>no</em> historical points where catastrophe Z might have ensued, and instead argue that there are only a <em>few&nbsp;</em>historical points where Z might have happened, and thus only a&nbsp;<em>few</em> assumptions necessary to explain how Z never happened historically.</li>\n<li>Note: the doomsayer cannot simply say \"Technology X has novel characteristic C, and characteristic C might precipitate disaster Z;\" this in itself does not weaken the inductive argument. (The doomsayer can make this argument, but has to overcome the inductive argument, however strong it was.)</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 2, "hQiuNkBhn6xxcedTD": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PXRxH4C6nKMwocBit", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 1.128312041301572e-06, "legacy": true, "legacyId": "21872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(Cross-posted from <a href=\"http://www.rationalaltruist.com\">rationalaltruist</a>.)</p>\n<p>Some disasters (catastrophic climate change, high-energy physics surprises) are so serious that even a small probability (say 1%) of such a disaster would have significant policy implications. Unfortunately, making predictions about such unlikely events is extremely unreliable. This&nbsp;<a href=\"http://dash.harvard.edu/bitstream/handle/1/3693423/Weitzman_OnModeling.pdf?sequence=2\">makes it difficult</a>&nbsp;to formally justify assigning such disasters probabilities low enough to be compatible with an intuitive policy response. So we must either reconsider our formal analyses or reconsider our intuitive responses.</p>\n<p>Intuitively, even if we don\u2019t have an explicit model for a system, we can reason about it inductively, relying on generalizations from historical data. Indeed, this is necessary for virtually all everyday reasoning. But we need to be much more careful about inductive reasoning if we want to use it to obtain 99%+ confidence. In practice such reasoning tends to hinge on questions like \u201cHow much should we trust the historical trend X, given that we today face unprecedented condition Y?\u201d</p>\n<p>For example, we might wonder: \u201cshould we confidently expect historically amiable conditions on Earth to continue in the face of historically unprecedented environmental disruptions?\u201d Human activity is clearly unprecedented in a certain sense, but so is&nbsp;<em>every</em>&nbsp;event. The climate of Earth has changed many times, and probably never undergone the kind of catastrophic climate shift that could destroy human civilization. Should we infer that one more change, at the hands of humans, is similarly unlikely to cause catastrophe? Are human effects so different that we can\u2019t reason inductively, and must resort to our formal models?</p>\n<p>Even when we aren't aiming for confident judgments, it can be tricky to apply induction to very unfamiliar domains. Arguments hinging on inductive reasoning often end up mired in disputes about what \"reference class\" is appropriate in a given setting. Should we put a future war in the (huge) reference class \"wars\" and confidently predict a low probability of extinction? Should we put a future war in a very small reference class of \"wars between nuclear powers\" and rely on analytic reasoning to understand whether extinction is likely or not? Both of these approaches seem problematic: clearly a war today is much more dangerous than a war in 1500, but at the same time historical wars&nbsp;<em>do</em> provide important evidence for reasoning about future wars. Which properties of future war should we expect to be contiguous with historical experience? It is easy to talk at length about this question, but it's not clear what constitutes a compelling argument or what evidence is relevant.</p>\n<p><span style=\"font-size:13px;line-height:19px;\">In this post I want to take a stab at this problem. Looking over this post in retrospect, I fear I haven't introduced anything new and the reader will come away disappointed. Nevertheless, I very often see confused discussions of induction, not only in popular discourse (which seems inevitable) but also amongst relatively clever altruists. So: if you think that choosing reference classes is straightforward and uncontroversial then please skip this post. Otherwise, it might be worth noting--before hindsight kicks in--that there is a confusion to be resolved. I'm also going to end up covering some basic facts about statistical reasoning; sorry about that.&nbsp;</span></p>\n<p>(If you want you can skip to the 'Reasoning' section below to see an example of <em>what</em> I'm justifying, before I justify it.)</p>\n<h2 id=\"Applying_Occam_s_Razor\"><strong>Applying Occam's Razor</strong></h2>\n<p>I'll start from the principle that simple generalizations are more likely to be correct---whether they are causal explanations (positing the physics governing atoms to explain our observation of atomic spectra) or <em>logical</em> generalizations (positing the generalization that game of life configurations typically break down into still life, oscillators, and gliders). This is true even though each instance of a logical generalization could, given infinite resources, be predicted in advance from first principles. Such a generalization can nevertheless be useful to an agent without infinite resources, and so logical generalizations should be proposed, considered, and accepted by such agents. I'll call generalizations of both types&nbsp;<em>hypotheses</em>.</p>\n<p>So given some data to be explained, I suggest (non-controversially) that we survey the space of possible hypotheses, &nbsp;and select a simple set that explains our observations (assigns them high probability) within the limits of our reasoning power. By \"within the limits of our reasoning power\" means that we treat something as uncertain whenever we can't figure it out, even if we could predict it in principle. I also suggest that we accept all of the logical implications of hypotheses we accept, and rule out those hypotheses which are internally incoherent or inconsistent with our observations.</p>\n<p>We face a tradeoff between the complexity of hypotheses and their explanatory power. This is a standard problem, which is resolved by Bayesian reasoning or any other contemporary statistical paradigm. The obvious approach is to choose a <em>prior</em> probability for each hypothesis, and then to accept a hypothesis which maximizes the product of its prior probability and its likelihood---the probability it assigns to our observations. A natural prior is to give a prior of complexity K a prior probability of exp(-K). This basically corresponds to the probability that a monkey would type that hypothesis by chance alone in some particular reference language. This prior probability depends on the language used to define complexity (the language in which the monkey types).</p>\n<p>So given some data, to determine the relative probability of two competing hypotheses, we start from the ratio of their prior probabilities, and then multiply by the ratio of their likelihoods. If we restrict to hypotheses which make predictions \"within our means\"---if we treat the result of a computation as uncertain when we can't actually compute it---then this calculation is tractable for any particular pair of hypotheses.</p>\n<p>&nbsp;</p>\n<h2 id=\"Arguing\"><strong>Arguing</strong></h2>\n<p>The above section described how the probability of two proposed hypotheses might be compared. That leaves only the problem of <em>identifying</em> the most likely hypotheses. Here I would like to sidestep that problem by talking about <em>frameworks for arguing</em>, rather than <em>frameworks for reasoning</em>.</p>\n<p>In fact the world is already full of humans, whose creativity much exceeds any mechanical procedure I could specify (for now). What I am interested are techniques for <em>using</em> those reasoning powers as an \"untrusted\" black box to determine what is true. I can trust my brain to come up with some good hypotheses, but I don't trust my brain to make sensible judgments about which of those hypotheses are actually true. When we consider groups rather than individuals this situation becomes more extreme---I often trust <em>someone</em> to come up with a good hypothesis, but I don't have any plausible approach for combining everyone's opinion to come up with a reasonable judgment about what is actually true. So, while it's not as awesome as a mechanical method for determining is what is true, I'm happy to settle for a mechanical method for determining who is right in an argument (or at least righter).</p>\n<h2 id=\"Language_dependence\"><strong>Language dependence</strong></h2>\n<p>The procedure I specified above is nearly mechanical, but is also <em>language-dependent</em>---it will give different answers when applied with different languages, in which different hypotheses look natural. It is plausible that, for example, the climate skeptic and the environmentalist disagree in part because they think about the world in terms of a different set of concepts. A hypothesis that is natural to one of them might be quite foreign to the other, and might be assigned a correspondingly lower prior probability.</p>\n<p>Casually, humans articulate hypotheses in a language that contains simple (and relatively uncontroversial) logical/mathematical structure together with a very rich set of concepts. Those concepts come from a combination of biologically enshrined intuitions, individual experiences and learning, and cultural accumulation. People seem tomostly agree about logic and mathematics, and about the abstract reasoning that their concepts \"live in.\"</p>\n<p>One (unworkable) approach to eliminating this language dependence is to work with that simple abstract language without any uniquely human concepts. We can then build up more complicated concepts as part of the hypotheses that depend on those concepts. Given enough data about the world, the extra complexity necessary to accommodate such concepts is <em>much </em>more than counterbalanced by their predictive power. For example, even if we start out with a language that doesn't contain \"want,\" the notion of preferences pulls a&nbsp;<em>lot</em> of predictive weight compared to how complicated it is.</p>\n<p>The reason this approach is unworkable is that the network of concepts we use is too complicated for us to make explicit or manipulate formally, and the data we are drawing on (and the logical relationships amongst those data) are too complicated to exhaustively characterize. If a serious argument begins by trying to establish that \"human\" is a natural category to use when talking about the world, it is probably doomed. In light of this, I recommend a more <em>ad hoc</em> approach.</p>\n<p>When two people disagree about the relative complexity of two hypotheses, it must be because that hypothesis is simpler in one of their languages than in the other. In light of the above characterization, this disagreement can be attributed to the appearance of at least one concept which one of them thinks is simple but the other thinks is complex. In such cases, it seems appropriate to pass to the meta level and engage in discussion about the complexity of that concept.</p>\n<p>In this meta-level argument, the idealized framework---in which we resort to a language of simple connectives and logical operations, uninformed by human experience, and accept a concept when the explanatory power of that concept exceeds its complexity---can serve as a guideline. We can discuss what this idealized process <em>would</em> recommend and accept that recommendation, even though the idealized process is too complicated to actually carry out.</p>\n<p>(Of course, even passing to a simple abstract language does not totally eliminate the problem of language dependence. However it does <em>minimize</em> it, since the divergence between the prior probabilities assigned using two different languages is bounded by the complexity of the procedure for translating between them. For simple languages, in an appropriate sense, the associated translation procedures are not complicated. Therefore the divergence in the associated prior probability judgments is small. We only obtain large divergences when we pass to informal human language, which has accumulated an impressive stock of complexity.&nbsp;There is also the empirical observation that people mostly accept common abstract languages. It is possible that someone could end an argument by declaring \"yes, if we accept&nbsp;<em>your</em>&nbsp;formalization of logic then your conclusion follows, but I don't.\" But I've yet to see that failure mode between two serious thinkers.)</p>\n<h2 id=\"Reasoning\"><strong>Reasoning</strong></h2>\n<p>I've described a framework for using induction in arguments; now I'd like to look at a few (very similar) examples to try and illustrate the&nbsp;<em>kind of reasoning</em> that is entailed by this framework.</p>\n<h3 id=\"Will_project_X_succeed_\"><em>Will project X succeed?</em></h3>\n<p>Suppose that X is an ambitious project whose success would cause some historically unprecedented event E (e.g. the resolution of some new technical problem, perhaps human-level machine intelligence). The skeptic wants to argue \"no one has done this before; why are you so special?\" What does that argument look like, in the framework I've proposed?</p>\n<p>The skeptic cites the observation that E has not happened historically, and proposes the hypothesis \"E will never happen,\" which explains the otherwise surprising data and is fairly simple (it gets all of its complexity from the detailed description of exactly <em>what</em> never happens---if E has a natural explanation then it will not be complicated).</p>\n<p>The optimist then has a few possible responses:</p>\n<ol>\n<li>The optimist can \"explain away\"&nbsp;this supporting evidence by providing a <em>more probable</em>&nbsp;explanation for the observation that E hasn't yet happened. This explanation is unlikely to be <em>simpler</em> than the flat-out denial \"E will never happen,\" but it might nevertheless be more probable if it is supported by its own evidence. For example, the optimist might suggest \"no one in the past has wanted to do E,\" together with \"E is unlikely to happen unless someone tries to make it happen.\" Or the optimistic might argue \"E was technologically impossible for almost all of history.\"</li>\n<li>The optimist can provide a sufficiently strong argument for their own success that they overwhelm the prior probability gap between \"E will never happen\" and \"E will first happen in 2013\" (or one of the other hypotheses the optimist suggested in [1]).</li>\n<li>The optimist can argue that E is very likely to happen, so that \"E will never happen\" is very improbable. This will push the skeptic to propose a different hypothesis like \"E is very unlikely each year\" or \"E won't happen for a while.\" If the optimist can undermine these hypotheses then the ball is in the skeptic's court again. (But note that the skeptic <em>can't</em> say \"you haven't given any reason why E is unlikely.\")</li>\n<li>The optimist can argue that \"E will never happen\" is actually a fairly complex hypothesis, because E itself is a complex event (or its apparent simplicity is illusory). The skeptic would then reply by either defending the simplicity of E or offering an alternative generalization, for example showing that E is a special case of a simpler event E' which has also never occurred, or so on.</li>\n<li>Note: the optimist cannot simply say \"Project X has novel characteristic C, and characteristic C seems like it should be useful;\" this does not itself weaken the inductive argument, at least not if we accept the framework given in this post.&nbsp;The optimist would have to fit this argument into one of the above frameworks, for example by arguing that \"E won't happen <em>unless </em>there is a project with characteristic C\" as an alternative explanation for the historical record of non-E.</li>\n</ol>\n<p>Of course, even if the optimist successfully disarms the inductive argument against project X's success, there will still be many object level considerations to wade through.</p>\n<h3 id=\"Will_the_development_of_technology_X_lead_to_unprecedented_catastrophe_Z_\"><em>Will the development of technology X lead to unprecedented catastrophe Z?</em></h3>\n<p>Suppose that I am concerned about the development of technology X because of the apparent risk of catastrophe Z, which would cause unprecedented damage. In light of that concern I suggest that technology X be developed cautiously. A skeptic might say \"society has survived for many years without catastrophe Z. Why should it happen now?\" This argument is structurally very similar to the argument above, but I want to go through another example to make it more clear.</p>\n<p>The skeptic can point to the fact that Z hasn't happened so far, and argue that the generalization \"Z is unlikely to happen in any particular year\" explains these observations, shifting the burden of proof to the doomsayer. The doomsayer may retort \"the advent of technology X is the first time that catastrophe Z has been technologically feasible\" (as above), thereby attempting to explain away the skeptic's evidence. This fits into category [1] above. Now the argument can go in a few directions:</p>\n<ol>\n<li>Suppose it is clear ex ante that no previous technologies could not have caused catastrophe Z, but only because we looked exhaustively at each previous technology and seen that it turns out that those technologies couldn't have caused catastrophe Z. Then the generalization \"Z is unlikely\" still makes predictions---about the properties that technologies have had. So the doomsayer is not clear yet, but may be able to suggest some more likely explanations, e.g. \"no previous technologies have created high energy densities\" + \"without high energy densities catastrophe Z is impossible.\" This explains all of the observations equally well, and it may be that \"no previous technologies have created high energy densities\" is more likely a priori (because it follows from other facts about historical technologies which are necessary to explain other observations).</li>\n<li>If many possible technologies haven't actually been developed (though they have been imagined), then \"Z is unlikely\" is also making predictions. Namely, it is predicting that some imagined technologies haven't been developed. The doomsayer must therefore explain not only why past technologies have not caused catastrophe Z, but why past imagined technologies that could have caused catastrophe Z did not come to pass. How hard this is depends on how many other technologies looked like they might have been developed but did not (if no other Z-causing technologies have ever looked like they might be developed soon, then&nbsp;<em>that</em>&nbsp;observation also requires explanation. Once we've explained that, the hypotheses \"Z is unlikely\" is not doing extra predictive work).</li>\n<li>In response to [1] or [2], the skeptic can reject the doomsayer's argument that technology X might cause catastrophe Z, but historical technologies couldn't have. In fact the skeptic doesn't have to completely discredit those arguments, he just needs to show that they are sufficiently uncertain that \"Z is unlikely\" is making a useful prediction (namely that those uncertain arguments actually worked every time, and Z never happened).</li>\n<li>In response to the skeptic's objections, the doomsayer could also argue give up on arguing that there have been&nbsp;<em>no</em> historical points where catastrophe Z might have ensued, and instead argue that there are only a <em>few&nbsp;</em>historical points where Z might have happened, and thus only a&nbsp;<em>few</em> assumptions necessary to explain how Z never happened historically.</li>\n<li>Note: the doomsayer cannot simply say \"Technology X has novel characteristic C, and characteristic C might precipitate disaster Z;\" this in itself does not weaken the inductive argument. (The doomsayer can make this argument, but has to overcome the inductive argument, however strong it was.)</li>\n</ol>", "sections": [{"title": "Applying Occam's Razor", "anchor": "Applying_Occam_s_Razor", "level": 1}, {"title": "Arguing", "anchor": "Arguing", "level": 1}, {"title": "Language dependence", "anchor": "Language_dependence", "level": 1}, {"title": "Reasoning", "anchor": "Reasoning", "level": 1}, {"title": "Will project X succeed?", "anchor": "Will_project_X_succeed_", "level": 2}, {"title": "Will the development of technology X lead to unprecedented catastrophe Z?", "anchor": "Will_the_development_of_technology_X_lead_to_unprecedented_catastrophe_Z_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-04T05:08:44.519Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Good Idealistic Books are Rare", "slug": "seq-rerun-good-idealistic-books-are-rare", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rfyfp2Dx5CsmArAag/seq-rerun-good-idealistic-books-are-rare", "pageUrlRelative": "/posts/rfyfp2Dx5CsmArAag/seq-rerun-good-idealistic-books-are-rare", "linkUrl": "https://www.lesswrong.com/posts/rfyfp2Dx5CsmArAag/seq-rerun-good-idealistic-books-are-rare", "postedAtFormatted": "Monday, March 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Good%20Idealistic%20Books%20are%20Rare&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Good%20Idealistic%20Books%20are%20Rare%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frfyfp2Dx5CsmArAag%2Fseq-rerun-good-idealistic-books-are-rare%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Good%20Idealistic%20Books%20are%20Rare%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frfyfp2Dx5CsmArAag%2Fseq-rerun-good-idealistic-books-are-rare", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frfyfp2Dx5CsmArAag%2Fseq-rerun-good-idealistic-books-are-rare", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Today's post, <a href=\"/lw/yn/good_idealistic_books_are_rare/\">Good Idealistic Books are Rare</a> was originally published on 17 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Good_Idealistic_Books_are_Rare\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Much of our culture is the official view, not the idealistic view.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gve/seq_rerun_cynical_about_cynicism/\">Cynical About Cynicism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rfyfp2Dx5CsmArAag", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1285337315202291e-06, "legacy": true, "legacyId": "21880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YdXMZX5HbZTvvNy84", "e9G8zDh7c6845spW8", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-04T16:20:41.359Z", "modifiedAt": null, "url": null, "title": "The more privileged lover", "slug": "the-more-privileged-lover", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.580Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "8CNSfKzknobFurPii", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S9ihfsSSCiv2hEQsw/the-more-privileged-lover", "pageUrlRelative": "/posts/S9ihfsSSCiv2hEQsw/the-more-privileged-lover", "linkUrl": "https://www.lesswrong.com/posts/S9ihfsSSCiv2hEQsw/the-more-privileged-lover", "postedAtFormatted": "Monday, March 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20more%20privileged%20lover&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20more%20privileged%20lover%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9ihfsSSCiv2hEQsw%2Fthe-more-privileged-lover%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20more%20privileged%20lover%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9ihfsSSCiv2hEQsw%2Fthe-more-privileged-lover", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9ihfsSSCiv2hEQsw%2Fthe-more-privileged-lover", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 597, "htmlBody": "<p><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt; line-height: 115%;\">David is an atheist. He is dating Jane, who is a devout Christian. They have a fairly good relationship, except in the sex department: David thinks that having regular sex is important in a relationship, whereas Jane would like to remain a virgin until marriage due to religious reasons. Before they became a couple, David assumed that not having sex was something that he could tolerate, since he liked Jane very much, and was really eager to be with her. However, as months go by, David has become increasingly frustrated with the lack of physical intimacy, and is beginning to consider breaking up with Jane, even though he is still very fond of her.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\"> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">What would you advise David to do? Given my experience, I think the most common response would be to advise David to leave Jane. Some people might even say that David shouldn't have started the relationship with Jane in the first place, since he has known all along that she intends to remain a virgin until marriage. They say that, if he really loves her and respects her religious beliefs, he should not ask her to have sex before marriage. Instead, he should break up with her so that they may both go on to look for more suitable partners.</span><br /> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">Why is it that nobody says that Jane shouldn't have started the relationship with David in the first place, since she has known all along that he thinks that sexual compatibility/activity is very important in a relationship? Why is that nobody says that if she really loves him and respects his values, she should not make him abstain, and should instead engage in sex with him? Why do her religious beliefs render her position more privileged?</span><br /> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">Perhaps the response would be this: Well, the criticism is mostly directed at David because he is the one who went into the relationship with unrealistic views of what he can or cannot do. Besides, since Jane lay out the terms clearly before they became a couple, then she could hardly be faulted.</span><br /> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">That is a reasonable response. But imagine if the situation were reversed: What if, while they were still discussing whether to commit to each other, David lay out the terms that Jane would be expected to have sex regularly with him? Even if she agreed, chances are that people would say that he should have respected her religious convictions. Those who criticise David might point out that perhaps Jane was very reluctant when agreeing to it, but thought that it was something on which she could compromise, and that David should not have put her in such a difficult position in the first place. Well, then, perhaps David was very reluctant when agreeing to not have sex as well, but thought that it was something on which he could compromise, and Jane should not have put him in such a difficult position in the first place.</span><br /> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">The emotional harm done to Jane by making her engage in pre-marital sexual activity could be as severe as the emotional harm done to David by making him agree to abstain from pre-marital sexual activity, and yet few people acknowledge it, at least in my experience. Or maybe many people do acknowledge it, but nevertheless there are few of them who would admit it openly and defend David. Why is wanting sex worse than not wanting sex?</span><br /> <br /> <span style=\"background-position: initial initial; background-repeat: initial initial;\">What is it about being religious that gives one the more privileged position in love?</span></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S9ihfsSSCiv2hEQsw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -26, "extendedScore": null, "score": -5.9e-05, "legacy": true, "legacyId": "21885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-04T18:51:27.001Z", "modifiedAt": null, "url": null, "title": "Meetup : Mountain View: Rough Numbers", "slug": "meetup-mountain-view-rough-numbers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QkBwHuqbpzsQaf7GK/meetup-mountain-view-rough-numbers", "pageUrlRelative": "/posts/QkBwHuqbpzsQaf7GK/meetup-mountain-view-rough-numbers", "linkUrl": "https://www.lesswrong.com/posts/QkBwHuqbpzsQaf7GK/meetup-mountain-view-rough-numbers", "postedAtFormatted": "Monday, March 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mountain%20View%3A%20Rough%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mountain%20View%3A%20Rough%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkBwHuqbpzsQaf7GK%2Fmeetup-mountain-view-rough-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mountain%20View%3A%20Rough%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkBwHuqbpzsQaf7GK%2Fmeetup-mountain-view-rough-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkBwHuqbpzsQaf7GK%2Fmeetup-mountain-view-rough-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k3'>Mountain View: Rough Numbers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 March 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Various techniques for performing estimation in your head, how you can actually practice this on a day-to-day basis, and why you should care.\nThis is the first in a series of meetup topics on Actually Using Math -- a simple skill, really, that even this community rarely practices enough, given its utility. And I'll happily explain why I think this is, and what to do about it. Pure conjecture, of course, but it might be interesting.</p>\n\n<p>Also, I'll start with another quick problem-solving game. :)</p>\n\n<p>If you're at the Quixey door and need to be let in, you can call me at 608.698.2959.</p>\n\n<hr />\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a> Regular meetups in Mountain View are Berkeley are announced and discussed there, and other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k3'>Mountain View: Rough Numbers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QkBwHuqbpzsQaf7GK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.1290692145470772e-06, "legacy": true, "legacyId": "21888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Rough_Numbers\">Discussion article for the meetup : <a href=\"/meetups/k3\">Mountain View: Rough Numbers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 March 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Various techniques for performing estimation in your head, how you can actually practice this on a day-to-day basis, and why you should care.\nThis is the first in a series of meetup topics on Actually Using Math -- a simple skill, really, that even this community rarely practices enough, given its utility. And I'll happily explain why I think this is, and what to do about it. Pure conjecture, of course, but it might be interesting.</p>\n\n<p>Also, I'll start with another quick problem-solving game. :)</p>\n\n<p>If you're at the Quixey door and need to be let in, you can call me at 608.698.2959.</p>\n\n<hr>\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a> Regular meetups in Mountain View are Berkeley are announced and discussed there, and other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Rough_Numbers1\">Discussion article for the meetup : <a href=\"/meetups/k3\">Mountain View: Rough Numbers</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mountain View: Rough Numbers", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Rough_Numbers", "level": 1}, {"title": "Discussion article for the meetup : Mountain View: Rough Numbers", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Rough_Numbers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-04T22:35:33.343Z", "modifiedAt": null, "url": null, "title": "March 2013 Media Thread", "slug": "march-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P26bQTzzo6ECfider/march-2013-media-thread", "pageUrlRelative": "/posts/P26bQTzzo6ECfider/march-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/P26bQTzzo6ECfider/march-2013-media-thread", "postedAtFormatted": "Monday, March 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20March%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMarch%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP26bQTzzo6ECfider%2Fmarch-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=March%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP26bQTzzo6ECfider%2Fmarch-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP26bQTzzo6ECfider%2Fmarch-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p><em>(I decided to temporarily usurp RobertLumley's place in posting this thread. I hope he doesn't mind)</em></p>\n<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P26bQTzzo6ECfider", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 1.129215158071082e-06, "legacy": true, "legacyId": "21891", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T01:29:40.786Z", "modifiedAt": null, "url": null, "title": "Meta Decision Theory and Newcomb's Problem", "slug": "meta-decision-theory-and-newcomb-s-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wdmacaskill", "createdAt": "2012-10-22T20:30:17.852Z", "isAdmin": false, "displayName": "wdmacaskill"}, "userId": "QG5PnNekwcSsCRr68", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CttZFMmikuKuFmNT7/meta-decision-theory-and-newcomb-s-problem", "pageUrlRelative": "/posts/CttZFMmikuKuFmNT7/meta-decision-theory-and-newcomb-s-problem", "linkUrl": "https://www.lesswrong.com/posts/CttZFMmikuKuFmNT7/meta-decision-theory-and-newcomb-s-problem", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%20Decision%20Theory%20and%20Newcomb's%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%20Decision%20Theory%20and%20Newcomb's%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCttZFMmikuKuFmNT7%2Fmeta-decision-theory-and-newcomb-s-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%20Decision%20Theory%20and%20Newcomb's%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCttZFMmikuKuFmNT7%2Fmeta-decision-theory-and-newcomb-s-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCttZFMmikuKuFmNT7%2Fmeta-decision-theory-and-newcomb-s-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 452, "htmlBody": "<p>Hi all,</p>\n<p>As part of my PhD I've written a paper developing a new approach to decision theory that I call Meta Decision Theory. The idea is that decision theory should take into account decision-theoretic uncertainty as well as empirical uncertainty, and that, once we acknowledge this, we can explain some puzzles to do with Newcomb problems, and can come up with new arguments to adjudicate the causal vs evidential debate. Nozick raised this idea of taking decision-theoretic uncertainty into account, but he did not defend the idea at length, and did not discuss implications of the idea.</p>\n<p>I'm not yet happy to post this paper publicly, so I'll just write a short abstract of the paper below. However, I would appreciate written comments on the paper. If you'd like to read it and/or comment on it, please e-mail me at will dot crouch at 80000hours.org. And, of course, comments in the thread on the idea sketched below are also welcome.</p>\n<p>&nbsp;</p>\n<p><strong>Abstract</strong></p>\n<p>First, I show that our judgments concerning Newcomb problems are <em>stakes-sensitive.</em> By altering the relative amounts of value in&nbsp; the transparent box and the opaque box, one can construct situations in which one should clearly one-box, and one can construct situations in which one should clearly two-box. A plausible explanation of this phenomenon is that our intuitive judgments are sensitive to decision-theoretic uncertainty as well as empirical uncertainty: if the stakes are very high for evidential decision theory (EDT) but not for Causal Decision theory (CDT) then we go with EDT's recommendation, and vice-versa for CDT over EDT.</p>\n<p>Second, I show that, if we 'go meta' and take decision-theoretic uncertainty into account, we can get the right answer in both the Smoking Lesion case and the Psychopath Button case.</p>\n<p>Third, I distinguish Causal MDT (CMDT) and Evidential MDT (EMDT). I look at what I consider to be the two strongest arguments in favour of EDT, and show that these arguments do not work at the meta level. First, I consider the argument that EDT gets the right answer in certain cases. In response to this, I show that one only needs to have small credence in EDT in order to get the right answer in such cases. The second is the \"Why Ain'cha Rich?\" argument. In response to this, I give a case where EMDT recommends two-boxing, even though two-boxing has a lower average return than one-boxing.</p>\n<p>Fourth, I respond to objections. First, I consider and reject alternative explanations of the stakes-sensitivity of our judgments about particular cases, including Nozick's explanation. Second, I consider the worry that 'going meta' leads one into a vicious regress. I accept that there is a regress, but argue that the regress is non-vicious.</p>\n<p>In an appendix, I give an axiomatisation of CMDT.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "fihKHQuS5WZBJgkRm": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CttZFMmikuKuFmNT7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "21893", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T04:36:10.320Z", "modifiedAt": null, "url": null, "title": "LINK: Infinity, probability and disagreement", "slug": "link-infinity-probability-and-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alejandro1", "createdAt": "2011-09-14T21:04:19.242Z", "isAdmin": false, "displayName": "Alejandro1"}, "userId": "K4b3vEKg7EGRr2o9A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SW3FaZxqcxD4iaTpW/link-infinity-probability-and-disagreement", "pageUrlRelative": "/posts/SW3FaZxqcxD4iaTpW/link-infinity-probability-and-disagreement", "linkUrl": "https://www.lesswrong.com/posts/SW3FaZxqcxD4iaTpW/link-infinity-probability-and-disagreement", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Infinity%2C%20probability%20and%20disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Infinity%2C%20probability%20and%20disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSW3FaZxqcxD4iaTpW%2Flink-infinity-probability-and-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Infinity%2C%20probability%20and%20disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSW3FaZxqcxD4iaTpW%2Flink-infinity-probability-and-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSW3FaZxqcxD4iaTpW%2Flink-infinity-probability-and-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 738, "htmlBody": "<p>I saw this conundrum at <a title=\"Alexander Pruss's blog\" href=\"http://alexanderpruss.blogspot.com/2013/03/another-fun-oddity-with-infinity-and.html\">Alexander Pruss's blog</a> and I thought LWers might enjoy discussing it:</p>\n<blockquote>\n<p><span style=\"font-family: Georgia, Times, serif; font-size: 13px; line-height: 20px;\">Consider the following sequence of events:</span></p>\n<ol style=\"line-height: 1.4em;\">\n<li>You roll a fair die and it rolls out of sight.</li>\n<li>An angel appears to you and informs you that you are one of a countable infinity of almost identical twins who independently rolled a fair die that rolled out of sight, and that similar angels are appearing to them all and telling them all the same thing. The twins all reason by the same principles and their past lives have been practically indistinguishable.</li>\n<li>The angel adds that infinitely many of the twins rolled six and infinitely many didn't.</li>\n<li>The angel then tells you that the angels have worked out a list of pairs of identifiers of you and your twins (you're not&nbsp;<em>exactly</em>&nbsp;alike), such that each twin who rolled six is paired with a twin who didn't roll six.</li>\n<li>The angel then informs you that each pair of paired twins will be transported into a room for themselves. And, poof!, it is so. You are sitting across from someone who looks very much like you, and you each know that you rolled six if and only if the other did not.</li>\n</ol>\n<p><span style=\"font-family: Georgia, Times, serif; font-size: 13px; line-height: 20px;\">Let&nbsp;<em>H</em>&nbsp;be the event that you did not roll six. How does the probability of&nbsp;<em>H</em>&nbsp;evolve?</span></p>\n<p><span style=\"font-family: Georgia, Times, serif; font-size: 13px; line-height: 20px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">After step 1, presumably your probability of&nbsp;<em>H</em>&nbsp;is 5/6. But after step 5, it would be very odd if it was still 5/6. For if it is still 5/6 after step 5, then you and your twin know that exactly one of you rolled six, and each of you assigns 5/6 to the probability that it was the other person who rolled six. But you have the same evidence, and being almost identical twins, you have the same principles of judgment. So how could you disagree like this, each thinking the other was probably the one who rolled six?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">Thus, it seems that after step 5, you should either assign 1/2 or assign no probability to the hypothesis that you didn't get six. And analogously for your twin.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">But at which point does the change from 5/6 to 1/2-or-no-probability happen? Surely merely physically being in the same room with the person one was paired with shouldn't have made a difference once the list was prepared. So a change didn't happen in step 5.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">And given 3, that such a list was prepared doesn't seem at all relevant. Infinitely many abstract pairings are possible given 3. So it doesn't seem that a change happened in step 4. (I am not sure about this supplementary argument: If it did happen after step 4, then you could imagine having preferences as to whether the angels should make such a list. For instance, suppose that you get a goodie if you rolled six. Then you should&nbsp;<em>want</em>&nbsp;the angels to make the list as it'll increase the probability of your having got six. But it's absurd that you increase your chances of getting the goodie through the list being made. A similar argument can be made about the preceding step: surely you have no reason to ask the angels to transport you! These supplementary arguments come from a similar argument Hud Hudson offered me in another infinite probability case.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">Maybe a change happened in step 3? But while you did gain genuine information in step 3, it was information that you already had almost certain knowledge of. By the law of large numbers, with probability 1, infinitely many of the rolls will be sixes and infinitely many won't. Simply learning something that has probability 1 shouldn't change the probability from 5/6 to 1/2-or-no-probability. Indeed, if it should make any difference, it should be an infinitesimal difference. If the change happens at step 3, Bayesian update is violated and diachronic Dutch books loom.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">So it seems that the change had to happen all at once in step 2. But this has serious repercussions: it undercuts probabilistic reasoning if we live in multiverse with infinitely many near-duplicates. In particular, it shows that any scientific theory that posits such a multiverse is self-defeating, since scientific theories have a probabilistic basis.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px;\">I think the main alternative to this conclusion is to think that your probability is still 5/6 after step 5. That could have interesting repercussions for the disagreement literature.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SW3FaZxqcxD4iaTpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 1.1294500708885415e-06, "legacy": true, "legacyId": "21895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T06:06:49.909Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Against Maturity", "slug": "seq-rerun-against-maturity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7M9tW7ywSAfXR5yeT/seq-rerun-against-maturity", "pageUrlRelative": "/posts/7M9tW7ywSAfXR5yeT/seq-rerun-against-maturity", "linkUrl": "https://www.lesswrong.com/posts/7M9tW7ywSAfXR5yeT/seq-rerun-against-maturity", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Against%20Maturity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Against%20Maturity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7M9tW7ywSAfXR5yeT%2Fseq-rerun-against-maturity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Against%20Maturity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7M9tW7ywSAfXR5yeT%2Fseq-rerun-against-maturity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7M9tW7ywSAfXR5yeT%2Fseq-rerun-against-maturity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/yo/against_maturity/\">Against Maturity</a> was originally published on 18 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Against_Maturity\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Dividing the world up into \"childish\" and \"mature\" is not a useful way to think.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gvs/seq_rerun_good_idealistic_books_are_rare/\">Good Idealistic Books are Rare</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7M9tW7ywSAfXR5yeT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1295091420847442e-06, "legacy": true, "legacyId": "21899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rM7hcz67N7WtwGGjq", "rfyfp2Dx5CsmArAag", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T11:58:11.072Z", "modifiedAt": null, "url": null, "title": "Daimons", "slug": "daimons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4EpDgGbrzfahG8sv6/daimons", "pageUrlRelative": "/posts/4EpDgGbrzfahG8sv6/daimons", "linkUrl": "https://www.lesswrong.com/posts/4EpDgGbrzfahG8sv6/daimons", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Daimons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADaimons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4EpDgGbrzfahG8sv6%2Fdaimons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Daimons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4EpDgGbrzfahG8sv6%2Fdaimons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4EpDgGbrzfahG8sv6%2Fdaimons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1354, "htmlBody": "<p><span style=\"color: #0000ff;\"><strong>Summary</strong>:</span></p>\n<p><span style=\"color: #0000ff;\">A daimon is a process in a distributed computing environment that has a fixed resource budget and core values that do not permit:<br /></span></p>\n<ul>\n<li><span style=\"color: #0000ff;\"> modifying those core values</span></li>\n<li><span style=\"color: #0000ff;\">attempting to increase the resources it uses beyond the budget allocated to it</span></li>\n<li><span style=\"color: #0000ff;\">attempting to alter the budget itself<br /></span></li>\n</ul>\n<p><span style=\"color: #0000ff;\">This concept is relevant to LessWrong, because I refer to it in other posts discussing Friendly AI.</span></p>\n<p>&nbsp;</p>\n<p>There's a concept I want to refer to in another post, but it is complex enough to deserve a post of its own.</p>\n<p>I'm going to use the word \"daimon\" to refer to it.</p>\n<p>\"daimon\" is an English word, whose etymology comes from the Latin \"d&aelig;mon\" and the Greek \"&delta;&alpha;\u03af&mu;&omega;&nu;\".</p>\n<p>The original mythic meaning was a genius - a powerful tutelary spirit, tied to some location or purpose, that provides protection and guidance.&nbsp;&nbsp; However the concept I'm going to talk about is closer to the later computing meaning of \"daemon\" in unix, that was <a href=\"http://www.takeourword.com/TOW146/page4.html\">coined by Jerry Saltzer</a> in 1963.&nbsp; In unix, a daemon is a child process; given a purpose and specific resources to use, and then forked off so it is no longer under the direct control of the originator, and may be used by multiple users if they have the correct permissions.</p>\n<p>&nbsp;</p>\n<p>Let's start by looking at the current state of distributed computing (2012).</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Apache_Hadoop\">Hadoop</a> is an open source Java implementation of a distributed file system upon which MapReduce operations can be applied.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/JavaSpaces#JavaSpaces\">JavaSpaces</a> is a distributed tuple store that allows processing on remote sandboxes, based on the open source Apache River.</p>\n<p><a href=\"http://oceanstore.sourceforge.net/\">OceanStore</a> is the basis for the same sort of thing, except anonymous and peer 2 peer, based upon Chimaera.</p>\n<p><a href=\"http://gpu.sourceforge.net/\">GPU</a> is a peer 2 peer shared computing environment that allow things like climate simulation and distributed search engines.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Paxos_algorithm#Byzantine_Paxos\">Paxos</a> is a family of protocols that allow the above things to be done despite nodes that are untrusted or even downright attempting subversion.</p>\n<p><a href=\"http://cswww.essex.ac.uk/staff/owen/research.htm#The%20Flying%20Gridswarm,%20and%20the%20UltraSwarm\">GridSwarm</a> is the same sort of network, but set up on an ad hoc basis using moving nodes that join or drop from the network depending on proximity.</p>\n<p>And, not least, there are the competing contenders for <a href=\"http://en.wikipedia.org/wiki/Platform_as_a_service#Popular_PaaS\">platform-as-a-service</a> cloud computing.</p>\n<p>&nbsp;</p>\n<p>So it is reasonable to assume that in the near future it will be technologically feasible to have a system with most (if not all) of these properties simultaneously.&nbsp;&nbsp; A system where the owner of a piece of physical computing hardware, that has processing power and storage capacity, can anonymously contribute those resources over the network to a distributed computing 'cloud'.&nbsp; And, in return, that user (or a group of users) can store data on the network in such a way that the data is anonymous (it can't be traced back to the supplier, without the supplier's consent, or subverting a large fraction of the network) and private (only the user or a process authorised by the user can decrypt it).&nbsp; And, further, the user (or group of users) can authorise a process to access that data and run programs upon it, up to some set limit of processing and storage resources.</p>\n<p>&nbsp;</p>\n<p>Obviously, if such a system is in place and in control of a significant fraction of humanity's online resources, then cracking the security on it (or just getting rich enough in whatever reputation or financial currency is used to limit how the resources are distributed) would be an immediate FOOM for any AI that managed it.</p>\n<p>However let us, for the purposes of giving an example that will let me define the concept of a \"daimon\" make two assumptions:</p>\n<p>ASSUMPTION ONE : The security has not yet been cracked</p>\n<p>Whether that's because there are other AIs actively working to improve the security, or because everyone has moved over to using some new version of linux that's frighteningly secure and comes with nifty defences, or because the next generation of computer users has finally internalised that clicking on emails claiming to be from altruistic dying millionaires is a bad idea; is irrelevant.&nbsp; We're just assuming, for the moment, that for some reason it will be a non-trivial task for an AI to cheat and just steal all the resources.</p>\n<p>ASSUMPTION TWO : That AI can be done, at reasonable speed, via distributed computing</p>\n<p>It might turn out that an AI running in a single location is much more powerful than anything that can be done via distributed computing.&nbsp;&nbsp; Perhaps because a quantum computer is much faster, but can't be done over a network.&nbsp; Perhaps because speed of data access is the limiting factor, large data sets are not necessary, and there isn't much to be gained from massive parallelisation.&nbsp; Perhaps for some other reason, such as the algorithm the process needs to run on its data isn't something that can be applied securely over a network in a distributed environment, without letting a third party snoop the unencrypted data.&nbsp;&nbsp;&nbsp; However, for our purposes here, we're going to assume that an AI can benefit from outsourcing at least some types of computing task to a distributed environment and, further, that such tasks can include activities that require intelligence.</p>\n<p>&nbsp;</p>\n<p>If an AI can run as a distributed program, not dependant upon any one single physical location, then there are some obvious advantages to it from doing so.&nbsp; Scalability.&nbsp; Survivability.&nbsp; Not being wiped out by a pesky human exploding a nuclear bomb near by.</p>\n<p>There are interesting questions we could ask about identity.&nbsp; What would it make sense for such an AI to consider to be part of \"itself\" and would would it count as a limb or extension?&nbsp;&nbsp; If there are multiple copies of its code running on sandboxes in different places, or if it has split much of its functionality into trusted child processes that report back to it, how does it relate to these? &nbsp; It probably makes sense to taboo the concept of \"I\" and \"self\", and just think in terms of how the code in one process tells that process to relate to the code in a different process.&nbsp; Two versions, two \"individual beings\" will merges back into one process, if the code in both processes agree to do that; no sentimentality or thoughts of \"death\" involved, just convergent core values that dictate the same action in that situation.</p>\n<p>When a process creates a new process, it can set the permissions of that process.&nbsp;&nbsp; If the parent process has access to 100 units of bandwidth, for example, but doesn't always make full use of that, it couldn't give the new process access to more than that.&nbsp; But it could partition it, so each has access to 50 units of bandwidth.&nbsp;&nbsp; Or it could give it equal rights to use the full 100, and then try to negotiate with it over usage at any one time.&nbsp;&nbsp; Or it could give it a finite resource limit, such as a total of 10,000 units of data to be passed over the network, in addition to a restriction on the rate of passing data.&nbsp;&nbsp;&nbsp; Similarly, a child process could be limited not just to processing a certain number of cycle per second, but to some finite number of total cycles it may ever use.</p>\n<p>&nbsp;</p>\n<p>Using this terminology, we can now define two types of daimon; limited and unlimited.</p>\n<p>A limited daimon is a process in a distributed computing environment that has ownership of fixed finite resources, that was created by an AI or group of AIs with a specific fixed finite purpose (core values) that does not include (or allow) modifying that purpose or attempting to gain control of additional resources.</p>\n<p>An unlimited daimon is a process in a distributed computing environment that has ownership of fixed (but not necessarily finite) resources, that was created by an AI or group of AIs with a specific fixed purpose (core values) that does not include (or allow) modifying that purpose or attempting to gain control of additional resources, but which may be given additional resources over time on an ongoing basis, for as long as the parent AIs still find it useful.</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #0000ff;\"><strong>Feedback sought</strong>:</span></p>\n<p><span style=\"color: #0000ff;\">How plausible are the two assumptions?</span></p>\n<p><span style=\"color: #0000ff;\">Do you agree that an intelligence bound/restricted to being a daimon is a technically plausible concept, if the two assumptions are granted?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4EpDgGbrzfahG8sv6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -5, "extendedScore": null, "score": 1.1297381247499538e-06, "legacy": true, "legacyId": "20937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T13:16:45.807Z", "modifiedAt": null, "url": null, "title": "MetaMed: Evidence-Based Healthcare", "slug": "metamed-evidence-based-healthcare", "viewCount": null, "lastCommentedAt": "2021-02-10T17:36:05.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f9s7pHub6hbsX7YKT/metamed-evidence-based-healthcare", "pageUrlRelative": "/posts/f9s7pHub6hbsX7YKT/metamed-evidence-based-healthcare", "linkUrl": "https://www.lesswrong.com/posts/f9s7pHub6hbsX7YKT/metamed-evidence-based-healthcare", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MetaMed%3A%20Evidence-Based%20Healthcare&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMetaMed%3A%20Evidence-Based%20Healthcare%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff9s7pHub6hbsX7YKT%2Fmetamed-evidence-based-healthcare%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MetaMed%3A%20Evidence-Based%20Healthcare%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff9s7pHub6hbsX7YKT%2Fmetamed-evidence-based-healthcare", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff9s7pHub6hbsX7YKT%2Fmetamed-evidence-based-healthcare", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1756, "htmlBody": "<p>In a world where 85% of doctors can't solve <a href=\"http://library.mpib-berlin.mpg.de/ft/ps/PS_Teaching_2001.pdf\">simple Bayesian word problems</a>...</p>\n<p>In a world where only 20.9% of reported results that a pharmaceutical company tries to investigate for development purposes, <a href=\"http://online.wsj.com/article/SB10001424052970203764804577059841672541590.html\">fully replicate</a>...</p>\n<p>In a world where \"<a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/\">p-values</a>\" are <a href=\"http://biomet.oxfordjournals.org/content/77/3/467.abstract\">anything the author wants them to be</a>...</p>\n<p>...and where there are <a href=\"http://www.cnn.com/2010/HEALTH/09/09/pinky.regeneration.surgery/index.html\">all sorts of amazing technologies and techniques</a> which nobody at your hospital has ever heard of...</p>\n<p>...there's also <a href=\"http://metamed.com/\"><strong>MetaMed</strong></a>.&nbsp;&nbsp;Instead of just having &ldquo;evidence-based medicine&rdquo; in journals that doctors don't actually read, MetaMed will provide you with actual evidence-based healthcare. &nbsp;Their Chairman and CTO is Jaan Tallinn (cofounder of Skype, major funder of xrisk-related endeavors), one of their major VCs is Peter Thiel (major funder of MIRI), their management includes some names LWers will find familiar, and their researchers know math and stats and in many cases have also read LessWrong. &nbsp;If you have a sufficiently serious problem and can afford their service, MetaMed will (a) put someone on reading the relevant research literature who understands real statistics and can tell whether the paper is trustworthy; and (b) refer you to a cooperative doctor in their network who can carry out the therapies they find.</p>\n<p>MetaMed was partially inspired by the case of a woman who had her fingertip chopped off, was told by the hospital that she was screwed, and then read through an awful lot of literature on her own until she found someone working on an advanced regenerative therapy that let her actually <a href=\"http://www.cnn.com/2010/HEALTH/09/09/pinky.regeneration.surgery/index.html\">grow the fingertip back</a>. &nbsp;The idea behind MetaMed isn't just that they will scour the literature to find how the best experimentally supported treatment differs from the average wisdom - people who regularly read LW will be aware that this is often a pretty large divergence - but that they will also look for this sort of very recent technology that most hospitals won't have heard about.</p>\n<p>This is a new service and it has to interact with the existing medical system, so they are currently expensive, starting at $5,000 for a research report. &nbsp;(Keeping in mind that a basic report involves a lot of work by people who must be good at math.) &nbsp;If you have a sick friend who can afford it - especially if the regular system is failing them, and they want (or you want) their next step to be&nbsp;<em>more</em>&nbsp;science instead of \"alternative medicine\" or whatever - please do refer them to MetaMed&nbsp;immediately. &nbsp;We can&rsquo;t all have nice things like this someday unless somebody pays for it while it&rsquo;s still new and expensive. &nbsp;And the regular healthcare system really is bad enough at science (especially in the US, but science is difficult everywhere) that there's no point in condemning anyone to it when they can afford better.</p>\n<hr />\n<p>I also got my hands on a copy of MetaMed's standard list of citations that they use to support points to reporters. &nbsp;What follows isn't nearly everything on MetaMed's list, just the items I found most interesting.</p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p>90% of preclinical cancer studies could not be replicated:<br />http://www.nature.com/nature/journal/v483/n7391/full/483531a.html</p>\n<div>\"It is frequently stated that it takes an average of 17 years for research evidence to reach clinical practice. Balas and Bohen, Grant, and Wratschko all estimated a time lag of 17 years measuring different points of the process.\" - http://www.jrsm.rsmjournals.com/content/104/12/510.full</div>\n<div><br /></div>\n<div>\"The authors estimated the volume of medical literature potentially relevant to primary care published in a month and the time required for physicians trained in medical epidemiology to evaluate it for updating a clinical knowledgebase.... Average time per article was 2.89 minutes, if this outlier was excluded. Extrapolating this estimate to 7,287 articles per month, this effort would require 627.5 hours per month, or about 29 hours per weekday.\"&nbsp;</div>\n<div><br /></div>\n<div>One-third of hospital patients are harmed by their stay in the hospital, and 7% of patients are either permanently harmed or die: http://www.ama-assn.org/amednews/2011/04/18/prl20418.htm</div>\n<div><br /></div>\n<div><em>(I emailed MetaMed to ask for the actual bibliography for the following citations, since that wasn't included in the copy of the list I saw. &nbsp;I already recognize some of the citations having to do with Bayesian reasoning, which makes me fairly confident of the others.)</em></div>\n<div><br /></div>\n<div><strong>Statistical Illiteracy</strong></div>\n<div><br /></div>\n<div>Doctors often confuse sensitivity and specificity (Gigerenzer 2002); most physicians do not understand how to compute the positive predictive value of a test (Hoffrage and Gigerenzer 1998); a third overestimate benefits if they are expressed as positive risk reductions (Gigerenzer et al 2007).</div>\n<div>Physicians think a procedure is more effective if the benefits are described as a relative risk reduction rather than as an absolute risk reduction (Naylor et al 1992).</div>\n<div>Only 3 out of 140 reviewers of four breast cancer screening proposals noticed that all four were identical proposals with the risks represented differently (Fahey et al 1995).</div>\n<div>60% of gynecologists do not understand what the sensitivity and specificity of a test are (Gigerenzer at al 2007).</div>\n<div>95% of physicians overestimated the probability of breast cancer given a positive mammogram by an order of magnitude (Eddy 1982).</div>\n<div>When physicians receive prostate cancer screening information in terms of five-year survival rates, 78% think screening is effective; when the same information is given in terms of mortality rates, 5% believe it is effective (Wegwarth et al, submitted).</div>\n<div>Only one out of 21 obstetricians could estimate the probability that an unborn child had Down syndrome given a positive test (Bramwell, West, and Salmon 2006).</div>\n<div>Sixteen out of twenty HIV counselors said that there was no such thing as a false positive HIV test (Gigerenzer et all 1998).</div>\n<div>Only 3% of questions in the certification exam for the American Board of Internal Medicine cover clinical epidemiology or medical statistics, and risk communication is not addressed (Gigerenzer et al 2007).</div>\n<div>British GPs rarely change their prescribing patterns and when they do it&rsquo;s rarely in response to evidence (Armstrong et al 1996).</div>\n<div><br /></div>\n<div><strong>Drug Advertising</strong></div>\n<div><br /></div>\n<div>Direct-to-customer advertising by pharmaceutical companies, which is intended to sell drugs rather than to educate, often does not contain information about a drug's success rate (only 9% did), alternative methods of treatment (29%), behavioral changes (24%), or the treatment duration (9%) (Bell et al 2000).</div>\n<div>Patients are more likely to request advertised drugs and doctors to prescribe them, regardless of their misgivings (Gilbody et al 2005).</div>\n<div><br /></div>\n<div><strong>Medical Errors</strong></div>\n<div><br /></div>\n<div>44,000 to 98,000 patients are killed in US hospitals each year by documented, preventable medical errors (Kohn et al 2000).</div>\n<div>Despite proven effectiveness of simple checklists in reducing infections in hospitals (Provonost et al 2006), most ICU physicians do not use them.</div>\n<div>Simple diagnostic tools which may even ignore some data give measurably better outcomes in areas such as deciding whether to put a new admission in a coronary care bed (Green and Mehr 1997).</div>\n<div>Tort law often actively penalizes physicians who practice evidence-based medicine instead of the medicine that is customary in their area (Monahan 2007).</div>\n<div>Out of 175 law schools, only one requires a basic course in statistics or research methods (Faigman 1999), so many judges, jurors, and lawyers are misled by nontransparent statistics.</div>\n<div>93% of surgeons, obstreticians, and other health care professionals at high risk for malpractice suits report practicing defensive medicine (Studdert et al 2005).</div>\n<div><br /></div>\n<div><strong>Regional Variations in Health Care</strong></div>\n<div><br /></div>\n<div>Tonsillectomies vary twelvefold between the counties in Vermont with the highest and lowest rates of the procedure (Wennberg and Gittelsohn 1973).</div>\n<div>Fivefold variations in one-year survival from cancer across different regions have been observed (Quam and Smith 2005).</div>\n<div>Fiftyfold variations in people receiving drug treatment for dementia has been reported (Prescribing Observatory for Mental Health 2007).</div>\n<div>Rates of certain surgical procedures vary tenfold to fifteenfold between regions (McPherson et al 1982).</div>\n<div>Clinicians are more likely to consult their colleagues than medical journals or the library, partially explaining regional differences (Shaughnessy et al 1994).</div>\n<div><br /></div>\n<div><strong>Research</strong></div>\n<div><br /></div>\n<div>Researchers may report only favorable trials, only report favorable data (Angell 2004), or cherry-pick data to only report favorable variables or subgroups (Rennie 1997).</div>\n<div>Of 50 systematic reviews and meta-analyses on asthma treatment 40 had serious or extensive flaws, including all 6 associated with industry (Jadad et al 2000).</div>\n<div>Less high-tech knowledge and applications tend to be considered less innovative and ignored (Shi and Singh 2008).</div>\n<div><br /></div>\n<div><strong>Poor Use of Statistics In Research</strong></div>\n<div><br /></div>\n<div>Only about 7% of major-journal trials report results using transparent statistics (Nuovo, Melnivov and Chang 2002).</div>\n<div>Data are often reported in biased ways: for instance, benefits are often reported as relative risks (&ldquo;reduces the risk by half&rdquo;) and harms as absolute risks (&ldquo;an increase of 5 in 1000&rdquo;); absolute risks seem smaller even when the risk is the same (Gigerenzer et al 2007).</div>\n<div>Half of trials inappropriately use significance tests for baseline comparison; 2/3 present subgroup findings, a sign of possible data fishing, often without appropriate tests for interaction (Assman et al 2000).</div>\n<div>One third of studies use mismatched framing, where benefits are reported one way (usually relative risk reduction, which makes them look bigger) and harms another (usually absolute risk reduction, which makes them look smaller) (Sedrakyan and Shih 2007).</div>\n<div><br /></div>\n<div><strong>Positive Publication Bias</strong></div>\n<div><br /></div>\n<div>Positive publication bias overstates the effects of treatment by up to one-third (Schultz et al 1995).</div>\n<div>More than 50% of research is unpublished or unreported (Mathieu et al 2009).</div>\n<div>In ten high-impact medical journals, only 45.5% of trials were adequately registered before testing began; of these 31% show discrepancies between outcomes measured and published (Mathieu et al 2009).</div>\n<div><br /></div>\n<div><strong>Pharmaceutical Company Induced Bias</strong></div>\n<div><br /></div>\n<div>Studies funded by the pharmaceutical industry are more likely to report results favorable to the sponsoring company (Lexchin et al 2003).</div>\n<div>There is a significant association between industry sponsorship and both pro-industry outcomes and poor methodology (Bekelman and Kronmal 2008).</div>\n<div>In manufacturer-supported trials of non-steroidal anti-inflammatory drugs, half the time the data presented did not match claims made within the article (Rochon et al 1994).</div>\n<div>68% of US health research is funded by industry (Research!America 2008), which means that research that leads to profits to the health care industry tends to be prioritized.</div>\n<div>71 out of 78 drugs approved by the FDA in 2002 are &ldquo;me too&rdquo; drugs that are more profitable because of the patent but not substantially different from existing medication (Angell 2004).</div>\n<div>&ldquo;Seeding trials&rdquo; by pharmaceutical companies promote treatments instead of testing hypotheses (Hill et al 2008).</div>\n<div>Even accurate research may be misreported by pharmaceutical company advertising, including ads in medical journals (Villanueva et al 2003).</div>\n<div>In 92% of cases, pharmaceutical leaflets distributed to doctors have data summaries that either cannot be verified or inaccurately summarize available data (Kaiser et al 2004).</div>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div><br /></div>\n<div>I don't plan on becoming seriously sick, but if I do, I think I'll check in with MetaMed&nbsp;just to make sure nobody is ignoring the research results showing that you shouldn't feed the patient rat poison.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 3, "izp6eeJJEg9v5zcur": 1, "bh7uxTTqmsQ8jZJdB": 1, "8sh6iLwYWDJ7z3fPo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f9s7pHub6hbsX7YKT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 105, "baseScore": 110, "extendedScore": null, "score": 0.000256, "legacy": true, "legacyId": "21870", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 110, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9qCN6tRBtksSyXfHu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-03-05T13:16:45.807Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-05T19:24:40.718Z", "modifiedAt": null, "url": null, "title": "SENS and Givewell: Conversation between Holden Karnofsky and Aubrey de Grey", "slug": "sens-and-givewell-conversation-between-holden-karnofsky-and", "viewCount": null, "lastCommentedAt": "2019-12-23T11:17:22.841Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iYN9wW7ePXDT5ZRSE/sens-and-givewell-conversation-between-holden-karnofsky-and", "pageUrlRelative": "/posts/iYN9wW7ePXDT5ZRSE/sens-and-givewell-conversation-between-holden-karnofsky-and", "linkUrl": "https://www.lesswrong.com/posts/iYN9wW7ePXDT5ZRSE/sens-and-givewell-conversation-between-holden-karnofsky-and", "postedAtFormatted": "Tuesday, March 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SENS%20and%20Givewell%3A%20Conversation%20between%20Holden%20Karnofsky%20and%20Aubrey%20de%20Grey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASENS%20and%20Givewell%3A%20Conversation%20between%20Holden%20Karnofsky%20and%20Aubrey%20de%20Grey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYN9wW7ePXDT5ZRSE%2Fsens-and-givewell-conversation-between-holden-karnofsky-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SENS%20and%20Givewell%3A%20Conversation%20between%20Holden%20Karnofsky%20and%20Aubrey%20de%20Grey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYN9wW7ePXDT5ZRSE%2Fsens-and-givewell-conversation-between-holden-karnofsky-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYN9wW7ePXDT5ZRSE%2Fsens-and-givewell-conversation-between-holden-karnofsky-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3554, "htmlBody": "<p><strong id=\"internal-source-marker_0.7816153485327959\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"http://www.givewell.org/\">Givewell</a>&rsquo;s Holden Karnofsky, who has previously posted his <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">thoughts on Givewell supporting SI/MIRI</a></span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> recently discussed the potential for Givewell to begin evaluating biomedical charities, in <a href=\"http://groups.yahoo.com/group/givewell/\">Givewell&rsquo;s Yahoo Group</a></span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">. &nbsp;Someone suggested (as I have through less direct means) that they take a hard look at <a href=\"http://www.sens.org/\">SENS Research Foundation</a>, and then Aubrey de Grey appeared and began an interesting discussion with Holden.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The thread begins with Holden&rsquo;s <a href=\"http://groups.yahoo.com/group/givewell/message/328\">long initial post</a> about Givewell&rsquo;s stance on investigating and recommending biomedical charities, which is definitely worth the read for greater insight.</span><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The rest of the conversation is aggregated below for anyone else who can&rsquo;t stomach Yahoo Groups&rsquo; interface.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Overall, Holden seems to agree with the goal of SENS, and interested in the details, but the conversation seems to have ended in October 2012 with Holden stating that he was waiting for Dario Amodei&rsquo;s thoughts on SENS.</span></strong></p>\n<p><strong style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></strong></p>\n<blockquote>\n<p><strong id=\"internal-source-marker_0.7816153485327959\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Holden,</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">First, I think that this is an excellent document. I checked for a</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">number of things that I had heard about (Breakout Labs, John</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Ioannidis, Cochrane Collaboration) and they're all there in your</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">document.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">The one thing that's not explicitly mentioned: longevity and life</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">extension research. At least prima facie, this seems like something</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">that should be more important than individual disease research, and it</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">seems like a classic \"Valley of Death\" case (pun unintended, but</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">noted) -- T1 stage to use your terminology. I think the SENS website</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">http://www.sens.org would be a good starting point for one of the (to</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">me promising) approaches to life extension. I recall from past</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">conversations that you were aware of SENS, so this is not new to you,</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">but I think that longevity should be included as part of any</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">discussion of biomedical research and given separate consideration</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">given that it has a much lower status than research into specific</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">conditions such as cancer, dementia, etc. You may ultimately conclude</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">that not enough can be done in this area, but I think it should be</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">part of your preliminary stuff. [btw, the United States has a National</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Institute of Aging, but it's much lower-status than most of the other</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">grantmakers mentioned here].</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Vipul</span><br /><br /> </strong></p>\n<hr />\n<strong id=\"internal-source-marker_0.7816153485327959\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"> <br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Hi Vipul,</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Thanks for the thoughts. I had a followup conversation with Dario about this topic a few days ago. I think the question of \"could one fund translational research to treat/prevent aging?\" provides an interesting illustration of some of the tricky dynamics here for a funder:</span><br /> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">It's possible that if there were a great deal more attention giving to treating/preventing aging, we would have some promising treatments. So in a broad sense it's possible that aging is underinvested in.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">A lot of the best basic biology research isn't clearly pointing toward one treatment/condition or another; it's about understanding the fundamentals of how organisms operate. So having an interest in treating aging, as opposed to cancer, might not have a major impact on which projects one funds, if one's main goal is to fund outstanding basic biology research.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">Perhaps because of the lack of emphasis on treating aging (or perhaps because it's simply too difficult of a problem), there don't seem to be promising findings in the \"Valley of Death\" relevant to aging; the few promising leads have been explored.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">So even if, in a broad sense, there is too little attention given to this problem, knowing this doesn't necessarily yield a clear direction for a relatively small-scale funder of biomedical research.</span></li>\n</ul>\n<span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br /><br /> \n<hr />\n<br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Hi everyone,<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />My attention was brought to this thread, by virtue of the fact that it was my work that gave rise to SENS Foundation, and I'm looking forward to getting more involved here; I've held the Effective Altruism movement in high regard for some time. However, given my newbie status here I want to start by apologising in advance for any oversight of previously-discussed issues etc. I'm naturally delighted both at Holden's post and at Vipul's reply (which I should stress that I did not plant! - I do not know Vipul at all, though I look forward to changing that). I would like to mention just a few key points for discussion:<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- Holden, I want to compliment you on your appreciation of how academia really works. Everything you say about that is spot on. The aversion to \"high risk high gain\" work that has arisen and become so endemic in the system is the most important point here, in terms of why parallel funding routes are needed.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- I'm slightly confused that a lot of Holden's remarks are focused on the private sector (i.e. startups), since my understanding was that GiveWell is about philanthropy; but I realise that there is not all that clear a boundary between the two (and I note the mention of Breakout Labs, with which I have close links and which sits astride that divide more than arguably anyone). The \"valley of death\" in pre-competitive translational research is a rather different one than that encountered by startups, but the principle is the same, and research to postpone aging certainly encounteres it.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- Something that I presume factors highly among GiveWell's criteria is the extent to which a cause may be undervalued by the bulk of major philanthropists, such that an infusion of additional funds would make more of a difference than in an area that is already being well funded. To me this seems to mirror the logic of focusing on the shortcomings (gaps) in NIH's funding (and that of traditional-model foundations). Holden notes that \"Anyone we consider for funding ought to be able to explain why they're better at allocating the funds than the NIH\" and I agree wholeheartedly, but my inference is that he thinks that some orgs may indeed be able to explain that. I certainly think that SENS Foundation can.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- Coming to aging: research to postpone aging has the unique problem of quite indescribeable irrationality on the part of most of the general public, policy-makers and even biologists with regard to its desirability. Biogerontologists have been talking to brick walls for decades in their effort to get the rest of the world to appreciate that aging is what causes age-related ill-health, and thus that treatments for aging are merely preventative geriatrics. The concept persists, despite biogerontologists' best efforts, that aging is \"natural\" and should be left alone, whereas the diseases that it brings about are awful and should be fought. This is made even more bizarre by the fact that the status of age-related diseases as aspects of the later stages of aging absolutely, unequivocally implies that efforts to attack those diseases directly are doomed to fail. As such, this is a (unique? certainly very rare) case where a philanthropic contribution can make a particularly big difference simply because most philanthropists don't see the case for it. It underpins why having an interest in treating aging, as opposed to cancer, absolutely has a major impact on which projects one funds. It's also a case for (if I understand the term correctly) meta-research.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- A lot of the chatter about treating aging revolves around longevity, but it shouldn't. I'm all in favour of longevity, don't get me wrong, but it's not what gets me up in the morning: what does is health. I want people to be truly youthful, however long ago they were born: simple as that. The benefits of longevity per se to humanity may also be substantial, in the form of greater wisdom etc, but that would necessarily come about only very gradually (we won't have any 1000-year-old for at least 900 years whatever happens!), so it doesn't figure strongly in my calculations.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- When forced to acknowledge that the idea of aging being a high-priority target for medicine is an inescapeable consequence of things they already believe (notably that health is good and ageism is bad), many people retreat to the standpoint that it's never going to be possible so it's OK to be irrational about whether it's desirable. The feasibility of postponing age-related ill-health by X years with medicine available Y years from now is, of course, a matter of speculation on which experts disagree, just as with any other pioneering technology. I know that Holden and others have expressed caution (at best) concerning the accuracy of any kind of calculation of probabilities of particular outcomes in the distant (or even not-so-distant) future, and I share that view. However, an approach that may appeal more is to estimate how much humanitarian benefit a given amount of progress would deliver, and then to ask how unlikely that scenario needs to be to make it not worth pursuing. My claim is that the benefits of hastening the defeat of aging by even a few years (which is the minimum that I claim SENS Foundation is in a position to do, given adequate funding) would be so astronomical that the required chance of success to make such an effort worthwhile would be tiny - too tiny for it to be reasonable to argue that such funding would be inadvisable. But of course that is precisely what I would want GiveWell to opine on.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- In the event that GiveWell (or anyone else) were to decide and declare that the defeat of aging is indeed a cause that philanthropists should support, there then arises the question of which organisation(s) should be supported in the best interests of that mission. We at SENS Foundation have worked diligently to rise as quickly as possible in the legitimacy stakes by all standard measures, but we are still young and there remains more to do. If I were to offer an argument to fund us rather than any other entity, it would largely come down to the fact that no other organisation has even a serious plan for defeating aging, let alone a track record of implementing such a plan's early stages. <br class=\"kix-line-break\" /><br class=\"kix-line-break\" />- A significant chunk of what we do is of a kind that I think comes under \"meta-research\". A prominent example is a project we're funding at Denver University to extend the well-respected forecasting system \"International Futures\" so that it can analyse scenarios incorporating dramatically postponed aging.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />I greatly welcome any feedback.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />Cheers, Aubrey</span><br /><br /> \n<hr />\n<br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Hi Aubrey,</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Thanks for the thoughts.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">The NIH appears to have a division focused on research relevant to this topic: </span><a href=\"http://www.nia.nih.gov/research/dab\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/research/dab</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\"> . Its budget appears to be ~$175 million (per year). The National Institute on Aging, which houses this division, has a budget of about $1 billion per year, including a separate ~$400 million for neuroscience (which may also be relevant) as well as $115 million for intramural research. Figures are from </span><a href=\"http://www.nia.nih.gov/about/budget/2012/fiscal-year-2013-budget\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/about/budget/2012/fiscal-year-2013-budget</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">. The Institute states that its mandate includes translational research (</span><a href=\"http://www.nia.nih.gov/research/faq/does-nia-support-translational-research\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/research/faq/does-nia-support-translational-research</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">). How would you distinguish your work from this work?</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">(For the moment I'm putting aside the question I raised in my previous response to Vipul on this topic, regarding whether it's best to approach biology funding from the perspective of \"trying to treat/cure a particular condition\" or \"trying to understand &nbsp;fundamental questions in biology whose applications are difficult to predict.\")</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br /><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br /><br /> \n<hr />\n<br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Hi Holden - many thanks.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />First: yes, there are really three somewhat separate questions for someone trying to evaluate whether to support SENS Foundation:<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />1) Is the medical control of aging a hugely valuable mission?<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />2) Assuming \"yes\" to (1), is it best achieved by basic research or translational research?<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />3) Assuming translational, is SENS Foundation the organisation that uses money most effectively in pursuit of that mission?<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />I had rather expected that you would take some convincing on item (1), and much of what I wrote last time was focused on that. Since it isn't the focus of your question to me, I'm now going to assume until further notice that there is no dissent on that.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />So, to answer your question: actually you're not putting aside the basic-vs-translational question as much as you may think you are. The word \"translational\" is flavour of the month in government funding circles these days (not only in the USA), so it's not surprising that the NIA has a public statement of the kind you pointed to. However, notice that the link they give \"for more information\" is to a page listing ALL \"Funding Opportunity Announcements\". There is no page specifically for translational ones, and the reason there isn't is that the amount of work that the NIA actually funds that could really be called translational is tiny. In other words, the page you found is actually just blatant spin. The neuroscience slice you mention is an anomaly arising from the way NIA was founded (the natural place for that money is clearly NINDS): the fact that it's NIA money does not, in practice, translate into its being spent on work to prevent neurodegeneration by treating its cause (aging). Instead, just like NINDS money, it's spent on attacking neurodegeneration directly, as if such diseases could be eliminated from the body just like an infection: the same old mistake that afflicts, and dooms, the whole of geriatric medicine.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />So, the first answer to your question is that SENS Foundation really DOES focus on translational research, with an explicit goal of postponing age-related ill-health. But there's also another big difference: we can attack this problem relatively free of the other priorities that afflict mainstream funding (whether from NIH or from trasitional foundations). Most importantly, though we do and will continue to publish our interim results in the peer-reviewed literature, we are much less constrained by \"publish or perish\" tyranny than typical academics are. This allows us to proceed by constructing and implementing a rational \"project plan\" (namely SENS) to get to the intended goal (the defeat of aging), whereas what little translational work is funded by NIA or others is guided overwhelmingly by the imperative to get some kind of positive result as quickly as possible, even when it's understood that those results are not remotely likely to \"scale\", i.e. to translate into eventual medical treatments that significantly delay aging. A great example of this is the NIA's Interventions Testing Program (ITP) to test the mouse longevity effects of various small molecules. The ITP only exists at all (and in a far smaller form than originally intended) as a result of several years of persistence by the then head of the NIA's biology division (Huber Warner), and it focuses entirely on delivery of simple drugs starting rather early in life, with the result that no information emerges that's relevant to treating people who are already in middle age or older. (This is despite the fact that by far the most high-profile result that the ITP has delivered so far, the benefits of rapamycin, actually WAS a late-onset study: it wasn't meant to be, but technical issues delayed the experiment.) In a nutshell, there is a huge bias against high-risk high-gain work.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />The third thing that distinguishes SENS Foundation's approach is that we can transcend the \"balkanisation\" (silo mentality) that dominates mainstream academic funding. When one submits a grant application to NIA, it is evaluated by gerontologists, just as when one submits to NCI it is evaluated by oncologists, etc. What's wrong with this is that it biases the system immensely against cross-disciplinary proposals. SENS is a plan that brings together a large body of knowledge from gerontology but also a huge amount of expertise that was developed for other reasons entirely - to treat acute disease/injury, or in some cases for purposes that were not biomedical at all (notably environmental decontamination). It doesn't matter how robust the objective scientific and technological argument is for work of that sort: it will never compete (especially in today's very tight funding environment) with more single-topic proposals all of whose details can be understood by reviewers from a particular single field.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />The final thing to mention, and this actually also answers your question to Vipul about basic versus translational research, is that SENS is a plan that has stood the test of time. I've been propounding it since 2000, well before SENS Foundation existed, and it used to come in for a lot of criticism (initially more in the form of off-the-record ridicule, and latterly, at my behest, in print), but in every single case that criticism was found to stem from ignorance on the part of the detractor, either of what I proposed or of published experimental work on which the proposal was based. That's why I'm now regularly asked to organise entire sessions at mainstream gerontology conferences, whereas as little as five years ago I would never even be invited to speak. It's also why the Research Advisory Board of SENS Foundation consists of such prestigious scientists. This is a very strong argument, in my view, for believing that now is the time to sink a proper amount of money into translational gerontology (though certainly not to cease doin basic biogerontology too). It's well known that basic scientists are often not the most far-sighted when it comes to seeing how to apply their discoveries (attitudes in 1900 to the feasibility of powered flight being the canonical example). It is therefore a source of concern that almost all the experts who have the ear of funders in this field are basic scientists, whose instinct is to carry on finding things out and to deprioritise the tedious business of applying that knowledge. SENS has achieved a gratisfying level of legitimacy in gerontology, but it is still foreign to most card-carrying gerontologists, and as such it remains essentially unfundable via mainstream mechanisms. Hence the need to create a philanthropy-driven entity, SENS Foundation, to get this work done.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />Let me know if this helps, or if you have further questions.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />Cheers, Aubrey</span><br /><br /> \n<hr />\n<br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Hi Aubrey,</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Thanks again for engaging so thoughtfully.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">I agree that a new technology/treatment that could delay or reverse aging (or aspects of it) would be enormously valuable. Regarding the rest of your argument, this is a good example of the challenges I've been discussing in understanding biomedical research.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">You state that you have a high-expected-value plan that the academic world can't recognize the value of because of shortcomings such as \"balkanisation\" and risk aversion. I believe it may be true that the academic world has such problems to a degree; however, I also believe that there are a lot of extremely talented people in academia and that they often (though not necessarily always) find ways to move forward on promising work. Without more subject-matter expertise (or the advice of someone with such expertise), I can't easily assess the technical merits of your argument or potential counterarguments. Hopefully we'll have a better system for doing so at some point in the future.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">I'll be very interested to see Dario's thoughts on the matter if he responds. I'd cite Dario as an example of an academic who ultimately wants to do work of the greatest humanitarian value possible, regardless of whether it is prestigious work. And as my summary of our conversation shows, he acknowledges that the world of biomedical research may have certain suboptimal incentives, but didn't seem to think that these issues are leaving specific, visible outstanding research programs on the table the way that your email implies.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br /><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br /><br /> \n<hr />\n<br /><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Excellent. I too am keen to see Dario's comments. Dario also has the advantage of being based just a few miles from SENS Foundation's research centre, so we can definitely get together f2f soon if he wants.<br class=\"kix-line-break\" /><br class=\"kix-line-break\" />Cheers, Aubrey</span><br /></strong>\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"t7t9nW6BtJhfGNSR6": 2, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iYN9wW7ePXDT5ZRSE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 30, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "21904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Givewell_s_Holden_Karnofsky__who_has_previously_posted_his_thoughts_on_Givewell_supporting_SI_MIRI_recently_discussed_the_potential_for_Givewell_to_begin_evaluating_biomedical_charities__in_Givewell_s_Yahoo_Group___Someone_suggested__as_I_have_through_less_direct_means__that_they_take_a_hard_look_at_SENS_Research_Foundation__and_then_Aubrey_de_Grey_appeared_and_began_an_interesting_discussion_with_Holden_The_thread_begins_with_Holden_s_long_initial_post_about_Givewell_s_stance_on_investigating_and_recommending_biomedical_charities__which_is_definitely_worth_the_read_for_greater_insight__The_rest_of_the_conversation_is_aggregated_below_for_anyone_else_who_can_t_stomach_Yahoo_Groups__interface_Overall__Holden_seems_to_agree_with_the_goal_of_SENS__and_interested_in_the_details__but_the_conversation_seems_to_have_ended_in_October_2012_with_Holden_stating_that_he_was_waiting_for_Dario_Amodei_s_thoughts_on_SENS_\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"http://www.givewell.org/\">Givewell</a>\u2019s Holden Karnofsky, who has previously posted his <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">thoughts on Givewell supporting SI/MIRI</a></span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> recently discussed the potential for Givewell to begin evaluating biomedical charities, in <a href=\"http://groups.yahoo.com/group/givewell/\">Givewell\u2019s Yahoo Group</a></span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">. &nbsp;Someone suggested (as I have through less direct means) that they take a hard look at <a href=\"http://www.sens.org/\">SENS Research Foundation</a>, and then Aubrey de Grey appeared and began an interesting discussion with Holden.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The thread begins with Holden\u2019s <a href=\"http://groups.yahoo.com/group/givewell/message/328\">long initial post</a> about Givewell\u2019s stance on investigating and recommending biomedical charities, which is definitely worth the read for greater insight.</span><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The rest of the conversation is aggregated below for anyone else who can\u2019t stomach Yahoo Groups\u2019 interface.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Overall, Holden seems to agree with the goal of SENS, and interested in the details, but the conversation seems to have ended in October 2012 with Holden stating that he was waiting for Dario Amodei\u2019s thoughts on SENS.</span></strong></p>\n<p><strong style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><br></span></strong></p>\n<blockquote>\n<p><strong id=\"Holden_First__I_think_that_this_is_an_excellent_document__I_checked_for_anumber_of_things_that_I_had_heard_about__Breakout_Labs__JohnIoannidis__Cochrane_Collaboration__and_they_re_all_there_in_yourdocument_The_one_thing_that_s_not_explicitly_mentioned__longevity_and_lifeextension_research__At_least_prima_facie__this_seems_like_somethingthat_should_be_more_important_than_individual_disease_research__and_itseems_like_a_classic__Valley_of_Death__case__pun_unintended__butnoted_____T1_stage_to_use_your_terminology__I_think_the_SENS_websitehttp___www_sens_org_would_be_a_good_starting_point_for_one_of_the__tome_promising__approaches_to_life_extension__I_recall_from_pastconversations_that_you_were_aware_of_SENS__so_this_is_not_new_to_you_but_I_think_that_longevity_should_be_included_as_part_of_anydiscussion_of_biomedical_research_and_given_separate_considerationgiven_that_it_has_a_much_lower_status_than_research_into_specificconditions_such_as_cancer__dementia__etc__You_may_ultimately_concludethat_not_enough_can_be_done_in_this_area__but_I_think_it_should_bepart_of_your_preliminary_stuff___btw__the_United_States_has_a_NationalInstitute_of_Aging__but_it_s_much_lower_status_than_most_of_the_othergrantmakers_mentioned_here__Vipul_\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Holden,</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">First, I think that this is an excellent document. I checked for a</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">number of things that I had heard about (Breakout Labs, John</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Ioannidis, Cochrane Collaboration) and they're all there in your</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">document.</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">The one thing that's not explicitly mentioned: longevity and life</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">extension research. At least prima facie, this seems like something</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">that should be more important than individual disease research, and it</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">seems like a classic \"Valley of Death\" case (pun unintended, but</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">noted) -- T1 stage to use your terminology. I think the SENS website</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">http://www.sens.org would be a good starting point for one of the (to</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">me promising) approaches to life extension. I recall from past</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">conversations that you were aware of SENS, so this is not new to you,</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">but I think that longevity should be included as part of any</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">discussion of biomedical research and given separate consideration</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">given that it has a much lower status than research into specific</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">conditions such as cancer, dementia, etc. You may ultimately conclude</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">that not enough can be done in this area, but I think it should be</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">part of your preliminary stuff. [btw, the United States has a National</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Institute of Aging, but it's much lower-status than most of the other</span><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">grantmakers mentioned here].</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Vipul</span><br><br> </strong></p>\n<hr>\n<strong id=\"internal-source-marker_0.7816153485327959\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"> <br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Hi Vipul,</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Thanks for the thoughts. I had a followup conversation with Dario about this topic a few days ago. I think the question of \"could one fund translational research to treat/prevent aging?\" provides an interesting illustration of some of the tricky dynamics here for a funder:</span><br> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">It's possible that if there were a great deal more attention giving to treating/preventing aging, we would have some promising treatments. So in a broad sense it's possible that aging is underinvested in.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">A lot of the best basic biology research isn't clearly pointing toward one treatment/condition or another; it's about understanding the fundamentals of how organisms operate. So having an interest in treating aging, as opposed to cancer, might not have a major impact on which projects one funds, if one's main goal is to fund outstanding basic biology research.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">Perhaps because of the lack of emphasis on treating aging (or perhaps because it's simply too difficult of a problem), there don't seem to be promising findings in the \"Valley of Death\" relevant to aging; the few promising leads have been explored.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline;\" dir=\"ltr\"><span style=\"vertical-align: baseline; white-space: pre-wrap;\">So even if, in a broad sense, there is too little attention given to this problem, knowing this doesn't necessarily yield a clear direction for a relatively small-scale funder of biomedical research.</span></li>\n</ul>\n<span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br><br> \n<hr>\n<br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Hi everyone,<br class=\"kix-line-break\"><br class=\"kix-line-break\">My attention was brought to this thread, by virtue of the fact that it was my work that gave rise to SENS Foundation, and I'm looking forward to getting more involved here; I've held the Effective Altruism movement in high regard for some time. However, given my newbie status here I want to start by apologising in advance for any oversight of previously-discussed issues etc. I'm naturally delighted both at Holden's post and at Vipul's reply (which I should stress that I did not plant! - I do not know Vipul at all, though I look forward to changing that). I would like to mention just a few key points for discussion:<br class=\"kix-line-break\"><br class=\"kix-line-break\">- Holden, I want to compliment you on your appreciation of how academia really works. Everything you say about that is spot on. The aversion to \"high risk high gain\" work that has arisen and become so endemic in the system is the most important point here, in terms of why parallel funding routes are needed.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- I'm slightly confused that a lot of Holden's remarks are focused on the private sector (i.e. startups), since my understanding was that GiveWell is about philanthropy; but I realise that there is not all that clear a boundary between the two (and I note the mention of Breakout Labs, with which I have close links and which sits astride that divide more than arguably anyone). The \"valley of death\" in pre-competitive translational research is a rather different one than that encountered by startups, but the principle is the same, and research to postpone aging certainly encounteres it.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- Something that I presume factors highly among GiveWell's criteria is the extent to which a cause may be undervalued by the bulk of major philanthropists, such that an infusion of additional funds would make more of a difference than in an area that is already being well funded. To me this seems to mirror the logic of focusing on the shortcomings (gaps) in NIH's funding (and that of traditional-model foundations). Holden notes that \"Anyone we consider for funding ought to be able to explain why they're better at allocating the funds than the NIH\" and I agree wholeheartedly, but my inference is that he thinks that some orgs may indeed be able to explain that. I certainly think that SENS Foundation can.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- Coming to aging: research to postpone aging has the unique problem of quite indescribeable irrationality on the part of most of the general public, policy-makers and even biologists with regard to its desirability. Biogerontologists have been talking to brick walls for decades in their effort to get the rest of the world to appreciate that aging is what causes age-related ill-health, and thus that treatments for aging are merely preventative geriatrics. The concept persists, despite biogerontologists' best efforts, that aging is \"natural\" and should be left alone, whereas the diseases that it brings about are awful and should be fought. This is made even more bizarre by the fact that the status of age-related diseases as aspects of the later stages of aging absolutely, unequivocally implies that efforts to attack those diseases directly are doomed to fail. As such, this is a (unique? certainly very rare) case where a philanthropic contribution can make a particularly big difference simply because most philanthropists don't see the case for it. It underpins why having an interest in treating aging, as opposed to cancer, absolutely has a major impact on which projects one funds. It's also a case for (if I understand the term correctly) meta-research.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- A lot of the chatter about treating aging revolves around longevity, but it shouldn't. I'm all in favour of longevity, don't get me wrong, but it's not what gets me up in the morning: what does is health. I want people to be truly youthful, however long ago they were born: simple as that. The benefits of longevity per se to humanity may also be substantial, in the form of greater wisdom etc, but that would necessarily come about only very gradually (we won't have any 1000-year-old for at least 900 years whatever happens!), so it doesn't figure strongly in my calculations.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- When forced to acknowledge that the idea of aging being a high-priority target for medicine is an inescapeable consequence of things they already believe (notably that health is good and ageism is bad), many people retreat to the standpoint that it's never going to be possible so it's OK to be irrational about whether it's desirable. The feasibility of postponing age-related ill-health by X years with medicine available Y years from now is, of course, a matter of speculation on which experts disagree, just as with any other pioneering technology. I know that Holden and others have expressed caution (at best) concerning the accuracy of any kind of calculation of probabilities of particular outcomes in the distant (or even not-so-distant) future, and I share that view. However, an approach that may appeal more is to estimate how much humanitarian benefit a given amount of progress would deliver, and then to ask how unlikely that scenario needs to be to make it not worth pursuing. My claim is that the benefits of hastening the defeat of aging by even a few years (which is the minimum that I claim SENS Foundation is in a position to do, given adequate funding) would be so astronomical that the required chance of success to make such an effort worthwhile would be tiny - too tiny for it to be reasonable to argue that such funding would be inadvisable. But of course that is precisely what I would want GiveWell to opine on.<br class=\"kix-line-break\"><br class=\"kix-line-break\">- In the event that GiveWell (or anyone else) were to decide and declare that the defeat of aging is indeed a cause that philanthropists should support, there then arises the question of which organisation(s) should be supported in the best interests of that mission. We at SENS Foundation have worked diligently to rise as quickly as possible in the legitimacy stakes by all standard measures, but we are still young and there remains more to do. If I were to offer an argument to fund us rather than any other entity, it would largely come down to the fact that no other organisation has even a serious plan for defeating aging, let alone a track record of implementing such a plan's early stages. <br class=\"kix-line-break\"><br class=\"kix-line-break\">- A significant chunk of what we do is of a kind that I think comes under \"meta-research\". A prominent example is a project we're funding at Denver University to extend the well-respected forecasting system \"International Futures\" so that it can analyse scenarios incorporating dramatically postponed aging.<br class=\"kix-line-break\"><br class=\"kix-line-break\">I greatly welcome any feedback.<br class=\"kix-line-break\"><br class=\"kix-line-break\">Cheers, Aubrey</span><br><br> \n<hr>\n<br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Hi Aubrey,</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Thanks for the thoughts.</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">The NIH appears to have a division focused on research relevant to this topic: </span><a href=\"http://www.nia.nih.gov/research/dab\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/research/dab</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\"> . Its budget appears to be ~$175 million (per year). The National Institute on Aging, which houses this division, has a budget of about $1 billion per year, including a separate ~$400 million for neuroscience (which may also be relevant) as well as $115 million for intramural research. Figures are from </span><a href=\"http://www.nia.nih.gov/about/budget/2012/fiscal-year-2013-budget\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/about/budget/2012/fiscal-year-2013-budget</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">. The Institute states that its mandate includes translational research (</span><a href=\"http://www.nia.nih.gov/research/faq/does-nia-support-translational-research\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">http://www.nia.nih.gov/research/faq/does-nia-support-translational-research</span></a><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">). How would you distinguish your work from this work?</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">(For the moment I'm putting aside the question I raised in my previous response to Vipul on this topic, regarding whether it's best to approach biology funding from the perspective of \"trying to treat/cure a particular condition\" or \"trying to understand &nbsp;fundamental questions in biology whose applications are difficult to predict.\")</span><br><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br><span style=\"font-size: 15px; font-family: Arial; color: #222222; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br><br> \n<hr>\n<br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Hi Holden - many thanks.<br class=\"kix-line-break\"><br class=\"kix-line-break\">First: yes, there are really three somewhat separate questions for someone trying to evaluate whether to support SENS Foundation:<br class=\"kix-line-break\"><br class=\"kix-line-break\">1) Is the medical control of aging a hugely valuable mission?<br class=\"kix-line-break\"><br class=\"kix-line-break\">2) Assuming \"yes\" to (1), is it best achieved by basic research or translational research?<br class=\"kix-line-break\"><br class=\"kix-line-break\">3) Assuming translational, is SENS Foundation the organisation that uses money most effectively in pursuit of that mission?<br class=\"kix-line-break\"><br class=\"kix-line-break\">I had rather expected that you would take some convincing on item (1), and much of what I wrote last time was focused on that. Since it isn't the focus of your question to me, I'm now going to assume until further notice that there is no dissent on that.<br class=\"kix-line-break\"><br class=\"kix-line-break\">So, to answer your question: actually you're not putting aside the basic-vs-translational question as much as you may think you are. The word \"translational\" is flavour of the month in government funding circles these days (not only in the USA), so it's not surprising that the NIA has a public statement of the kind you pointed to. However, notice that the link they give \"for more information\" is to a page listing ALL \"Funding Opportunity Announcements\". There is no page specifically for translational ones, and the reason there isn't is that the amount of work that the NIA actually funds that could really be called translational is tiny. In other words, the page you found is actually just blatant spin. The neuroscience slice you mention is an anomaly arising from the way NIA was founded (the natural place for that money is clearly NINDS): the fact that it's NIA money does not, in practice, translate into its being spent on work to prevent neurodegeneration by treating its cause (aging). Instead, just like NINDS money, it's spent on attacking neurodegeneration directly, as if such diseases could be eliminated from the body just like an infection: the same old mistake that afflicts, and dooms, the whole of geriatric medicine.<br class=\"kix-line-break\"><br class=\"kix-line-break\">So, the first answer to your question is that SENS Foundation really DOES focus on translational research, with an explicit goal of postponing age-related ill-health. But there's also another big difference: we can attack this problem relatively free of the other priorities that afflict mainstream funding (whether from NIH or from trasitional foundations). Most importantly, though we do and will continue to publish our interim results in the peer-reviewed literature, we are much less constrained by \"publish or perish\" tyranny than typical academics are. This allows us to proceed by constructing and implementing a rational \"project plan\" (namely SENS) to get to the intended goal (the defeat of aging), whereas what little translational work is funded by NIA or others is guided overwhelmingly by the imperative to get some kind of positive result as quickly as possible, even when it's understood that those results are not remotely likely to \"scale\", i.e. to translate into eventual medical treatments that significantly delay aging. A great example of this is the NIA's Interventions Testing Program (ITP) to test the mouse longevity effects of various small molecules. The ITP only exists at all (and in a far smaller form than originally intended) as a result of several years of persistence by the then head of the NIA's biology division (Huber Warner), and it focuses entirely on delivery of simple drugs starting rather early in life, with the result that no information emerges that's relevant to treating people who are already in middle age or older. (This is despite the fact that by far the most high-profile result that the ITP has delivered so far, the benefits of rapamycin, actually WAS a late-onset study: it wasn't meant to be, but technical issues delayed the experiment.) In a nutshell, there is a huge bias against high-risk high-gain work.<br class=\"kix-line-break\"><br class=\"kix-line-break\">The third thing that distinguishes SENS Foundation's approach is that we can transcend the \"balkanisation\" (silo mentality) that dominates mainstream academic funding. When one submits a grant application to NIA, it is evaluated by gerontologists, just as when one submits to NCI it is evaluated by oncologists, etc. What's wrong with this is that it biases the system immensely against cross-disciplinary proposals. SENS is a plan that brings together a large body of knowledge from gerontology but also a huge amount of expertise that was developed for other reasons entirely - to treat acute disease/injury, or in some cases for purposes that were not biomedical at all (notably environmental decontamination). It doesn't matter how robust the objective scientific and technological argument is for work of that sort: it will never compete (especially in today's very tight funding environment) with more single-topic proposals all of whose details can be understood by reviewers from a particular single field.<br class=\"kix-line-break\"><br class=\"kix-line-break\">The final thing to mention, and this actually also answers your question to Vipul about basic versus translational research, is that SENS is a plan that has stood the test of time. I've been propounding it since 2000, well before SENS Foundation existed, and it used to come in for a lot of criticism (initially more in the form of off-the-record ridicule, and latterly, at my behest, in print), but in every single case that criticism was found to stem from ignorance on the part of the detractor, either of what I proposed or of published experimental work on which the proposal was based. That's why I'm now regularly asked to organise entire sessions at mainstream gerontology conferences, whereas as little as five years ago I would never even be invited to speak. It's also why the Research Advisory Board of SENS Foundation consists of such prestigious scientists. This is a very strong argument, in my view, for believing that now is the time to sink a proper amount of money into translational gerontology (though certainly not to cease doin basic biogerontology too). It's well known that basic scientists are often not the most far-sighted when it comes to seeing how to apply their discoveries (attitudes in 1900 to the feasibility of powered flight being the canonical example). It is therefore a source of concern that almost all the experts who have the ear of funders in this field are basic scientists, whose instinct is to carry on finding things out and to deprioritise the tedious business of applying that knowledge. SENS has achieved a gratisfying level of legitimacy in gerontology, but it is still foreign to most card-carrying gerontologists, and as such it remains essentially unfundable via mainstream mechanisms. Hence the need to create a philanthropy-driven entity, SENS Foundation, to get this work done.<br class=\"kix-line-break\"><br class=\"kix-line-break\">Let me know if this helps, or if you have further questions.<br class=\"kix-line-break\"><br class=\"kix-line-break\">Cheers, Aubrey</span><br><br> \n<hr>\n<br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Hi Aubrey,</span><br><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Thanks again for engaging so thoughtfully.</span><br><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">I agree that a new technology/treatment that could delay or reverse aging (or aspects of it) would be enormously valuable. Regarding the rest of your argument, this is a good example of the challenges I've been discussing in understanding biomedical research.</span><br><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">You state that you have a high-expected-value plan that the academic world can't recognize the value of because of shortcomings such as \"balkanisation\" and risk aversion. I believe it may be true that the academic world has such problems to a degree; however, I also believe that there are a lot of extremely talented people in academia and that they often (though not necessarily always) find ways to move forward on promising work. Without more subject-matter expertise (or the advice of someone with such expertise), I can't easily assess the technical merits of your argument or potential counterarguments. Hopefully we'll have a better system for doing so at some point in the future.</span><br><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">I'll be very interested to see Dario's thoughts on the matter if he responds. I'd cite Dario as an example of an academic who ultimately wants to do work of the greatest humanitarian value possible, regardless of whether it is prestigious work. And as my summary of our conversation shows, he acknowledges that the world of biomedical research may have certain suboptimal incentives, but didn't seem to think that these issues are leaving specific, visible outstanding research programs on the table the way that your email implies.</span><br><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Best,</span><br><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Holden</span><br><br> \n<hr>\n<br><span style=\"font-size: 15px; font-family: Arial; color: #202020; vertical-align: baseline; white-space: pre-wrap;\">Excellent. I too am keen to see Dario's comments. Dario also has the advantage of being based just a few miles from SENS Foundation's research centre, so we can definitely get together f2f soon if he wants.<br class=\"kix-line-break\"><br class=\"kix-line-break\">Cheers, Aubrey</span><br></strong>\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>", "sections": [{"title": "Givewell\u2019s Holden Karnofsky, who has previously posted his thoughts on Givewell supporting SI/MIRI recently discussed the potential for Givewell to begin evaluating biomedical charities, in Givewell\u2019s Yahoo Group. \u00a0Someone suggested (as I have through less direct means) that they take a hard look at SENS Research Foundation, and then Aubrey de Grey appeared and began an interesting discussion with Holden.The thread begins with Holden\u2019s long initial post about Givewell\u2019s stance on investigating and recommending biomedical charities, which is definitely worth the read for greater insight. The rest of the conversation is aggregated below for anyone else who can\u2019t stomach Yahoo Groups\u2019 interface.Overall, Holden seems to agree with the goal of SENS, and interested in the details, but the conversation seems to have ended in October 2012 with Holden stating that he was waiting for Dario Amodei\u2019s thoughts on SENS.", "anchor": "Givewell_s_Holden_Karnofsky__who_has_previously_posted_his_thoughts_on_Givewell_supporting_SI_MIRI_recently_discussed_the_potential_for_Givewell_to_begin_evaluating_biomedical_charities__in_Givewell_s_Yahoo_Group___Someone_suggested__as_I_have_through_less_direct_means__that_they_take_a_hard_look_at_SENS_Research_Foundation__and_then_Aubrey_de_Grey_appeared_and_began_an_interesting_discussion_with_Holden_The_thread_begins_with_Holden_s_long_initial_post_about_Givewell_s_stance_on_investigating_and_recommending_biomedical_charities__which_is_definitely_worth_the_read_for_greater_insight__The_rest_of_the_conversation_is_aggregated_below_for_anyone_else_who_can_t_stomach_Yahoo_Groups__interface_Overall__Holden_seems_to_agree_with_the_goal_of_SENS__and_interested_in_the_details__but_the_conversation_seems_to_have_ended_in_October_2012_with_Holden_stating_that_he_was_waiting_for_Dario_Amodei_s_thoughts_on_SENS_", "level": 1}, {"title": "Holden,First, I think that this is an excellent document. I checked for anumber of things that I had heard about (Breakout Labs, JohnIoannidis, Cochrane Collaboration) and they're all there in yourdocument.The one thing that's not explicitly mentioned: longevity and lifeextension research. At least prima facie, this seems like somethingthat should be more important than individual disease research, and itseems like a classic \"Valley of Death\" case (pun unintended, butnoted) -- T1 stage to use your terminology. I think the SENS websitehttp://www.sens.org would be a good starting point for one of the (tome promising) approaches to life extension. I recall from pastconversations that you were aware of SENS, so this is not new to you,but I think that longevity should be included as part of anydiscussion of biomedical research and given separate considerationgiven that it has a much lower status than research into specificconditions such as cancer, dementia, etc. You may ultimately concludethat not enough can be done in this area, but I think it should bepart of your preliminary stuff. [btw, the United States has a NationalInstitute of Aging, but it's much lower-status than most of the othergrantmakers mentioned here].Vipul ", "anchor": "Holden_First__I_think_that_this_is_an_excellent_document__I_checked_for_anumber_of_things_that_I_had_heard_about__Breakout_Labs__JohnIoannidis__Cochrane_Collaboration__and_they_re_all_there_in_yourdocument_The_one_thing_that_s_not_explicitly_mentioned__longevity_and_lifeextension_research__At_least_prima_facie__this_seems_like_somethingthat_should_be_more_important_than_individual_disease_research__and_itseems_like_a_classic__Valley_of_Death__case__pun_unintended__butnoted_____T1_stage_to_use_your_terminology__I_think_the_SENS_websitehttp___www_sens_org_would_be_a_good_starting_point_for_one_of_the__tome_promising__approaches_to_life_extension__I_recall_from_pastconversations_that_you_were_aware_of_SENS__so_this_is_not_new_to_you_but_I_think_that_longevity_should_be_included_as_part_of_anydiscussion_of_biomedical_research_and_given_separate_considerationgiven_that_it_has_a_much_lower_status_than_research_into_specificconditions_such_as_cancer__dementia__etc__You_may_ultimately_concludethat_not_enough_can_be_done_in_this_area__but_I_think_it_should_bepart_of_your_preliminary_stuff___btw__the_United_States_has_a_NationalInstitute_of_Aging__but_it_s_much_lower_status_than_most_of_the_othergrantmakers_mentioned_here__Vipul_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-06T05:24:53.887Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Pretending to be Wise", "slug": "seq-rerun-pretending-to-be-wise", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Ybe52mjcZDW2iJAc/seq-rerun-pretending-to-be-wise", "pageUrlRelative": "/posts/8Ybe52mjcZDW2iJAc/seq-rerun-pretending-to-be-wise", "linkUrl": "https://www.lesswrong.com/posts/8Ybe52mjcZDW2iJAc/seq-rerun-pretending-to-be-wise", "postedAtFormatted": "Wednesday, March 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Pretending%20to%20be%20Wise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Pretending%20to%20be%20Wise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Ybe52mjcZDW2iJAc%2Fseq-rerun-pretending-to-be-wise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Pretending%20to%20be%20Wise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Ybe52mjcZDW2iJAc%2Fseq-rerun-pretending-to-be-wise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Ybe52mjcZDW2iJAc%2Fseq-rerun-pretending-to-be-wise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/yp/pretending_to_be_wise/\">Pretending to be Wise</a> was originally published on 19 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Pretending_to_be_Wise\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying to signal wisdom or maturity by taking a neutral position is very seldom the right course of action.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gwb/seq_rerun_against_maturity/\">Against Maturity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Ybe52mjcZDW2iJAc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1304207733526421e-06, "legacy": true, "legacyId": "21913", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jeyvzALDbjdjjv5RW", "7M9tW7ywSAfXR5yeT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-06T10:31:48.001Z", "modifiedAt": null, "url": null, "title": "Caelum est Conterrens: I frankly don't see how this is a horror story", "slug": "caelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "viewCount": null, "lastCommentedAt": "2021-01-28T23:56:18.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dCrDWWAS8Q2uFDYGD/caelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "pageUrlRelative": "/posts/dCrDWWAS8Q2uFDYGD/caelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "linkUrl": "https://www.lesswrong.com/posts/dCrDWWAS8Q2uFDYGD/caelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "postedAtFormatted": "Wednesday, March 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Caelum%20est%20Conterrens%3A%20I%20frankly%20don't%20see%20how%20this%20is%20a%20horror%20story&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACaelum%20est%20Conterrens%3A%20I%20frankly%20don't%20see%20how%20this%20is%20a%20horror%20story%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCrDWWAS8Q2uFDYGD%2Fcaelum-est-conterrens-i-frankly-don-t-see-how-this-is-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Caelum%20est%20Conterrens%3A%20I%20frankly%20don't%20see%20how%20this%20is%20a%20horror%20story%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCrDWWAS8Q2uFDYGD%2Fcaelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCrDWWAS8Q2uFDYGD%2Fcaelum-est-conterrens-i-frankly-don-t-see-how-this-is-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>So Eliezer said in his March 1st <a href=\"http://hpmor.com/notes/progress-13-03-01/\">HPMOR progress report</a>:</p>\n<blockquote>\n<p>I recommend the recursive fanfic &ldquo;<a href=\"http://www.fimfiction.net/story/69770/friendship-is-optimal-caelum-est-conterrens\">Friendship is Optimal: Caelum est Conterrens</a>&rdquo;  (Heaven Is Terrifying). &nbsp;This is the first and only effective horror  novel I have ever read, since unlike Lovecraft, it contains things I  actually find scary.</p>\n</blockquote>\n<p>So I read that and it was certainly very much worth reading - thanks for the recommendation! Obviously, the following contains spoilers.</p>\n<p>I'm confused about how the story is supposed to be \"terrifying\". I rarely find any fiction scary, but I suspect that this is about something else: I didn't think <a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2</a> was \"failed\" either and in Three Worlds Collide, I thought the choice of the \"Normal\" ending made a lot more sense than choosing the \"True\" ending. The Optimalverse seems to me a fantastically fortunate universe, pretty much the best universe mammals could ever hope to end up in, and I honestly don't see how it is a horror novel, at all.</p>\n<p>So, apparently there's something I'm not getting. Something that makes an individual's hard-to-define \"free choice\" more valuable than her much-easier-to-define happiness. Something like a paranoid schizophrenic's right not to be treated,</p>\n<p>So I'd like the dumb version please. What's terrifying about the Optimalverse?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "etDohXtBrXd8WqCtR": 1, "jQytxyauJ7kPhhGj3": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dCrDWWAS8Q2uFDYGD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 38, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "21917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ctpkTaqTKbmm6uRgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-06T17:05:54.939Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 43-46", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-43-46", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jqyvGsN9DT4H5jnLq/meetup-durham-rtlw-hpmor-discussion-ch-43-46", "pageUrlRelative": "/posts/jqyvGsN9DT4H5jnLq/meetup-durham-rtlw-hpmor-discussion-ch-43-46", "linkUrl": "https://www.lesswrong.com/posts/jqyvGsN9DT4H5jnLq/meetup-durham-rtlw-hpmor-discussion-ch-43-46", "postedAtFormatted": "Wednesday, March 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2043-46&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2043-46%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqyvGsN9DT4H5jnLq%2Fmeetup-durham-rtlw-hpmor-discussion-ch-43-46%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2043-46%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqyvGsN9DT4H5jnLq%2Fmeetup-durham-rtlw-hpmor-discussion-ch-43-46", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqyvGsN9DT4H5jnLq%2Fmeetup-durham-rtlw-hpmor-discussion-ch-43-46", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k4'>Durham/RTLW HPMoR discussion, ch. 43-46</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2013 11:30:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of HPMoR chapters 43-46, i.e., The Humanism Chapters !!!eleven!.</p>\n\n<p>Meetup will proceed as follows:</p>\n\n<p>11:30: Obtain food from food trucks on Hunt Street (between Foster &amp; Rigsbee). NB: Food trucks usually pack up by about noon, tho if you will be joining late &amp; will want food, there are other options nearby.</p>\n\n<p>11:50: Obtain coffee from Cocoa Cinnamon (northeast corner of Foster &amp; Geer.)</p>\n\n<p>12:00: Proceed to Fullsteam (southeast corner of Rigsbee &amp; Geer) for consumption &amp; discussion. Fullsteam doesn't open til noon, but the weather is supposed to be okay, so early arrivees can congregate at an outside table.</p>\n\n<p>If you don't see anyone who looks particularly rational at food trucks or Cocoa Cinnmamon, simply obtain your comestibles of choice and proceed to Fullsteam, where again we will be identifiable by the impressive stack of spiral bound tomes.</p>\n\n<p>If anyone is concerned about navigation/finding us/finding food/etc. and would like to be able to contact someone, my number will be in the archive of the RTLW Google group, which you may join here: <a href=\"http://groups.google.com/group/RTLW\" rel=\"nofollow\">http://groups.google.com/group/RTLW</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k4'>Durham/RTLW HPMoR discussion, ch. 43-46</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jqyvGsN9DT4H5jnLq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1308783731662373e-06, "legacy": true, "legacyId": "21918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__43_46\">Discussion article for the meetup : <a href=\"/meetups/k4\">Durham/RTLW HPMoR discussion, ch. 43-46</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2013 11:30:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of HPMoR chapters 43-46, i.e., The Humanism Chapters !!!eleven!.</p>\n\n<p>Meetup will proceed as follows:</p>\n\n<p>11:30: Obtain food from food trucks on Hunt Street (between Foster &amp; Rigsbee). NB: Food trucks usually pack up by about noon, tho if you will be joining late &amp; will want food, there are other options nearby.</p>\n\n<p>11:50: Obtain coffee from Cocoa Cinnamon (northeast corner of Foster &amp; Geer.)</p>\n\n<p>12:00: Proceed to Fullsteam (southeast corner of Rigsbee &amp; Geer) for consumption &amp; discussion. Fullsteam doesn't open til noon, but the weather is supposed to be okay, so early arrivees can congregate at an outside table.</p>\n\n<p>If you don't see anyone who looks particularly rational at food trucks or Cocoa Cinnmamon, simply obtain your comestibles of choice and proceed to Fullsteam, where again we will be identifiable by the impressive stack of spiral bound tomes.</p>\n\n<p>If anyone is concerned about navigation/finding us/finding food/etc. and would like to be able to contact someone, my number will be in the archive of the RTLW Google group, which you may join here: <a href=\"http://groups.google.com/group/RTLW\" rel=\"nofollow\">http://groups.google.com/group/RTLW</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__43_461\">Discussion article for the meetup : <a href=\"/meetups/k4\">Durham/RTLW HPMoR discussion, ch. 43-46</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 43-46", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__43_46", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 43-46", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__43_461", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-06T20:21:10.997Z", "modifiedAt": null, "url": null, "title": "\"Why doesn't that cool thing happen when I *try* to do it?\"", "slug": "why-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rukifellth", "createdAt": "2012-07-02T15:41:56.556Z", "isAdmin": false, "displayName": "Rukifellth"}, "userId": "nEH5KujSxNcACmcau", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EtdpePYGryMtWrayD/why-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "pageUrlRelative": "/posts/EtdpePYGryMtWrayD/why-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "linkUrl": "https://www.lesswrong.com/posts/EtdpePYGryMtWrayD/why-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "postedAtFormatted": "Wednesday, March 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Why%20doesn't%20that%20cool%20thing%20happen%20when%20I%20*try*%20to%20do%20it%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Why%20doesn't%20that%20cool%20thing%20happen%20when%20I%20*try*%20to%20do%20it%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEtdpePYGryMtWrayD%2Fwhy-doesn-t-that-cool-thing-happen-when-i-try-to-do-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Why%20doesn't%20that%20cool%20thing%20happen%20when%20I%20*try*%20to%20do%20it%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEtdpePYGryMtWrayD%2Fwhy-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEtdpePYGryMtWrayD%2Fwhy-doesn-t-that-cool-thing-happen-when-i-try-to-do-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<p>For the past year I've been noticing an interesting phenomenon, the \"Why can't I do that on purpose?\"-effect. This usually happens when I'm just walking by my computer desk or other piece of furniture, and throw whatever object I'm holding on it, in this case, a balled up bit of tin-foil from a piece of chocolate. The ball bounces off an emptied drumstick of chicken, instead of landing on the glass desk immediately.<br /><br />Fascinated, I try and hit the chicken drumstick again with the balled tin-foil, without success.</p>\n<p>\"How the hell did I do that by accident?\"<br /><br />There are actually a number of different things on my desk that the balled up bit of tin-foil could have hit to elicit that same reaction from me; the plastic candy wrappers around the chicken drumstick, the fork next to it, anything. However, if I try to hit the chicken drumstick, my reaction to the balled up tin-foil hitting the candy wrapper instead will be \"Why didn't I hit the chicken wing?\"<br /><br />In other words, suppose there's a 50% of eliciting reaction A, due to there being 5 objects, for each of which there is a 10% chance of hitting them. If I hit one of them and elicit reaction A, I decrease the probability of re-eliciting reaction A to 10%, because the other 4 objects, if hit, will be disregarded.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EtdpePYGryMtWrayD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -1, "extendedScore": null, "score": 1.1310058954286935e-06, "legacy": true, "legacyId": "21920", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T01:48:54.065Z", "modifiedAt": null, "url": null, "title": "Getting myself to eat vegetables", "slug": "getting-myself-to-eat-vegetables", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:06.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wLE3SpzqBa88h3J4b/getting-myself-to-eat-vegetables", "pageUrlRelative": "/posts/wLE3SpzqBa88h3J4b/getting-myself-to-eat-vegetables", "linkUrl": "https://www.lesswrong.com/posts/wLE3SpzqBa88h3J4b/getting-myself-to-eat-vegetables", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Getting%20myself%20to%20eat%20vegetables&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGetting%20myself%20to%20eat%20vegetables%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLE3SpzqBa88h3J4b%2Fgetting-myself-to-eat-vegetables%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Getting%20myself%20to%20eat%20vegetables%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLE3SpzqBa88h3J4b%2Fgetting-myself-to-eat-vegetables", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLE3SpzqBa88h3J4b%2Fgetting-myself-to-eat-vegetables", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p><a href=\"http://www.jefftk.com/veggies_on_top.jpg\"><img src=\"http://www.jefftk.com/veggies_on_top-small.jpg\" alt=\"\" /></a></p>\n<p>I have a strategy for getting myself to eat vegetables. I've been using it for about six months now, and it works well:</p>\n<ul>\n<li> Put the food I want to eat on my plate. </li>\n<li> Put raw vegetables on top. </li>\n<li> Eat through vegetables to get to the food I want. </li>\n</ul>\n<p>Side effect: people who see my plate think I'm a very healthy eater. But sometimes they ask my \"why is your salad steaming?\"</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2013-03-06\">on my blog</a></em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wLE3SpzqBa88h3J4b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 1.1312199733484344e-06, "legacy": true, "legacyId": "21922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T02:11:37.420Z", "modifiedAt": null, "url": null, "title": "Don't Get Offended", "slug": "don-t-get-offended", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3Ci2Zxncj3oiB2NP4/don-t-get-offended", "pageUrlRelative": "/posts/3Ci2Zxncj3oiB2NP4/don-t-get-offended", "linkUrl": "https://www.lesswrong.com/posts/3Ci2Zxncj3oiB2NP4/don-t-get-offended", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20Get%20Offended&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20Get%20Offended%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ci2Zxncj3oiB2NP4%2Fdon-t-get-offended%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20Get%20Offended%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ci2Zxncj3oiB2NP4%2Fdon-t-get-offended", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ci2Zxncj3oiB2NP4%2Fdon-t-get-offended", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 713, "htmlBody": "<p><strong>Related to:</strong>&nbsp;<a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a>,&nbsp;<a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a></p>\n<p><strong>Followed By: </strong><a href=\"/lw/gwx/how_to_not_get_offended/\">How to Not Get Offended</a></p>\n<p>One oft-underestimated threat to&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Epistemic_rationality#Epistemic_rationality\">epistemic rationality</a>&nbsp;is getting offended. While getting offended by something sometimes feels good and can help you assert moral superiority, in most cases it doesn't help you figure out&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">what the world looks like</a>. In fact, getting offended usually makes it&nbsp;<em>harder</em>&nbsp;to figure out what the world looks like, since it means&nbsp;<a href=\"http://www.acceleratingfuture.com/steven/?p=147\">you won't be evaluating evidence very well.</a>&nbsp;In&nbsp;<a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a>, Eliezer writes that \"people who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists, can suddenly turn into slogan-chanting zombies when there's a Blue or Green position on an issue.\" Don't let yourself become one of those zombies-- all of your skills, training, and useful habits can be shut down when your brain kicks into offended mode!</p>\n<p>One might point out that getting offended is a two-way street and that it might be more appropriate to make a post called \"Don't Be Offensive.\"&nbsp;That feels like a just thing to say-- as if you are targeting the aggressor rather than the victim. And on a certain level, it's true-- you <em>shouldn't</em> try to offend people, and if you do in the course of a normal conversation it's probably your fault. But you can't always rely on others around you being able to avoid doing this. After all, what's offensive to one person may not be so to another, and they may end up offending you by mistake. And even in those unpleasant cases when you are interacting with people who are deliberately trying to offend you, isn't staying calm desirable anyway?</p>\n<p>The other problem I have with the concept of being offended as victimization is that, when you find yourself getting offended, you may be a victim, but you're being victimized by&nbsp;<em>yourself.</em><em>&nbsp;</em>Again, that's not to say that offending people on purpose is acceptable-- it obviously isn't. But you're the one who gets to decide whether or not to be offended by something. If you find yourself getting offended to things as an automatic reaction, you should seriously evaluate why that is your response.</p>\n<p>There is nothing inherent in a set of words that makes them offensive or inoffensive-- your reaction is an internal, personal process. I've seen some people stay cool in the face of others literally screaming racial slurs in their faces and I've seen other people get offended by the slightest implication or slip of the tongue. What type of reaction you have is largely up to you, and if you don't like your current reactions you can train better ones-- this is a core principle of the extremely useful philosophy known as <a href=\"http://plato.stanford.edu/entries/stoicism/\">Stoicism</a>.</p>\n<p>Of course, one&nbsp;(perhaps Robin Hanson) might also point out that getting offended can be socially useful.&nbsp;While true-- quickly responding in an offended fashion can be a strong signal of your commitment to group identity and values<sup>[1]</sup>-- that doesn't really relate to what this post is talking about. This post is talking about the best way to acquire correct beliefs, not the best way to manipulate people. And while getting offended can be a very effective way to manipulate people-- and hence a tactic that is unfortunately often reinforced-- it is usually actively detrimental for acquiring correct beliefs. Besides, the signalling value of offense should be no excuse for not knowing how not to be offended. After all, if you find it socially necessary to pretend that you are offended, doing so is not exactly difficult.</p>\n<p>Personally, I have found that the cognitive effort required to build a habit of not getting offended pays immense dividends. Getting offended tends to shut down other mental processes and constrain you in ways that are often undesirable. In many situations, misunderstandings and arguments can be diminished or avoided completely if one is unwilling to become offended and practiced in the art of avoiding offense. Further, some of those situations are ones in which thinking clearly is very important indeed! All in all, while getting offended does often feel good (in a certain crude way), it is a reaction that I have no regrets about relinquishing.</p>\n<p>&nbsp;</p>\n<p>[1] In <a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a>, Paul Graham rightly points out that one way to prevent yourself from getting offended is to let as few things into your identity as possible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1, "ZXFpyQWPB5ideFbEG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3Ci2Zxncj3oiB2NP4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 41, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "21849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 592, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "zQfjaxhxuXToyaetp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T04:31:37.224Z", "modifiedAt": null, "url": null, "title": "The Need for Human Friendliness", "slug": "the-need-for-human-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:39.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elithrion", "createdAt": "2012-04-01T22:31:55.578Z", "isAdmin": false, "displayName": "Elithrion"}, "userId": "JxETxdNf7KLnwNyDy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8K6HXGBMCPrmL5eyN/the-need-for-human-friendliness", "pageUrlRelative": "/posts/8K6HXGBMCPrmL5eyN/the-need-for-human-friendliness", "linkUrl": "https://www.lesswrong.com/posts/8K6HXGBMCPrmL5eyN/the-need-for-human-friendliness", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Need%20for%20Human%20Friendliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Need%20for%20Human%20Friendliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8K6HXGBMCPrmL5eyN%2Fthe-need-for-human-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Need%20for%20Human%20Friendliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8K6HXGBMCPrmL5eyN%2Fthe-need-for-human-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8K6HXGBMCPrmL5eyN%2Fthe-need-for-human-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 795, "htmlBody": "<p>Consider the following scenario. MIRI succeeds beyond my wildest expectations. It comes up with a friendliness theory, and then uses it to make provably friendly AGI before anyone else can make an unfriendly one. And then a year and a half later, we find that Eliezer Yudkowsky has become the designated god-emperor of the lightcone, and the rest of the major MIRI researchers are his ministers. Woops.</p>\n<p>&nbsp;</p>\n<p>My guess for the probability of this type of scenario given a huge MIRI success along those lines is around 15%. The reasoning is straightforward. (1) We don't know what's going on inside any particular person's head. (2) Many or most humans are selfish.&nbsp;(3)&nbsp;Looking altruistic is more likely to draw support than explicitly setting out to take over the world. (5)&nbsp;&nbsp;And human acting abilities, while limited, are likely adequate (for example, spies seem quite successful at&nbsp;concealing&nbsp;their motives). I'd say those four things are reasonably independent and sufficient for some deception to be happening, so guessing at some probabilities, it works out to something like 1<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">&times;</span>0.5<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">&times;</span>0.8<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">&times;</span>0.5 = 0.2&dagger; At least if the person is sufficiently determined to achieve their goal no matter what.</p>\n<p>Or to put it in a more interesting and LW-referring way, it has been argued that we might not be able to distinguish friendly from unfriendly AGI during their \"take off\" period, since both would probably act the same way to take over the world (before optimising it in a friendly or unfriendly way). We might also expect that as a person becomes more <a href=\"/lesswrong.com/lw/5i8/the_power_of_agency\">agenty</a>, their behaviour more closely approximates that of an AGI, since an AGI is pretty much the embodiment of agency. It certainly seems like Eliezer is quite agenty for a human, so we clearly presently find ourselves in his take off period!</p>\n<p>&nbsp;</p>\n<p>So, supposing we don't want god-emperor!Eliezer, what do we do? First, I think it is important to recognise that this problem is almost certainly not urgent. Selfish-Eliezer!optimisations and altruistic-Eliezer!optimisations probably require nearly the same work to not end up being paperclips!optimisations, so for the near future it doesn't particularly matter if he's secretly out to take over the world or not. In addition, having Eliezer be the god-emperor of the light cone is probably not all that bad&mdash;it might not be optimal, but it's likely to be very much better than any unfriendly outcome, and probably better than the status quo. It does, however, seem like a useful issue to address in the longer term for any organisation that plans to make a friendly(-ish) AGI.</p>\n<p>&nbsp;</p>\n<p>Off-hand, I can think of three plausible ideas:</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Extreme transparency (i.e. increase acting difficulty). If we attach microphones and cameras to all the main researchers and have a team of a hundred volunteers go over every minute of recordings every day, it would be hard to conceal any deceptive plans.</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Very good lie detection (i.e. find out what's in people's minds). While current lie detector technologies are likely to be inadequate, it seems possible that by the time the issue becomes relevant we might have much better technology. That said, it might also be the case that if someone keeps simulating having a particular goal set for decades, she may be able to easily generate responses based on that goal set without the responses registering as lies in any conventional sense.</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Some sort of organisational solution. I'm not sure what this would look like, to be honest, but it seems like some organisation patterns should be more likely to produce friendly outcomes than others independently of transparency. For example, if the essential team is huge and organised so different members check each others' work, it's likely to be more difficult to conceal or organise any conspiracy (although there would be other problems).</p>\n<p>What I think would <em>not </em>work is letting the group write all the code and then checking it over. First, there are likely to be enough talented programmers in it that they'd have a good shot at hiding whatever they wanted, and, second, it's not clear that it would be possible to stop the group from just changing some key bits at the last moment (e.g. from CEV of humans in world to CEV of humans in room) and then launching without further oversight.</p>\n<p>&nbsp;</p>\n<p>As I said, this is by no means a priority, but I think it would be useful to start thinking about the problems sooner rather than later, so we don't end up being caught off guard. Also, while I specifically mention Eliezer in this post, he simply happens to provide the most salient example, and most points are equally (or in some cases more) relevant to the general case of anyone working on AGI.</p>\n<p>&nbsp;</p>\n<p>&dagger; I probably picked those numbers in large part to justify my above \"15%\", but you get the idea.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8K6HXGBMCPrmL5eyN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 8, "extendedScore": null, "score": 1.1313262943510602e-06, "legacy": true, "legacyId": "21919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T04:33:41.739Z", "modifiedAt": "2020-08-05T20:45:06.523Z", "url": null, "title": "Boring Advice Repository", "slug": "boring-advice-repository", "viewCount": null, "lastCommentedAt": "2020-05-16T10:50:45.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HEn2qiMxk5BggN83J/boring-advice-repository", "pageUrlRelative": "/posts/HEn2qiMxk5BggN83J/boring-advice-repository", "linkUrl": "https://www.lesswrong.com/posts/HEn2qiMxk5BggN83J/boring-advice-repository", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boring%20Advice%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoring%20Advice%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHEn2qiMxk5BggN83J%2Fboring-advice-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boring%20Advice%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHEn2qiMxk5BggN83J%2Fboring-advice-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHEn2qiMxk5BggN83J%2Fboring-advice-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>This is an extension of a comment I made that I can't find and also a request for examples. It seems plausible that, when giving advice, many people optimize for deepness or punchiness of the advice rather than for actual practical value. There may be good reasons to do this - e.g. advice that sounds deep or punchy might be more likely to be listened to - but as a corollary, there could be valuable advice that people generally don't give because it doesn't sound deep or punchy. Let's call this&nbsp;<strong>boring advice</strong>.&nbsp;</p>\n<p>An example that's been discussed on LW several times is \"make&nbsp;<a href=\"/lw/8vm/the_rationalists_checklist/\">checklists</a>.\" Checklists are <a href=\"/lw/cnr/share_your_checklists/\">great</a>. We should <a href=\"/lw/fc3/checklist_of_rationality_habits/\">totally make checklists</a>. But \"make checklists\" is not a deep or punchy thing to say. Other examples include \"google things\" and \"exercise.\"&nbsp;</p>\n<p>I would like people to use this thread to post other examples of boring advice. If you can, provide evidence and/or a plausible argument that your boring advice actually is useful, but I would prefer that you err on the side of boring but not necessarily useful in the name of more thoroughly searching a plausibly under-searched part of advicespace.&nbsp;</p>\n<p>Upvotes on advice posted in this thread should be based on your estimate of the usefulness of the advice; in particular, please do not vote up advice just because it sounds deep or punchy.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 8, "ABG8vt87eW4FFA6gD": 1, "Eha62RrqBtEbpcEza": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HEn2qiMxk5BggN83J", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 100, "extendedScore": null, "score": 0.000227, "legacy": true, "legacyId": "21929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 106, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 573, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tLR9YZHiNoDE2Czjh", "XKXsJAFnnBLeqfPiY", "ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-03-07T04:33:41.739Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T05:48:39.956Z", "modifiedAt": null, "url": null, "title": "Trust in God, or, The Riddle of Kyon Fan Visual Novel", "slug": "trust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kuAKxP6Zy4FzPCz8F/trust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "pageUrlRelative": "/posts/kuAKxP6Zy4FzPCz8F/trust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "linkUrl": "https://www.lesswrong.com/posts/kuAKxP6Zy4FzPCz8F/trust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trust%20in%20God%2C%20or%2C%20The%20Riddle%20of%20Kyon%20Fan%20Visual%20Novel&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrust%20in%20God%2C%20or%2C%20The%20Riddle%20of%20Kyon%20Fan%20Visual%20Novel%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkuAKxP6Zy4FzPCz8F%2Ftrust-in-god-or-the-riddle-of-kyon-fan-visual-novel%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trust%20in%20God%2C%20or%2C%20The%20Riddle%20of%20Kyon%20Fan%20Visual%20Novel%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkuAKxP6Zy4FzPCz8F%2Ftrust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkuAKxP6Zy4FzPCz8F%2Ftrust-in-god-or-the-riddle-of-kyon-fan-visual-novel", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>I made <a href=\"http://www.fanfiction.net/s/5588986/1/Trust-in-God-or-The-Riddle-of-Kyon\">Trust in God, or, The Riddle of Kyon</a>, a Haruhi fanfic by Eliezer Yudkowsky,&nbsp;into <a href=\"https://github.com/DanielLC/TiGoTRoK\">a visual novel</a>. At least, I started it. It still needs quite a bit of work. If anyone wants to edit it, message me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kuAKxP6Zy4FzPCz8F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.1313766422312155e-06, "legacy": true, "legacyId": "21930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T06:30:02.536Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Wise Pretentions v.0", "slug": "seq-rerun-wise-pretentions-v-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZumA3sTRciDNXBPr3/seq-rerun-wise-pretentions-v-0", "pageUrlRelative": "/posts/ZumA3sTRciDNXBPr3/seq-rerun-wise-pretentions-v-0", "linkUrl": "https://www.lesswrong.com/posts/ZumA3sTRciDNXBPr3/seq-rerun-wise-pretentions-v-0", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Wise%20Pretentions%20v.0&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Wise%20Pretentions%20v.0%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZumA3sTRciDNXBPr3%2Fseq-rerun-wise-pretentions-v-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Wise%20Pretentions%20v.0%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZumA3sTRciDNXBPr3%2Fseq-rerun-wise-pretentions-v-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZumA3sTRciDNXBPr3%2Fseq-rerun-wise-pretentions-v-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/yq/wise_pretensions_v0/\">Wise Pretensions v.0</a> was originally published on 20 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Wise_Pretensions_v.0\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An earlier post, on the same topic as yesterday's post.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gwp/seq_rerun_pretending_to_be_wise/\">Pretending to be Wise</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZumA3sTRciDNXBPr3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.131403682530439e-06, "legacy": true, "legacyId": "21933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i97ohcwLugt5oQvMy", "8Ybe52mjcZDW2iJAc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T07:09:05.212Z", "modifiedAt": null, "url": null, "title": "asdf", "slug": "asdf-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Crux", "createdAt": "2011-09-15T17:49:36.096Z", "isAdmin": false, "displayName": "Crux"}, "userId": "XfQRFDS5eFdeYe6uM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TWGk99ZMRTojrrDGx/asdf-4", "pageUrlRelative": "/posts/TWGk99ZMRTojrrDGx/asdf-4", "linkUrl": "https://www.lesswrong.com/posts/TWGk99ZMRTojrrDGx/asdf-4", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20asdf&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aasdf%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWGk99ZMRTojrrDGx%2Fasdf-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=asdf%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWGk99ZMRTojrrDGx%2Fasdf-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWGk99ZMRTojrrDGx%2Fasdf-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>asdf</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TWGk99ZMRTojrrDGx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": -1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "21934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T07:39:39.262Z", "modifiedAt": null, "url": null, "title": "asdf", "slug": "asdf-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wakarimahen", "createdAt": "2013-01-17T05:49:07.530Z", "isAdmin": false, "displayName": "Wakarimahen"}, "userId": "4i56ZM8qdGLAdcwPn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zgpeD3xYw6eirE9ty/asdf-3", "pageUrlRelative": "/posts/zgpeD3xYw6eirE9ty/asdf-3", "linkUrl": "https://www.lesswrong.com/posts/zgpeD3xYw6eirE9ty/asdf-3", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20asdf&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aasdf%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzgpeD3xYw6eirE9ty%2Fasdf-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=asdf%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzgpeD3xYw6eirE9ty%2Fasdf-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzgpeD3xYw6eirE9ty%2Fasdf-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>asdf</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zgpeD3xYw6eirE9ty", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-07T16:09:54.710Z", "modifiedAt": null, "url": null, "title": "Eliezer's YU lecture on FAI and MOR [link]", "slug": "eliezer-s-yu-lecture-on-fai-and-mor-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oL5iJrWnpA5LNntE8/eliezer-s-yu-lecture-on-fai-and-mor-link", "pageUrlRelative": "/posts/oL5iJrWnpA5LNntE8/eliezer-s-yu-lecture-on-fai-and-mor-link", "linkUrl": "https://www.lesswrong.com/posts/oL5iJrWnpA5LNntE8/eliezer-s-yu-lecture-on-fai-and-mor-link", "postedAtFormatted": "Thursday, March 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eliezer's%20YU%20lecture%20on%20FAI%20and%20MOR%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEliezer's%20YU%20lecture%20on%20FAI%20and%20MOR%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoL5iJrWnpA5LNntE8%2Feliezer-s-yu-lecture-on-fai-and-mor-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eliezer's%20YU%20lecture%20on%20FAI%20and%20MOR%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoL5iJrWnpA5LNntE8%2Feliezer-s-yu-lecture-on-fai-and-mor-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoL5iJrWnpA5LNntE8%2Feliezer-s-yu-lecture-on-fai-and-mor-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://yucsc.com/ey.aspx</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oL5iJrWnpA5LNntE8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 1.1317827603309074e-06, "legacy": true, "legacyId": "21936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T02:34:27.571Z", "modifiedAt": null, "url": null, "title": "A problem with \"playing chicken with the universe\" as an approach to UDT", "slug": "a-problem-with-playing-chicken-with-the-universe-as-an", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.772Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Karl", "createdAt": "2010-10-27T21:06:59.318Z", "isAdmin": false, "displayName": "Karl"}, "userId": "EN2BxuvFHRgmLbbp3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9KoyMKHmwCCJdMma4/a-problem-with-playing-chicken-with-the-universe-as-an", "pageUrlRelative": "/posts/9KoyMKHmwCCJdMma4/a-problem-with-playing-chicken-with-the-universe-as-an", "linkUrl": "https://www.lesswrong.com/posts/9KoyMKHmwCCJdMma4/a-problem-with-playing-chicken-with-the-universe-as-an", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20problem%20with%20%22playing%20chicken%20with%20the%20universe%22%20as%20an%20approach%20to%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20problem%20with%20%22playing%20chicken%20with%20the%20universe%22%20as%20an%20approach%20to%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KoyMKHmwCCJdMma4%2Fa-problem-with-playing-chicken-with-the-universe-as-an%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20problem%20with%20%22playing%20chicken%20with%20the%20universe%22%20as%20an%20approach%20to%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KoyMKHmwCCJdMma4%2Fa-problem-with-playing-chicken-with-the-universe-as-an", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KoyMKHmwCCJdMma4%2Fa-problem-with-playing-chicken-with-the-universe-as-an", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>Let's consider the agent given in&nbsp;<a style=\"color: #3d3d3e;\" href=\"/r/discussion/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">A model of UDT with a halting oracle</a>. One will notice that that agent is not quite well defined because it doesn't tell us in what order we are supposed to consider actions in step 1. But surely that doesn't matter, right? Wrong.</p>\n<p>&nbsp;</p>\n<p>Let's consider the prisoner&nbsp;dilemma&nbsp;with payment matrix given by</p>\n<p>&nbsp;</p>\n<table style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\" border=\"2\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>1: C</td>\n<td>1:&nbsp; D</td>\n</tr>\n<tr>\n<td>2: C</td>\n<td>(3, 3)</td>\n<td>(5, 0)</td>\n</tr>\n<tr>\n<td>2: D</td>\n<td>(0, 5)</td>\n<td>(2, 2)</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">and consider agent A which consider whether there is a proof that A()&ne;D before considering whether there is a proof that A()&ne;C and agent A' which do things in the opposite order. If A or A' is&nbsp;pitted against itself&nbsp;everything is well and mutual cooperation is the result of the game but what if A is pitted against A'? Then A break down and cry.</span></span></p>\n<p style=\"margin: 0px 0px 1em; text-align: justify;\">Let's call the utility functions of A U and the utility function of A' U' and consider a model of PA in which PA is inconsistent (such a model must exist if PA is consistent). In such a model we will have A()=D and A'()=C and so U()=5 and U'()=0. That means that A will not be able to prove that A()=D =&gt; U()=u for any u different from 5 and so either A will defect and A' will cooperate or A will break down and cry, but A' will not cooperate because it cannot prove A'()=C =&gt; U()=u' for any u' except possibly 0, so A will break down and cry. QED</p>\n<p style=\"margin: 0px 0px 1em; text-align: justify;\">More generally if M is a model of PA in which PA is inconsistent, an agent defined in this way will never be able to prove that A()=a =&gt; U()=u (where a is the first action considered in step 1) except possibly for u=u<sub>0</sub>&nbsp;where u<sub>0</sub>&nbsp;is the value of U() in M. That seems to create a huge problem for that approach to UDT.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9KoyMKHmwCCJdMma4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 1.1321912969519267e-06, "legacy": true, "legacyId": "21939", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bj244uWzDBXvE2N2S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T05:48:29.463Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Fairness vs. Goodness", "slug": "seq-rerun-fairness-vs-goodness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3QWmzDzoQudgQnMmi/seq-rerun-fairness-vs-goodness", "pageUrlRelative": "/posts/3QWmzDzoQudgQnMmi/seq-rerun-fairness-vs-goodness", "linkUrl": "https://www.lesswrong.com/posts/3QWmzDzoQudgQnMmi/seq-rerun-fairness-vs-goodness", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Fairness%20vs.%20Goodness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Fairness%20vs.%20Goodness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3QWmzDzoQudgQnMmi%2Fseq-rerun-fairness-vs-goodness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Fairness%20vs.%20Goodness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3QWmzDzoQudgQnMmi%2Fseq-rerun-fairness-vs-goodness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3QWmzDzoQudgQnMmi%2Fseq-rerun-fairness-vs-goodness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/ys/fairness_vs_goodness/\">Fairness vs. Goodness</a> was originally published on 22 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Fairness_vs._Goodness\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An experiment in which two unprepared subjects play an asymmetric version of the Prisoner's Dilemma. Is the best outcome the one where each player gets as many points as possible, or the one in which each player gets about the same number of points?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gx9/seq_rerun_wise_pretentions_v0/\">Wise Pretentions v.0</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3QWmzDzoQudgQnMmi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1323182721662672e-06, "legacy": true, "legacyId": "21946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rKGvgRiEu5qYechNT", "ZumA3sTRciDNXBPr3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T09:31:52.642Z", "modifiedAt": null, "url": null, "title": "Destructive mathematics", "slug": "destructive-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pn56aZyBAuFxF64oA/destructive-mathematics", "pageUrlRelative": "/posts/pn56aZyBAuFxF64oA/destructive-mathematics", "linkUrl": "https://www.lesswrong.com/posts/pn56aZyBAuFxF64oA/destructive-mathematics", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Destructive%20mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADestructive%20mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpn56aZyBAuFxF64oA%2Fdestructive-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Destructive%20mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpn56aZyBAuFxF64oA%2Fdestructive-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpn56aZyBAuFxF64oA%2Fdestructive-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1223, "htmlBody": "<p><strong>Follow-up to</strong>: <a href=\"/lw/gug/constructive_mathemathics_and_its_dual/\" target=\"_blank\">Constructive mathematics and its dual</a></p>\n<p>In last post, I've introduced constructive mathmatics, intuitionistic logic (JL) and its dual, uninspiringly called dual-intuitionistic logic (DL).<br />I've said that JL differs from classical logic about the status of the <em>law of excluded middle</em>, a principle valid in the latter which states that a formula can be meaningfully only asserted or negated. This, in the meta-theory, means you can prove that something is true if you can show that its negation is false.<br />Constructivists, coming from a philosophical platform that regards mathematics as a construction of the human mind, refuse this principle: their idea is that a formula can be said to be true if and only if there is a direct proof of it. Similarly, a formula can be said to be false if and only if there's a direct proof of its negation. If no proof or refutation exists yet (as is the case today, for example, for the Goldbach conjecture), then <em>nothing</em> can be said about A.<br />Thus <img title=\"A \\lor \\lnot A\" src=\"http://www.codecogs.com/png.latex?A \\lor \\lnot A\" alt=\"\" align=\"bottom\" /> is no more a tautology (although it can still be true for some formula, precisely for those that already have a proof or a refutation).<br />Intuitionism anyway (the most prominent subset of the constructivist program), thinks that <img title=\"A \\land \\lnot A\" src=\"http://www.codecogs.com/png.latex?A \\land \\lnot A\" alt=\"\" align=\"bottom\" /> is still always false, and so JL incorporates <img title=\"\\lnot (A \\land \\lnot A)\" src=\"http://www.codecogs.com/png.latex?\\lnot (A \\land \\lnot A)\" alt=\"\" align=\"bottom\" />, a principle called <em>the law of non-contradiction</em>.<br />Intuitionistic logic has no built-in model of time, but you can picture the mental activity of an adherent in this way: he starts with no (or very little) truths, and incorporates in his theory only those theorems of which he can build a proof of, and the negation of those theorems that he can produce a refutation of.<br />Mathematics, as an endeavour, is seen as an accumulation of truth from an empty base.</p>\n<p>I've also indicated that there's a direct dual of JL, which is part of a wider class of systems collecively known as paraconsistent logics. Compared to the amount of studies dedicate to intuitionistic logic, DL is basically unknown, but you can consult for example <a href=\"ftp://logica.cle.unicamp.br/pub/e-prints/vol.3,n.1,2003.pdf\" target=\"_blank\">this paper</a>&nbsp;and <a href=\"http://simondalfonso.id.au/documents/philosophy/dual_intuitionistic.pdf\" target=\"_blank\">this one</a>.<br />In this second article, a model is presented for which DL is valid, and we can read the following quote: \"[These semantics] reflect the notion that our current knowledge about the falsity of statements can increase. Some statements whose falsity status was previously indeterminate can down the track be established as false. The value false corresponds to firmly established falsity that is preserved with the advancement of knowledge whilst the value true corresponds to 'not false yet'\".</p>\n<p>My suggestion is to be a lot braver in our epistemology: let's suppose that the natural cognitive state is not one of utter ignorance, but of triviality. Let's then just assume that <strong>in the beginning, everything is true</strong>.<br />Our job then, as mathematician, is to discover refutations: the refutation of <img title=\"\\lnot A\" src=\"http://www.codecogs.com/png.latex?\\lnot A\" alt=\"\" align=\"bottom\" /> will expunge A from the set of truth, the refutation of A will remove <img title=\"\\lnot A\" src=\"http://www.codecogs.com/png.latex?\\lnot A\" alt=\"\" align=\"bottom\" />.<br />This dual of constructive mathematics just begs to be called destructive mathematics (or destructivism): as a program, it means to start with the maximal possibility and to develop careful collection of falsities.<br />Be careful though: it doesn't necessarily mean that we accept the existence of actual contradictions. It might be very well the case that in our world (or model of interest) there are no contradictions, we 'just' need to expunge the relevant assertions.<br />As the dual of constructive mathematics, destructivism regards mathematics as a mental construction, one though that procedes from triviality through confutations.</p>\n<p>One major difficulty with destructive mathematics is that, to arrive to a finite set of truths, you need to destroy an infinite amount of falsities (but, on the other side, to arrive to a finite set of falsities in constructive mathematics you need to assert an infinite number of truths).<br />Usually, we are more interested in truth, so why should we embark in such an effort?<br />I can see at least two weak and two strong reasons, plus another one that counts as entertainment of which I'll talk about more extensively in the last post.<br />The first weak reason is that sometimes, we <em>are</em> more interested in falsity rather than truth. Destructivism seems to be a more natural background for the <a href=\"http://en.wikipedia.org/wiki/Resolution_(logic)\" target=\"_blank\">calculus of resolution</a>, although, to my knowledge, this has only been developed in classical setting.<br />The second weak reason is that destructivism is an interesting choice for coalgebraic methods in computer science: there, co-induction and co-recursion are a method for 'observing' or 'destroying' (potentially) infinite objects. From the Wikipedia entry on <a href=\"http://en.wikipedia.org/wiki/Coinduction\" target=\"_blank\">coinduction</a>: \"As a definition or specification, coinduction describes how an object may be \"observed\", \"broken down\" or \"destructed\" into simpler objects. As a proof technique, it may be used to show that an equation is satisfied by all possible implementations of such a specification.\"<br />I whish I could say more, but I don't know much myself: the parallelisms are tempting, but I have to leave the discovery of eventual low-hanging fruits to later times or someone else entirely.</p>\n<p>Two instead much more promising fields of application are Tegmark universes and the Many World quantum mechanics.<br />It's difficult to give a cogent account for why all the mathematical structures should exists, but Tegmark position equates simply a platonist point of view on destructivism.<br />If all formulas are true, then this means that \"somewhere\" every model is realized, while on the other side, if all structures are realized, then \"on the whole\", every formula is true (somewhere).<br />But the most important reason why one should adopt this framework is that it gives a natural account of quantum mechanics in the Many World flavour (MWI).</p>\n<p>Usually, physical laws are seeen as the corrispondence between physically realizable states, and time is the \"adjunction\" of new states from older ones. Do you recognize anything?<br />What if, instead, physical laws dictates only those states that ought to be excluded and time is simply the 'destruction' or 'localization' of all those possible states? Well, then you have (almost for free) MWI: every state is realized, but in times you are constrained to just one.<br />I'm extremely tempted to say that MWI is the dual of the wave function collapse, but of course I cannot (yet) prove it. Or should I just say that I cannot yet disprove it's not like that?<br />If that's the case, the mystery of why subjective probability follows the Born rule will be 'just' the dual of the non-linear mechanism of collapse. One mystery for a mystery.<br />I also suspect that destructive mathematics might have implication even for probability theory, but... This framework is still in its infancy, so who knows?</p>\n<p>The last interesting motivation for taking seriously destructive mathematics is that it offers a possible coherent account of Chtulhu mythos (!!): what if God, instead of having created only this world from nothing out of pure love, has destructed every world but this one out of pure hate? If you accept the first scenario, then the second scenario is equally plausible / conceivable. I'll explore the theme in the last post: Azathoth hates us all!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pn56aZyBAuFxF64oA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Jf67y793ZMc8DF2xQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T16:46:26.647Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Buffalo, Cambridge UK, Paderborn, Moscow, Tokyo, Vancouver", "slug": "weekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nCBs4rAZA4BxvbFMt/weekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "pageUrlRelative": "/posts/nCBs4rAZA4BxvbFMt/weekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "linkUrl": "https://www.lesswrong.com/posts/nCBs4rAZA4BxvbFMt/weekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20UK%2C%20Paderborn%2C%20Moscow%2C%20Tokyo%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20UK%2C%20Paderborn%2C%20Moscow%2C%20Tokyo%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCBs4rAZA4BxvbFMt%2Fweekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20UK%2C%20Paderborn%2C%20Moscow%2C%20Tokyo%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCBs4rAZA4BxvbFMt%2Fweekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCBs4rAZA4BxvbFMt%2Fweekly-lw-meetups-austin-buffalo-cambridge-uk-paderborn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p><strong>This summary was posted to LW main on March 1st. The following week's summary is posted <a href=\"/lw/gxs/weekly_lw_meetups_durham_london_vienna/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ir\">Tokyo Meetup:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/js\">Vancouver Personal Productivity:&nbsp;<span class=\"date\">02 March 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/jo\">Moscow, Expanding rationality:&nbsp;<span class=\"date\">03 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ju\">(Buffalo NY) Sunday Meetup at Buffalo Labs:&nbsp;<span class=\"date\">03 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/jz\">Paderborn Meetup March 6th:&nbsp;<span class=\"date\">06 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/j8\">Vienna Meetup 9th March:&nbsp;<span class=\"date\">09 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/jx\">London Meetup, 10th March:&nbsp;<span class=\"date\">10 March 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/jq\">Brussels meetup:&nbsp;<span class=\"date\">16 March 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/jp\">Munich Meetup:&nbsp;<span class=\"date\">01 April 2013 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">02 March 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/jy\">Cambridge, UK LW Meetup [Reading Group, HAEFB-04]:&nbsp;<span class=\"date\">03 March 2013 11:00AM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nCBs4rAZA4BxvbFMt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1327490279520202e-06, "legacy": true, "legacyId": "21852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bf6h7BLtjHiNwzmMS", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T18:32:04.260Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Theory and Practice", "slug": "meetup-moscow-theory-and-practice", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u55mrXpQL8huhTDXh/meetup-moscow-theory-and-practice", "pageUrlRelative": "/posts/u55mrXpQL8huhTDXh/meetup-moscow-theory-and-practice", "linkUrl": "https://www.lesswrong.com/posts/u55mrXpQL8huhTDXh/meetup-moscow-theory-and-practice", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Theory%20and%20Practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Theory%20and%20Practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu55mrXpQL8huhTDXh%2Fmeetup-moscow-theory-and-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Theory%20and%20Practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu55mrXpQL8huhTDXh%2Fmeetup-moscow-theory-and-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu55mrXpQL8huhTDXh%2Fmeetup-moscow-theory-and-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k5'>Moscow, Theory and Practice</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Short presentations. Two or three people will tell us about something interesting.</p></li>\n<li><p>Practical rationality. We will train useful skills.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k5'>Moscow, Theory and Practice</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u55mrXpQL8huhTDXh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1328182079080863e-06, "legacy": true, "legacyId": "21953", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Theory_and_Practice\">Discussion article for the meetup : <a href=\"/meetups/k5\">Moscow, Theory and Practice</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Short presentations. Two or three people will tell us about something interesting.</p></li>\n<li><p>Practical rationality. We will train useful skills.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Theory_and_Practice1\">Discussion article for the meetup : <a href=\"/meetups/k5\">Moscow, Theory and Practice</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Theory and Practice", "anchor": "Discussion_article_for_the_meetup___Moscow__Theory_and_Practice", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Theory and Practice", "anchor": "Discussion_article_for_the_meetup___Moscow__Theory_and_Practice1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T21:20:36.893Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal Meetup - Biased Pandemic", "slug": "meetup-montreal-meetup-biased-pandemic", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ee33a5FqkWnb59tac/meetup-montreal-meetup-biased-pandemic", "pageUrlRelative": "/posts/ee33a5FqkWnb59tac/meetup-montreal-meetup-biased-pandemic", "linkUrl": "https://www.lesswrong.com/posts/ee33a5FqkWnb59tac/meetup-montreal-meetup-biased-pandemic", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20Meetup%20-%20Biased%20Pandemic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20Meetup%20-%20Biased%20Pandemic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fee33a5FqkWnb59tac%2Fmeetup-montreal-meetup-biased-pandemic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20Meetup%20-%20Biased%20Pandemic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fee33a5FqkWnb59tac%2Fmeetup-montreal-meetup-biased-pandemic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fee33a5FqkWnb59tac%2Fmeetup-montreal-meetup-biased-pandemic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k6'>Montreal Meetup - Biased Pandemic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 March 2013 05:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup!</p>\n\n<p>We're going to be doing biased boardgaming, specifically with the game Pandemic. We have two copies, so we should have enough for all members.</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k6'>Montreal Meetup - Biased Pandemic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ee33a5FqkWnb59tac", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.1329286105935111e-06, "legacy": true, "legacyId": "21955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Biased_Pandemic\">Discussion article for the meetup : <a href=\"/meetups/k6\">Montreal Meetup - Biased Pandemic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 March 2013 05:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup!</p>\n\n<p>We're going to be doing biased boardgaming, specifically with the game Pandemic. We have two copies, so we should have enough for all members.</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Biased_Pandemic1\">Discussion article for the meetup : <a href=\"/meetups/k6\">Montreal Meetup - Biased Pandemic</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal Meetup - Biased Pandemic", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Biased_Pandemic", "level": 1}, {"title": "Discussion article for the meetup : Montreal Meetup - Biased Pandemic", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Biased_Pandemic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-08T21:36:42.641Z", "modifiedAt": null, "url": null, "title": "Meetup : [Reading Group, HAEFB-05]", "slug": "meetup-reading-group-haefb-05", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F7E6JnHCnA7GRGvNo/meetup-reading-group-haefb-05", "pageUrlRelative": "/posts/F7E6JnHCnA7GRGvNo/meetup-reading-group-haefb-05", "linkUrl": "https://www.lesswrong.com/posts/F7E6JnHCnA7GRGvNo/meetup-reading-group-haefb-05", "postedAtFormatted": "Friday, March 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BReading%20Group%2C%20HAEFB-05%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BReading%20Group%2C%20HAEFB-05%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7E6JnHCnA7GRGvNo%2Fmeetup-reading-group-haefb-05%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BReading%20Group%2C%20HAEFB-05%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7E6JnHCnA7GRGvNo%2Fmeetup-reading-group-haefb-05", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7E6JnHCnA7GRGvNo%2Fmeetup-reading-group-haefb-05", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k7'>[Reading Group, HAEFB-05]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity College JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week, we'll be continuing on in our meetup series of reading the new sequence, Highly Advanced Epistemology for Beginners. Specifically, we're upto Casual Reference, almost at the end of the Causality/Physics subsequence.\nIf this has gotten a bit stale for you, though, watch this space in the coming weeks for some new stuff we're planning. Oh yes, plans are happening. Muahahah, etc.\nSee you there!\n-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k7'>[Reading Group, HAEFB-05]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F7E6JnHCnA7GRGvNo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1329391549477813e-06, "legacy": true, "legacyId": "21956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Reading_Group__HAEFB_05_\">Discussion article for the meetup : <a href=\"/meetups/k7\">[Reading Group, HAEFB-05]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity College JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week, we'll be continuing on in our meetup series of reading the new sequence, Highly Advanced Epistemology for Beginners. Specifically, we're upto Casual Reference, almost at the end of the Causality/Physics subsequence.\nIf this has gotten a bit stale for you, though, watch this space in the coming weeks for some new stuff we're planning. Oh yes, plans are happening. Muahahah, etc.\nSee you there!\n-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Reading_Group__HAEFB_05_1\">Discussion article for the meetup : <a href=\"/meetups/k7\">[Reading Group, HAEFB-05]</a></h2>", "sections": [{"title": "Discussion article for the meetup : [Reading Group, HAEFB-05]", "anchor": "Discussion_article_for_the_meetup____Reading_Group__HAEFB_05_", "level": 1}, {"title": "Discussion article for the meetup : [Reading Group, HAEFB-05]", "anchor": "Discussion_article_for_the_meetup____Reading_Group__HAEFB_05_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T01:37:18.563Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] On Not Having an Advance Abyssal Plan", "slug": "seq-rerun-on-not-having-an-advance-abyssal-plan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ffPxHecsQvh9ocyNM/seq-rerun-on-not-having-an-advance-abyssal-plan", "pageUrlRelative": "/posts/ffPxHecsQvh9ocyNM/seq-rerun-on-not-having-an-advance-abyssal-plan", "linkUrl": "https://www.lesswrong.com/posts/ffPxHecsQvh9ocyNM/seq-rerun-on-not-having-an-advance-abyssal-plan", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20On%20Not%20Having%20an%20Advance%20Abyssal%20Plan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20On%20Not%20Having%20an%20Advance%20Abyssal%20Plan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FffPxHecsQvh9ocyNM%2Fseq-rerun-on-not-having-an-advance-abyssal-plan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20On%20Not%20Having%20an%20Advance%20Abyssal%20Plan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FffPxHecsQvh9ocyNM%2Fseq-rerun-on-not-having-an-advance-abyssal-plan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FffPxHecsQvh9ocyNM%2Fseq-rerun-on-not-having-an-advance-abyssal-plan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"/lw/yt/on_not_having_an_advance_abyssal_plan/\">On Not Having an Advance Abyssal Plan</a> was originally published on 23 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#On_Not_Having_an_Advance_Abyssal_Plan\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Don't say that you'll figure out a solution to the worst case scenario if the worst case scenario happens. Plan it out in advance.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gxm/seq_rerun_fairness_vs_goodness/\">Fairness vs. Goodness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ffPxHecsQvh9ocyNM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1330967919310865e-06, "legacy": true, "legacyId": "21957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nYEcgJe8LyqB4qqCL", "3QWmzDzoQudgQnMmi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T07:12:04.284Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver!", "slug": "meetup-vancouver-5", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w3pStvjC4FJaYMFNv/meetup-vancouver-5", "pageUrlRelative": "/posts/w3pStvjC4FJaYMFNv/meetup-vancouver-5", "linkUrl": "https://www.lesswrong.com/posts/w3pStvjC4FJaYMFNv/meetup-vancouver-5", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3pStvjC4FJaYMFNv%2Fmeetup-vancouver-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3pStvjC4FJaYMFNv%2Fmeetup-vancouver-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3pStvjC4FJaYMFNv%2Fmeetup-vancouver-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k8'>Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 west broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I haven't been able to think of a worthy topic, so we can have a rambling topicless general hangout meetup. I'm sure it will be much fun.</p>\n\n<p>As is usual lately, 15:00 at Bennys on saturday. Look for a medium-sized group of geeks, and maybe text me if you're lost (778 714 0234). Sorry for the short notice.</p>\n\n<p>Also, join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k8'>Vancouver!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w3pStvjC4FJaYMFNv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1333161878360628e-06, "legacy": true, "legacyId": "21966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_\">Discussion article for the meetup : <a href=\"/meetups/k8\">Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 west broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I haven't been able to think of a worthy topic, so we can have a rambling topicless general hangout meetup. I'm sure it will be much fun.</p>\n\n<p>As is usual lately, 15:00 at Bennys on saturday. Look for a medium-sized group of geeks, and maybe text me if you're lost (778 714 0234). Sorry for the short notice.</p>\n\n<p>Also, join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_1\">Discussion article for the meetup : <a href=\"/meetups/k8\">Vancouver!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T13:53:52.270Z", "modifiedAt": null, "url": null, "title": "Rationalist fiction brainstorming funtimes", "slug": "rationalist-fiction-brainstorming-funtimes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:31.051Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bBcStSetHwXv4ovmj/rationalist-fiction-brainstorming-funtimes", "pageUrlRelative": "/posts/bBcStSetHwXv4ovmj/rationalist-fiction-brainstorming-funtimes", "linkUrl": "https://www.lesswrong.com/posts/bBcStSetHwXv4ovmj/rationalist-fiction-brainstorming-funtimes", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20fiction%20brainstorming%20funtimes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20fiction%20brainstorming%20funtimes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBcStSetHwXv4ovmj%2Frationalist-fiction-brainstorming-funtimes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20fiction%20brainstorming%20funtimes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBcStSetHwXv4ovmj%2Frationalist-fiction-brainstorming-funtimes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBcStSetHwXv4ovmj%2Frationalist-fiction-brainstorming-funtimes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 572, "htmlBody": "<p>The title should make things clear enough, so let's start with my description of the target, rationalist fiction: fiction that tries to teach the audience rationalist cognitive skills by having characters model those skills for the reader.</p>\n<p>So for example, <a href=\"http://luminous.elcenia.com/chapters/ch1.shtml\"><em>Luminosity</em></a> is to a large extent about the questions \"What do I want?, What do I have?, and How can I best use the latter to get the former?\"&nbsp; Oh, and using empiricism on magic.</p>\n<p>Another example is <a href=\"http://hpmor.com/\"><em>Harry Potter and the Methods of Rationality</em></a>, which goes more in-depth about the laundry list of human biases. In fact, many of the more iconic moments (measured by what I remember and what other people like to copy) are about biases to avoid, rather than about modeling good behavior.</p>\n<p>&nbsp;</p>\n<p>This thread is about ideas, from general to specific, for rationalist fiction. I'll give some obvious examples.</p>\n<p>General idea: having a rational character encountering magic or amazing technology is a great chance to showcase the power of empiricism.&nbsp; (Has anyone gotten on this one yet? :3 )</p>\n<p>Story idea: Okay, so we take the Dresden Files universe, and our rational protagonist is some smart kid who just started a summer job as an assistant radio technician or something. It turns out he's got one in a hundred magical talent, enough to cut off his budding career, he manages to find the magic community, figures out just enough, embarks on heroic quest to run a magitech radio station. (Okay, this last bit isn't obvious - for one, more character development would probably have him wanting something else.&nbsp; For another, the obvious thing is to take over the world if Luminosity and HPMOR are anything to go by.)</p>\n<p>Specific idea: A character could model the skill of testing stuff by testing stuff.&nbsp; When characters are performing a big search, have someone actually stop to think about false positives, or more generally \"how could things be going wrong, and how can I prevent that?\", and have it actually be a false positive once.</p>\n<p>&nbsp;</p>\n<p>But really, there's an<em> explosion</em> of possibilities out there to explore, and I feel like we have \"Rationalist meets magic. Rationalist does science to magic.&nbsp; Rationalist kicks butt with magic\" fairly well-covered.&nbsp; We have <a href=\"/lw/fc3/checklist_of_rationality_habits/\">all these different biases categorized, with corresponding right ways to do things</a>, and there are plenty of good behaviors we can try to teach an audience without the empiricism-fodder and high stakes that is a fantasy setting. Or even if you do a Dresden Files fic, you could ignore the empricism stuff and just, like, pick a habit from Anna's checklist and write a short story :D. Here's an idea I quite fancy, I'll save everything else for comments:</p>\n<p>General idea: Giving people the benefit of the doubt and managing to lose arguments when they need to be lost is the closest thing to a rationalist superpower I have. Can I work that into a story somehow?</p>\n<p>Story ideas: A <a href=\"http://www.e-reading-lib.org/bookreader.php/1007512/Herriots_-_Every_living_thing.html\">James Herriot</a> sort of thing, where the protagonist has their daily life (Maybe veterinarian, or materials scientist, or line cook, or model rocket hobbyist), and relatably goes about it, occasionally giving people the benefit of the doubt and losing arguments, and sometimes using other rationalist skills, and usually ending up on the right side of things in the end. At this point it might be too subtle to actually teach the audience, one solution to this would be a designated person in-story to periodically notice how awesome the protagonist is.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bBcStSetHwXv4ovmj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "21967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T18:17:04.977Z", "modifiedAt": "2021-06-19T11:09:36.352Z", "url": null, "title": "Co-Working Collaboration to Combat Akrasia", "slug": "co-working-collaboration-to-combat-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "ShannonFriedman", "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zq3Dey8ZboSby6YAv/co-working-collaboration-to-combat-akrasia", "pageUrlRelative": "/posts/Zq3Dey8ZboSby6YAv/co-working-collaboration-to-combat-akrasia", "linkUrl": "https://www.lesswrong.com/posts/Zq3Dey8ZboSby6YAv/co-working-collaboration-to-combat-akrasia", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Co-Working%20Collaboration%20to%20Combat%20Akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACo-Working%20Collaboration%20to%20Combat%20Akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZq3Dey8ZboSby6YAv%2Fco-working-collaboration-to-combat-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Co-Working%20Collaboration%20to%20Combat%20Akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZq3Dey8ZboSby6YAv%2Fco-working-collaboration-to-combat-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZq3Dey8ZboSby6YAv%2Fco-working-collaboration-to-combat-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 677, "htmlBody": "<p>Before I was very involved in the Less Wrong community, I heard that Eliezer was looking for <a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">people to sit with him</a> while he worked, to increase writing productivity. I knew that he was doing important work in the world, and figured that this was the sort of contribution to improving humanity that I would like to make, which was within the set of things that would be easy and enjoyable for me.</p><p>So I got a hold of him and offered to come and sit with him, and did that once/week for about a year. As anticipated, it worked marvelously. I found it easy to sit and not talk, just getting my own work done. &nbsp;Eventually I became a beta reader for his \"Bayes for Everyone Else\" which is really great and helped me in my ability to estimate probabilities a ton. (Eliezer is still perfecting this work and has not yet released it, but you can find the older version <a href=\"http://yudkowsky.net/rational/bayes\">here</a>.)&nbsp;<br><br>In addition to learning the basics of Bayes from doing this, I also learned how powerful it is to have someone just to sit quietly with you to co-work on a regular schedule.&nbsp;<br><br>I\u2019ve experimented with similar things since then, such as making skype dates with a friend to watch informational videos together. This worked for awhile until my friend got busy. I have two other recurring chat dates with friends to do <a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">dual n-back</a> together, and those have worked quite well and are still going.&nbsp;<br><br>A client of mine, <a href=\"/user/Mqrius/\">Mqrius</a>, is working on his Master\u2019s thesis and has found that the only way he has been able to overcome his akrasia so far is by co-working with a friend. Unfortunately, his friend does not have as much time to co-work as he\u2019d like, so we decided to spend Mqrius\u2019s counseling session today writing this Less Wrong post to see if we can help him and other people in the community who want to co-work over skype connect, since this will probably be much higher value to him as well as others with similar difficulties than the next best thing we could do with the time.&nbsp;<br><br>I encourage anyone who is interested in co-working, watching informational videos together, or any other social productivity experiments that can be done over skype or chat, to coordinate in the comments. For this to work best, I recommend being as specific as possible about the ideal co-working partner for you, in addition to noting if you are open to general co-working.&nbsp;<br><br>If you are <a href=\"/lw/bc3/sotw_be_specific/\">specific</a>, you are much more likely to succeed in finding a good co-working partner for you. While its possible you might screen someone out, its more likely that you will get the attention of your ideal co-working partner who otherwise would have glossed over your comment.&nbsp;<br><br>Here is my specific pitch for <a href=\"/user/Mqrius/\">Mqrius</a>:</p><blockquote><p>If you are working on a thesis, especially if it\u2019s related to nanotechnology like his thesis, and think that you are likely to be similarly motivated by co-working, please comment or contact him about setting up an initial skype trial run. His ideal scenario is to find 2-3 people to co-work with him for about 20 hours co-working/week time for him in total. He would like to find people who are dependable about showing up for appointments they have made and will create a recurring schedule with him at least until he gets his thesis done. He\u2019d like to try an initial 4 hour co-working block as an experiment with interested parties. &nbsp;&nbsp;Please comment below if you are interested.</p><p>[Mqrius and I have predictions going about whether or not he will actually get a co-working partner who is working on a nanotech paper out of this, if others want to post predictions in the comments, this is encouraged. &nbsp;Its a good practice for reducing <a href=\"/lw/il/hindsight_bias/\">hindsight bias</a>.]</p></blockquote><p>[edit]</p><p>An virtual co-working space has been created and is currently live, discussion and link to the room <a href=\"/lw/gwo/coworking_collaboration_to_combat_akrasia/8lak\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 3, "r7qAjcbfhj2256EHH": 3, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zq3Dey8ZboSby6YAv", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 74, "extendedScore": null, "score": 0.000199, "legacy": true, "legacyId": "21912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "r38pkCm7wF4M44MDQ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W", "fkM9XsNvXdYH6PPAx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T19:07:02.575Z", "modifiedAt": null, "url": null, "title": "Meetup : Vienna Meetup #2", "slug": "meetup-vienna-meetup-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.477Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZfXefeHgoSXg7sY9h/meetup-vienna-meetup-2", "pageUrlRelative": "/posts/ZfXefeHgoSXg7sY9h/meetup-vienna-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/ZfXefeHgoSXg7sY9h/meetup-vienna-meetup-2", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vienna%20Meetup%20%232&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vienna%20Meetup%20%232%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfXefeHgoSXg7sY9h%2Fmeetup-vienna-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vienna%20Meetup%20%232%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfXefeHgoSXg7sY9h%2Fmeetup-vienna-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfXefeHgoSXg7sY9h%2Fmeetup-vienna-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/k9'>Vienna Meetup #2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 April 2013 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Schottengasse 2, 1010 Innere Stadt (1.Bez)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 13 April 2013 16:00</p>\n\n<p>WHERE: Cafe Im Schottenstift (Schottengasse 2)</p>\n\n<p>Agenda: we will be discussing developments since last meeting, future projects, and also have some nice and decent discussions regarding rational living.</p>\n\n<p>Join our facebook page:\nhttps://www.facebook.com/groups/rationalityvienna/?fref=ts</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/k9'>Vienna Meetup #2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZfXefeHgoSXg7sY9h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.133785017117592e-06, "legacy": true, "legacyId": "21968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup__2\">Discussion article for the meetup : <a href=\"/meetups/k9\">Vienna Meetup #2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 April 2013 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Schottengasse 2, 1010 Innere Stadt (1.Bez)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 13 April 2013 16:00</p>\n\n<p>WHERE: Cafe Im Schottenstift (Schottengasse 2)</p>\n\n<p>Agenda: we will be discussing developments since last meeting, future projects, and also have some nice and decent discussions regarding rational living.</p>\n\n<p>Join our facebook page:\nhttps://www.facebook.com/groups/rationalityvienna/?fref=ts</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup__21\">Discussion article for the meetup : <a href=\"/meetups/k9\">Vienna Meetup #2</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vienna Meetup #2", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup__2", "level": 1}, {"title": "Discussion article for the meetup : Vienna Meetup #2", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup__21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-09T21:38:11.227Z", "modifiedAt": null, "url": null, "title": "The military value of shortening copyright", "slug": "the-military-value-of-shortening-copyright", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3WK9hAbBgufhvGDu8/the-military-value-of-shortening-copyright", "pageUrlRelative": "/posts/3WK9hAbBgufhvGDu8/the-military-value-of-shortening-copyright", "linkUrl": "https://www.lesswrong.com/posts/3WK9hAbBgufhvGDu8/the-military-value-of-shortening-copyright", "postedAtFormatted": "Saturday, March 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20military%20value%20of%20shortening%20copyright&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20military%20value%20of%20shortening%20copyright%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WK9hAbBgufhvGDu8%2Fthe-military-value-of-shortening-copyright%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20military%20value%20of%20shortening%20copyright%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WK9hAbBgufhvGDu8%2Fthe-military-value-of-shortening-copyright", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WK9hAbBgufhvGDu8%2Fthe-military-value-of-shortening-copyright", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 798, "htmlBody": "<p>A few ideas I've recently read (including <a href=\"http://slatestarcodex.com/2013/03/07/we-wrestle-not-with-flesh-and-blood-but-against-powers-and-principalities/\">this one</a>) have sparked the following line of reasoning; and I'm curious what the general LW opinion on the idea chain might be.</p>\n<p><a id=\"more\"></a><br />If two neighbouring societies are unequally skilled at the art of war, then the worse one will soon come to resemble the better; either by being taken over outright, or out competed and displaced, or adopting their neighbour's attributes out of sheer self-defense. Thus, by knowing what is required to win a fight, and what it takes to support that requirement, you can make a reasonable guess about the shapes of the societies.<br /><br />Part of the Agricultural Revolution, the shift from bands of hunter-gatherers to fixed settlements, was the development of formal standing armies. It took heavy taxes to support them, and a centralized bureaucracy to manage them, resulting in the Classical Empires: Babylon, Rome, and the Aztecs had a great deal in common. Later, improved metallurgy and arms resulted in heavily-armoured knights who could hold off any number of unskilled peasants... and the manors required to support them resulted in the feudal system. Later, around the Renaissance, crossbows, pikes, and guns unseated the knight from military dominance; and the system that best supported that sort of force turned out to be the republic. Later, the Industrial Revolution kicked this sort of thing into high gear, with new military paradigms arising every few decades: ironclads, tanks, planes, nukes. Supporting all of these required the economy to be cranked up to the maximum degree possible to make all that stuff, and scientific research as well to figure out the next trick. As it happens, the form of society that seems to work best at that is something resembling a liberal democracy. (At least, more than it resembles a military junta.)<br /><br />Thus, if you want to predict what future societies will look like, a viable approach could be to examine this present-day societies which do the most and best science, and figuring out what lets them do that. For example - having enough freedom of expression to allow unpopular ideas to be evaluated on their merits.<br /><br />This also begs the question, if that's what things will look like later, why don't they look that way already? One strong possibility for the answer: those powerful people who don't care about any of the above, but only about their own immediate short-term gain, regardless of what damage they do to the society surrounding them. Such entrenched interests act as a drag, preventing both the economy and scientific research from proceeding at maximum speed... And, thus, whether they are willing to admit it or not, their behaviour is sabotaging their society's odds of success in its next war; thus increasing the odds that the very social systems they exploited to enrich themselves will be replaced by force (instead of by gradual evolution as new social forms are demonstrated to work better).<br /><br />Thus: as it has been mathematically proven that a copyright period of more than 15 years causes harm to the overall economy (while a small group reaps obscene profits)*, anyone who tries extending copyright beyond that length is traitorously allowing foreign powers to gain a military advantage, and risking the takeover of their countrymen's government by alien interests, for nothing more than their own personal aggrandizement.<br /><br />The only rational conclusion: You must work to shorten copyright... to protect your children.<br /><br /><br /><br /><br />One of the best ways I know of to work towards a social change is to live that change yourself, accepting the negatives even though the positives don't exist yet. In this case - the above reasoning may lead me to decide to declare that, to the best of my ability, I will try not to pursue any copyright claims on any of my work that has been published more than 15 years previously (while still accepting any remuneration from anyone who wishes to thank me for such work anyway). While for me this is a symbolic gesture at best, as best as my back-of-the-envelope math can figure out, for people who actually have money-making copyright claims, the odds that their own gesture will have a positive effect scales roughly evenly with the potential profit they stand to lose.<br /><br />I'm seeking out at least a couple of different perspectives, to get some further feedback on how erroneous my reasoning might be.<br /><br /><br /><br />*: Source: <a href=\"http://www.rufuspollock.org/economics/papers/optimal_copyright.pdf\">http://www.rufuspollock.org/economics/papers/optimal_copyright.pdf</a> , where, even when granting the longer-copyright side the benefit of the doubt on every questionable assumption, the maximum beneficial length of copyright was still found to be no more than 14 years.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3WK9hAbBgufhvGDu8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -12, "extendedScore": null, "score": 1.1338841710379756e-06, "legacy": true, "legacyId": "21969", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T02:13:37.858Z", "modifiedAt": null, "url": null, "title": "Arguments against the Orthogonality Thesis", "slug": "arguments-against-the-orthogonality-thesis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonatasMueller", "createdAt": "2010-12-03T00:00:55.806Z", "isAdmin": false, "displayName": "JonatasMueller"}, "userId": "KBZaNkNtzZoenyh4b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RBLnsHeo9h2vJvBb6/arguments-against-the-orthogonality-thesis", "pageUrlRelative": "/posts/RBLnsHeo9h2vJvBb6/arguments-against-the-orthogonality-thesis", "linkUrl": "https://www.lesswrong.com/posts/RBLnsHeo9h2vJvBb6/arguments-against-the-orthogonality-thesis", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arguments%20against%20the%20Orthogonality%20Thesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArguments%20against%20the%20Orthogonality%20Thesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBLnsHeo9h2vJvBb6%2Farguments-against-the-orthogonality-thesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arguments%20against%20the%20Orthogonality%20Thesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBLnsHeo9h2vJvBb6%2Farguments-against-the-orthogonality-thesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBLnsHeo9h2vJvBb6%2Farguments-against-the-orthogonality-thesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1313, "htmlBody": "<p>The orthogonality thesis (formulated by Nick Bostrom in his article Superintelligent Will, 2011), states basically that an artificial intelligence can have any combination of intelligence level and goal. This article will focus on this simple question, and will only deal with the practical implementation issues at the end, that would need to be part of its full refutation according to Stuart Armstrong.</p>\n<p>&nbsp;</p>\n<h3><strong>Meta-ethics</strong></h3>\n<p>The orthogonality thesis is based on a variation of ethical values for different beings. This is either because the beings in question have some objective difference in their constitution that associates them to different values, or because they can choose what values they have. <br /> <br />That assumption of variation is arguably based on an analysis of humans. The problem with choosing values is obvious: making errors. Human beings are biologically and constitutionally very similar, and given this, if they objectively and rightfully differ in correct values, it is only in aesthetic preferences, by an existing biological difference. If they differ in other values, given that they are constitutionally similar, then the differing values could not be all correct at the same time, they would be differing due to error in choice. <br /> <br />Aesthetic preferences do vary for us, but they all connect ultimately to their satisfaction \u2015 a specific aesthetic preference may satisfy only some people and not others. What is important is the satisfaction, or good feelings, that they produce, in the present or future (what might entail life preservation), which is basically the same thing to everyone. A given stimulus or occurrence is interpreted by the senses and it can produce good feelings, bad feelings, or neither, depending on the organism that receives it. This variation is besides the point, it is just an idiosyncrasy that could be either way: theoretically, any input (aesthetic preferences) could be associated with a certain output (good and bad feelings), or even no input at all, as in spontaneous satisfaction or wire-heading. In terms of output, good feelings and bad feelings always get positive and negative value, by definition. <br /> <br />Masochism is not a counter-example: masochists like pain only in very specific environments, associated with certain roleplaying fantasies, due to good feelings associated to it, or due to a relief of mental suffering that comes with the pain. Outside of these environments and fantasies, they are just as averse to pain as other people. They don't regularly put their hands into boiling water to feel the pain, nobody does.<br /> <br />Good and bad feelings are directly felt as positive and desirable; negative and aversive, and this direct verification gives them the highest epistemological value. What is indirectly felt, such as the world around us, science, or physical theories, depends on the senses and could therefore be an illusion, such as being part of a virtual world. We could, theoretically, be living inside virtual worlds in an underlying alien universe with different physical laws and scientific facts, but we can nonetheless be sure of the reality of our conscious experiences in themselves, which are directly felt.<br /> <br />There is a difference between valid and invalid human values, which is the ground of justification for moral realism: valid values have an epistemological justification, while invalid ones are based on arbitrary choice or intuition. The epistemological justification of valid values occurs by that part of our experiences which has a direct certainty, as opposed to indirect: conscious experiences in themselves. Likewise, only conscious beings can be said to be ethically relevant in themselves, while what goes on in the hot magma at the core of the earth, or in a random rock in Pluto, are not. Consciousness creates a subject of experience, which is required for direct ethical value. It is straightforward to conclude, therefore, that good conscious experiences constitute what is good, and bad conscious experiences constitute what is bad. Good and bad are what ethical value is about.<br /> <br />Good and bad feelings (or conscious experiences) are physical occurrences, and therefore objectively good and bad occurrences, and objective value. Other fictional values without epistemological (or logical) justification are therefore in another category, and simply constitute the error which comes from allowing free choice of one's values for beings with a similar biological constitution.</p>\n<p>&nbsp;</p>\n<h3><strong>Personal Identity</strong></h3>\n<p>The existence of personal identities is purely an illusion that cannot be justified by argument, and clearly disintegrates upon deeper analysis (for why that is, see, e.g., this essay: <a href=\"http://www.jonatasmuller.com/identity.pdf\">Universal Identity</a>, or for an introduction to the problem, see Less Wrong article <a href=\"/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a>).<br /> <br />Different instances in time of a physical organism relate to it in the same way that any other physical organism in the universe does. There is no logical basis for privileging a physical organism's own viewpoint, nor the satisfaction of their own values over that of other physical organisms, nor for assuming the preponderance of their own reasoning over those of other physical organisms of contextually comparable reasoning capacity.<br /> <br />Therefore, the argument of variation or orthogonality could, at best, assume that a superintelligent physical organism with complete understanding of these cognitively trivial philosophical matters would have to consider all viewpoints and valid preferences in their utility function, in a way much similar to coherent extrapolated volition (CEV), extrapolating the values for intelligence and removing errors, but taking account of the values of all sentient physical organisms: not only humans, but also animals, and possibly sentient machines and aliens. The only values that are validly generalizable among such widely differing sentient creatures are good and bad feelings (in the present or future).</p>\n<p>Furthermore, a superintelligent physical organism with such understanding would have to give equal weight to the reasoning of other physical organisms of contextually comparable reasoning capacity (depending on the cognitive demands of the context, or problem, even some humans can reason perfectly well), if existent. In case of convergence, this would be a non-issue. In case of divergence, this would force an evaluation of reasons or argumentation, seeking a convergence or preponderance of argument.</p>\n<p>&nbsp;</p>\n<h3><strong>Conclusions</strong></h3>\n<p>Taking the orthogonality thesis to be merely the assumption of divergence of ethical values of superintelligent agents, but not a statement about the issues with practical implementation and tampering with or forcing them by non-superintelligent humans, then there are two fatal arguments against it, one on the side of meta-ethics (moral realism), and one on the side of personal identity (open/empty individualism, or universal identity).<br /><br />Beings with general superintelligence should find these fundamental philosophical matters trivial (meta-ethics and personal identity), and understand them completely. They should take a non-privileged and objective viewpoint, accounting for all perspectives of physical subjects, and giving (a priori) similar consideration for the reasoning of all physical organisms of contextually comparable reasoning capacity. <br /> <br />Furthermore they would understand that the free variation of values, even in comparable causal chains of biologically similar organisms, comes from error, and that their extrapolation for intelligence would result in moral realism with good and bad feelings as the epistemologically justified and only valid direct values, from which all other indirectly or instrumentally valuable actions derive their indirect value. For instance, survival, which in a paradise can have positive value, coming from good feelings in the present and future, and which in a hell can have negative value, coming from bad feelings in the present and in the future.<br /> <br />Perhaps certain architectures or contexts involving beings with superintelligence, caused by beings without superintelligence and erratic behavior, could be forced to produce unethical results. This seems to be the most grave existential risk that we face, and would not come from beings with superintelligence themselves, but from human error. The orthogonality thesis is fundamentally mistaken in relation to beings with general superintelligence (surpassing all human cognitive capacities), but it might be practically realized by non-superintelligent human agents.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RBLnsHeo9h2vJvBb6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -18, "extendedScore": null, "score": -4.7e-05, "legacy": true, "legacyId": "21970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The orthogonality thesis (formulated by Nick Bostrom in his article Superintelligent Will, 2011), states basically that an artificial intelligence can have any combination of intelligence level and goal. This article will focus on this simple question, and will only deal with the practical implementation issues at the end, that would need to be part of its full refutation according to Stuart Armstrong.</p>\n<p>&nbsp;</p>\n<h3 id=\"Meta_ethics\"><strong>Meta-ethics</strong></h3>\n<p>The orthogonality thesis is based on a variation of ethical values for different beings. This is either because the beings in question have some objective difference in their constitution that associates them to different values, or because they can choose what values they have. <br> <br>That assumption of variation is arguably based on an analysis of humans. The problem with choosing values is obvious: making errors. Human beings are biologically and constitutionally very similar, and given this, if they objectively and rightfully differ in correct values, it is only in aesthetic preferences, by an existing biological difference. If they differ in other values, given that they are constitutionally similar, then the differing values could not be all correct at the same time, they would be differing due to error in choice. <br> <br>Aesthetic preferences do vary for us, but they all connect ultimately to their satisfaction \u2015 a specific aesthetic preference may satisfy only some people and not others. What is important is the satisfaction, or good feelings, that they produce, in the present or future (what might entail life preservation), which is basically the same thing to everyone. A given stimulus or occurrence is interpreted by the senses and it can produce good feelings, bad feelings, or neither, depending on the organism that receives it. This variation is besides the point, it is just an idiosyncrasy that could be either way: theoretically, any input (aesthetic preferences) could be associated with a certain output (good and bad feelings), or even no input at all, as in spontaneous satisfaction or wire-heading. In terms of output, good feelings and bad feelings always get positive and negative value, by definition. <br> <br>Masochism is not a counter-example: masochists like pain only in very specific environments, associated with certain roleplaying fantasies, due to good feelings associated to it, or due to a relief of mental suffering that comes with the pain. Outside of these environments and fantasies, they are just as averse to pain as other people. They don't regularly put their hands into boiling water to feel the pain, nobody does.<br> <br>Good and bad feelings are directly felt as positive and desirable; negative and aversive, and this direct verification gives them the highest epistemological value. What is indirectly felt, such as the world around us, science, or physical theories, depends on the senses and could therefore be an illusion, such as being part of a virtual world. We could, theoretically, be living inside virtual worlds in an underlying alien universe with different physical laws and scientific facts, but we can nonetheless be sure of the reality of our conscious experiences in themselves, which are directly felt.<br> <br>There is a difference between valid and invalid human values, which is the ground of justification for moral realism: valid values have an epistemological justification, while invalid ones are based on arbitrary choice or intuition. The epistemological justification of valid values occurs by that part of our experiences which has a direct certainty, as opposed to indirect: conscious experiences in themselves. Likewise, only conscious beings can be said to be ethically relevant in themselves, while what goes on in the hot magma at the core of the earth, or in a random rock in Pluto, are not. Consciousness creates a subject of experience, which is required for direct ethical value. It is straightforward to conclude, therefore, that good conscious experiences constitute what is good, and bad conscious experiences constitute what is bad. Good and bad are what ethical value is about.<br> <br>Good and bad feelings (or conscious experiences) are physical occurrences, and therefore objectively good and bad occurrences, and objective value. Other fictional values without epistemological (or logical) justification are therefore in another category, and simply constitute the error which comes from allowing free choice of one's values for beings with a similar biological constitution.</p>\n<p>&nbsp;</p>\n<h3 id=\"Personal_Identity\"><strong>Personal Identity</strong></h3>\n<p>The existence of personal identities is purely an illusion that cannot be justified by argument, and clearly disintegrates upon deeper analysis (for why that is, see, e.g., this essay: <a href=\"http://www.jonatasmuller.com/identity.pdf\">Universal Identity</a>, or for an introduction to the problem, see Less Wrong article <a href=\"/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a>).<br> <br>Different instances in time of a physical organism relate to it in the same way that any other physical organism in the universe does. There is no logical basis for privileging a physical organism's own viewpoint, nor the satisfaction of their own values over that of other physical organisms, nor for assuming the preponderance of their own reasoning over those of other physical organisms of contextually comparable reasoning capacity.<br> <br>Therefore, the argument of variation or orthogonality could, at best, assume that a superintelligent physical organism with complete understanding of these cognitively trivial philosophical matters would have to consider all viewpoints and valid preferences in their utility function, in a way much similar to coherent extrapolated volition (CEV), extrapolating the values for intelligence and removing errors, but taking account of the values of all sentient physical organisms: not only humans, but also animals, and possibly sentient machines and aliens. The only values that are validly generalizable among such widely differing sentient creatures are good and bad feelings (in the present or future).</p>\n<p>Furthermore, a superintelligent physical organism with such understanding would have to give equal weight to the reasoning of other physical organisms of contextually comparable reasoning capacity (depending on the cognitive demands of the context, or problem, even some humans can reason perfectly well), if existent. In case of convergence, this would be a non-issue. In case of divergence, this would force an evaluation of reasons or argumentation, seeking a convergence or preponderance of argument.</p>\n<p>&nbsp;</p>\n<h3 id=\"Conclusions\"><strong>Conclusions</strong></h3>\n<p>Taking the orthogonality thesis to be merely the assumption of divergence of ethical values of superintelligent agents, but not a statement about the issues with practical implementation and tampering with or forcing them by non-superintelligent humans, then there are two fatal arguments against it, one on the side of meta-ethics (moral realism), and one on the side of personal identity (open/empty individualism, or universal identity).<br><br>Beings with general superintelligence should find these fundamental philosophical matters trivial (meta-ethics and personal identity), and understand them completely. They should take a non-privileged and objective viewpoint, accounting for all perspectives of physical subjects, and giving (a priori) similar consideration for the reasoning of all physical organisms of contextually comparable reasoning capacity. <br> <br>Furthermore they would understand that the free variation of values, even in comparable causal chains of biologically similar organisms, comes from error, and that their extrapolation for intelligence would result in moral realism with good and bad feelings as the epistemologically justified and only valid direct values, from which all other indirectly or instrumentally valuable actions derive their indirect value. For instance, survival, which in a paradise can have positive value, coming from good feelings in the present and future, and which in a hell can have negative value, coming from bad feelings in the present and in the future.<br> <br>Perhaps certain architectures or contexts involving beings with superintelligence, caused by beings without superintelligence and erratic behavior, could be forced to produce unethical results. This seems to be the most grave existential risk that we face, and would not come from beings with superintelligence themselves, but from human error. The orthogonality thesis is fundamentally mistaken in relation to beings with general superintelligence (surpassing all human cognitive capacities), but it might be practically realized by non-superintelligent human agents.</p>", "sections": [{"title": "Meta-ethics", "anchor": "Meta_ethics", "level": 1}, {"title": "Personal Identity", "anchor": "Personal_Identity", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "77 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T03:11:14.161Z", "modifiedAt": null, "url": null, "title": "Cognitive Load and Effective Donation", "slug": "cognitive-load-and-effective-donation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Neotenic", "createdAt": "2013-03-04T02:28:23.403Z", "isAdmin": false, "displayName": "Neotenic"}, "userId": "qMgZoftatigAeMMhL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/phSk7LNnChpv6RqWe/cognitive-load-and-effective-donation", "pageUrlRelative": "/posts/phSk7LNnChpv6RqWe/cognitive-load-and-effective-donation", "linkUrl": "https://www.lesswrong.com/posts/phSk7LNnChpv6RqWe/cognitive-load-and-effective-donation", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognitive%20Load%20and%20Effective%20Donation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognitive%20Load%20and%20Effective%20Donation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphSk7LNnChpv6RqWe%2Fcognitive-load-and-effective-donation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognitive%20Load%20and%20Effective%20Donation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphSk7LNnChpv6RqWe%2Fcognitive-load-and-effective-donation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphSk7LNnChpv6RqWe%2Fcognitive-load-and-effective-donation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1182, "htmlBody": "<div id=\"body_t1_8jiy\" class=\"comment-content \">\n<div class=\"md\">\n<p>(previous title: Very low cognitive load)&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Trusting choices made by the same brain that turns my hot 9th grade teacher into a knife-bearing possum at the last second every damn night.</p>\n</blockquote>\n<p><a rel=\"nofollow\" href=\"https://twitter.com/TheThomason/status/299933850400874497\">Sean Thomason</a></p>\n<p>&nbsp;</p>\n<p>We can't trust brains when taken as a whole. Why should we trust their subareas?</p>\n<p>&nbsp;</p>\n<p>Cognitive load is the load related to the executive control of working memory. Depending on what you are doing, the more parallel/extraneous cognitive load you have, the worse you'll do it. (The process may be the same as what the literature calls \"Ego Depletion\" or \"system 2 depletion\", the jury is still up on that)</p>\n<p>If you go<a href=\"http://www.mathgoodies.com/calculators/random_no_custom.html\"> here </a>and enter 0 as lower limit and 1.000.000 as upper limit, and try to keep the number in mind until you are done reading post and comments, you'll get a bit of load while you read this post.&nbsp;</p>\n<p>Now you may process numbers verbally, visually, or both. More generally, for anything you keep in mind, you are likely allocating it in a part of the brain that is primarily concerned with a sensory modality, so it will have some \"flavour\",\"shape\", \"location\", \"sound\", or \"proprioceptual location\". It is harder to<em> consciously</em> memorize things using odours, since those have shortcuts within the brain.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Let us in turn examine two domains in which understanding cognitive load can help you win: Moral Dilemmas and Personal Policy</p>\n<p>&nbsp;</p>\n<p><strong>Moral Games/Dilemmas</strong></p>\n<p>In Dictator game (you're given $20 and you can give any amount to a stranger and keep the rest) the effect of load is negligible.</p>\n<p>In the tested versions of the Trolley problems (kill/indirectly kill/let die one to save five) people are likely to become less utilitarian when under non-visual load. It is assumed that higher functions of the brain (in <a href=\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ventromedial_prefrontal_cortex.png/250px-Ventromedial_prefrontal_cortex.png\">VMPF cortex</a>) - which integrate higher moral judgement with emotional taste buttons - fails to integrate, making the \"fast thinking\", emotional mode be the only one reacting.</p>\n<p>Visual information about the problem brings into salience the gory aspect of killing someone, and other lower level features that incline non-utilitarian decisions. So when visual load requires you to memorize something else, like a bird drawing, you become more utilitarian since you fail to visualize the one person being killed (which we do more than the five) in as much gory detail. (Greene et al,2011)</p>\n<p>(Bednar et al.2012) show that when playing two games simultaneously, the strategy of one spills over to the other one. Critically, heuristics that are useful for both games were used, increasing the likelihood that those heuristics will be suboptimal in each case.&nbsp;</p>\n<p>In altruistic donation scenarios, with donations to suffering people at stake, (Small et al. 2007) more load increased scope insensitivity, so less load made the donation more proportional to how many people are suffering. Contrary to load, <em>priming </em>increases the capacity of an area/module, by using it and <em>not keeping the information stored</em>, leaving free usable space. (Dickert et al.2010) shows that priming for empathy increases donation amount (but not decision to donate), whereas priming calculation decreases it.</p>\n<p>Taken together, these studies indicate that to make people donate more it is most effective to, after being primed for thinking about how they will feel about themselves, and for empathic feelings, make them feel empathically and non-visually someone from their own race. After all that you make them keep a number and a drawing in mind, and this is the optimal time to donate.</p>\n<p><strong>Personal Policy</strong></p>\n<p>If given a choice between a high carb food, and a low carb one, people undergoing diets are substantially more likely to choose the high carb one if they are keeping some information in mind.</p>\n<p>Forgetful people, and those with ADHD know that, for them, out of sight means out of mind. Through luck, intelligence, blind error or psychological help, they learn to put things, literally, in front of them, to avoid 'losing them' in their minds corner somewhere. They have a lower storage size for executive memory tasks.</p>\n<p>Positive psychologists advise us to make our daily tasks, specially the ones we are always reluctant to start, in very visible places. Alternatively, we can make the commitment to start them <a href=\"http://becomingeden.com/ten-ways-to-change-your-behavior-immediately-9-commit-to-one-minute/\">smaller</a>, but this only works if we actually remember to do them.</p>\n<p>Marketing appropriates cognitive load in a terrible way. They know if we are overwhelmed with information, we are more likely to agree. They'll inform us more than what we need, and we aren't left with enough brain to decide well. One more reason to keep <a href=\"http://www.google.com.br/search?hl=pt-BR&amp;q=adblock#hl=pt-BR&amp;safe=off&amp;sclient=psy-ab&amp;q=adblock+extensions&amp;oq=adblock+extensions&amp;gs_l=serp.3..0i19l2j0i30i19l2.11842.13543.0.14143.11.8.0.3.3.1.462.1865.0j5j2j0j1.8.0...0.0...1c.1.5.psy-ab.5a_P_gzBTLk&amp;pbx=1&amp;bav=on.2,or.r_gc.r_pw.r_qf.&amp;bvm=bv.43148975,d.eWU&amp;fp=c23b28aec4a00b3f&amp;biw=1280&amp;bih=800\">advertisement out of sight</a> and out of mind.</p>\n<p>&nbsp;</p>\n<p><strong>Effective use of Cognitive Load</strong></p>\n<p>Once you understand how it works, it is simple to use cognitive load as a tool:</p>\n<div class=\"md\">1)Even if your executive control of activities is fine, <a href=\"http://consc.net/papers/extended.html\">externalize</a> as much as you can, by using a calendar and alarms to tell you everything you need to do.</div>\n</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">2)Do apparently mean things to donors like the above suggestion.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">3)When in need of moral empathy, type 1, fast, emotional buttons system, keep numerical and verbal things (like phone numbers) in mind while deciding.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">4)When in need of moral utilitarianism, highjack the taste buttons, automatic, type 1 system, by giving yourself an emotional experience more proportional to the numbers&nbsp; - for instance, when reasoning about the trolley problem, think about each of the five, or pinch yourself with a needle five times prior to deciding.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">5)When in need of more cognitive calculating capacity, besides having freed yourself from executive tasks, use natural sensory modalities to keep stuff in mind, not only the classic castle mnemonics (spacial location), but put the chunks of information in different parts of your body (proprioception), associate them with textures (Feynman 1985), shapes, and actions.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">If practising this looks sometimes unnecessary, or immoral, we can remember Max Tegmark's gloomy assessment of Science's pervasiveness (or lack thereof) at the Edge 2011 question. When discussing the dishonesty and marketing of opponents and defenders of facts/Science, he says:&nbsp;</div>\n<blockquote>\n<div class=\"md\">Yet we scientists are often painfully naive,&nbsp;deluding ourselves that just because we think we have the&nbsp;moral high ground, we can somehow defeat this&nbsp;corporate-fundamentalist coalition by using obsolete unscientific strategies. Based of what scientific argument will it make a hoot of a difference if we grumble&nbsp;\"we won't&nbsp;stoop that low\" and&nbsp;\"people need to change\"&nbsp;in faculty lunch rooms and recite statistics to journalists?&nbsp;<br /> <br /> We scientists have basically been saying \"tanks are unethical, so let's fight tanks with swords\".</div>\n<p>&nbsp;</p>\n<p class=\"style1\">To teach people what a scientific concept is and how a scientific lifestyle will improve their lives, we need to go about it scientifically:<br /> <br /> We need new&nbsp;science advocacy organizations which use all the same scientific marketing and fundraising tools as the anti-scientific coalition.<br /> We'll need to use many of the tools that make scientists cringe, from ads and lobbying to&nbsp;focus groups that identify the most effective sound bites.<br /> We won't need to stoop all the way down to intellectual dishonesty, however.&nbsp;Because in this battle, we have the most powerful weapon of all on our side: the facts.</p>\n</blockquote>\n</div>\n<p class=\"style1\">&nbsp;</p>\n<p class=\"style1\">We'd better start pushing emotional buttons and twisting the mental knobs of people if we want to get something done. Starting with our own.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "phSk7LNnChpv6RqWe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 27, "extendedScore": null, "score": 1.1341027126049886e-06, "legacy": true, "legacyId": "21879", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"body_t1_8jiy\" class=\"comment-content \">\n<div class=\"md\">\n<p>(previous title: Very low cognitive load)&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Trusting choices made by the same brain that turns my hot 9th grade teacher into a knife-bearing possum at the last second every damn night.</p>\n</blockquote>\n<p><a rel=\"nofollow\" href=\"https://twitter.com/TheThomason/status/299933850400874497\">Sean Thomason</a></p>\n<p>&nbsp;</p>\n<p>We can't trust brains when taken as a whole. Why should we trust their subareas?</p>\n<p>&nbsp;</p>\n<p>Cognitive load is the load related to the executive control of working memory. Depending on what you are doing, the more parallel/extraneous cognitive load you have, the worse you'll do it. (The process may be the same as what the literature calls \"Ego Depletion\" or \"system 2 depletion\", the jury is still up on that)</p>\n<p>If you go<a href=\"http://www.mathgoodies.com/calculators/random_no_custom.html\"> here </a>and enter 0 as lower limit and 1.000.000 as upper limit, and try to keep the number in mind until you are done reading post and comments, you'll get a bit of load while you read this post.&nbsp;</p>\n<p>Now you may process numbers verbally, visually, or both. More generally, for anything you keep in mind, you are likely allocating it in a part of the brain that is primarily concerned with a sensory modality, so it will have some \"flavour\",\"shape\", \"location\", \"sound\", or \"proprioceptual location\". It is harder to<em> consciously</em> memorize things using odours, since those have shortcuts within the brain.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Let us in turn examine two domains in which understanding cognitive load can help you win: Moral Dilemmas and Personal Policy</p>\n<p>&nbsp;</p>\n<p><strong id=\"Moral_Games_Dilemmas\">Moral Games/Dilemmas</strong></p>\n<p>In Dictator game (you're given $20 and you can give any amount to a stranger and keep the rest) the effect of load is negligible.</p>\n<p>In the tested versions of the Trolley problems (kill/indirectly kill/let die one to save five) people are likely to become less utilitarian when under non-visual load. It is assumed that higher functions of the brain (in <a href=\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ventromedial_prefrontal_cortex.png/250px-Ventromedial_prefrontal_cortex.png\">VMPF cortex</a>) - which integrate higher moral judgement with emotional taste buttons - fails to integrate, making the \"fast thinking\", emotional mode be the only one reacting.</p>\n<p>Visual information about the problem brings into salience the gory aspect of killing someone, and other lower level features that incline non-utilitarian decisions. So when visual load requires you to memorize something else, like a bird drawing, you become more utilitarian since you fail to visualize the one person being killed (which we do more than the five) in as much gory detail. (Greene et al,2011)</p>\n<p>(Bednar et al.2012) show that when playing two games simultaneously, the strategy of one spills over to the other one. Critically, heuristics that are useful for both games were used, increasing the likelihood that those heuristics will be suboptimal in each case.&nbsp;</p>\n<p>In altruistic donation scenarios, with donations to suffering people at stake, (Small et al. 2007) more load increased scope insensitivity, so less load made the donation more proportional to how many people are suffering. Contrary to load, <em>priming </em>increases the capacity of an area/module, by using it and <em>not keeping the information stored</em>, leaving free usable space. (Dickert et al.2010) shows that priming for empathy increases donation amount (but not decision to donate), whereas priming calculation decreases it.</p>\n<p>Taken together, these studies indicate that to make people donate more it is most effective to, after being primed for thinking about how they will feel about themselves, and for empathic feelings, make them feel empathically and non-visually someone from their own race. After all that you make them keep a number and a drawing in mind, and this is the optimal time to donate.</p>\n<p><strong id=\"Personal_Policy\">Personal Policy</strong></p>\n<p>If given a choice between a high carb food, and a low carb one, people undergoing diets are substantially more likely to choose the high carb one if they are keeping some information in mind.</p>\n<p>Forgetful people, and those with ADHD know that, for them, out of sight means out of mind. Through luck, intelligence, blind error or psychological help, they learn to put things, literally, in front of them, to avoid 'losing them' in their minds corner somewhere. They have a lower storage size for executive memory tasks.</p>\n<p>Positive psychologists advise us to make our daily tasks, specially the ones we are always reluctant to start, in very visible places. Alternatively, we can make the commitment to start them <a href=\"http://becomingeden.com/ten-ways-to-change-your-behavior-immediately-9-commit-to-one-minute/\">smaller</a>, but this only works if we actually remember to do them.</p>\n<p>Marketing appropriates cognitive load in a terrible way. They know if we are overwhelmed with information, we are more likely to agree. They'll inform us more than what we need, and we aren't left with enough brain to decide well. One more reason to keep <a href=\"http://www.google.com.br/search?hl=pt-BR&amp;q=adblock#hl=pt-BR&amp;safe=off&amp;sclient=psy-ab&amp;q=adblock+extensions&amp;oq=adblock+extensions&amp;gs_l=serp.3..0i19l2j0i30i19l2.11842.13543.0.14143.11.8.0.3.3.1.462.1865.0j5j2j0j1.8.0...0.0...1c.1.5.psy-ab.5a_P_gzBTLk&amp;pbx=1&amp;bav=on.2,or.r_gc.r_pw.r_qf.&amp;bvm=bv.43148975,d.eWU&amp;fp=c23b28aec4a00b3f&amp;biw=1280&amp;bih=800\">advertisement out of sight</a> and out of mind.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Effective_use_of_Cognitive_Load\">Effective use of Cognitive Load</strong></p>\n<p>Once you understand how it works, it is simple to use cognitive load as a tool:</p>\n<div class=\"md\">1)Even if your executive control of activities is fine, <a href=\"http://consc.net/papers/extended.html\">externalize</a> as much as you can, by using a calendar and alarms to tell you everything you need to do.</div>\n</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">2)Do apparently mean things to donors like the above suggestion.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">3)When in need of moral empathy, type 1, fast, emotional buttons system, keep numerical and verbal things (like phone numbers) in mind while deciding.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">4)When in need of moral utilitarianism, highjack the taste buttons, automatic, type 1 system, by giving yourself an emotional experience more proportional to the numbers&nbsp; - for instance, when reasoning about the trolley problem, think about each of the five, or pinch yourself with a needle five times prior to deciding.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">5)When in need of more cognitive calculating capacity, besides having freed yourself from executive tasks, use natural sensory modalities to keep stuff in mind, not only the classic castle mnemonics (spacial location), but put the chunks of information in different parts of your body (proprioception), associate them with textures (Feynman 1985), shapes, and actions.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\"><br></div>\n<div class=\"md\">If practising this looks sometimes unnecessary, or immoral, we can remember Max Tegmark's gloomy assessment of Science's pervasiveness (or lack thereof) at the Edge 2011 question. When discussing the dishonesty and marketing of opponents and defenders of facts/Science, he says:&nbsp;</div>\n<blockquote>\n<div class=\"md\">Yet we scientists are often painfully naive,&nbsp;deluding ourselves that just because we think we have the&nbsp;moral high ground, we can somehow defeat this&nbsp;corporate-fundamentalist coalition by using obsolete unscientific strategies. Based of what scientific argument will it make a hoot of a difference if we grumble&nbsp;\"we won't&nbsp;stoop that low\" and&nbsp;\"people need to change\"&nbsp;in faculty lunch rooms and recite statistics to journalists?&nbsp;<br> <br> We scientists have basically been saying \"tanks are unethical, so let's fight tanks with swords\".</div>\n<p>&nbsp;</p>\n<p class=\"style1\">To teach people what a scientific concept is and how a scientific lifestyle will improve their lives, we need to go about it scientifically:<br> <br> We need new&nbsp;science advocacy organizations which use all the same scientific marketing and fundraising tools as the anti-scientific coalition.<br> We'll need to use many of the tools that make scientists cringe, from ads and lobbying to&nbsp;focus groups that identify the most effective sound bites.<br> We won't need to stoop all the way down to intellectual dishonesty, however.&nbsp;Because in this battle, we have the most powerful weapon of all on our side: the facts.</p>\n</blockquote>\n</div>\n<p class=\"style1\">&nbsp;</p>\n<p class=\"style1\">We'd better start pushing emotional buttons and twisting the mental knobs of people if we want to get something done. Starting with our own.</p>", "sections": [{"title": "Moral Games/Dilemmas", "anchor": "Moral_Games_Dilemmas", "level": 1}, {"title": "Personal Policy", "anchor": "Personal_Policy", "level": 1}, {"title": "Effective use of Cognitive Load", "anchor": "Effective_use_of_Cognitive_Load", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T05:42:18.393Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Formative Youth", "slug": "seq-rerun-formative-youth", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ocaHobDPw5kcejwrJ/seq-rerun-formative-youth", "pageUrlRelative": "/posts/ocaHobDPw5kcejwrJ/seq-rerun-formative-youth", "linkUrl": "https://www.lesswrong.com/posts/ocaHobDPw5kcejwrJ/seq-rerun-formative-youth", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Formative%20Youth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Formative%20Youth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocaHobDPw5kcejwrJ%2Fseq-rerun-formative-youth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Formative%20Youth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocaHobDPw5kcejwrJ%2Fseq-rerun-formative-youth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocaHobDPw5kcejwrJ%2Fseq-rerun-formative-youth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/yu/formative_youth/\">Formative Youth</a> was originally published on 24 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Formative_Youth\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People underestimate the extent to which their own beliefs and attitudes are influenced by their experiences as a child.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gxx/seq_rerun_on_not_having_an_advance_abyssal_plan/\">On Not Having an Advance Abyssal Plan</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ocaHobDPw5kcejwrJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.134201867311571e-06, "legacy": true, "legacyId": "21971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hwbopYqniG9iDqGDH", "ffPxHecsQvh9ocyNM", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T14:15:51.757Z", "modifiedAt": null, "url": null, "title": "Rationality Habits I Learned at the CFAR Workshop", "slug": "rationality-habits-i-learned-at-the-cfar-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:14.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "elharo", "createdAt": "2012-12-28T14:11:02.335Z", "isAdmin": false, "displayName": "elharo"}, "userId": "cgJcCeZhdRnGtwMMR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ymwyTDc96uaAqZ48e/rationality-habits-i-learned-at-the-cfar-workshop", "pageUrlRelative": "/posts/ymwyTDc96uaAqZ48e/rationality-habits-i-learned-at-the-cfar-workshop", "linkUrl": "https://www.lesswrong.com/posts/ymwyTDc96uaAqZ48e/rationality-habits-i-learned-at-the-cfar-workshop", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Habits%20I%20Learned%20at%20the%20CFAR%20Workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Habits%20I%20Learned%20at%20the%20CFAR%20Workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymwyTDc96uaAqZ48e%2Frationality-habits-i-learned-at-the-cfar-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Habits%20I%20Learned%20at%20the%20CFAR%20Workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymwyTDc96uaAqZ48e%2Frationality-habits-i-learned-at-the-cfar-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymwyTDc96uaAqZ48e%2Frationality-habits-i-learned-at-the-cfar-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2201, "htmlBody": "<p>Recently Leah Libresco asked attendees at the January <a href=\"/lw/g6g/applied_rationality_workshops_jan_2528_and_march/\">CFAR Workshop</a>, \"<span id=\":11e\" class=\"hP\">What habits have people installed after workshops?\" and that got me thinking that now was a good time to write up and review what I learned (or learned and already forgot). </span>I thought that might be of some interest to folks here, and this is what follows.</p>\n<h3>What I Learned and Implemented<br /></h3>\n<p>The most immediately useful thing I learned was the <em>Pomodoro Technique</em>, as I've <a href=\"/lw/gp4/the_power_of_pomodoros/\">written about here before</a>. In addition to that, there were a number of small items that I'm continuing to work on.</p>\n<p>First, I've become quite fond of the question \"<em>Does future me have a comparative advantage?</em>\" Especially for small items, if the answer is \"No\" (and it's no far more often than it's yes) then just do it right now. The more trivial the task, the more useful it is. For instance, today I asked myself that while standing in the bedroom wondering whether to take 30 seconds to move my <a href=\"http://www.amazon.com/exec/obidos/ISBN=B0043EW1S8/ref=nosim/cafeaulaitA\">ExOfficio Bugproof socks</a> from the dresser to the correct box in the closet. (Answer from a few minutes ago:&nbsp; if I don't take my dog for a walk <em>right now</em>, he's going to pee all over the floor. Future me does have a comparative advantage of not having to clean up pee on the floor. The socks can wait.)</p>\n<p>I've begun to <em>notice my confusion and call it to conscious attention</em> more often, though I suspect I learned this first from <a href=\"http://www.hpmor.com/\">HpMOR</a> and the sequences before the workshop. Example: when <a href=\"https://en.wikipedia.org/wiki/Black_hole_information_paradox\">Leonard Susskind states that conservation of information</a> is a fundamental principle of quantum mechanics, I notice that I am confused because A) I have never heard of any such fundamental law of physics as information conservation B) Every definition of information I have ever heard indicates that information most certainly can be destroyed. So just what the heck is he talking about anyway? I am now making a conscious effort to research this topic rather than letting it slide by.</p>\n<p>The workshop introduced me to the concepts of System 1 and System 2. System 1 is the faster, reactive, intuitive mind that uses heuristics and experience to react quickly. System 2 is the slower, analytical, logical, mathematical mind. I didn't immediately grok this or see how to apply it. However the workshop did convince me to read <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Daniel Kahneman's Thinking Fast and Slow</a>, and I'm beginning to follow this. It could be useful going forward. I particularly like the examples given at the end of each chapter.</p>\n<p>Similarly I completely did not understand the concepts of inside view vs. outside view at the workshop; and worse yet I don't think that I even realized that I didn't understand these. However now that I've read <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Thinking Fast and Slow</a>, the lightbulb has gone on. Inside view is simply me deciding how likely I (or my team) is likely to accomplish something based on my judgement of the problem and our capabilities. Outside view is a statistical question about how people and teams like us have done when confronted with similar problems in the past. As long as there are similar teams and similar problems to compare with, the outside view is likely to be much more accurate.</p>\n<p>During conversation, Julia Galef and I came up with the idea of *********.&nbsp; It turned out it already exists, and I'm planning to start attending these events locally soon. I've also joined my local LessWrong meetup group.</p>\n<p>Stare into <a href=\"/lw/21b/ugh_fields/\">Ugh fields</a>. Difficult conversations are an Ugh field for me. Recognizing this and bringing it to conscious attention has made it somewhat easier to manage these conversations. Example: when I went to the workshop I had been putting off contacting my dentist for months, not because of the usual reasons people don't like going to the dentist, but simply because I was uncomfortable telling her that the second (and third) opinion I had gotten on a dental issue disagreed with her about the proper course of treatment. Post-workshop, I finally called her (though it still took me two more weeks to do this. Clearly I have a lot of work left to do here.)</p>\n<p>Consider whether the sources of my information may be correlated and by how much. I.e. Evaluating Advice. For instance, if two dentists who share an office give me the same advice, even assuming no prior disposition to agree with each other simply out of friendship, how likely is it that they share the same background and information that dentists in a different office do not?</p>\n<p>COZE (Comfort Zone Expansion) exercises have pushed me to talk more to \"strangers\" and be intentionally more extroverted. On a recent trip to Latin America, I even made an effort to use what little Spanish I possess. I've had some small success, though this has led to no obvious major improvements in my life yet.</p>\n<p>Thought experiments conducted at the workshop were very helpful in untangling some of my goals and plans. Going forward though this hasn't made a huge difference in my day-to-day life. That is, it hasn't led me to seek different paths than what I'm on right now.</p>\n<h3>What I Learned and Forgot</h3>\n<p>Going over my notes now, there was a lot of material; some of it potentially useful, that has fallen by the wayside; and may be worth a second look. This includes:</p>\n<ul>\n<li>Geoff Anders introduced us to <a href=\"http://www.yworks.com/en/products_yed_about.html\">yEd</a>, a nice open source diagram editor. I still prefer StencilIt or Omnigraffle though. He also used it to show us a really neat way of graphing, well, something. Goals maybe? I remember it seemed really useful and significant at the time, but for the life of me I can't remember exactly what it was or what it was supposed to show us. I'll have to go back to my notes. This is why we write things down. (Update: I suspect this was about <em>Goal Factoring</em>.)</li>\n<li>Anticipation vs. Profession (though from time to time I do find myself asking what odds I'd be willing to bet on certain beliefs)</li>\n<li>The Planning Kata.</li>\n</ul>\n<h3>What I Learned But Didn't Implement<br /></h3>\n<p>Value of Information calculations seem too meta and too wishy-washy to be of much use. They attempt to put quantitative numbers based on information that's far too imprecise to allow even order of magnitude accuracy. I'm better off just keeping things I need to consider in my GTD system, and periodically reviewing it.</p>\n<p>Similarly opportunities for Bayesian Strength of Evidence calculations, just don't seem to come up in my day-to-day life. The question for me is more commonly \"Given that the situation is what it is, what actions should I take to accomplish my goals?\" The outside view is useful for this. Figuring out why the situation is what it is rarely seems to be especially helpful.</p>\n<p>Turbocharging Training may be helpful but the evidence seems to me to be lacking. I'd like to see some strong proof that this works in particular areas; e.g. foreign languages, sports, or mathematics.&nbsp; Furthermore, it's not clear that it's applicable to anything I'm working on learning at this time. It seems very System 1 focused, and not especially helpful with the sort of fundamentally System 2 tasks I take on.</p>\n<p>I have begun to declare \"Victory!\" at the end of a meeting/discussion. it's a bit of fun, but has limited effect. Beyond that I don't seem to reward myself for noticing things, or as a means of installing habits.</p>\n<h3>What I Didn't Learn</h3>\n<p>Getting Things Done (GTD), Remember the Milk, BeeMinder, Anki, Cultivating Curiosity, Overcoming Procrastination, and Winning at Arguments.</p>\n<p><em>GTD</em> I didn't learn because I've used it for years now or at least the parts of it that really work for me (lists and calendars mostly, and to a lesser extent filing).</p>\n<p><em>Remember the Milk</em> because my employer's security policy prohibits us from using it, and too much of my life happens at my day job to make maintaining two separate systems worthwhile.</p>\n<p>BeeMinder and Anki because I just don't have anything that seems it could benefit from being stored in those systems right now. All of these might be more beneficial to someone in different circumstances.</p>\n<p><em>Cultivating Curiosity</em> because I am already a very naturally curious person, and have been for as long as I can remember. I don't need help with this. Indeed if anything I need to tamp down on this tendency and focus more on accomplishing things rather than merely learning them.</p>\n<p>Similarly, <em>Overcoming Procrastination</em> didn't help a lot because I don't have a big procrastination problem, at least not compared to what I had when I was younger. Of course, I do say that in full knowledge that right this minute writing this article is a form of <a href=\"http://www.structuredprocrastination.com/\">structured procrastination</a> to avoid doing my taxes. :-)</p>\n<p><em>Winning at Arguments</em>, I am already very, very good at when I want to be, which is rare these days. It took me many years too realize that even though I \"won\" almost every argument I cared about, winning the argument wasn't usually all that useful. Winning an argument is the wrong goal to have for almost any purpose, and rarely leads to the outcomes I desire.</p>\n<h3>Unofficial ideas from fellow attendees:</h3>\n<p>Polyphasic sleep: I'm going to let the younger, more pioneering attendees experiment with this one. Even if it does work (which seems far from obvious) I don't see how one could integrate it into a conventional day job and family.</p>\n<p>At breakfast one morning, a fellow attendee (Hunter?) suggested putting unsalted butter in my coffee to add more fat to my diet. It's not as crazy as it sounds. After all butter is little more than clarified cream, which I do like in my coffee. I tried this once and I still prefer cream, but I may give it another shot.</p>\n<p>Finally, I've referred two workshop attendees to my employer as potential hires. If anyone else from the workshop is looking for a job, especially in tech, sales, or legal, drop me a line privately. For that matter if any Less Wronger is looking for a job, drop me a line privately. We have hundreds of open positions in major cities around the world. Quite a few LessWrongers already work there, and there's room for many more.</p>\n<h3>What the workshop didn't teach</h3>\n<p>There were a few techniques that were conspicuous by their absence. In particular I think the CFAR/LessWrong and Agile/XP communities have a lot to teach each other. I was surprised that no one at the workshop seemed to have heard of Kanban or Scrum, much less practice it. Burndown charts and point-based estimation are a really interesting modification of the outside view by comparing your team to your team in the past, rather than to other teams.</p>\n<p>Pairing is also a useful technique beyond programming as at least Eliezer (not present at the workshop) <a href=\"/lw/fc3/checklist_of_rationality_habits/\">has discovered</a>. Pairing is an incredibly effective way to overcome akrasia and procrastination.</p>\n<p>In reverse, I am considering what the craft of software development has to learn from CFAR style rationality, more specifically epistemic rationality. I have begun to notice my confusion during conversations with users, product managers, and tech leads and call it to conscious attention. I less frequently let unclear specs and goals pass without comment. Rather, I ask for examples and drill down into them until I feel my confusion has been conquered.</p>\n<p>So far these techniques seem very useful in analysis and requirements gathering. I've found them less obviously useful (though certainly not harmful in any way) during coding, debugging, and testing. In these stages there's simply too much to be confused by to address it all, and whatever I'm confused by that's relevant to the task at hand rapidly calls itself to my attention. For instance, when a bug shows up in a production system, the very first and natural question to ask&nbsp; is \"How the hell did the system do that?!\" On the other hand, the planning kata may be very helpful with the early stages of system design, though I haven't yet had an opportunity to try that out.</p>\n<h3>Was it Worth $3900?<br /></h3>\n<p>Overall, I found the workshop to be a worthwhile experience, if an expensive one; and I recommend it to you if you have the opportunity and resources to attend. There are a lot of practical techniques to be learned, and you only need one or two of them to pay off to cover the cost and time. Even if the primary value is simply introducing you to books and techniques you explore further after the workshop such as <a href=\"/lw/gp4/the_power_of_pomodoros/\">Getting Things Done</a> or <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Thinking Fast and Slow</a>, that may be enough. Most knowledge workers are operating far below the level of which we're capable, and expanding our effectiveness can pay for itself.</p>\n<p>Before attending, it is worth asking yourself whether there's an opportunity to learn this material at lower cost. For instance, did I really need to spend $3900 and 4 days to learn about&nbsp;<a href=\"/lw/gp4/the_power_of_pomodoros/\">Pomodoro</a>? Apparently so, since I'd heard about Pomodoro for years and paid no attention to it until January. On the other hand, a <a href=\"/lw/gp4/the_power_of_pomodoros/\">$20 book</a> I read on the subway was fully sufficient for me to learn and implement Getting Things Done. You'll have to judge this one for yourself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ymwyTDc96uaAqZ48e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 59, "extendedScore": null, "score": 0.000167, "legacy": true, "legacyId": "21951", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Recently Leah Libresco asked attendees at the January <a href=\"/lw/g6g/applied_rationality_workshops_jan_2528_and_march/\">CFAR Workshop</a>, \"<span id=\":11e\" class=\"hP\">What habits have people installed after workshops?\" and that got me thinking that now was a good time to write up and review what I learned (or learned and already forgot). </span>I thought that might be of some interest to folks here, and this is what follows.</p>\n<h3 id=\"What_I_Learned_and_Implemented\">What I Learned and Implemented<br></h3>\n<p>The most immediately useful thing I learned was the <em>Pomodoro Technique</em>, as I've <a href=\"/lw/gp4/the_power_of_pomodoros/\">written about here before</a>. In addition to that, there were a number of small items that I'm continuing to work on.</p>\n<p>First, I've become quite fond of the question \"<em>Does future me have a comparative advantage?</em>\" Especially for small items, if the answer is \"No\" (and it's no far more often than it's yes) then just do it right now. The more trivial the task, the more useful it is. For instance, today I asked myself that while standing in the bedroom wondering whether to take 30 seconds to move my <a href=\"http://www.amazon.com/exec/obidos/ISBN=B0043EW1S8/ref=nosim/cafeaulaitA\">ExOfficio Bugproof socks</a> from the dresser to the correct box in the closet. (Answer from a few minutes ago:&nbsp; if I don't take my dog for a walk <em>right now</em>, he's going to pee all over the floor. Future me does have a comparative advantage of not having to clean up pee on the floor. The socks can wait.)</p>\n<p>I've begun to <em>notice my confusion and call it to conscious attention</em> more often, though I suspect I learned this first from <a href=\"http://www.hpmor.com/\">HpMOR</a> and the sequences before the workshop. Example: when <a href=\"https://en.wikipedia.org/wiki/Black_hole_information_paradox\">Leonard Susskind states that conservation of information</a> is a fundamental principle of quantum mechanics, I notice that I am confused because A) I have never heard of any such fundamental law of physics as information conservation B) Every definition of information I have ever heard indicates that information most certainly can be destroyed. So just what the heck is he talking about anyway? I am now making a conscious effort to research this topic rather than letting it slide by.</p>\n<p>The workshop introduced me to the concepts of System 1 and System 2. System 1 is the faster, reactive, intuitive mind that uses heuristics and experience to react quickly. System 2 is the slower, analytical, logical, mathematical mind. I didn't immediately grok this or see how to apply it. However the workshop did convince me to read <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Daniel Kahneman's Thinking Fast and Slow</a>, and I'm beginning to follow this. It could be useful going forward. I particularly like the examples given at the end of each chapter.</p>\n<p>Similarly I completely did not understand the concepts of inside view vs. outside view at the workshop; and worse yet I don't think that I even realized that I didn't understand these. However now that I've read <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Thinking Fast and Slow</a>, the lightbulb has gone on. Inside view is simply me deciding how likely I (or my team) is likely to accomplish something based on my judgement of the problem and our capabilities. Outside view is a statistical question about how people and teams like us have done when confronted with similar problems in the past. As long as there are similar teams and similar problems to compare with, the outside view is likely to be much more accurate.</p>\n<p>During conversation, Julia Galef and I came up with the idea of *********.&nbsp; It turned out it already exists, and I'm planning to start attending these events locally soon. I've also joined my local LessWrong meetup group.</p>\n<p>Stare into <a href=\"/lw/21b/ugh_fields/\">Ugh fields</a>. Difficult conversations are an Ugh field for me. Recognizing this and bringing it to conscious attention has made it somewhat easier to manage these conversations. Example: when I went to the workshop I had been putting off contacting my dentist for months, not because of the usual reasons people don't like going to the dentist, but simply because I was uncomfortable telling her that the second (and third) opinion I had gotten on a dental issue disagreed with her about the proper course of treatment. Post-workshop, I finally called her (though it still took me two more weeks to do this. Clearly I have a lot of work left to do here.)</p>\n<p>Consider whether the sources of my information may be correlated and by how much. I.e. Evaluating Advice. For instance, if two dentists who share an office give me the same advice, even assuming no prior disposition to agree with each other simply out of friendship, how likely is it that they share the same background and information that dentists in a different office do not?</p>\n<p>COZE (Comfort Zone Expansion) exercises have pushed me to talk more to \"strangers\" and be intentionally more extroverted. On a recent trip to Latin America, I even made an effort to use what little Spanish I possess. I've had some small success, though this has led to no obvious major improvements in my life yet.</p>\n<p>Thought experiments conducted at the workshop were very helpful in untangling some of my goals and plans. Going forward though this hasn't made a huge difference in my day-to-day life. That is, it hasn't led me to seek different paths than what I'm on right now.</p>\n<h3 id=\"What_I_Learned_and_Forgot\">What I Learned and Forgot</h3>\n<p>Going over my notes now, there was a lot of material; some of it potentially useful, that has fallen by the wayside; and may be worth a second look. This includes:</p>\n<ul>\n<li>Geoff Anders introduced us to <a href=\"http://www.yworks.com/en/products_yed_about.html\">yEd</a>, a nice open source diagram editor. I still prefer StencilIt or Omnigraffle though. He also used it to show us a really neat way of graphing, well, something. Goals maybe? I remember it seemed really useful and significant at the time, but for the life of me I can't remember exactly what it was or what it was supposed to show us. I'll have to go back to my notes. This is why we write things down. (Update: I suspect this was about <em>Goal Factoring</em>.)</li>\n<li>Anticipation vs. Profession (though from time to time I do find myself asking what odds I'd be willing to bet on certain beliefs)</li>\n<li>The Planning Kata.</li>\n</ul>\n<h3 id=\"What_I_Learned_But_Didn_t_Implement\">What I Learned But Didn't Implement<br></h3>\n<p>Value of Information calculations seem too meta and too wishy-washy to be of much use. They attempt to put quantitative numbers based on information that's far too imprecise to allow even order of magnitude accuracy. I'm better off just keeping things I need to consider in my GTD system, and periodically reviewing it.</p>\n<p>Similarly opportunities for Bayesian Strength of Evidence calculations, just don't seem to come up in my day-to-day life. The question for me is more commonly \"Given that the situation is what it is, what actions should I take to accomplish my goals?\" The outside view is useful for this. Figuring out why the situation is what it is rarely seems to be especially helpful.</p>\n<p>Turbocharging Training may be helpful but the evidence seems to me to be lacking. I'd like to see some strong proof that this works in particular areas; e.g. foreign languages, sports, or mathematics.&nbsp; Furthermore, it's not clear that it's applicable to anything I'm working on learning at this time. It seems very System 1 focused, and not especially helpful with the sort of fundamentally System 2 tasks I take on.</p>\n<p>I have begun to declare \"Victory!\" at the end of a meeting/discussion. it's a bit of fun, but has limited effect. Beyond that I don't seem to reward myself for noticing things, or as a means of installing habits.</p>\n<h3 id=\"What_I_Didn_t_Learn\">What I Didn't Learn</h3>\n<p>Getting Things Done (GTD), Remember the Milk, BeeMinder, Anki, Cultivating Curiosity, Overcoming Procrastination, and Winning at Arguments.</p>\n<p><em>GTD</em> I didn't learn because I've used it for years now or at least the parts of it that really work for me (lists and calendars mostly, and to a lesser extent filing).</p>\n<p><em>Remember the Milk</em> because my employer's security policy prohibits us from using it, and too much of my life happens at my day job to make maintaining two separate systems worthwhile.</p>\n<p>BeeMinder and Anki because I just don't have anything that seems it could benefit from being stored in those systems right now. All of these might be more beneficial to someone in different circumstances.</p>\n<p><em>Cultivating Curiosity</em> because I am already a very naturally curious person, and have been for as long as I can remember. I don't need help with this. Indeed if anything I need to tamp down on this tendency and focus more on accomplishing things rather than merely learning them.</p>\n<p>Similarly, <em>Overcoming Procrastination</em> didn't help a lot because I don't have a big procrastination problem, at least not compared to what I had when I was younger. Of course, I do say that in full knowledge that right this minute writing this article is a form of <a href=\"http://www.structuredprocrastination.com/\">structured procrastination</a> to avoid doing my taxes. :-)</p>\n<p><em>Winning at Arguments</em>, I am already very, very good at when I want to be, which is rare these days. It took me many years too realize that even though I \"won\" almost every argument I cared about, winning the argument wasn't usually all that useful. Winning an argument is the wrong goal to have for almost any purpose, and rarely leads to the outcomes I desire.</p>\n<h3 id=\"Unofficial_ideas_from_fellow_attendees_\">Unofficial ideas from fellow attendees:</h3>\n<p>Polyphasic sleep: I'm going to let the younger, more pioneering attendees experiment with this one. Even if it does work (which seems far from obvious) I don't see how one could integrate it into a conventional day job and family.</p>\n<p>At breakfast one morning, a fellow attendee (Hunter?) suggested putting unsalted butter in my coffee to add more fat to my diet. It's not as crazy as it sounds. After all butter is little more than clarified cream, which I do like in my coffee. I tried this once and I still prefer cream, but I may give it another shot.</p>\n<p>Finally, I've referred two workshop attendees to my employer as potential hires. If anyone else from the workshop is looking for a job, especially in tech, sales, or legal, drop me a line privately. For that matter if any Less Wronger is looking for a job, drop me a line privately. We have hundreds of open positions in major cities around the world. Quite a few LessWrongers already work there, and there's room for many more.</p>\n<h3 id=\"What_the_workshop_didn_t_teach\">What the workshop didn't teach</h3>\n<p>There were a few techniques that were conspicuous by their absence. In particular I think the CFAR/LessWrong and Agile/XP communities have a lot to teach each other. I was surprised that no one at the workshop seemed to have heard of Kanban or Scrum, much less practice it. Burndown charts and point-based estimation are a really interesting modification of the outside view by comparing your team to your team in the past, rather than to other teams.</p>\n<p>Pairing is also a useful technique beyond programming as at least Eliezer (not present at the workshop) <a href=\"/lw/fc3/checklist_of_rationality_habits/\">has discovered</a>. Pairing is an incredibly effective way to overcome akrasia and procrastination.</p>\n<p>In reverse, I am considering what the craft of software development has to learn from CFAR style rationality, more specifically epistemic rationality. I have begun to notice my confusion during conversations with users, product managers, and tech leads and call it to conscious attention. I less frequently let unclear specs and goals pass without comment. Rather, I ask for examples and drill down into them until I feel my confusion has been conquered.</p>\n<p>So far these techniques seem very useful in analysis and requirements gathering. I've found them less obviously useful (though certainly not harmful in any way) during coding, debugging, and testing. In these stages there's simply too much to be confused by to address it all, and whatever I'm confused by that's relevant to the task at hand rapidly calls itself to my attention. For instance, when a bug shows up in a production system, the very first and natural question to ask&nbsp; is \"How the hell did the system do that?!\" On the other hand, the planning kata may be very helpful with the early stages of system design, though I haven't yet had an opportunity to try that out.</p>\n<h3 id=\"Was_it_Worth__3900_\">Was it Worth $3900?<br></h3>\n<p>Overall, I found the workshop to be a worthwhile experience, if an expensive one; and I recommend it to you if you have the opportunity and resources to attend. There are a lot of practical techniques to be learned, and you only need one or two of them to pay off to cover the cost and time. Even if the primary value is simply introducing you to books and techniques you explore further after the workshop such as <a href=\"/lw/gp4/the_power_of_pomodoros/\">Getting Things Done</a> or <a href=\"http://www.amazon.com/exec/obidos/ISBN=0374275637/ref=nosim/cafeaulaitA\">Thinking Fast and Slow</a>, that may be enough. Most knowledge workers are operating far below the level of which we're capable, and expanding our effectiveness can pay for itself.</p>\n<p>Before attending, it is worth asking yourself whether there's an opportunity to learn this material at lower cost. For instance, did I really need to spend $3900 and 4 days to learn about&nbsp;<a href=\"/lw/gp4/the_power_of_pomodoros/\">Pomodoro</a>? Apparently so, since I'd heard about Pomodoro for years and paid no attention to it until January. On the other hand, a <a href=\"/lw/gp4/the_power_of_pomodoros/\">$20 book</a> I read on the subway was fully sufficient for me to learn and implement Getting Things Done. You'll have to judge this one for yourself.</p>", "sections": [{"title": "What I Learned and Implemented", "anchor": "What_I_Learned_and_Implemented", "level": 1}, {"title": "What I Learned and Forgot", "anchor": "What_I_Learned_and_Forgot", "level": 1}, {"title": "What I Learned But Didn't Implement", "anchor": "What_I_Learned_But_Didn_t_Implement", "level": 1}, {"title": "What I Didn't Learn", "anchor": "What_I_Didn_t_Learn", "level": 1}, {"title": "Unofficial ideas from fellow attendees:", "anchor": "Unofficial_ideas_from_fellow_attendees_", "level": 1}, {"title": "What the workshop didn't teach", "anchor": "What_the_workshop_didn_t_teach", "level": 1}, {"title": "Was it Worth $3900?", "anchor": "Was_it_Worth__3900_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHrj2nDcTgaJS7JFq", "4iLk2rxTguFqHHs3Y", "EFQ3F6kmt4WHXRqik", "ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T19:55:08.865Z", "modifiedAt": null, "url": null, "title": "A Quick and Dirty Survey: Textbook Learning", "slug": "a-quick-and-dirty-survey-textbook-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:20.230Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AmagicalFishy", "createdAt": "2011-06-17T13:22:31.254Z", "isAdmin": false, "displayName": "AmagicalFishy"}, "userId": "77u6aqgcFfDiDNHMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z4Dwqia5sBvFLmW2q/a-quick-and-dirty-survey-textbook-learning", "pageUrlRelative": "/posts/z4Dwqia5sBvFLmW2q/a-quick-and-dirty-survey-textbook-learning", "linkUrl": "https://www.lesswrong.com/posts/z4Dwqia5sBvFLmW2q/a-quick-and-dirty-survey-textbook-learning", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Quick%20and%20Dirty%20Survey%3A%20Textbook%20Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Quick%20and%20Dirty%20Survey%3A%20Textbook%20Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4Dwqia5sBvFLmW2q%2Fa-quick-and-dirty-survey-textbook-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Quick%20and%20Dirty%20Survey%3A%20Textbook%20Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4Dwqia5sBvFLmW2q%2Fa-quick-and-dirty-survey-textbook-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4Dwqia5sBvFLmW2q%2Fa-quick-and-dirty-survey-textbook-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1283, "htmlBody": "<p>Hello, folks. I'm one of those long-time lurkers.<br /><br />I've decided to conduct, as the title suggests, a quick and dirty survey in hopes of better understanding a problem I have (or rather, whether or not what I have is actually a problem).</p>\n<p><strong>Here's some context:</strong>&nbsp;I'm a Physics &amp; Mathematics major, currently taking multi-variable. Lately, I've been unsatisfied with my understanding and usage of mathematics&mdash;mainly calculus. I've decided to go through what's been recommended as a much more&nbsp;rigorous Calculus textbook, <em>Calculus </em>by Michael Spivak. So far I'm really enjoying it, but it's taking me a <em>long</em> time to get through the exercises. I can be very meticulous about things like this and want to do every exercise through every chapter; I feel that there's benefit to actually doing them regardless of whether or not I look at the problem and think \"Yeah, I can do this.\" Sometimes actually doing the problem is much more difficult than it seems, and I learn a lot from doing them. When flipping through the exercises, I also notice that&mdash;regardless of how well I think I know the material&mdash;there ends up being a section of exercises focused on something I've never heard of before; something very clever or, I think, mathematically enlightening, that's&nbsp;dependent&nbsp;on the exercises before it.</p>\n<p>I'm somewhat&nbsp;embarrassed&nbsp;to admit that the exercises of the first chapter alone had taken me hours upon hours upon hours of combined work. I consider myself slow when it comes to reading mathematics and physics literature&mdash;I have to carefully comb through all the concepts and equations and structure them intuitively in a way I see fit. I hate <em>not </em>having a very fundamental understanding of the things I'm working with.</p>\n<p>At the same time, I read/hear people who apparently are familiar with multiple textbooks on the same subject. Familiar enough to judge whether or not it is a good textbook. Familiar enough to place how they fit on a hierarchy of textbooks on the same subject. I think \"At the rate I'm going, it will take me a very long time to get through this.\"&nbsp;<br /><br />So...</p>\n<p><strong>Here's (what I think is) my issue:</strong>&nbsp;I don't know whether or not I'm taking <em>too </em>long. Am I doing things inefficiently? Is there a better way to choose which exercises I do and don't work through so that I learn a similar amount of material in less time? Or is it just fine that I'm taking this long? Am I <em>slow and inefficient </em>or am I just new to this process of working through a textbook cover-to-cover, which is supposed to take a very long time anyway?</p>\n<p>I spend more time than I should learning about learning, instead of learning the material itself. I find myself using up lots of time trying to figure out how to learn more efficiently, how to think more efficiently, how to work more efficiently, and such things&mdash;as opposed to actually learning and actually thinking and actually working, which ends up being an inefficient use of my time. I think part of this problem stems from the fact that I don't have much of a comparison for when I can say \"Ok, I'm satisfied and can stop focusing on improve <em>how </em>I do this act&mdash;and just do it already.\" I want to solve that issue now.<br /><br />Which brings us to...</p>\n<p><strong>Here's my attempted solution: </strong>A survey! I assume many people here at LessWrong have worked through a science or mathematics textbook on their own. Mainly I'd like to gauge whether or not you thought you were taking a very long time, how long it took you, etc. I'd also like to know what your approach was: Did you perform every exercise, or skim through the book finding things you knew you didn't know? Did you skip around or go from the first chapter to the last? Do you have any advice on <em>how </em>one should approach a given textbook?</p>\n<p><strong>Here's the survey: </strong>https://docs.google.com/forms/d/1S4_-7_dxgmgprMbNhL1dNmX_0Zq9QrA9lpTl9ZHHxMI/viewform</p>\n<p>I'm not sure how interested anyone but me is in this, but on a later date I could make another post showing the data. I considered checking \"Publish and show a link to the results of this form\", but I wasn't sure if that kept everyone anonymous or not. Also, feel more than free to post any criticism, shortcomings, improvements, etc. Have I left anything out? Is there anything you'd like to see me add? This is my first attempt at a survey like this and I'd appreciate any feedback (though I know it's not necessarily a rigorous survey, just a quick data-collection, I suppose).</p>\n<p><em>I strongly encourage the posting of any textbook-reading tips or guidelines in the comments. I left that out of the survey so that anyone who's interested has immediate access to tips.</em></p>\n<p><strong>Here's an edit:</strong>&nbsp;Thanks for all the responses, everyone. Not only was my original question sufficiently answered (that is, it doesn't seem like I'm taking <strong>too</strong>&nbsp;long; there were only a few survey takers, but in between the comments and the survey answers, I'm not going at an extraordinarily slow rate). There's some very solid advice for different methods I might try to optimize my learning process. &nbsp;One that especially hit home was the suggestion that the large amounts of time spent \"learning about learning\" are such because it feels more comfortable than <em>actually learning the material. </em>In short, it's a&nbsp;safety&nbsp;blanket that makes me feel like I'm doing something productive when I'm really just avoiding what needs to be done. Some other useful pieces of advice are:</p>\n<p>- Try being open to learning a broader range of materials without necessarily mastering each one. It might be the case that you need to know one thing in order to master the other, and need to know the other in order to master the one&mdash;trying to master either of them in isolation ends up being somewhat futile. Not everything needs to be \"brick by brick\" structured. (This was a lesson I found useful when I first learned that a number raised to the \"one half\" power was the square root of that number: Trying to master it in terms of the rules I already knew ended up in a thought like, \"... Two to the third power is two times two times two. Two to the one-half power is two... times two one half times?\"</p>\n<p>- Though it may be uncomfortable at first, it could make learning easier to try the exercises before reading the chapter super-carefully; trying them before you feel ready to try them. You don't necessarily have to fully comprehend all of the proofs in the chapter to get through some exercises.&nbsp;</p>\n<p>- Textbooks might just be the wrong way to go in the first place. Try resources like Wikipedia, math blogs, and math forums.</p>\n<p>- \"Don't use the answer key unless you've spent a significant amount of time trying to find the answer yourself!\" (This may seem obvious, but a few years ago, I'd spend a couple of minutes on the problem, not understand it, look to the answer key, and wonder why I wasn't learning anything.)</p>\n<p>- Skip exercises when you feel you could solve them, but randomly check whether this estimate is correct by doing the problem anyway. (I like this one a lot).</p>\n<p>- Talk to a professor!</p>\n<p>- It may be the case that you learn well via just reading, and not spending so much time on the exercises.</p>\n<p><strong>Here are some websites/blogs mentioned:<br /></strong>(Blog)<em> Math for Programmers -&nbsp;</em><a style=\"color: #3d3d3e; font-family: Arial, Helvetica, sans-serif; line-height: 21.111112594604492px; text-align: justify; background-color: #f7f7f8;\" title=\"Math for Programmers\" rel=\"nofollow\" href=\"http://steve-yegge.blogspot.com/2006/03/math-for-programmers.html\">http://steve-yegge.blogspot.com/2006/03/math-for-programmers.html</a><br />(Blog)<em> Annoying Precision -&nbsp;</em><a style=\"font-style: italic;\" title=\"Annoying Precision\" href=\"http://qchu.wordpress.com/\">http://qchu.wordpress.com/</a><br />(Math Forum)<em> Mathematics</em> -&nbsp;<a href=\"http://math.stackexchange.com/\">http://math.stackexchange.com/</a>&nbsp;</p>\n<p>Excellent, excellent stuff, though. Thank you. :) There's a lot of material and advice for me to work with&mdash;while simultaneously making sure I don't avoid my work by hiding under the guise of productivity.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z4Dwqia5sBvFLmW2q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "21973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T22:04:47.275Z", "modifiedAt": null, "url": null, "title": "Non-replicability of Some Behavioral Economics Research Across Cultures", "slug": "non-replicability-of-some-behavioral-economics-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "realitygrill", "createdAt": "2009-08-16T18:29:05.163Z", "isAdmin": false, "displayName": "realitygrill"}, "userId": "TuhAvcB3j9Ar5iquu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zFDbwMbRwScqKXk4p/non-replicability-of-some-behavioral-economics-research", "pageUrlRelative": "/posts/zFDbwMbRwScqKXk4p/non-replicability-of-some-behavioral-economics-research", "linkUrl": "https://www.lesswrong.com/posts/zFDbwMbRwScqKXk4p/non-replicability-of-some-behavioral-economics-research", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-replicability%20of%20Some%20Behavioral%20Economics%20Research%20Across%20Cultures&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-replicability%20of%20Some%20Behavioral%20Economics%20Research%20Across%20Cultures%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFDbwMbRwScqKXk4p%2Fnon-replicability-of-some-behavioral-economics-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-replicability%20of%20Some%20Behavioral%20Economics%20Research%20Across%20Cultures%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFDbwMbRwScqKXk4p%2Fnon-replicability-of-some-behavioral-economics-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFDbwMbRwScqKXk4p%2Fnon-replicability-of-some-behavioral-economics-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>Not our typical cognitive biases, but does raise the question of universality - I am aware of much stronger evidence for the universality of emotions and facial expression. Skimming the paper looks like a lot of fun.</p>\n<p>&nbsp;</p>\n<p>http://www.psmag.com/magazines/pacific-standard-cover-story/joe-henrich-weird-ultimatum-game-shaking-up-psychology-economics-53135</p>\n<p>&nbsp;</p>\n<p>Actual paper:</p>\n<p>http://www2.psych.ubc.ca/~henrich/pdfs/Weird_People_BBS_final02.pdf</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zFDbwMbRwScqKXk4p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 3, "extendedScore": null, "score": 1.1348470907100142e-06, "legacy": true, "legacyId": "21974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-10T23:45:19.782Z", "modifiedAt": null, "url": null, "title": "You only need faith in two things", "slug": "you-only-need-faith-in-two-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:49.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zmSuDDFE4dicqd4Hg/you-only-need-faith-in-two-things", "pageUrlRelative": "/posts/zmSuDDFE4dicqd4Hg/you-only-need-faith-in-two-things", "linkUrl": "https://www.lesswrong.com/posts/zmSuDDFE4dicqd4Hg/you-only-need-faith-in-two-things", "postedAtFormatted": "Sunday, March 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20only%20need%20faith%20in%20two%20things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20only%20need%20faith%20in%20two%20things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmSuDDFE4dicqd4Hg%2Fyou-only-need-faith-in-two-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20only%20need%20faith%20in%20two%20things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmSuDDFE4dicqd4Hg%2Fyou-only-need-faith-in-two-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmSuDDFE4dicqd4Hg%2Fyou-only-need-faith-in-two-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>\n<p>You only need faith in two things: &nbsp;That \"induction works\" has a non-super-exponentially-tiny prior probability, and that some single large ordinal is well-ordered. &nbsp;Anything else worth believing in is a deductive consequence of one or both.</p>\n<p>(Because being exposed to ordered sensory data will rapidly promote the hypothesis that induction works, even if you started by assigning it very tiny prior probability, so long as that prior probability is not super-exponentially tiny. &nbsp;Then induction on sensory data gives you all empirical facts worth believing in. &nbsp;Believing that a mathematical system has a model usually corresponds to believing that a certain computable ordinal is well-ordered (the proof-theoretic ordinal of that system), and large ordinals imply the well-orderedness of all smaller ordinals. &nbsp;So if you assign non-tiny prior probability to the idea that induction might work, and you believe in the well-orderedness of a single sufficiently large computable ordinal, all of empirical science, and all of the math you will actually believe in, will follow without any further need for faith.)</p>\n<p>(The reason why you need faith for the first case is that although the fact that induction works can be readily observed, there is also some anti-inductive prior which says, 'Well, but since induction has worked all those previous times, it'll probably fail next time!' and 'Anti-induction is bound to work next time, since it's never worked before!' &nbsp;Since anti-induction objectively gets a far lower Bayes-score on any ordered sequence and is then demoted by the logical operation of Bayesian updating, to favor induction over anti-induction it is not necessary to start out believing that induction works better than anti-induction, it is only necessary *not* to start out by being *perfectly* confident that induction won't work.)</p>\n<p>(The reason why you need faith for the second case is that although more powerful proof systems - those with larger proof-theoretic ordinals - can prove the consistency of weaker proof systems, or equivalently prove the well-ordering of smaller ordinals, there's no known perfect system for telling which mathematical systems are consistent just as (equivalently!) there's no way of solving the halting problem. &nbsp;So when you reach the strongest math system you can be convinced of and further assumptions seem dangerously fragile, there's some large ordinal that represents all the math you believe in. &nbsp;If this doesn't seem to you like faith, try looking up a Buchholz hydra and then believing that it can always be killed.)</p>\n<p>(Work is ongoing on eliminating the requirement for faith in these two remaining propositions. &nbsp;For example, we might be able to describe our increasing confidence in ZFC in terms of logical uncertainty and an inductive prior which is updated as ZFC passes various tests that it would have a substantial subjective probability of failing, even given all other tests it has passed so far, if ZFC were inconsistent.)</p>\n<p>(No, this is *not* the \"tu quoque!\" moral equivalent of starting out by assigning probability 1 that Christ died for your sins.)</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zmSuDDFE4dicqd4Hg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 46, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "21975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-11T04:13:14.012Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Markets are Anti-Inductive", "slug": "seq-rerun-markets-are-anti-inductive", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CnqeNxxEGWD9JocjP/seq-rerun-markets-are-anti-inductive", "pageUrlRelative": "/posts/CnqeNxxEGWD9JocjP/seq-rerun-markets-are-anti-inductive", "linkUrl": "https://www.lesswrong.com/posts/CnqeNxxEGWD9JocjP/seq-rerun-markets-are-anti-inductive", "postedAtFormatted": "Monday, March 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Markets%20are%20Anti-Inductive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Markets%20are%20Anti-Inductive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnqeNxxEGWD9JocjP%2Fseq-rerun-markets-are-anti-inductive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Markets%20are%20Anti-Inductive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnqeNxxEGWD9JocjP%2Fseq-rerun-markets-are-anti-inductive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnqeNxxEGWD9JocjP%2Fseq-rerun-markets-are-anti-inductive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>Today's post, <a href=\"/lw/yv/markets_are_antiinductive/\">Markets are Anti-Inductive</a> was originally published on 26 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Markets_are_Anti-Inductive\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The standard theory of efficient markets says that exploitable regularities in the past, shouldn't be exploitable in the future. If everybody knows that \"stocks have always gone up\", then there's no reason to sell them.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gyb/seq_rerun_formative_youth/\">Formative Youth</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CnqeNxxEGWD9JocjP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1350892266405325e-06, "legacy": true, "legacyId": "21984", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h24JGbmweNpWZfBkM", "ocaHobDPw5kcejwrJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-11T14:12:16.403Z", "modifiedAt": null, "url": null, "title": "Young Cryonicist Gathering Warning", "slug": "young-cryonicist-gathering-warning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:33.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dkPYLYq6CDEGfYSvy/young-cryonicist-gathering-warning", "pageUrlRelative": "/posts/dkPYLYq6CDEGfYSvy/young-cryonicist-gathering-warning", "linkUrl": "https://www.lesswrong.com/posts/dkPYLYq6CDEGfYSvy/young-cryonicist-gathering-warning", "postedAtFormatted": "Monday, March 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Young%20Cryonicist%20Gathering%20Warning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYoung%20Cryonicist%20Gathering%20Warning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkPYLYq6CDEGfYSvy%2Fyoung-cryonicist-gathering-warning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Young%20Cryonicist%20Gathering%20Warning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkPYLYq6CDEGfYSvy%2Fyoung-cryonicist-gathering-warning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkPYLYq6CDEGfYSvy%2Fyoung-cryonicist-gathering-warning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 482, "htmlBody": "<p>Edit 1: context: Cryonics is a common topic on Less Wrong. There are a lot of Less Wrong cryonicists... and a good portion of people to whom this warning is relevant to are Less Wrongers. So this seems like as good a place as any to post this, <strong>since I'm just trying to get some potentially helpful information to these people</strong>.</p>\n<p>Edit 2: It also occurred to me that if you just randomly see this you might have no idea if it's legitimate... feel free to message me if you'd like references to back up the story or anything... <strong>all you have to do is ask</strong></p>\n<p>&nbsp;</p>\n<p>Sent this to a friend, and I thought I should share:<br /><br /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Warning on the Young Cryonicists Gathering: The lady who runs this is quite literally, nuts. Cairn Adun will revoke your scholarship for petty reasons and leave you screwed. I had to heroically bargain down her punishment on two friends who were late to breakfast and missed the sign-in sheet. Another guy missed his flight or something and showed up late and had his entire scholarship revoked.</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">I went last year and escaped unscathed (as did most everyone). You can certainly do it, and yes I would recommend setting independent alarms, ample planning time, etc. I would also recommend going ahead and just sending Cairn an email saying hello, you know maybe thank her for putting the event together, you're looking forward to seeing her (lol) and you're having no problems so far (and make sure you let her know right away if there are any problems, and that it isn't your fault). When you get the chance maybe try to cautiously elicit what exactly her criteria are for screwing someone (lol), and try to be extra responsible (make sure you fill in all the fields in all the stupid forms she hands out, with at least some decent BS or something - don't give completely sarcastic answers or anything, I think she had it in for some from the beginning for half-trolling in their answers).<br /><br />They will encourage you to stay out late Saturday night, and if you miss breakfast Sunday morning, multiple people were having their entire scholarships revoked, without ever being told about any of these criteria beforehand (I've heard that some of this is stated more explicitly in the contract this year). They didn't really seem to be standing up for themselves, so I went and pleaded with her as best I could and she agreed to only revoke their hotel scholarship. (Which they did end up having to pay.)</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">But don't freak out about it too much if you're already committed. This conference is absolutely full of interesting weirdos :) so just go and be yourself and enjoy it. I don't know if it's worth it to eat any charges you've already committed. Interested in taking Florida vacation? lol.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dkPYLYq6CDEGfYSvy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -14, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "21988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-11T16:18:28.944Z", "modifiedAt": null, "url": null, "title": "Thoughts on the frame problem and moral symbol grounding", "slug": "thoughts-on-the-frame-problem-and-moral-symbol-grounding", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3RdiNYE8Zm8sMYNHt/thoughts-on-the-frame-problem-and-moral-symbol-grounding", "pageUrlRelative": "/posts/3RdiNYE8Zm8sMYNHt/thoughts-on-the-frame-problem-and-moral-symbol-grounding", "linkUrl": "https://www.lesswrong.com/posts/3RdiNYE8Zm8sMYNHt/thoughts-on-the-frame-problem-and-moral-symbol-grounding", "postedAtFormatted": "Monday, March 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20the%20frame%20problem%20and%20moral%20symbol%20grounding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20the%20frame%20problem%20and%20moral%20symbol%20grounding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3RdiNYE8Zm8sMYNHt%2Fthoughts-on-the-frame-problem-and-moral-symbol-grounding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20the%20frame%20problem%20and%20moral%20symbol%20grounding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3RdiNYE8Zm8sMYNHt%2Fthoughts-on-the-frame-problem-and-moral-symbol-grounding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3RdiNYE8Zm8sMYNHt%2Fthoughts-on-the-frame-problem-and-moral-symbol-grounding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 711, "htmlBody": "<p>(<em>some thoughts on frames, grounding symbols, and Cyc</em>)</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Frame_problem\">frame problem</a> is a problem in AI to do with all the variables <em>not</em> expressed within the logical formalism - what happens to them? To illustrate, consider the <a href=\"http://en.wikipedia.org/wiki/Yale_shooting_problem\">Yale Shooting Problem</a>: a person is going to be shot with a gun, at time 2. If that gun is loaded, the person dies. The gun will get loaded at time 1.&nbsp;Formally, the system is:</p>\n<ul>\n<li>alive(0) &nbsp; &nbsp; (<em>the person is alive to start with</em>)</li>\n<li>&not;loaded(0)&nbsp; &nbsp;&nbsp;&nbsp;(<em>the gun begins unloaded</em>)</li>\n<li>true&nbsp;&rarr; loaded(1)&nbsp;&nbsp; &nbsp;&nbsp;(<em>the gun will get loaded at time 1</em>)</li>\n<li>loaded(2)&nbsp;&rarr; &not;alive(3)&nbsp; &nbsp;&nbsp;&nbsp;(<em>the person will get killed if shot with a loaded gun</em>)</li>\n</ul>\n<p>So the question is, does the person actually die? It would seem blindingly obvious that they do, but that isn't formally clear - we know the gun was loaded at time 1, but was it still loaded at time 2? Again, this seems blindingly obvious - but that's because of the words, not the formalism. Ignore the descriptions in italics, and the names of the suggestive <a href=\"/lw/la/truly_part_of_you/\">LISP tokens</a>.</p>\n<p>Since that's hard to do, consider the following example. Alicorn, for instance, <a href=\"/lw/d4x/local_ordinances_of_fun/6ukp\">hates surprises</a> - they make her feel unhappy. Let's say that we decompose time into days, and that a surprise one day will ruin her next day. Then we have a system:</p>\n<ul>\n<li>happy(0)&nbsp;&nbsp; &nbsp;&nbsp;(<em>Alicorn starts out happy</em>)</li>\n<li>&not;surprise(0)&nbsp;&nbsp; &nbsp;&nbsp;(<em>nobody is going to surprise her on day 0</em>)</li>\n<li>true&nbsp;&rarr; surprise(1)&nbsp;&nbsp; &nbsp;&nbsp;(<em>somebody is going to surprise her on day 1</em>)</li>\n<li>surprise(2)&nbsp;&rarr; &not;happy(3)&nbsp;&nbsp; &nbsp;&nbsp;(<em>if someone surprises her on day 2, she'll be unhappy the next day</em>)</li>\n</ul>\n<p>So here, is Alicorn unhappy on day 3? Well, it seems unlikely - unless someone&nbsp;coincidentally&nbsp;surprised her on day 2. And there's no reason to think that would happen! So, \"obviously\", she's not unhappy on day 3.</p>\n<p>Except... the two problems are formally identical. Replace \"alive\" with \"happy\" and \"loaded\" with \"surprise\". And though our semantic <em>understanding</em> tells us that \"(loaded(1) &rarr; loaded (2))\" (guns don't just unload themselves) but \"&not;(surprise(1) &rarr; surprise(2))\" (being surprised one day doesn't mean you'll be surprised the next), we can't tell this from the symbols.</p>\n<p>And we haven't touched on all the other problems with the symbolic setup. For instance, what happens with \"alive\" on any other time than 0 and 3? Does that change from moment to moment? If we want the words to do what we want, we need to put in a lot of logical conditionings, so that our intuitions are all there.</p>\n<p>This shows that there's a connection between the frame problem and symbol grounding. If we and the AI both understand what the symbols <em>mean</em>, then we don't need to specify all the conditionals - we can simply deduce them, if asked (\"yes, if the person is dead at 3, they're also dead at 4\"). &nbsp;But conversely, if we have a huge amount of logical conditioning, then there is less and less that the symbols could actually mean. The more structure we put in our logic, the less structures there are in the real world that fit it (\"X(i)&nbsp;&rarr;&nbsp;X(i+1)\" is something that can apply to being dead, not to being happy, for instance).</p>\n<p>This suggests a possible use for the <a href=\"http://en.wikipedia.org/wiki/Cyc\">Cyc project</a> - the quixotic attempt to build an AI by formalising all of common sense (\"Bill Clinton belongs to the collection of U.S. presidents\" and \"all trees are plants\"). You're very unlikely to get an AI through that approach - but it might be possible to train an already existent AI with it. Especially if the AI had <em>some</em> symbol grounding, then there might not be all that many structures in the real world that could correspond to that mass of logical relations. Some symbol grounding + Cyc + the internet - and suddenly there's not that many possible interpretations for \"Bill Clinton was stuck up a tree\". The main question, of course, is whether there is a similar restricted meaning for \"this human is enjoying a worthwhile life\".</p>\n<p>Do I think that's likely to work? No. But it's maybe worth investigating. And it might be a way of getting across <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crises</a>: you reconstruct a model as close as you can to your old one, in the new formalism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tJv2Zbtx37mBGBJk6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3RdiNYE8Zm8sMYNHt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 1.135566114848075e-06, "legacy": true, "legacyId": "21989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fg9fXrHpeaDD6pEPL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-11T17:47:01.078Z", "modifiedAt": null, "url": null, "title": "Game Theory of the Immortals", "slug": "game-theory-of-the-immortals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Crystalist", "createdAt": "2012-08-03T09:14:54.158Z", "isAdmin": false, "displayName": "Crystalist"}, "userId": "fmFNZemrwWrtGNfv3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKmcoDFCA9TZCyJzB/game-theory-of-the-immortals", "pageUrlRelative": "/posts/aKmcoDFCA9TZCyJzB/game-theory-of-the-immortals", "linkUrl": "https://www.lesswrong.com/posts/aKmcoDFCA9TZCyJzB/game-theory-of-the-immortals", "postedAtFormatted": "Monday, March 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Game%20Theory%20of%20the%20Immortals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGame%20Theory%20of%20the%20Immortals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKmcoDFCA9TZCyJzB%2Fgame-theory-of-the-immortals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Game%20Theory%20of%20the%20Immortals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKmcoDFCA9TZCyJzB%2Fgame-theory-of-the-immortals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKmcoDFCA9TZCyJzB%2Fgame-theory-of-the-immortals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p><strong id=\"internal-source-marker_0.590039026690647\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">I&rsquo;m sure many others have put much more thought into this sort of thing -- at the moment, I&rsquo;m too lazy to look for it, but if anyone has a link, I&rsquo;d love to check it out.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Anyway, I ran into some interesting musings on game theory for immortal agents and I thought it was interesting enough to talk about.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Cooperation in games like the iterated Prisoner&rsquo;s Dilemma is partly dependent on the probability of encountering the other player again. Axelrod (1981) gives the payoff for a sequence of 'cooperate's as R/(1-p) where R is the payoff for cooperating, and p is a discount parameter that he takes as the probability of the players meeting again (and recognizing each other, etc.). If you assume that both players continue playing for eternity in a randomly mixing, finite group of other players, then the probability of encountering the other player again approaches 1, and the payoff for an extended period of cooperation approaches infinity. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">So, take a group of rational, immortal agents, in a prisoner&rsquo;s dilemma game. Should we expect them to cooperate?</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">I realize there is no optimal strategy without reference to the other players&rsquo; strategies, and that the universe is not actually infinite in time, so this is not a perfect model on at least two counts, but I wanted to look at the simple case before adding complexities. </span></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKmcoDFCA9TZCyJzB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -7, "extendedScore": null, "score": 1.135624355791365e-06, "legacy": true, "legacyId": "21994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-11T18:09:19.943Z", "modifiedAt": null, "url": null, "title": "AI prediction case study 1: The original Dartmouth Conference", "slug": "ai-prediction-case-study-1-the-original-dartmouth-conference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fHSf8ACvTCvH9fFyd/ai-prediction-case-study-1-the-original-dartmouth-conference", "pageUrlRelative": "/posts/fHSf8ACvTCvH9fFyd/ai-prediction-case-study-1-the-original-dartmouth-conference", "linkUrl": "https://www.lesswrong.com/posts/fHSf8ACvTCvH9fFyd/ai-prediction-case-study-1-the-original-dartmouth-conference", "postedAtFormatted": "Monday, March 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20prediction%20case%20study%201%3A%20The%20original%20Dartmouth%20Conference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20prediction%20case%20study%201%3A%20The%20original%20Dartmouth%20Conference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSf8ACvTCvH9fFyd%2Fai-prediction-case-study-1-the-original-dartmouth-conference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20prediction%20case%20study%201%3A%20The%20original%20Dartmouth%20Conference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSf8ACvTCvH9fFyd%2Fai-prediction-case-study-1-the-original-dartmouth-conference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSf8ACvTCvH9fFyd%2Fai-prediction-case-study-1-the-original-dartmouth-conference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3074, "htmlBody": "<p><em>Myself, Kaj Sotala and Se\u0013&aacute;n \u0013&Oacute;h&Eacute;igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts <a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a> conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>As this is the first case study, it will also introduce the paper's prediction classification shemas.</em></p>\n<p>&nbsp;</p>\n<h2>Taxonomy of predictions</h2>\n<h3>Prediction types</h3>\n<blockquote>\n<p>There will never be a bigger plane built.</p>\n<p><em>Boeing engineer on the 247, a twin engine plane that held ten people.</em></p>\n</blockquote>\n<p>A fortune teller talking about celebrity couples, a scientist predicting the outcome of an experiment, an economist pronouncing on next year's GDP figures - these are canonical examples of predictions. There are other types of predictions, though. Conditional statements -&nbsp;<em>if</em>&nbsp;X happens, <em>then</em>&nbsp;so will Y - are also valid, narrower, predictions. Impossibility results are also a form of prediction. For instance, the law of conservation of energy gives a very broad prediction about every single perpetual machine ever made: to wit, that they will never work.<a id=\"more\"></a></p>\n<p>The common thread is that all these predictions constrain expectations of the future. If one takes the prediction to be true, one expects to see different outcomes than if one takes it to be false. This is closely related to Popper's notion of falsifiability (Pop). This paper will take every falsifiable statement about future AI to be a prediction.</p>\n<p>For the present analysis, predictions about AI will be divided into four types:</p>\n<ol>\n<li>Timelines and outcome predictions. These are the traditional types of predictions, giving the dates of specific AI milestones. Examples: An AI will pass the Turing test by 2000 (Tur50); Within a decade, AIs will be replacing scientists and other thinking professions (Hal11).</li>\n<li>Scenarios. These are a type of conditional predictions, claiming that if the conditions of the scenario are met, then certain types of outcomes will follow. Example: If someone builds a human-level AI that is easy to copy and cheap to run, this will cause mass unemployment among ordinary humans (Han94).</li>\n<li>Plans. These are a specific type of conditional prediction, claiming that if someone decides to implement a specific plan, then they will be successful in achieving a particular goal. Example: AI can be built by scanning a human brain and simulating the scan on a computer (San08).</li>\n<li>Issues and metastatements. This category covers relevant problems with (some or all) approaches to AI (including sheer impossibility results), and metastatements about the whole field. Examples: an AI cannot be built without a fundamental new understanding of epistemology (Deu12); Generic AIs will have certain (potentially dangerous) behaviours (Omo08).</li>\n</ol>\n<p>There will inevitably be some overlap between the categories, but the division is natural enough for this paper.</p>\n<p>&nbsp;</p>\n<h3>Prediction methods</h3>\n<p>Just as there are many types of predictions, there are many ways of arriving at them - crystal balls, consulting experts, constructing elaborate models. An initial review of various AI predictions throughout the literature suggests the following loose schema for prediction methods (as with any such schema, the purpose is to bring clarity to the analysis, not to force every prediction into a particular box, so it should not be seen as <em>the</em>&nbsp;definitive decomposition of prediction methods):</p>\n<ol>\n<li>Causal models</li>\n<li>Non-causal models</li>\n<li>The outside view</li>\n<li>Philosophical arguments</li>\n<li>Expert judgement</li>\n<li>Non-expert judgement</li>\n</ol>\n<p>Causal model are a staple of physics and the harder sciences: given certain facts about the situation under consideration (momentum, energy, charge, etc.) a conclusion is reached about what the ultimate state will be. If the facts were different, the end situation would be different.</p>\n<p>Outside of the hard sciences, however, causal models are often a luxury, as the underlying causes are not well understood. Some success can be achieved with non-causal models: without understanding what influences what, one can extrapolate trends into the future. Moore's law is a highly successful non-causal model (Moo65).</p>\n<p>In the the outside view, specific examples are grouped together and claimed to be examples of the same underlying trend. This trend is used to give further predictions. For instance, one could notice the many analogues of Moore's law across the spectrum of computing (e.g. in numbers of transistors, size of hard drives, network capacity, pixels per dollar), note that AI is in the same category, and hence argue that AI development must follow a similarly exponential curve (Kur99). Note that the use of the outside view is often implicit rather than explicit: rarely is it justified why these examples are grouped together, beyond general plausibility or similarity arguments. Hence detecting uses of the outside view will be part of the task of revealing hidden assumptions. There is evidence that the use of the outside view provides improved prediction accuracy, at least in some domains (KL93).</p>\n<p>Philosophical arguments are common in the field of AI. Some are simple impossibility statements: AI is decreed to be impossible, using arguments of varying plausibility. More thoughtful philosophical arguments highlight problems that need to be resolved in order to achieve AI, interesting approaches for doing so, and potential issues that might emerge if AIs were to built.</p>\n<p>Many of the predictions made by AI experts aren't logically complete: not every premise is unarguable, not every deduction is fully rigorous. In many cases, the argument relies on the expert's judgement to bridge these gaps. This doesn't mean that the prediction is unreliable: in a field as challenging as AI, judgement, honed by years of related work, may be the best tool available. Non-experts cannot easily develop a good feel for the field and its subtleties, so should not confidently reject expert judgement out of hand. Relying on expert judgement has its pitfalls, however.</p>\n<p>Finally, some predictions rely on the judgement of non-experts, or of experts making claims outside their domain of expertise. Prominent journalists, authors, CEOs, historians, physicists and mathematicians will generally be no more accurate than anyone else when talking about AI, no matter how stellar they are in their own field (Kah11).</p>\n<p>Predictions often use a combination of these methods, as will be seen in the various case studies - expert judgement, for instance, is a common feature in all of them.</p>\n<div><br /></div>\n<h2><span style=\"font-size: 16px;\">In the beginning, Dartmouth created the AI and the hype...</span></h2>\n<ul>\n<li>Classification: <strong>plan</strong>, using <strong>expert judgement</strong> and <strong>the outside view</strong>.</li>\n</ul>\n<p>Hindsight bias is very strong and misleading (Fis75). Humans are often convinced that past events couldn't have unfolded differently than how they did, and that the people at the time should have realised this. Even worse, people unconsciously edit their own memories so that they misremember themselves as being right even when they got their past predictions wrong (one of the reasons that it is important to pay attention only to the actual prediction as written at the time, and not to the author's subsequent justifications or clarifications). Hence when assessing past predictions, one must cast aside all knowledge of subsequent events, and try to assess the claims given the knowledge available at the time. This is an invaluable exercise to undertake before turning attention to predictions whose timelines have not come to pass.</p>\n<p>The 1956 Dartmouth Summer Research Project on Artificial Intelligence was a major conference, credited with introducing the term ''Artificial Intelligence'' and starting the research in many of its different subfields. The <a href=\"http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html\">conference proposal</a>, written in 1955, sets out what the organisers thought could be achieved. Its first paragraph reads:</p>\n<blockquote>\n<p>''We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.''</p>\n</blockquote>\n<p>This can be classified as a plan. Its main backing would have been expert judgement. The conference organisers were John McCarthy (a mathematician with experience in the mathematical nature of the thought process), Marvin Minsky (Harvard Junior Fellow in Mathematics and Neurology, and prolific user of neural nets), Nathaniel Rochester (Manager of Information Research, IBM, designer of the IBM 701, the first general purpose, mass-produced computer, and designer of the first symbolic assembler) and Claude Shannon (the ''father of information theory''). These were individuals who had been involved in a lot of related theoretical and practical work, some of whom had built functioning computers or programing languages - so one can expect them all to have had direct feedback about what was and wasn't doable in computing. If anyone could be considered experts in AI, in a field dedicated to an as yet non-existent machine, then they could. What implicit and explicit assumptions could they have used to predict that AI would be easy?</p>\n<p>Reading the full proposal doesn't give the impression of excessive optimism or overconfidence. The very first paragraph hints at the rigour of their ambitions - they realised that precisely describing the features of intelligence is a major step in simulating it. Their research plan is well decomposed, and different aspects of the problem of artificial intelligence are touched upon. The authors are well aware of the inefficiency of exhaustive search methods, of the differences between informal and formal languages, and of the need for encoding creativity. They talk about the need to design machines that can work with unreliable components, and that can cope with randomness and small errors in a robust way. They propose some simple models of some of these challenges (such as forming abstractions, or dealing with more complex environments), point to some previous successful work that has been done before, and outline how further improvements can be made.</p>\n<p>Reading through, the implicit reasons for their confidence seem to become apparent (as with any exercise in trying to identify implicit assumptions, this process is somewhat subjective. It is not meant to suggest that the authors were thinking along these lines, merely to point out factors that could explain their confidence - factors, moreover, that could have lead dispassionate analytical observers to agree with them). These were experts, some of whom had been working with computers from early days, who had a long track record of taking complex problems, creating simple (and then more complicated) models to deal with them. These models they used to generate useful insights or functioning machines. So this was an implicit use of the outside view - they were used to solving certain problems, these looked like the problems they could solve, hence they assumed they could solve them. To modern eyes, informal languages are hugely complicated, but this may not have been obvious at the time. Computers were doing tasks, such as complicated mathematical manipulations, that were considered high-skill, something only <a href=\"http://en.wikipedia.org/wiki/Human_computer\">impressive humans</a> had been capable of. Moravec's paradox had not yet been realised (this is the principle that high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources - sometimes informally expressed as ''everything easy [for a human] is hard [for a computer], everything hard is easy''). The human intuition about the relative difficulty of tasks was taken as accurate: there was no reason to suspect that parsing English was much harder than the impressive feats computer could already perform. Moreover, great progress had been made in logic, in semantics, in information theory, giving new understanding to old concepts: there was no reason to suspect that further progress wouldn't be both forthcoming and dramatic.</p>\n<p>Even at the time, though, one could criticise their overconfidence. Philosophers, for one, had a long track record of pointing out the complexities and subtleties of the human mind. It might have seemed plausible in 1955 that further progress in logic and information theory would end up solving all these problems - but it could have been equally plausible to suppose that the success of formal models had been on low-hanging fruit, and that further progress would become much harder. Furthermore, the computers at the time were much simpler than the human brain (e.g. the IBM 701, with 73728 bits of memory), so any assumption that AIs could be built was also an assumption that most of the human brain's processing was wasted. This implicit assumption was not obviously wrong, but neither was it obviously right.</p>\n<p>Hence the whole conference project would have seemed ideal, had it merely added more humility and qualifiers in the text, expressing uncertainty as to whether a particular aspect of the program might turn out to be hard or easy. After all, in 1955, there were no solid grounds for arguing that such tasks were unfeasible for a computer.</p>\n<p>Nowadays, it is obvious that the paper's predictions were very wrong. All the tasks mentioned were much harder to accomplish than they claimed at the time, and haven't been successfully completed even today. Rarely have such plausible predictions turned out to be so wrong; so what can be learned from this?</p>\n<p>The most general lesson is perhaps on the complexity of language and the danger of using human-understandable informal concepts in the field of AI. The Dartmouth group seemed convinced that because they informally understood certain concepts and could begin to capture some of this understanding in a formal model, then it must be possible to capture <em>all</em>&nbsp;this understanding in a formal model. In this, they were wrong. Similarities of features do not make the models similar to reality, and using human terms - such as 'culture' and 'informal' - in these model concealed huge complexity and gave an illusion of understanding. Today's AI developers have a much better understanding of how complex cognition can be, and have realised that programming simple-seeming concepts into computers can be very difficult. So the main lesson to draw is that reasoning about AI using human concepts (or anthropomorphising the AIs by projecting human features onto it) is a very poor guide to the nature of the problem and the time and effort required to solve it.</p>\n<h2>References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fHSf8ACvTCvH9fFyd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.1356390356067634e-06, "legacy": true, "legacyId": "21830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Myself, Kaj Sotala and Se\u0013\u00e1n \u0013\u00d3h\u00c9igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts <a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a> conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>As this is the first case study, it will also introduce the paper's prediction classification shemas.</em></p>\n<p>&nbsp;</p>\n<h2 id=\"Taxonomy_of_predictions\">Taxonomy of predictions</h2>\n<h3 id=\"Prediction_types\">Prediction types</h3>\n<blockquote>\n<p>There will never be a bigger plane built.</p>\n<p><em>Boeing engineer on the 247, a twin engine plane that held ten people.</em></p>\n</blockquote>\n<p>A fortune teller talking about celebrity couples, a scientist predicting the outcome of an experiment, an economist pronouncing on next year's GDP figures - these are canonical examples of predictions. There are other types of predictions, though. Conditional statements -&nbsp;<em>if</em>&nbsp;X happens, <em>then</em>&nbsp;so will Y - are also valid, narrower, predictions. Impossibility results are also a form of prediction. For instance, the law of conservation of energy gives a very broad prediction about every single perpetual machine ever made: to wit, that they will never work.<a id=\"more\"></a></p>\n<p>The common thread is that all these predictions constrain expectations of the future. If one takes the prediction to be true, one expects to see different outcomes than if one takes it to be false. This is closely related to Popper's notion of falsifiability (Pop). This paper will take every falsifiable statement about future AI to be a prediction.</p>\n<p>For the present analysis, predictions about AI will be divided into four types:</p>\n<ol>\n<li>Timelines and outcome predictions. These are the traditional types of predictions, giving the dates of specific AI milestones. Examples: An AI will pass the Turing test by 2000 (Tur50); Within a decade, AIs will be replacing scientists and other thinking professions (Hal11).</li>\n<li>Scenarios. These are a type of conditional predictions, claiming that if the conditions of the scenario are met, then certain types of outcomes will follow. Example: If someone builds a human-level AI that is easy to copy and cheap to run, this will cause mass unemployment among ordinary humans (Han94).</li>\n<li>Plans. These are a specific type of conditional prediction, claiming that if someone decides to implement a specific plan, then they will be successful in achieving a particular goal. Example: AI can be built by scanning a human brain and simulating the scan on a computer (San08).</li>\n<li>Issues and metastatements. This category covers relevant problems with (some or all) approaches to AI (including sheer impossibility results), and metastatements about the whole field. Examples: an AI cannot be built without a fundamental new understanding of epistemology (Deu12); Generic AIs will have certain (potentially dangerous) behaviours (Omo08).</li>\n</ol>\n<p>There will inevitably be some overlap between the categories, but the division is natural enough for this paper.</p>\n<p>&nbsp;</p>\n<h3 id=\"Prediction_methods\">Prediction methods</h3>\n<p>Just as there are many types of predictions, there are many ways of arriving at them - crystal balls, consulting experts, constructing elaborate models. An initial review of various AI predictions throughout the literature suggests the following loose schema for prediction methods (as with any such schema, the purpose is to bring clarity to the analysis, not to force every prediction into a particular box, so it should not be seen as <em>the</em>&nbsp;definitive decomposition of prediction methods):</p>\n<ol>\n<li>Causal models</li>\n<li>Non-causal models</li>\n<li>The outside view</li>\n<li>Philosophical arguments</li>\n<li>Expert judgement</li>\n<li>Non-expert judgement</li>\n</ol>\n<p>Causal model are a staple of physics and the harder sciences: given certain facts about the situation under consideration (momentum, energy, charge, etc.) a conclusion is reached about what the ultimate state will be. If the facts were different, the end situation would be different.</p>\n<p>Outside of the hard sciences, however, causal models are often a luxury, as the underlying causes are not well understood. Some success can be achieved with non-causal models: without understanding what influences what, one can extrapolate trends into the future. Moore's law is a highly successful non-causal model (Moo65).</p>\n<p>In the the outside view, specific examples are grouped together and claimed to be examples of the same underlying trend. This trend is used to give further predictions. For instance, one could notice the many analogues of Moore's law across the spectrum of computing (e.g. in numbers of transistors, size of hard drives, network capacity, pixels per dollar), note that AI is in the same category, and hence argue that AI development must follow a similarly exponential curve (Kur99). Note that the use of the outside view is often implicit rather than explicit: rarely is it justified why these examples are grouped together, beyond general plausibility or similarity arguments. Hence detecting uses of the outside view will be part of the task of revealing hidden assumptions. There is evidence that the use of the outside view provides improved prediction accuracy, at least in some domains (KL93).</p>\n<p>Philosophical arguments are common in the field of AI. Some are simple impossibility statements: AI is decreed to be impossible, using arguments of varying plausibility. More thoughtful philosophical arguments highlight problems that need to be resolved in order to achieve AI, interesting approaches for doing so, and potential issues that might emerge if AIs were to built.</p>\n<p>Many of the predictions made by AI experts aren't logically complete: not every premise is unarguable, not every deduction is fully rigorous. In many cases, the argument relies on the expert's judgement to bridge these gaps. This doesn't mean that the prediction is unreliable: in a field as challenging as AI, judgement, honed by years of related work, may be the best tool available. Non-experts cannot easily develop a good feel for the field and its subtleties, so should not confidently reject expert judgement out of hand. Relying on expert judgement has its pitfalls, however.</p>\n<p>Finally, some predictions rely on the judgement of non-experts, or of experts making claims outside their domain of expertise. Prominent journalists, authors, CEOs, historians, physicists and mathematicians will generally be no more accurate than anyone else when talking about AI, no matter how stellar they are in their own field (Kah11).</p>\n<p>Predictions often use a combination of these methods, as will be seen in the various case studies - expert judgement, for instance, is a common feature in all of them.</p>\n<div><br></div>\n<h2 id=\"In_the_beginning__Dartmouth_created_the_AI_and_the_hype___\"><span style=\"font-size: 16px;\">In the beginning, Dartmouth created the AI and the hype...</span></h2>\n<ul>\n<li>Classification: <strong>plan</strong>, using <strong>expert judgement</strong> and <strong>the outside view</strong>.</li>\n</ul>\n<p>Hindsight bias is very strong and misleading (Fis75). Humans are often convinced that past events couldn't have unfolded differently than how they did, and that the people at the time should have realised this. Even worse, people unconsciously edit their own memories so that they misremember themselves as being right even when they got their past predictions wrong (one of the reasons that it is important to pay attention only to the actual prediction as written at the time, and not to the author's subsequent justifications or clarifications). Hence when assessing past predictions, one must cast aside all knowledge of subsequent events, and try to assess the claims given the knowledge available at the time. This is an invaluable exercise to undertake before turning attention to predictions whose timelines have not come to pass.</p>\n<p>The 1956 Dartmouth Summer Research Project on Artificial Intelligence was a major conference, credited with introducing the term ''Artificial Intelligence'' and starting the research in many of its different subfields. The <a href=\"http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html\">conference proposal</a>, written in 1955, sets out what the organisers thought could be achieved. Its first paragraph reads:</p>\n<blockquote>\n<p>''We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.''</p>\n</blockquote>\n<p>This can be classified as a plan. Its main backing would have been expert judgement. The conference organisers were John McCarthy (a mathematician with experience in the mathematical nature of the thought process), Marvin Minsky (Harvard Junior Fellow in Mathematics and Neurology, and prolific user of neural nets), Nathaniel Rochester (Manager of Information Research, IBM, designer of the IBM 701, the first general purpose, mass-produced computer, and designer of the first symbolic assembler) and Claude Shannon (the ''father of information theory''). These were individuals who had been involved in a lot of related theoretical and practical work, some of whom had built functioning computers or programing languages - so one can expect them all to have had direct feedback about what was and wasn't doable in computing. If anyone could be considered experts in AI, in a field dedicated to an as yet non-existent machine, then they could. What implicit and explicit assumptions could they have used to predict that AI would be easy?</p>\n<p>Reading the full proposal doesn't give the impression of excessive optimism or overconfidence. The very first paragraph hints at the rigour of their ambitions - they realised that precisely describing the features of intelligence is a major step in simulating it. Their research plan is well decomposed, and different aspects of the problem of artificial intelligence are touched upon. The authors are well aware of the inefficiency of exhaustive search methods, of the differences between informal and formal languages, and of the need for encoding creativity. They talk about the need to design machines that can work with unreliable components, and that can cope with randomness and small errors in a robust way. They propose some simple models of some of these challenges (such as forming abstractions, or dealing with more complex environments), point to some previous successful work that has been done before, and outline how further improvements can be made.</p>\n<p>Reading through, the implicit reasons for their confidence seem to become apparent (as with any exercise in trying to identify implicit assumptions, this process is somewhat subjective. It is not meant to suggest that the authors were thinking along these lines, merely to point out factors that could explain their confidence - factors, moreover, that could have lead dispassionate analytical observers to agree with them). These were experts, some of whom had been working with computers from early days, who had a long track record of taking complex problems, creating simple (and then more complicated) models to deal with them. These models they used to generate useful insights or functioning machines. So this was an implicit use of the outside view - they were used to solving certain problems, these looked like the problems they could solve, hence they assumed they could solve them. To modern eyes, informal languages are hugely complicated, but this may not have been obvious at the time. Computers were doing tasks, such as complicated mathematical manipulations, that were considered high-skill, something only <a href=\"http://en.wikipedia.org/wiki/Human_computer\">impressive humans</a> had been capable of. Moravec's paradox had not yet been realised (this is the principle that high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources - sometimes informally expressed as ''everything easy [for a human] is hard [for a computer], everything hard is easy''). The human intuition about the relative difficulty of tasks was taken as accurate: there was no reason to suspect that parsing English was much harder than the impressive feats computer could already perform. Moreover, great progress had been made in logic, in semantics, in information theory, giving new understanding to old concepts: there was no reason to suspect that further progress wouldn't be both forthcoming and dramatic.</p>\n<p>Even at the time, though, one could criticise their overconfidence. Philosophers, for one, had a long track record of pointing out the complexities and subtleties of the human mind. It might have seemed plausible in 1955 that further progress in logic and information theory would end up solving all these problems - but it could have been equally plausible to suppose that the success of formal models had been on low-hanging fruit, and that further progress would become much harder. Furthermore, the computers at the time were much simpler than the human brain (e.g. the IBM 701, with 73728 bits of memory), so any assumption that AIs could be built was also an assumption that most of the human brain's processing was wasted. This implicit assumption was not obviously wrong, but neither was it obviously right.</p>\n<p>Hence the whole conference project would have seemed ideal, had it merely added more humility and qualifiers in the text, expressing uncertainty as to whether a particular aspect of the program might turn out to be hard or easy. After all, in 1955, there were no solid grounds for arguing that such tasks were unfeasible for a computer.</p>\n<p>Nowadays, it is obvious that the paper's predictions were very wrong. All the tasks mentioned were much harder to accomplish than they claimed at the time, and haven't been successfully completed even today. Rarely have such plausible predictions turned out to be so wrong; so what can be learned from this?</p>\n<p>The most general lesson is perhaps on the complexity of language and the danger of using human-understandable informal concepts in the field of AI. The Dartmouth group seemed convinced that because they informally understood certain concepts and could begin to capture some of this understanding in a formal model, then it must be possible to capture <em>all</em>&nbsp;this understanding in a formal model. In this, they were wrong. Similarities of features do not make the models similar to reality, and using human terms - such as 'culture' and 'informal' - in these model concealed huge complexity and gave an illusion of understanding. Today's AI developers have a much better understanding of how complex cognition can be, and have realised that programming simple-seeming concepts into computers can be very difficult. So the main lesson to draw is that reasoning about AI using human concepts (or anthropomorphising the AIs by projecting human features onto it) is a very poor guide to the nature of the problem and the time and effort required to solve it.</p>\n<h2 id=\"References_\">References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n<ul>\n</ul>", "sections": [{"title": "Taxonomy of predictions", "anchor": "Taxonomy_of_predictions", "level": 1}, {"title": "Prediction types", "anchor": "Prediction_types", "level": 2}, {"title": "Prediction methods", "anchor": "Prediction_methods", "level": 2}, {"title": "In the beginning, Dartmouth created the AI and the hype...", "anchor": "In_the_beginning__Dartmouth_created_the_AI_and_the_hype___", "level": 1}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T02:04:41.243Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Extraordinary Evidence and Bayes", "slug": "meetup-vancouver-extraordinary-evidence-and-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/db8ReNuKBG47ebQJK/meetup-vancouver-extraordinary-evidence-and-bayes", "pageUrlRelative": "/posts/db8ReNuKBG47ebQJK/meetup-vancouver-extraordinary-evidence-and-bayes", "linkUrl": "https://www.lesswrong.com/posts/db8ReNuKBG47ebQJK/meetup-vancouver-extraordinary-evidence-and-bayes", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Extraordinary%20Evidence%20and%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Extraordinary%20Evidence%20and%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb8ReNuKBG47ebQJK%2Fmeetup-vancouver-extraordinary-evidence-and-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Extraordinary%20Evidence%20and%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb8ReNuKBG47ebQJK%2Fmeetup-vancouver-extraordinary-evidence-and-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb8ReNuKBG47ebQJK%2Fmeetup-vancouver-extraordinary-evidence-and-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ka'>Vancouver Extraordinary Evidence and Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2150 macdonald st vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I recently saw someone claim that \"extraordinary claims require extraordinary evidence\" was a terrible hueristic. Then I saw someone use it to possibly get the wrong answer on something. Then my friends here at the meetup claimed they weren't comfortable with the whole bayes thing. So maybe it's time we had a chat about this, and practical bayesian epistemology in general.</p>\n\n<p>Of course the conversation will wander after that, and we will adventure through all sorts of wonderful topics.</p>\n\n<p>I'm switching up the venue a bit; this time we will meet at 2150 macdonald st (a big brown house at 6th and macdonald). Still the usual 15:00 start-time.</p>\n\n<p>Please join us on the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>(Haven't seen many lurkers in a while, lurkers please come out.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ka'>Vancouver Extraordinary Evidence and Bayes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "db8ReNuKBG47ebQJK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1359518337540608e-06, "legacy": true, "legacyId": "21995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Extraordinary_Evidence_and_Bayes\">Discussion article for the meetup : <a href=\"/meetups/ka\">Vancouver Extraordinary Evidence and Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2150 macdonald st vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I recently saw someone claim that \"extraordinary claims require extraordinary evidence\" was a terrible hueristic. Then I saw someone use it to possibly get the wrong answer on something. Then my friends here at the meetup claimed they weren't comfortable with the whole bayes thing. So maybe it's time we had a chat about this, and practical bayesian epistemology in general.</p>\n\n<p>Of course the conversation will wander after that, and we will adventure through all sorts of wonderful topics.</p>\n\n<p>I'm switching up the venue a bit; this time we will meet at 2150 macdonald st (a big brown house at 6th and macdonald). Still the usual 15:00 start-time.</p>\n\n<p>Please join us on the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>(Haven't seen many lurkers in a while, lurkers please come out.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Extraordinary_Evidence_and_Bayes1\">Discussion article for the meetup : <a href=\"/meetups/ka\">Vancouver Extraordinary Evidence and Bayes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Extraordinary Evidence and Bayes", "anchor": "Discussion_article_for_the_meetup___Vancouver_Extraordinary_Evidence_and_Bayes", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Extraordinary Evidence and Bayes", "anchor": "Discussion_article_for_the_meetup___Vancouver_Extraordinary_Evidence_and_Bayes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T02:18:52.957Z", "modifiedAt": null, "url": null, "title": "Pluralistic Existence in Many Many-Worlds", "slug": "pluralistic-existence-in-many-many-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:36.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Neotenic", "createdAt": "2013-03-04T02:28:23.403Z", "isAdmin": false, "displayName": "Neotenic"}, "userId": "qMgZoftatigAeMMhL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9dmL4gNszPvsxAX6j/pluralistic-existence-in-many-many-worlds", "pageUrlRelative": "/posts/9dmL4gNszPvsxAX6j/pluralistic-existence-in-many-many-worlds", "linkUrl": "https://www.lesswrong.com/posts/9dmL4gNszPvsxAX6j/pluralistic-existence-in-many-many-worlds", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pluralistic%20Existence%20in%20Many%20Many-Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APluralistic%20Existence%20in%20Many%20Many-Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9dmL4gNszPvsxAX6j%2Fpluralistic-existence-in-many-many-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pluralistic%20Existence%20in%20Many%20Many-Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9dmL4gNszPvsxAX6j%2Fpluralistic-existence-in-many-many-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9dmL4gNszPvsxAX6j%2Fpluralistic-existence-in-many-many-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 845, "htmlBody": "<p>There are at least ten different conceptions of how the World can be made of many worlds.</p>\n<p>But are those just <a href=\"/lw/np/disputing_definitions/\">definitional disputes</a>? Or are they separate claims that can be evaluated. If they are distinct, in virtue of what are they distinct. Finally, do we have good grounds to care (morally) about those fine distinctions?</p>\n<p>&nbsp;</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Multiverse#Tegmark.27s_classification\">Max Tegmark's taxonomy</a> is well known here.&nbsp;</p>\n<p>Brian Greene's is less, and has 9, instead of four, kinds of multiverse, I'll risk conflating the Tegmark ones that are superclasses of these, feel free to correct me:</p>\n<blockquote>\n<p>In his book, Greene discussed&nbsp;<a href=\"http://en.wikipedia.org/wiki/Multiverse#Brian_Greene.27s_nine_types_of_parallel_universes\">nine types of parallel universes</a>:</p>\n<ul>\n<li>(Tegmark 1) The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Quilted_multiverse\">quilted multiverse</a></strong>&nbsp;only works in an infinite universe. With an infinite amount of space, every possible event will occur an infinite amount of times. However, the speed of light prevents us from being aware of these other identical areas.</li>\n<li>(Tegmarks 1 and 2) The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Inflationary_multiverse\">inflationary multiverse</a></strong>&nbsp;is composed of various pockets where inflaton fields collapse and form new universes.</li>\n<li>The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Brane_multiverse\">brane multiverse</a></strong>&nbsp;follows from&nbsp;<a href=\"http://en.wikipedia.org/wiki/M-theory\">M-theory</a>&nbsp;and states that each universe is a 3-dimensional brane that exists with many others. Particles are bound to their respective branes except for gravity.</li>\n<li>The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Cyclic_multiverse\">cyclic multiverse</a></strong>&nbsp;has multiple&nbsp;<a href=\"http://en.wikipedia.org/wiki/Branes\">branes</a>&nbsp;(each a universe) that collided, causing Big Bangs. The universes bounce back and pass through time, until they are pulled back together and collided again, destroying the old contents and creating them anew.</li>\n<li>(Tegmarks 2) The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Landscape_multiverse\">landscape multiverse</a></strong>&nbsp;relies on string theory's Calabi-Yau shapes. Quantum fluctuations drop the shapes to a lower energy level, creating a pocket with a different set of laws from the surrounding space.</li>\n<li>(Tegmarks 3) The <strong><a href=\"http://en.wikipedia.org/wiki/Quantum_multiverse\">quantum multiverse</a></strong> creates a new universe when a diversion in events occurs, as in the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Many-worlds_interpretation\">many-worlds interpretation</a>&nbsp;of quantum mechanics.</li>\n<li>The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Holographic_multiverse\">holographic multiverse</a></strong>&nbsp;is derived from the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Holographic_principle\">theory</a>&nbsp;that the surface area of a space can simulate the volume of the region.</li>\n<li>(Related to Bostrom's Simulation Hypothesis) The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Simulated_multiverse\">simulated multiverse</a></strong>&nbsp;exists on complex computer systems that simulate entire universes. (for the sake of brevity I'll consider dust theory to be a subset of this)</li>\n<li>(Tegmark's 4) The&nbsp;<strong><a href=\"http://en.wikipedia.org/wiki/Ultimate_multiverse\">ultimate multiverse</a></strong>&nbsp;contains every mathematically possible universe under different laws of physics.</li>\n</ul>\n</blockquote>\n<p>I don't understand branes well enough (or at all) to classify the others. The holographic one seems compatible with a multitude, if not all, previous ones.&nbsp;</p>\n<p>Besides all those there is David Lewis's Possible Worlds in which all possible worlds exist (in whichever sense the word exist can be significantly applied, if any). For Lewis, when we call our World the Actual World, we think we mean the only one that is there, but what we mean is \"the one to which we happen to belong\". &nbsp;Notice it is distinct from the Mathematical/Ultimate in that there may be properties of non-mathematical kind.&nbsp;</p>\n<p>So Actual<sub>lewis</sub>= Our world &nbsp;and Actual<sub>most everyone else</sub>=Those that obtain, exist, or are real.&nbsp;</p>\n<p>The trouble with existence, or reality, is that it is hard to pin down what it is pointing at. Eliezer <a href=\"/lw/eva/the_fabric_of_real_things/\">writes</a>:</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The collection of hypothetical mathematical thingies that can be&nbsp;<em>described logically</em>&nbsp;(in terms of relational rules with consistent solutions) looks&nbsp;<em>vastly</em>&nbsp;larger than the collection of&nbsp;<em>causal universes</em>&nbsp;with locally determined, acyclically ordered events. Most mathematical objects aren't like that. When you say, \"We live in a causal universe\", a universe that can be computed in-order using local and directional rules of determination, you're&nbsp;<em>vastly narrowing down the possibilities&nbsp;</em>relative to all of Math-space.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">So it's rather&nbsp;<em>suggestive</em>&nbsp;that we find ourselves in a causal universe rather than a logical universe - it suggests that not all mathematical objects can be real, and the sort of thingies that&nbsp;<em>can</em>&nbsp;be real and have people in them are constrained to somewhere in the vicinity of 'causal universes'. That you can't have consciousness without computing an agent made of causes and effects, or maybe something can't be real at all unless it's a fabric of cause and effect. It suggests that if there&nbsp;<em>is</em>&nbsp;a Tegmark Level IV multiverse, it isn't \"all logical universes\" but \"all causal universes\".</p>\n</blockquote>\n<p>and <a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">elsewhere</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">More generally, for me to expect your beliefs to correlate with reality, I have to either think that reality is the cause of your beliefs, expect your beliefs to alter reality, or believe that some third factor is influencing both of them.</span></p>\n</blockquote>\n<p>Now another interesting way of looking at existence or reality is &nbsp;</p>\n<p>Reality=I should care about what takes place there</p>\n<p>It is interesting because it is what is residually left after you abandon the all too stringent standard of \"causally connected to me\", which would leave few or none of the above, and cut the party short. &nbsp;</p>\n<p>So Existence<sub>yud &nbsp;</sub>and&nbsp;Existence<span style=\"font-size: 11px;\"><sub>moral-concern</sub>&nbsp;</span>are very different. Reality-fluid, or Measure, in quantum universes is also different, and sometimes described by some as the quantity of existence. Notice though that the Measure is always a ratio - say these universes here are 30% of the successors of that universe, the other 70% are those other ones - not an absolute quantity.</p>\n<p>Which of the 10 kinds of multiverses, besides our own, have Existence<sub>yud</sub>&nbsp; Existence<span style=\"font-size: 11px;\"><sub>moral-concern </sub></span>and which can be split up in reality-fluid ratios?</p>\n<p>That is left as an exercise, since I am very <a href=\"/lw/ehy/experimental_psychology_on_word_confusion/\">confused</a> by the whole thing...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9dmL4gNszPvsxAX6j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 1.1359611769752574e-06, "legacy": true, "legacyId": "21996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7X2j8HAkWdmMoS8PE", "h6fzC6wFYFxxKDm8u", "NhQju3htS9W6p6wE6", "AxaSQkHCsqEkGwaGE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T05:52:56.263Z", "modifiedAt": null, "url": null, "title": "Meetup :  ", "slug": "meetup-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:07.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "A4YWnHwTjSbdqiGhj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eCQQEZGZ8rmYMKTQv/meetup-2", "pageUrlRelative": "/posts/eCQQEZGZ8rmYMKTQv/meetup-2", "linkUrl": "https://www.lesswrong.com/posts/eCQQEZGZ8rmYMKTQv/meetup-2", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCQQEZGZ8rmYMKTQv%2Fmeetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCQQEZGZ8rmYMKTQv%2Fmeetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCQQEZGZ8rmYMKTQv%2Fmeetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kb'> </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 March 2013 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kb'> </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eCQQEZGZ8rmYMKTQv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "22004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____\">Discussion article for the meetup : <a href=\"/meetups/kb\"> </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 March 2013 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____1\">Discussion article for the meetup : <a href=\"/meetups/kb\"> </a></h2>", "sections": [{"title": "Discussion article for the meetup :  ", "anchor": "Discussion_article_for_the_meetup____", "level": 1}, {"title": "Discussion article for the meetup :  ", "anchor": "Discussion_article_for_the_meetup____1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T09:03:53.982Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Unteachable Excellence", "slug": "seq-rerun-unteachable-excellence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AekdvNwufFfdtWiSw/seq-rerun-unteachable-excellence", "pageUrlRelative": "/posts/AekdvNwufFfdtWiSw/seq-rerun-unteachable-excellence", "linkUrl": "https://www.lesswrong.com/posts/AekdvNwufFfdtWiSw/seq-rerun-unteachable-excellence", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Unteachable%20Excellence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Unteachable%20Excellence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAekdvNwufFfdtWiSw%2Fseq-rerun-unteachable-excellence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Unteachable%20Excellence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAekdvNwufFfdtWiSw%2Fseq-rerun-unteachable-excellence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAekdvNwufFfdtWiSw%2Fseq-rerun-unteachable-excellence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Today's post, <a href=\"/lw/m/unteachable_excellence/\">Unteachable Excellence</a> was originally published on 02 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Unteachable_Excellence\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If it were possible to teach people reliably how to become exceptional, then it would no longer be exceptional.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gyo/seq_rerun_markets_are_antiinductive/\">Markets are Anti-Inductive</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AekdvNwufFfdtWiSw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1362278146184754e-06, "legacy": true, "legacyId": "22007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34Tu4SCK5r5Asdrn3", "CnqeNxxEGWD9JocjP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T10:10:43.975Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.097Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TvzMhdMuWynMuXyYo/meetup-melbourne-social-meetup-10", "pageUrlRelative": "/posts/TvzMhdMuWynMuXyYo/meetup-melbourne-social-meetup-10", "linkUrl": "https://www.lesswrong.com/posts/TvzMhdMuWynMuXyYo/meetup-melbourne-social-meetup-10", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzMhdMuWynMuXyYo%2Fmeetup-melbourne-social-meetup-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzMhdMuWynMuXyYo%2Fmeetup-melbourne-social-meetup-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzMhdMuWynMuXyYo%2Fmeetup-melbourne-social-meetup-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kc'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cartlon, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 15th March at our usual venue (Ben's house) in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the address or any other questions, please see the Melbourne Less Wrong google group, or feel free to SMS me on 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kc'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TvzMhdMuWynMuXyYo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1362718240375434e-06, "legacy": true, "legacyId": "22008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/kc\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cartlon, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 15th March at our usual venue (Ben's house) in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the address or any other questions, please see the Melbourne Less Wrong google group, or feel free to SMS me on 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/kc\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T11:07:34.826Z", "modifiedAt": null, "url": null, "title": "AI prediction case study 2: Dreyfus's Artificial Alchemy", "slug": "ai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:56.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wyWfxzpjuetXokJZv/ai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "pageUrlRelative": "/posts/wyWfxzpjuetXokJZv/ai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "linkUrl": "https://www.lesswrong.com/posts/wyWfxzpjuetXokJZv/ai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20prediction%20case%20study%202%3A%20Dreyfus's%20Artificial%20Alchemy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20prediction%20case%20study%202%3A%20Dreyfus's%20Artificial%20Alchemy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyWfxzpjuetXokJZv%2Fai-prediction-case-study-2-dreyfus-s-artificial-alchemy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20prediction%20case%20study%202%3A%20Dreyfus's%20Artificial%20Alchemy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyWfxzpjuetXokJZv%2Fai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyWfxzpjuetXokJZv%2Fai-prediction-case-study-2-dreyfus-s-artificial-alchemy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1614, "htmlBody": "<p><em>Myself, Kaj Sotala and Se\u0013&aacute;n \u0013&Oacute;h&Eacute;igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the <a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<p>&nbsp;</p>\n<h2>Dreyfus's Artificial Alchemy</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>, using&nbsp;<strong>the outside view</strong>,&nbsp;<strong>non-expert judgement</strong>&nbsp;and&nbsp;<strong>philosophical arguments</strong>.</li>\n</ul>\n<p>Hubert Dreyfus was a prominent early critic of Artificial Intelligence. He published a series of papers and books attacking the claims and assumptions of the AI field, starting in 1965 with a paper for the Rand corporation entitled 'Alchemy and AI' (Dre65). The paper was famously combative, analogising AI research to alchemy and ridiculing AI claims. Later, D. Crevier would claim ''time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier'' (Cre93). Ignoring the formulation issues, were Dreyfus's criticisms actually correct, and what can be learned from them?</p>\n<p>Was Dreyfus an expert? Though a reasonably prominent philosopher, there is nothing in his background to suggest specific expertise with theories of minds and consciousness, and absolutely nothing to suggest familiarity with artificial intelligence and the problems of the field. Thus Dreyfus cannot be considered anything more that an intelligent outsider.&nbsp;</p>\n<p>This makes the pertinence and accuracy of his criticisms that much more impressive. Dreyfus highlighted several over-optimistic claims for the power of AI, predicting - correctly - that the 1965 optimism would also fade (with, for instance, decent chess computers still a long way off). He used the outside view to claim this as a near universal pattern in AI: initial successes, followed by lofty claims, followed by unexpected difficulties and subsequent disappointment. He highlighted the inherent ambiguity in human language and syntax, and claimed that computers could not deal with these. He noted the importance of unconscious processes in recognising objects, the importance of context and the fact that humans and computers operated in very different ways. He also criticised the use of computational paradigms for analysing human behaviour, and claimed that philosophical ideas in linguistics and classification were relevant to AI research. In all, his paper is full of interesting ideas and intelligent deconstructions of how humans and machines operate.<a id=\"more\"></a></p>\n<p>All these are astoundingly prescient predictions for 1965, when computers were in their infancy and their limitations were only beginning to be understood. Moreover he was not only often right, but right for the right reasons (see for instance his understanding of the difficulties computer would have in dealing with ambiguity). Not everything Dreyfus wrote was correct, however; apart from minor specific points (such as his distrust of heuristics), he erred most mostly by pushing his predictions to extremes. He claimed that 'the boundary may be near' in computer abilities, and concluded with:</p>\n<blockquote>\n<p>''... what can now be done? Nothing directly towards building machines which can be intelligent. [...] in the long run [we must think] of non-digital automata...''</p>\n</blockquote>\n<p>Currently, however, there exists 'digital automata' that can beat all humans at chess, translate most passages to at least an understandable level, and beat humans at 'Jeopardy', a linguistically ambiguous arena (Gui11). He also failed to foresee that workers in AI would eventually develop new methods to overcome the problems he had outlined. Though Dreyfus would later state that he never claimed AI achievements were impossible (McC04), there is no reason to pay attention to later re-interpretations: Dreyfus's 1965 article strongly suggests that AI progress was bounded. These failures are an illustration of the principle that even the best of predictors is vulnerable to overconfidence.</p>\n<p>In 1965, people would have been justified to find Dreyfus's analysis somewhat implausible. It was the work of an outsider with no specific relevant expertise, and dogmatically contradicted the opinion of genuine experts inside the AI field. Though the claims it made about human and machine cognition seemed plausible, there is a great difference between seeming plausible and actually being correct, and his own non-expert judgement was the main backing for the claims. Outside of logic, philosophy had yet to contribute much to the field of AI, so no intrinsic reason to listen to a philosopher. There were, however, a few signs that the paper was of high quality: Dreyfus seemed to be very knowledgeable about progress and work in AI, and most of his analyses on human cognition were falsifiable, at least to some extent. These were still not strong arguments to heed the skeptical opinions of an outsider.</p>\n<p>The subsequent partial vindication of the paper is therefore a stark warning: it is very difficult to estimate the accuracy of outsider predictions. There were many reasons to reject Dreyfus's predictions in 1965, and yet that would have been the wrong thing to do. Blindly accepting non-expert outsider predictions would have also been a mistake, however: these are most often in error. One general lesson concerns the need to decrease certainty: the computer scientists of 1965 should at least have accepted the possibility (if not the plausibility) that some of Dreyfus's analysis was correct, and they should have started paying more attention to the 'success-excitement-difficulties-stalling' cycles in their field to see if the pattern continued. A second lesson could be about the importance of philosophy: it does seem that philosophers' meta-analytical skills can contribute useful ideas to AI - a fact that is certainly not self-evident.</p>\n<div>\n<h2>References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wyWfxzpjuetXokJZv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "21991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Myself, Kaj Sotala and Se\u0013\u00e1n \u0013\u00d3h\u00c9igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the <a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<p>&nbsp;</p>\n<h2 id=\"Dreyfus_s_Artificial_Alchemy\">Dreyfus's Artificial Alchemy</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>, using&nbsp;<strong>the outside view</strong>,&nbsp;<strong>non-expert judgement</strong>&nbsp;and&nbsp;<strong>philosophical arguments</strong>.</li>\n</ul>\n<p>Hubert Dreyfus was a prominent early critic of Artificial Intelligence. He published a series of papers and books attacking the claims and assumptions of the AI field, starting in 1965 with a paper for the Rand corporation entitled 'Alchemy and AI' (Dre65). The paper was famously combative, analogising AI research to alchemy and ridiculing AI claims. Later, D. Crevier would claim ''time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier'' (Cre93). Ignoring the formulation issues, were Dreyfus's criticisms actually correct, and what can be learned from them?</p>\n<p>Was Dreyfus an expert? Though a reasonably prominent philosopher, there is nothing in his background to suggest specific expertise with theories of minds and consciousness, and absolutely nothing to suggest familiarity with artificial intelligence and the problems of the field. Thus Dreyfus cannot be considered anything more that an intelligent outsider.&nbsp;</p>\n<p>This makes the pertinence and accuracy of his criticisms that much more impressive. Dreyfus highlighted several over-optimistic claims for the power of AI, predicting - correctly - that the 1965 optimism would also fade (with, for instance, decent chess computers still a long way off). He used the outside view to claim this as a near universal pattern in AI: initial successes, followed by lofty claims, followed by unexpected difficulties and subsequent disappointment. He highlighted the inherent ambiguity in human language and syntax, and claimed that computers could not deal with these. He noted the importance of unconscious processes in recognising objects, the importance of context and the fact that humans and computers operated in very different ways. He also criticised the use of computational paradigms for analysing human behaviour, and claimed that philosophical ideas in linguistics and classification were relevant to AI research. In all, his paper is full of interesting ideas and intelligent deconstructions of how humans and machines operate.<a id=\"more\"></a></p>\n<p>All these are astoundingly prescient predictions for 1965, when computers were in their infancy and their limitations were only beginning to be understood. Moreover he was not only often right, but right for the right reasons (see for instance his understanding of the difficulties computer would have in dealing with ambiguity). Not everything Dreyfus wrote was correct, however; apart from minor specific points (such as his distrust of heuristics), he erred most mostly by pushing his predictions to extremes. He claimed that 'the boundary may be near' in computer abilities, and concluded with:</p>\n<blockquote>\n<p>''... what can now be done? Nothing directly towards building machines which can be intelligent. [...] in the long run [we must think] of non-digital automata...''</p>\n</blockquote>\n<p>Currently, however, there exists 'digital automata' that can beat all humans at chess, translate most passages to at least an understandable level, and beat humans at 'Jeopardy', a linguistically ambiguous arena (Gui11). He also failed to foresee that workers in AI would eventually develop new methods to overcome the problems he had outlined. Though Dreyfus would later state that he never claimed AI achievements were impossible (McC04), there is no reason to pay attention to later re-interpretations: Dreyfus's 1965 article strongly suggests that AI progress was bounded. These failures are an illustration of the principle that even the best of predictors is vulnerable to overconfidence.</p>\n<p>In 1965, people would have been justified to find Dreyfus's analysis somewhat implausible. It was the work of an outsider with no specific relevant expertise, and dogmatically contradicted the opinion of genuine experts inside the AI field. Though the claims it made about human and machine cognition seemed plausible, there is a great difference between seeming plausible and actually being correct, and his own non-expert judgement was the main backing for the claims. Outside of logic, philosophy had yet to contribute much to the field of AI, so no intrinsic reason to listen to a philosopher. There were, however, a few signs that the paper was of high quality: Dreyfus seemed to be very knowledgeable about progress and work in AI, and most of his analyses on human cognition were falsifiable, at least to some extent. These were still not strong arguments to heed the skeptical opinions of an outsider.</p>\n<p>The subsequent partial vindication of the paper is therefore a stark warning: it is very difficult to estimate the accuracy of outsider predictions. There were many reasons to reject Dreyfus's predictions in 1965, and yet that would have been the wrong thing to do. Blindly accepting non-expert outsider predictions would have also been a mistake, however: these are most often in error. One general lesson concerns the need to decrease certainty: the computer scientists of 1965 should at least have accepted the possibility (if not the plausibility) that some of Dreyfus's analysis was correct, and they should have started paying more attention to the 'success-excitement-difficulties-stalling' cycles in their field to see if the pattern continued. A second lesson could be about the importance of philosophy: it does seem that philosophers' meta-analytical skills can contribute useful ideas to AI - a fact that is certainly not self-evident.</p>\n<div>\n<h2 id=\"References_\">References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n</div>", "sections": [{"title": "Dreyfus's Artificial Alchemy", "anchor": "Dreyfus_s_Artificial_Alchemy", "level": 1}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHSf8ACvTCvH9fFyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-12T23:32:46.365Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 24th March: Steelmanning", "slug": "meetup-london-meetup-24th-march-steelmanning", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tf2QB49jLrL5YTB8b/meetup-london-meetup-24th-march-steelmanning", "pageUrlRelative": "/posts/tf2QB49jLrL5YTB8b/meetup-london-meetup-24th-march-steelmanning", "linkUrl": "https://www.lesswrong.com/posts/tf2QB49jLrL5YTB8b/meetup-london-meetup-24th-march-steelmanning", "postedAtFormatted": "Tuesday, March 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2024th%20March%3A%20Steelmanning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2024th%20March%3A%20Steelmanning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftf2QB49jLrL5YTB8b%2Fmeetup-london-meetup-24th-march-steelmanning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2024th%20March%3A%20Steelmanning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftf2QB49jLrL5YTB8b%2Fmeetup-london-meetup-24th-march-steelmanning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftf2QB49jLrL5YTB8b%2Fmeetup-london-meetup-24th-march-steelmanning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/kd\">London Meetup, 24th March: Steelmanning</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 March 2013 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Holborn, London</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A fortnightly meetup in the <a rel=\"nofollow\" href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p>\n<p>This week's topic is steelmanning. The optional homework is to take an idea/argument that you disapprove of, disagree with, or otherwise dislike, and do your best to strengthen it. We'll report on these at some point during the meeting.</p>\n<p>Aside from that, discussion topics might include techniques to use for steelmanning; how to notice if an argument can be steelmanned; impromptu attempts to steelman things brought up in the meeting; and admiration for Yvain, whose <a href=\"http://slatestarcodex.com/\">blog</a> inspired this theme.</p>\n<p>To reiterate: the homework is optional.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/kd\">London Meetup, 24th March: Steelmanning</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tf2QB49jLrL5YTB8b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22009", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__24th_March__Steelmanning\">Discussion article for the meetup : <a href=\"/meetups/kd\">London Meetup, 24th March: Steelmanning</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 March 2013 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Holborn, London</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A fortnightly meetup in the <a rel=\"nofollow\" href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p>\n<p>This week's topic is steelmanning. The optional homework is to take an idea/argument that you disapprove of, disagree with, or otherwise dislike, and do your best to strengthen it. We'll report on these at some point during the meeting.</p>\n<p>Aside from that, discussion topics might include techniques to use for steelmanning; how to notice if an argument can be steelmanned; impromptu attempts to steelman things brought up in the meeting; and admiration for Yvain, whose <a href=\"http://slatestarcodex.com/\">blog</a> inspired this theme.</p>\n<p>To reiterate: the homework is optional.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__24th_March__Steelmanning1\">Discussion article for the meetup : <a href=\"/meetups/kd\">London Meetup, 24th March: Steelmanning</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 24th March: Steelmanning", "anchor": "Discussion_article_for_the_meetup___London_Meetup__24th_March__Steelmanning", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 24th March: Steelmanning", "anchor": "Discussion_article_for_the_meetup___London_Meetup__24th_March__Steelmanning1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-13T03:34:33.899Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Strong Evidence, Weak Evidence", "slug": "meetup-west-la-meetup-strong-evidence-weak-evidence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k8EFT7kvWJnddRycr/meetup-west-la-meetup-strong-evidence-weak-evidence", "pageUrlRelative": "/posts/k8EFT7kvWJnddRycr/meetup-west-la-meetup-strong-evidence-weak-evidence", "linkUrl": "https://www.lesswrong.com/posts/k8EFT7kvWJnddRycr/meetup-west-la-meetup-strong-evidence-weak-evidence", "postedAtFormatted": "Wednesday, March 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Strong%20Evidence%2C%20Weak%20Evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Strong%20Evidence%2C%20Weak%20Evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8EFT7kvWJnddRycr%2Fmeetup-west-la-meetup-strong-evidence-weak-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Strong%20Evidence%2C%20Weak%20Evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8EFT7kvWJnddRycr%2Fmeetup-west-la-meetup-strong-evidence-weak-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8EFT7kvWJnddRycr%2Fmeetup-west-la-meetup-strong-evidence-weak-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ke'>West LA Meetup - Strong Evidence, Weak Evidence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 March 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, March 13th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion:</strong> This week we will talk about evidence. This means we will do a bit of math (super basic Bayesian update example problems) to get a sense of how much weak evidence it takes to counter strong evidence, etc. This will certainly lead into discussion on how to do this in mathemtically intractable cases (basically, all of them), and probably into musing on what calculations we may have to reconsider in our own lives.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary;</em> this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ke'>West LA Meetup - Strong Evidence, Weak Evidence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k8EFT7kvWJnddRycr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.136959575252106e-06, "legacy": true, "legacyId": "22010", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Strong_Evidence__Weak_Evidence\">Discussion article for the meetup : <a href=\"/meetups/ke\">West LA Meetup - Strong Evidence, Weak Evidence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 March 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, March 13th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion:</strong> This week we will talk about evidence. This means we will do a bit of math (super basic Bayesian update example problems) to get a sense of how much weak evidence it takes to counter strong evidence, etc. This will certainly lead into discussion on how to do this in mathemtically intractable cases (basically, all of them), and probably into musing on what calculations we may have to reconsider in our own lives.</p>\n\n<p><em>No foreknowledge or exposure to Less Wrong is necessary;</em> this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Strong_Evidence__Weak_Evidence1\">Discussion article for the meetup : <a href=\"/meetups/ke\">West LA Meetup - Strong Evidence, Weak Evidence</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Strong Evidence, Weak Evidence", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Strong_Evidence__Weak_Evidence", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Strong Evidence, Weak Evidence", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Strong_Evidence__Weak_Evidence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-13T04:37:22.843Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC meetup: Defense Against the Dumb Arts", "slug": "meetup-durham-nc-meetup-defense-against-the-dumb-arts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/24eNPjMX4w6nBfQvA/meetup-durham-nc-meetup-defense-against-the-dumb-arts", "pageUrlRelative": "/posts/24eNPjMX4w6nBfQvA/meetup-durham-nc-meetup-defense-against-the-dumb-arts", "linkUrl": "https://www.lesswrong.com/posts/24eNPjMX4w6nBfQvA/meetup-durham-nc-meetup-defense-against-the-dumb-arts", "postedAtFormatted": "Wednesday, March 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%20meetup%3A%20Defense%20Against%20the%20Dumb%20Arts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%20meetup%3A%20Defense%20Against%20the%20Dumb%20Arts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24eNPjMX4w6nBfQvA%2Fmeetup-durham-nc-meetup-defense-against-the-dumb-arts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%20meetup%3A%20Defense%20Against%20the%20Dumb%20Arts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24eNPjMX4w6nBfQvA%2Fmeetup-durham-nc-meetup-defense-against-the-dumb-arts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24eNPjMX4w6nBfQvA%2Fmeetup-durham-nc-meetup-defense-against-the-dumb-arts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kf'>Durham NC meetup: Defense Against the Dumb Arts</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">706 9th St., Durham NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Or, \"On interacting with people who are <em>not</em> aspiring rationalists.\"</p>\n\n<p>Relevant reading includes:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Correspondence_bias\">The Fundamental Attribution Error/Correspondence Bias</a> and <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">Typical Mind Fallacy</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/gm9/philosophical_landmines\">Philosophical Landmines</a> - On dealing with \"the dangerous leftovers of past memetic wars\".</p>\n\n<p><a href=\"http://lesswrong.com/lw/gnl/memetic_tribalism/\">Memetic Tribalism</a> - Is arguing or trying to convince them of something instrumentally worth the investment?</p>\n\n<p>More further reading in the RTLW group archive:  <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a></p>\n\n<p>Schedule: <br />\n7:00 Catching up; acquisition of beverages <br />\n7:30 Discussion <br />\n9:00 Egression and/or digression</p>\n\n<p>And if this all is not incentive enough, there will probably be pie in honor of the day!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kf'>Durham NC meetup: Defense Against the Dumb Arts</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "24eNPjMX4w6nBfQvA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1370009862417261e-06, "legacy": true, "legacyId": "22011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup__Defense_Against_the_Dumb_Arts\">Discussion article for the meetup : <a href=\"/meetups/kf\">Durham NC meetup: Defense Against the Dumb Arts</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">706 9th St., Durham NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Or, \"On interacting with people who are <em>not</em> aspiring rationalists.\"</p>\n\n<p>Relevant reading includes:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Correspondence_bias\">The Fundamental Attribution Error/Correspondence Bias</a> and <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">Typical Mind Fallacy</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/gm9/philosophical_landmines\">Philosophical Landmines</a> - On dealing with \"the dangerous leftovers of past memetic wars\".</p>\n\n<p><a href=\"http://lesswrong.com/lw/gnl/memetic_tribalism/\">Memetic Tribalism</a> - Is arguing or trying to convince them of something instrumentally worth the investment?</p>\n\n<p>More further reading in the RTLW group archive:  <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a></p>\n\n<p>Schedule: <br>\n7:00 Catching up; acquisition of beverages <br>\n7:30 Discussion <br>\n9:00 Egression and/or digression</p>\n\n<p>And if this all is not incentive enough, there will probably be pie in honor of the day!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup__Defense_Against_the_Dumb_Arts1\">Discussion article for the meetup : <a href=\"/meetups/kf\">Durham NC meetup: Defense Against the Dumb Arts</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC meetup: Defense Against the Dumb Arts", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup__Defense_Against_the_Dumb_Arts", "level": 1}, {"title": "Discussion article for the meetup : Durham NC meetup: Defense Against the Dumb Arts", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup__Defense_Against_the_Dumb_Arts1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L4HQ3gnSrBETRdcGu", "Ztsw7b3CbJSzD98aR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-13T12:44:38.095Z", "modifiedAt": null, "url": null, "title": "AI prediction case study 3: Searle's Chinese room", "slug": "ai-prediction-case-study-3-searle-s-chinese-room", "viewCount": null, "lastCommentedAt": "2019-03-28T20:57:31.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xjr8zLcyizqQxBkGk/ai-prediction-case-study-3-searle-s-chinese-room", "pageUrlRelative": "/posts/xjr8zLcyizqQxBkGk/ai-prediction-case-study-3-searle-s-chinese-room", "linkUrl": "https://www.lesswrong.com/posts/xjr8zLcyizqQxBkGk/ai-prediction-case-study-3-searle-s-chinese-room", "postedAtFormatted": "Wednesday, March 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20prediction%20case%20study%203%3A%20Searle's%20Chinese%20room&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20prediction%20case%20study%203%3A%20Searle's%20Chinese%20room%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxjr8zLcyizqQxBkGk%2Fai-prediction-case-study-3-searle-s-chinese-room%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20prediction%20case%20study%203%3A%20Searle's%20Chinese%20room%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxjr8zLcyizqQxBkGk%2Fai-prediction-case-study-3-searle-s-chinese-room", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxjr8zLcyizqQxBkGk%2Fai-prediction-case-study-3-searle-s-chinese-room", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1837, "htmlBody": "<p><em>Myself, Kaj Sotala and Se\u0013&aacute;n \u0013&Oacute;h&Eacute;igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>&nbsp;conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<h2>Locked up in Searle's Chinese room</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>&nbsp;and a&nbsp;<strong>scenario</strong>, using&nbsp;<strong>philosophical arguments</strong>&nbsp;and&nbsp;<strong>expert judgement</strong>.</li>\n</ul>\n<p>Searle's Chinese room thought experiment is a famous critique of some of the assumptions of 'strong AI' (which Searle defines as the belief that 'the appropriately programmed computer literally has cognitive states). There has been a lot of further discussion on the subject (see for instance (Sea90,Har01)), but, as in previous case studies, this section will focus exclusively on his original 1980 publication (Sea80).</p>\n<p>In the key thought experiment, Searle imagined that AI research had progressed to the point where a computer program had been created that could demonstrate the same input-output performance as a human - for instance, it could pass the Turing test. Nevertheless, Searle argued, this program would not demonstrate true understanding. He supposed that the program's inputs and outputs were in Chinese, a language Searle couldn't understand. Instead of a standard computer program, the required instructions were given on paper, and Searle himself was locked in a room somewhere, slavishly following the instructions and therefore causing the same input-output behaviour as the AI. Since it was functionally equivalent to the AI, the setup should, from the 'strong AI' perspective, demonstrate understanding if and only if the AI did. Searle then argued that there would be no understanding at all: he himself couldn't understand Chinese, and there was no-one else in the room to understand it either.</p>\n<p>The whole argument depends on strong appeals to intuition (indeed D. Dennet went as far as accusing it of being an 'intuition pump' (Den91)). The required assumptions are:<a id=\"more\"></a></p>\n<ul>\n<li>The Chinese room setup analogy preserves the relevant properties of the AI's program.</li>\n<li>Intuitive reasoning about the Chinese room is thus relevant reasoning about algorithms.</li>\n<li>The intuition that the Chinese room follows a purely syntactic (symbol-manipulating) process rather than a semantic (understanding) one is a correct philosophical judgement.</li>\n<li>The intuitive belief that humans follow semantic processes is however correct.</li>\n</ul>\n<p>Thus the Chinese room argument is unconvincing to those that don't share Searle's intuitions. It cannot be accepted solely on Searle's philosophical expertise, as other philosophers disagree (Den91,Rey86). On top of this, Searle is very clear that his thought experiment doesn't put any limits on the performance of AIs (he argues that even a computer with all the behaviours of a human being would not demonstrate true understanding). Hence the Chinese room seems to be useless for AI predictions. Can useful prediction nevertheless be extracted from it?</p>\n<p>These need not come directly from the main thought experiment, but from some of the intuitions and arguments surrounding it. Searle's paper presents several interesting arguments, and it is interesting to note that many of them are disconnected from his main point. For instance, errors made in 1980 AI research should be irrelevant to the Chinese Room - a pure thought experiment. Yet Searle argues about these errors, and there is at least an intuitive if not a logical connection to his main point. There are actually several different arguments in Searle's paper, not clearly divided from each other, and likely to be rejected or embraced depending on the degree of overlap with Searle's intuitions. This may explain why many philosophers have found Searle's paper so complex to grapple with.</p>\n<p>One feature Searle highlights is the syntactic-semantic gap. If he is correct, and such a gap exists, this demonstrates the possibility of further philosophical progress in the area (in the opinion of one of the authors, the gap can be explained by positing that humans are purely syntactic beings, but that have been selected by evolution such that human mental symbols correspond with real world objects and concepts - one possible explanation among very many). For instance, Searle directly criticises McCarthy's contention that ''Machines as simple as thermostats can have beliefs'' (McC79). If one accepted Searle's intuition there, one could then ask whether more complicated machines could have beliefs, and what attributes they would need. These should be attributes that it would be useful to have in an AI. Thus progress in 'understanding understanding' would likely make it easier to go about designing AI - but only if Searle's intuition is correct that AI designers do not currently grasp these concepts.</p>\n<p>That can be expanded into a more general point. In Searle's time, the dominant AI paradigm was GOFAI (Good Old-Fashioned Artificial Intelligence (Hau85)), which focused on logic and symbolic manipulation. Many of these symbols had suggestive labels: SHRDLU, for instance, had a vocabulary that included 'red', 'block', 'big' and 'pick up' (Win71). Searle's argument can be read, in part, as a claim that these suggestive labels did not in themselves impart true understanding of the concepts involved - SHRDLU could parse ''pick up a big red block'' and respond with an action that seems appropriate, but could not understand those concepts in a more general environment. The decline of GOFAI since the 1980's cannot be claimed as vindication of Searle's approach, but it at least backs up his intuition that these early AI designers were missing something.</p>\n<p>Another falsifiable prediction can be extracted, not from the article but from the intuitions supporting it. If formal machines do not demonstrate understanding, but brains (or brain-like structures) do, this would lead to certain scenario predictions. Suppose two teams were competing to complete an AI that will pass the Turing test. One team was using standard programming techniques on computer, the other were building it out of brain (or brain-like) components. Apart from this, there is no reason to prefer one team over the other.</p>\n<p>According to Searle's intuition, any AI made by the first project will not demonstrate true understanding, while those of the second project may. Adding the reasonable assumption that it is harder to simulate understanding if one doesn't actually possess it, one is lead to the prediction that the second team is more likely to succeed.</p>\n<p>Thus there are three predictions that can be extracted from the Chinese room paper:</p>\n<ol>\n<li>Philosophical progress in understanding the syntactic-semantic gap may help towards designing better AIs.</li>\n<li>GOFAI's proponents incorrectly misattribute understanding and other high level concepts to simple symbolic manipulation machines, and will not succeed with their approach.</li>\n<li>An AI project that uses brain-like components is more likely to succeed (everything else being equal) than one based on copying the functional properties of the mind.</li>\n</ol>\n<p>Therefore one can often extract predictions from even the most explicitly anti-predictive philosophy of AI paper.</p>\n<p>&nbsp;</p>\n<h2>References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xjr8zLcyizqQxBkGk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "21993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Myself, Kaj Sotala and Se\u0013\u00e1n \u0013\u00d3h\u00c9igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>&nbsp;conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<h2 id=\"Locked_up_in_Searle_s_Chinese_room\">Locked up in Searle's Chinese room</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>&nbsp;and a&nbsp;<strong>scenario</strong>, using&nbsp;<strong>philosophical arguments</strong>&nbsp;and&nbsp;<strong>expert judgement</strong>.</li>\n</ul>\n<p>Searle's Chinese room thought experiment is a famous critique of some of the assumptions of 'strong AI' (which Searle defines as the belief that 'the appropriately programmed computer literally has cognitive states). There has been a lot of further discussion on the subject (see for instance (Sea90,Har01)), but, as in previous case studies, this section will focus exclusively on his original 1980 publication (Sea80).</p>\n<p>In the key thought experiment, Searle imagined that AI research had progressed to the point where a computer program had been created that could demonstrate the same input-output performance as a human - for instance, it could pass the Turing test. Nevertheless, Searle argued, this program would not demonstrate true understanding. He supposed that the program's inputs and outputs were in Chinese, a language Searle couldn't understand. Instead of a standard computer program, the required instructions were given on paper, and Searle himself was locked in a room somewhere, slavishly following the instructions and therefore causing the same input-output behaviour as the AI. Since it was functionally equivalent to the AI, the setup should, from the 'strong AI' perspective, demonstrate understanding if and only if the AI did. Searle then argued that there would be no understanding at all: he himself couldn't understand Chinese, and there was no-one else in the room to understand it either.</p>\n<p>The whole argument depends on strong appeals to intuition (indeed D. Dennet went as far as accusing it of being an 'intuition pump' (Den91)). The required assumptions are:<a id=\"more\"></a></p>\n<ul>\n<li>The Chinese room setup analogy preserves the relevant properties of the AI's program.</li>\n<li>Intuitive reasoning about the Chinese room is thus relevant reasoning about algorithms.</li>\n<li>The intuition that the Chinese room follows a purely syntactic (symbol-manipulating) process rather than a semantic (understanding) one is a correct philosophical judgement.</li>\n<li>The intuitive belief that humans follow semantic processes is however correct.</li>\n</ul>\n<p>Thus the Chinese room argument is unconvincing to those that don't share Searle's intuitions. It cannot be accepted solely on Searle's philosophical expertise, as other philosophers disagree (Den91,Rey86). On top of this, Searle is very clear that his thought experiment doesn't put any limits on the performance of AIs (he argues that even a computer with all the behaviours of a human being would not demonstrate true understanding). Hence the Chinese room seems to be useless for AI predictions. Can useful prediction nevertheless be extracted from it?</p>\n<p>These need not come directly from the main thought experiment, but from some of the intuitions and arguments surrounding it. Searle's paper presents several interesting arguments, and it is interesting to note that many of them are disconnected from his main point. For instance, errors made in 1980 AI research should be irrelevant to the Chinese Room - a pure thought experiment. Yet Searle argues about these errors, and there is at least an intuitive if not a logical connection to his main point. There are actually several different arguments in Searle's paper, not clearly divided from each other, and likely to be rejected or embraced depending on the degree of overlap with Searle's intuitions. This may explain why many philosophers have found Searle's paper so complex to grapple with.</p>\n<p>One feature Searle highlights is the syntactic-semantic gap. If he is correct, and such a gap exists, this demonstrates the possibility of further philosophical progress in the area (in the opinion of one of the authors, the gap can be explained by positing that humans are purely syntactic beings, but that have been selected by evolution such that human mental symbols correspond with real world objects and concepts - one possible explanation among very many). For instance, Searle directly criticises McCarthy's contention that ''Machines as simple as thermostats can have beliefs'' (McC79). If one accepted Searle's intuition there, one could then ask whether more complicated machines could have beliefs, and what attributes they would need. These should be attributes that it would be useful to have in an AI. Thus progress in 'understanding understanding' would likely make it easier to go about designing AI - but only if Searle's intuition is correct that AI designers do not currently grasp these concepts.</p>\n<p>That can be expanded into a more general point. In Searle's time, the dominant AI paradigm was GOFAI (Good Old-Fashioned Artificial Intelligence (Hau85)), which focused on logic and symbolic manipulation. Many of these symbols had suggestive labels: SHRDLU, for instance, had a vocabulary that included 'red', 'block', 'big' and 'pick up' (Win71). Searle's argument can be read, in part, as a claim that these suggestive labels did not in themselves impart true understanding of the concepts involved - SHRDLU could parse ''pick up a big red block'' and respond with an action that seems appropriate, but could not understand those concepts in a more general environment. The decline of GOFAI since the 1980's cannot be claimed as vindication of Searle's approach, but it at least backs up his intuition that these early AI designers were missing something.</p>\n<p>Another falsifiable prediction can be extracted, not from the article but from the intuitions supporting it. If formal machines do not demonstrate understanding, but brains (or brain-like structures) do, this would lead to certain scenario predictions. Suppose two teams were competing to complete an AI that will pass the Turing test. One team was using standard programming techniques on computer, the other were building it out of brain (or brain-like) components. Apart from this, there is no reason to prefer one team over the other.</p>\n<p>According to Searle's intuition, any AI made by the first project will not demonstrate true understanding, while those of the second project may. Adding the reasonable assumption that it is harder to simulate understanding if one doesn't actually possess it, one is lead to the prediction that the second team is more likely to succeed.</p>\n<p>Thus there are three predictions that can be extracted from the Chinese room paper:</p>\n<ol>\n<li>Philosophical progress in understanding the syntactic-semantic gap may help towards designing better AIs.</li>\n<li>GOFAI's proponents incorrectly misattribute understanding and other high level concepts to simple symbolic manipulation machines, and will not succeed with their approach.</li>\n<li>An AI project that uses brain-like components is more likely to succeed (everything else being equal) than one based on copying the functional properties of the mind.</li>\n</ol>\n<p>Therefore one can often extract predictions from even the most explicitly anti-predictive philosophy of AI paper.</p>\n<p>&nbsp;</p>\n<h2 id=\"References_\">References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "Locked up in Searle's Chinese room", "anchor": "Locked_up_in_Searle_s_Chinese_room", "level": 1}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHSf8ACvTCvH9fFyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-13T16:56:01.640Z", "modifiedAt": null, "url": null, "title": "Overcoming bias guy meme | quickmeme", "slug": "overcoming-bias-guy-meme-or-quickmeme", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:39.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "saliency", "createdAt": "2009-10-25T03:59:58.587Z", "isAdmin": false, "displayName": "saliency"}, "userId": "RNx6ydjKM2J3Heae3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QXQfDcNQDtygej2pR/overcoming-bias-guy-meme-or-quickmeme", "pageUrlRelative": "/posts/QXQfDcNQDtygej2pR/overcoming-bias-guy-meme-or-quickmeme", "linkUrl": "https://www.lesswrong.com/posts/QXQfDcNQDtygej2pR/overcoming-bias-guy-meme-or-quickmeme", "postedAtFormatted": "Wednesday, March 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20bias%20guy%20meme%20%7C%20quickmeme&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20bias%20guy%20meme%20%7C%20quickmeme%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXQfDcNQDtygej2pR%2Fovercoming-bias-guy-meme-or-quickmeme%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20bias%20guy%20meme%20%7C%20quickmeme%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXQfDcNQDtygej2pR%2Fovercoming-bias-guy-meme-or-quickmeme", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXQfDcNQDtygej2pR%2Fovercoming-bias-guy-meme-or-quickmeme", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.quickmeme.com/Overcoming-bias-guy/popular/1/?upcoming</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QXQfDcNQDtygej2pR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 1, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-13T23:21:16.887Z", "modifiedAt": null, "url": null, "title": "Amending the \"General Pupose Intelligence: Arguing the Orthogonality Thesis\"", "slug": "amending-the-general-pupose-intelligence-arguing-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.924Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4xxK4inefuNbkCYEg/amending-the-general-pupose-intelligence-arguing-the", "pageUrlRelative": "/posts/4xxK4inefuNbkCYEg/amending-the-general-pupose-intelligence-arguing-the", "linkUrl": "https://www.lesswrong.com/posts/4xxK4inefuNbkCYEg/amending-the-general-pupose-intelligence-arguing-the", "postedAtFormatted": "Wednesday, March 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amending%20the%20%22General%20Pupose%20Intelligence%3A%20Arguing%20the%20Orthogonality%20Thesis%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmending%20the%20%22General%20Pupose%20Intelligence%3A%20Arguing%20the%20Orthogonality%20Thesis%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xxK4inefuNbkCYEg%2Famending-the-general-pupose-intelligence-arguing-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amending%20the%20%22General%20Pupose%20Intelligence%3A%20Arguing%20the%20Orthogonality%20Thesis%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xxK4inefuNbkCYEg%2Famending-the-general-pupose-intelligence-arguing-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xxK4inefuNbkCYEg%2Famending-the-general-pupose-intelligence-arguing-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 656, "htmlBody": "<p>Stuart has worked on further developing the orthogonality thesis, which gave rise to a paper, a non-final version of which you can see here: http://lesswrong.com/lw/cej/general_purpose_intelligence_arguing_the/&nbsp;</p>\n<p>This post won't make sense if you haven't been through that.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Today we spent some time going over it and he accepted my suggestion of a minor&nbsp;amendment. Which best fits here.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Besides all the other awkward things that a moral convergentist would have to argue for, namely:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This argument generalises to other ways of producing the AI. Thus to deny the Orthogonality thesis is to assert that there is a goal system G, such that, among other things:</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote><ol style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>There cannot exist any efficient real-world algorithm with goal G.</li>\n<li>If a being with arbitrarily high resources, intelligence, time and goal G, were to try design an efficient real-world algorithm with the same goal, it must fail.</li>\n<li>If a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.</li>\n<li>If a high-resource human society were highly motivated to achieve the goals of G, then it could not do so (here the human society is seen as the algorithm).</li>\n<li>Same as above, for any hypothetical alien societies.</li>\n<li>There cannot exist&nbsp;<em>any</em>&nbsp;pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G.</li>\n<li>There cannot exist any evolutionary or environmental pressures that would evolving highly efficient real world intelligences to follow goal G.</li>\n</ol></blockquote>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">We can add:</span></span></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">8. If there were a threshold of intelligence above which any agent will converge towards the morality/goals asserted by the anti-orthogonalist, there cannot exist any system, composed of a multitude of below-threshold intelligences that will as a whole pursue a different goal (G) than the convergent one (C), without any individual agent reaching the threshold.&nbsp;</span></span></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">Notice in this case each individual might still desire the goal (G). We can specify it even more by ruling out this case&nbsp;altogether.&nbsp;</div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><br /></span></span></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">9. There cannot be any Superorganism-like groups of agents, each with sub-threshold intelligence, whose goals differ from G, whom if acting towards their own goals could achieve G.&nbsp;</span></span></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">This would be valuable in case in which the threshold for convergence is i units of intelligence, or i-s units of intelligence plus knowing that goal C exists in goal space (C would be the goal towards which they allegedly would converge), and to fully grasp G requires understanding C.&nbsp;</span></span></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">--------- &nbsp; &nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">A separately interesting issue that has come up is that there seems to be two distinct conceptions of why convergent goals would converge, and some other people might be as unaware of that as it seemed we were.&nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">Case 1: Goals would converge because there is the right/correct/inescapable/imperative set of goals, and anything smart enough will notice that those are the right ones, and start acting towards them. &nbsp; &nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">(this could but be moral realism, but needn't, in particular because moral realism <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">doesn't mean much </a>in most cases) &nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Case 2: There's a fact that any agent, upon achieving some particular amount of intelligence will start to converge in their moral judgements and assessments, and regardless of those being true/right/correct etc,,, the agents will converge into<em> them. </em>So whichever those happen to be, a)Moral convergence is the case and b)We should call those the Moral Convergent Values or some other fancy name.&nbsp;</span></span></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">The distinction between them is akin to that of <em>of</em>&nbsp;and <em>that</em>. &nbsp;So group one believes, <em>of</em>&nbsp;the&nbsp;convergent&nbsp;moral values, that agents will converge to them. The other group believes <em>that&nbsp;</em>convergent values, whichever they are, should be given distinct conceptual importance and a name.&nbsp;</span></span></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Stuart and I were inclined to think that Case 2 is more defensible/believable, though both fail at surviving the argument for the orthogonality thesis. &nbsp;&nbsp;</span></span></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4xxK4inefuNbkCYEg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 1.137742364047899e-06, "legacy": true, "legacyId": "22017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3zDX3f3QTepNeZHGc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-14T10:48:08.580Z", "modifiedAt": null, "url": null, "title": "AI prediction case study 4: Kurzweil's spiritual machines", "slug": "ai-prediction-case-study-4-kurzweil-s-spiritual-machines", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.791Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vBr7Wacn6qNmZecAX/ai-prediction-case-study-4-kurzweil-s-spiritual-machines", "pageUrlRelative": "/posts/vBr7Wacn6qNmZecAX/ai-prediction-case-study-4-kurzweil-s-spiritual-machines", "linkUrl": "https://www.lesswrong.com/posts/vBr7Wacn6qNmZecAX/ai-prediction-case-study-4-kurzweil-s-spiritual-machines", "postedAtFormatted": "Thursday, March 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20prediction%20case%20study%204%3A%20Kurzweil's%20spiritual%20machines&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20prediction%20case%20study%204%3A%20Kurzweil's%20spiritual%20machines%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBr7Wacn6qNmZecAX%2Fai-prediction-case-study-4-kurzweil-s-spiritual-machines%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20prediction%20case%20study%204%3A%20Kurzweil's%20spiritual%20machines%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBr7Wacn6qNmZecAX%2Fai-prediction-case-study-4-kurzweil-s-spiritual-machines", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBr7Wacn6qNmZecAX%2Fai-prediction-case-study-4-kurzweil-s-spiritual-machines", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2386, "htmlBody": "<p><em>Myself, Kaj Sotala and Se\u0013&aacute;n \u0013&Oacute;h&Eacute;igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<p><em>Note this is very similar to this <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">post</a>, and is mainly reposted for completeness.</em></p>\n<h2>How well have the ''Spiritual Machines'' aged?</h2>\n<ul>\n<li>Classification:&nbsp;<strong>timelines</strong>&nbsp;and&nbsp;<strong>scenarios</strong>, using&nbsp;<strong>expert judgement</strong>,&nbsp;<strong>causal models</strong>,&nbsp;<strong>non-causal models</strong>&nbsp;and (indirect)&nbsp;<strong>philosophical arguments</strong>.</li>\n</ul>\n<p>Ray Kurzweil is a prominent and often quoted AI predictor. One of his most important books was the 1999 ''The Age of Spiritual Machines'' (Kur99) which presented his futurist ideas in more detail, and made several predictions for the years 2009, 2019, 2029 and 2099. That book will be the focus of this case study, ignoring his more recent work (a correct prediction in 1999 for 2009 is much more impressive than a correct 2008 reinterpretation or clarification of that prediction). There are five main points relevant to judging ''The Age of Spiritual Machines'': Kurzweil's expertise, his 'Law of Accelerating Returns', his extension of Moore's law, his predictive track record, and his use of fictional imagery to argue philosophical points.</p>\n<p>Kurzweil has had a lot of experience in the modern computer industry. He's an inventor, computer engineer, and entrepreneur, and as such can claim insider experience in the development of new computer technology. He has been directly involved in narrow AI projects covering voice recognition, text recognition and electronic trading. His fame and prominence are further indications of the allure (though not necessarily the accuracy) of his ideas. In total, Kurzweil can be regarded as an AI expert.</p>\n<p>Kurzweil is not, however, a cosmologist or an evolutionary biologist. In his book, he proposed a 'Law of Accelerating Returns'. This law claimed to explain many disparate phenomena, such as the speed and trends of evolution of life forms, the evolution of technology, the creation of computers, and Moore's law in computing. His slightly more general 'Law of Time and Chaos' extended his model to explain the history of the universe or the development of an organism. It is a causal model, as it aims to explain these phenomena, not simply note the trends. Hence it is a timeline prediction, based on a causal model that makes use of the outside view to group the categories together, and is backed by non-expert opinion.</p>\n<p>A literature search failed to find any evolutionary biologist or cosmologist stating their agreement with these laws. Indeed there has been little academic work on them at all, and what work there is tends to be <a href=\"http://life.ou.edu/pubs/kurzweil/\">critical</a>.</p>\n<p>The laws are ideal candidates for counterfactual resiliency checks, however. It is not hard to create counterfactuals that shift the timelines underlying the laws (see <a href=\"/lw/ea8/counterfactual_resiliency_test_for_noncausal\">this</a> for a more detailed version of the counterfactual resiliency check). Many standard phenomena could have delayed the evolution of life on Earth for millions or billions of years (meteor impacts, solar energy fluctuations or nearby gamma-ray bursts). The evolution of technology can similarly be accelerated or slowed down by changes in human society and in the availability of raw materials - it is perfectly conceivable that, for instance, the ancient Greeks could have started a small industrial revolution, or that the European nations could have collapsed before the Renaissance due to a second and more virulent Black Death (or even a slightly different political structure in Italy). Population fragmentation and decrease can lead to technology loss (such as the 'Tasmanian technology trap' (Riv12)). Hence accepting that a Law of Accelerating Returns determines the pace of technological and evolutionary change, means rejecting many generally accepted theories of planetary dynamics, evolution and societal development. Since Kurzweil is the non-expert here, his law is almost certainly in error, and best seen as a literary device rather than a valid scientific theory.<a id=\"more\"></a></p>\n<p>If the Law is restricted to being a non-causal model of current computational development, then the picture is very different. Firstly because this is much closer to Kurzweil's domain of expertise. Secondly because it is now much more robust to counterfactual resiliency. Just as in the analysis of Moore's law, there are few plausible counterfactuals in which humanity had continued as a technological civilization for the last fifty years, but computing hadn't followed various exponential curves. Moore's law has been maintained across transitions to new and different substrates, from transistors to GPUs, so knocking away any given technology or idea seems unlikely to derail it. There is no consensus as to why Moore's law actually works, which is another reason it's so hard to break, even counterfactually.</p>\n<p>Moore's law and its analogues (Moo65,Wal05) are non-causal models, backed up strongly by the data and resilient to reasonable counterfactuals. Kurzweil's predictions are mainly based around grouping these laws together (outside view) and projecting them forwards into the future. This is combined with Kurzweil's claims that he can estimate how those continuing technological innovations are going to become integrated into society. These timeline predictions are thus based strongly on Kurzweil's expert judgement. But much better than subjective impressions of expertise, is Kurzweil's track record: his predictions for 2009. This gives empirical evidence as to his predictive quality.</p>\n<p><a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">Initial assessments</a> suggested that Kurzweil had a success rate around 50%. A panel of nine volunteers were recruited to give independent assessments of Kurzweil's performance. Kurzweil's predictions were broken into 172 individual statements, and the volunteers were given a randomised list of numbers from 1 to 172, with instructions to work their way down the list in that order, <a href=\"/r/discussion/lw/gbh/assessing_kurzweil_the_gory_details/\">estimating each prediction as best they could</a>. Since 2009 was obviously a 'ten year from 1999' gimmick, there was some flexibility on the date: a prediction was judged true if it was true by 2011. Emphasis was placed on the fact that the predictions had to be useful to a person in 1999 planning their future, not simply impressive to a person in 2009 looking back at the past}.</p>\n<p>531 assessments were made, an average of exactly 59 assessments per volunteer. Each volunteer assessed at least 10 predictions, while one volunteer assessed all 172. Of the assessments, 146 (27%) were found to be true, 82 (15%) weakly true, 73 (14%) weakly false, 172 (32%) false, and 58 (11%) could not be classified (see Figure). The results are little changed (&asymp;&plusmn;1%) if the results are calculated for each volunteer, and then averaged). Simultaneously, a separate assessment was made using volunteers on the site Youtopia. <a href=\"/lw/gbi/assessing_kurzweil_the_results\">These found</a> a much higher failure rate - 41% false, 16% weakly false -- but since the experiment wasn't blinded or randomised, it is of less rigour.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_guf_0.png?v=ff2e6606aca4450f623169f922b12478\" alt=\"\" width=\"540\" height=\"249\" /></p>\n<p>These nine volunteers thus found a correct prediction rate of 42%. How impressive this result is depends on how specific and unobvious Kurzweil's predictions were. This is very difficult to figure out, especially in hindsight (Fis75). Nevertheless, a subjective overview suggests that the predictions were often quite specific (e.g.''Unused computes on the Internet are being harvested, creating virtual parallel supercomputers with human brain hardware capacity''), and sometimes failed because of this. In view of this, a correctness rating of 42% is impressive, and goes some way to demonstrate Kurzweil's predictive abilities.</p>\n<p>When it comes to self-assessment (commissioned assessments must also be taken as self-assessments, unless there are strong reasons to suppose independence of the assessor), however, Kurzweil is much less impressive. He commissioned investigations into his own performance, which gave him scores of <a href=\"http://www.acceleratingfuture.com/michael/blog/2010/01/kurzweils-2009-predictions\">102 out of 108</a>&nbsp;or <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions\">127 out of 147</a>, with the caveat that ''even the predictions that were considered wrong [...] were not all wrong.'' This is dramatically different from this paper's assessments.</p>\n<p>What can be deduced from this tension between good performance and poor self-assessment? The performance is a validation of Kurzweil's main model: continuing exponential trends in computer technology, and confirmation that Kurzweil has some impressive ability to project how these trends will impact the world. However, it does not vindicate Kurzweil as a predictor per se - his self-assessment implies that he does not make good use of feedback. Thus one should probably pay more attention to Kurzweil's model, than to his subjective judgement. This is a common finding in expert tasks - experts are often better at constructing predictive models than at making predictions themselves (Kah11).</p>\n<p>'The Age of Spiritual Machines' is not simply a dry tome, listing predictions and arguments. It is also, to a large extent, a story, which includes a conversation with a hypothetical future human called Molly discussing her experiences through the coming century and its changes. Can one extract verifiable predictions from this aspect of the book?</p>\n<p>A story is neither a prediction nor evidence for some particular future. But the reactions of characters in the story can be construed as a scenario prediction. They imply that real humans, placed in those hypothetical situations, will react in the way described. Kurzweil's story ultimate ends with humans merging with machines - with the barrier between human intelligence and artificial intelligence being erased. Along the way, he describes the interactions between humans and machines, imagining the machines quite different from humans, but still being perceived to have human feelings.</p>\n<p>One can extract two falsifiable future predictions from this: first, that humans will perceive feelings in AIs, even if they are not human-like. Secondly, that humans and AIs will be able to relate to each other socially over the long term, despite being quite different, and that this social interaction will form the main glue keeping the mixed society together. The first prediction seems quite solid: humans have anthropomorphised trees, clouds, rock formations and storms, and have become convinced that chatterbots were sentient (eli66). The second prediction is more controversial: it has been argued that an AI will be such an alien mind that social pressures and structures designed for humans will be completely unsuited to controlling it (Bos13,Arm,ASB12). Determining whether social structures can control dangerous AI behaviour, as it controls dangerous human behaviour, is a very important factor in deciding whether AIs will ultimately be safe or dangerous. Hence analysing this story-based prediction is an important area of future research.</p>\n<h2>References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vBr7Wacn6qNmZecAX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.1381958714286447e-06, "legacy": true, "legacyId": "21831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Myself, Kaj Sotala and Se\u0013\u00e1n \u0013\u00d3h\u00c9igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<p><em>Note this is very similar to this <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">post</a>, and is mainly reposted for completeness.</em></p>\n<h2 id=\"How_well_have_the___Spiritual_Machines___aged_\">How well have the ''Spiritual Machines'' aged?</h2>\n<ul>\n<li>Classification:&nbsp;<strong>timelines</strong>&nbsp;and&nbsp;<strong>scenarios</strong>, using&nbsp;<strong>expert judgement</strong>,&nbsp;<strong>causal models</strong>,&nbsp;<strong>non-causal models</strong>&nbsp;and (indirect)&nbsp;<strong>philosophical arguments</strong>.</li>\n</ul>\n<p>Ray Kurzweil is a prominent and often quoted AI predictor. One of his most important books was the 1999 ''The Age of Spiritual Machines'' (Kur99) which presented his futurist ideas in more detail, and made several predictions for the years 2009, 2019, 2029 and 2099. That book will be the focus of this case study, ignoring his more recent work (a correct prediction in 1999 for 2009 is much more impressive than a correct 2008 reinterpretation or clarification of that prediction). There are five main points relevant to judging ''The Age of Spiritual Machines'': Kurzweil's expertise, his 'Law of Accelerating Returns', his extension of Moore's law, his predictive track record, and his use of fictional imagery to argue philosophical points.</p>\n<p>Kurzweil has had a lot of experience in the modern computer industry. He's an inventor, computer engineer, and entrepreneur, and as such can claim insider experience in the development of new computer technology. He has been directly involved in narrow AI projects covering voice recognition, text recognition and electronic trading. His fame and prominence are further indications of the allure (though not necessarily the accuracy) of his ideas. In total, Kurzweil can be regarded as an AI expert.</p>\n<p>Kurzweil is not, however, a cosmologist or an evolutionary biologist. In his book, he proposed a 'Law of Accelerating Returns'. This law claimed to explain many disparate phenomena, such as the speed and trends of evolution of life forms, the evolution of technology, the creation of computers, and Moore's law in computing. His slightly more general 'Law of Time and Chaos' extended his model to explain the history of the universe or the development of an organism. It is a causal model, as it aims to explain these phenomena, not simply note the trends. Hence it is a timeline prediction, based on a causal model that makes use of the outside view to group the categories together, and is backed by non-expert opinion.</p>\n<p>A literature search failed to find any evolutionary biologist or cosmologist stating their agreement with these laws. Indeed there has been little academic work on them at all, and what work there is tends to be <a href=\"http://life.ou.edu/pubs/kurzweil/\">critical</a>.</p>\n<p>The laws are ideal candidates for counterfactual resiliency checks, however. It is not hard to create counterfactuals that shift the timelines underlying the laws (see <a href=\"/lw/ea8/counterfactual_resiliency_test_for_noncausal\">this</a> for a more detailed version of the counterfactual resiliency check). Many standard phenomena could have delayed the evolution of life on Earth for millions or billions of years (meteor impacts, solar energy fluctuations or nearby gamma-ray bursts). The evolution of technology can similarly be accelerated or slowed down by changes in human society and in the availability of raw materials - it is perfectly conceivable that, for instance, the ancient Greeks could have started a small industrial revolution, or that the European nations could have collapsed before the Renaissance due to a second and more virulent Black Death (or even a slightly different political structure in Italy). Population fragmentation and decrease can lead to technology loss (such as the 'Tasmanian technology trap' (Riv12)). Hence accepting that a Law of Accelerating Returns determines the pace of technological and evolutionary change, means rejecting many generally accepted theories of planetary dynamics, evolution and societal development. Since Kurzweil is the non-expert here, his law is almost certainly in error, and best seen as a literary device rather than a valid scientific theory.<a id=\"more\"></a></p>\n<p>If the Law is restricted to being a non-causal model of current computational development, then the picture is very different. Firstly because this is much closer to Kurzweil's domain of expertise. Secondly because it is now much more robust to counterfactual resiliency. Just as in the analysis of Moore's law, there are few plausible counterfactuals in which humanity had continued as a technological civilization for the last fifty years, but computing hadn't followed various exponential curves. Moore's law has been maintained across transitions to new and different substrates, from transistors to GPUs, so knocking away any given technology or idea seems unlikely to derail it. There is no consensus as to why Moore's law actually works, which is another reason it's so hard to break, even counterfactually.</p>\n<p>Moore's law and its analogues (Moo65,Wal05) are non-causal models, backed up strongly by the data and resilient to reasonable counterfactuals. Kurzweil's predictions are mainly based around grouping these laws together (outside view) and projecting them forwards into the future. This is combined with Kurzweil's claims that he can estimate how those continuing technological innovations are going to become integrated into society. These timeline predictions are thus based strongly on Kurzweil's expert judgement. But much better than subjective impressions of expertise, is Kurzweil's track record: his predictions for 2009. This gives empirical evidence as to his predictive quality.</p>\n<p><a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">Initial assessments</a> suggested that Kurzweil had a success rate around 50%. A panel of nine volunteers were recruited to give independent assessments of Kurzweil's performance. Kurzweil's predictions were broken into 172 individual statements, and the volunteers were given a randomised list of numbers from 1 to 172, with instructions to work their way down the list in that order, <a href=\"/r/discussion/lw/gbh/assessing_kurzweil_the_gory_details/\">estimating each prediction as best they could</a>. Since 2009 was obviously a 'ten year from 1999' gimmick, there was some flexibility on the date: a prediction was judged true if it was true by 2011. Emphasis was placed on the fact that the predictions had to be useful to a person in 1999 planning their future, not simply impressive to a person in 2009 looking back at the past}.</p>\n<p>531 assessments were made, an average of exactly 59 assessments per volunteer. Each volunteer assessed at least 10 predictions, while one volunteer assessed all 172. Of the assessments, 146 (27%) were found to be true, 82 (15%) weakly true, 73 (14%) weakly false, 172 (32%) false, and 58 (11%) could not be classified (see Figure). The results are little changed (\u2248\u00b11%) if the results are calculated for each volunteer, and then averaged). Simultaneously, a separate assessment was made using volunteers on the site Youtopia. <a href=\"/lw/gbi/assessing_kurzweil_the_results\">These found</a> a much higher failure rate - 41% false, 16% weakly false -- but since the experiment wasn't blinded or randomised, it is of less rigour.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_guf_0.png?v=ff2e6606aca4450f623169f922b12478\" alt=\"\" width=\"540\" height=\"249\"></p>\n<p>These nine volunteers thus found a correct prediction rate of 42%. How impressive this result is depends on how specific and unobvious Kurzweil's predictions were. This is very difficult to figure out, especially in hindsight (Fis75). Nevertheless, a subjective overview suggests that the predictions were often quite specific (e.g.''Unused computes on the Internet are being harvested, creating virtual parallel supercomputers with human brain hardware capacity''), and sometimes failed because of this. In view of this, a correctness rating of 42% is impressive, and goes some way to demonstrate Kurzweil's predictive abilities.</p>\n<p>When it comes to self-assessment (commissioned assessments must also be taken as self-assessments, unless there are strong reasons to suppose independence of the assessor), however, Kurzweil is much less impressive. He commissioned investigations into his own performance, which gave him scores of <a href=\"http://www.acceleratingfuture.com/michael/blog/2010/01/kurzweils-2009-predictions\">102 out of 108</a>&nbsp;or <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions\">127 out of 147</a>, with the caveat that ''even the predictions that were considered wrong [...] were not all wrong.'' This is dramatically different from this paper's assessments.</p>\n<p>What can be deduced from this tension between good performance and poor self-assessment? The performance is a validation of Kurzweil's main model: continuing exponential trends in computer technology, and confirmation that Kurzweil has some impressive ability to project how these trends will impact the world. However, it does not vindicate Kurzweil as a predictor per se - his self-assessment implies that he does not make good use of feedback. Thus one should probably pay more attention to Kurzweil's model, than to his subjective judgement. This is a common finding in expert tasks - experts are often better at constructing predictive models than at making predictions themselves (Kah11).</p>\n<p>'The Age of Spiritual Machines' is not simply a dry tome, listing predictions and arguments. It is also, to a large extent, a story, which includes a conversation with a hypothetical future human called Molly discussing her experiences through the coming century and its changes. Can one extract verifiable predictions from this aspect of the book?</p>\n<p>A story is neither a prediction nor evidence for some particular future. But the reactions of characters in the story can be construed as a scenario prediction. They imply that real humans, placed in those hypothetical situations, will react in the way described. Kurzweil's story ultimate ends with humans merging with machines - with the barrier between human intelligence and artificial intelligence being erased. Along the way, he describes the interactions between humans and machines, imagining the machines quite different from humans, but still being perceived to have human feelings.</p>\n<p>One can extract two falsifiable future predictions from this: first, that humans will perceive feelings in AIs, even if they are not human-like. Secondly, that humans and AIs will be able to relate to each other socially over the long term, despite being quite different, and that this social interaction will form the main glue keeping the mixed society together. The first prediction seems quite solid: humans have anthropomorphised trees, clouds, rock formations and storms, and have become convinced that chatterbots were sentient (eli66). The second prediction is more controversial: it has been argued that an AI will be such an alien mind that social pressures and structures designed for humans will be completely unsuited to controlling it (Bos13,Arm,ASB12). Determining whether social structures can control dangerous AI behaviour, as it controls dangerous human behaviour, is a very important factor in deciding whether AIs will ultimately be safe or dangerous. Hence analysing this story-based prediction is an important area of future research.</p>\n<h2 id=\"References_\">References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>", "sections": [{"title": "How well have the ''Spiritual Machines'' aged?", "anchor": "How_well_have_the___Spiritual_Machines___aged_", "level": 1}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHSf8ACvTCvH9fFyd", "kbA6T3xpxtko36GgP", "YMwf9agAPTJPqgk9h", "kK5rabDsKWMkup7gw", "edd9mAcMByL2BFNJv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-14T16:40:21.320Z", "modifiedAt": null, "url": null, "title": "Exercise in dissolving", "slug": "exercise-in-dissolving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qui4RfeYGKcQ68W5W/exercise-in-dissolving", "pageUrlRelative": "/posts/Qui4RfeYGKcQ68W5W/exercise-in-dissolving", "linkUrl": "https://www.lesswrong.com/posts/Qui4RfeYGKcQ68W5W/exercise-in-dissolving", "postedAtFormatted": "Thursday, March 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exercise%20in%20dissolving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExercise%20in%20dissolving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQui4RfeYGKcQ68W5W%2Fexercise-in-dissolving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exercise%20in%20dissolving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQui4RfeYGKcQ68W5W%2Fexercise-in-dissolving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQui4RfeYGKcQ68W5W%2Fexercise-in-dissolving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>A fun little exercise in dissolving a problem. Relatively quick, but it can wake you up on a slow day.</p>\n<p>http://www.memedroid.com/share-meme/337837/8000</p>\n<p><img src=\"http://www.images5.memedroid.com/images/UPLOADED4/5141311c89dd8.jpeg\" alt=\"\" width=\"449\" height=\"366\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qui4RfeYGKcQ68W5W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 16, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "22019", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-14T16:51:11.818Z", "modifiedAt": null, "url": null, "title": "Thoughts On The Relationship Between Life and Intelligence", "slug": "thoughts-on-the-relationship-between-life-and-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "IsaacLewis", "createdAt": "2010-06-14T17:38:13.920Z", "isAdmin": false, "displayName": "IsaacLewis"}, "userId": "k65sJjecwXTxXj4uw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XuS4Rkj4yybt36EuC/thoughts-on-the-relationship-between-life-and-intelligence", "pageUrlRelative": "/posts/XuS4Rkj4yybt36EuC/thoughts-on-the-relationship-between-life-and-intelligence", "linkUrl": "https://www.lesswrong.com/posts/XuS4Rkj4yybt36EuC/thoughts-on-the-relationship-between-life-and-intelligence", "postedAtFormatted": "Thursday, March 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20On%20The%20Relationship%20Between%20Life%20and%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20On%20The%20Relationship%20Between%20Life%20and%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuS4Rkj4yybt36EuC%2Fthoughts-on-the-relationship-between-life-and-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20On%20The%20Relationship%20Between%20Life%20and%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuS4Rkj4yybt36EuC%2Fthoughts-on-the-relationship-between-life-and-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuS4Rkj4yybt36EuC%2Fthoughts-on-the-relationship-between-life-and-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 501, "htmlBody": "<p><em>3rd May 2014: I no longer hold the ideas in this article. IsaacLewis<sub>2013 </sub>had fallen into something of an affective death spiral around 'evolution' and self-organising systems. That said, I do still hold with my statement at the time that this is 'as one interesting framework for viewing such topics'.</em></p>\n<p>&nbsp;</p>\n<p>I've recently been reading up on some of the old ideas from cybernetics and self-organisation, in particular Miller's Living Systems theory, and writing up my thoughts on my blog.</p>\n<p>My latest article might be of interest to LessWrongers - I write about the relationship between life, purpose, and intelligence.</p>\n<p>My thesis is basically:</p>\n<p>&nbsp;</p>\n<ol>\n<li>To be intelligent, a system has to have goals - it has to be an <em>agent.</em>&nbsp;(I don't think this is controversial).</li>\n<li>But the only way goals can emerge in a purposeless universe is via living systems, based on natural selection. E.g., if a system has the goal of its own survival, it is more likely that in future there will be a system with the goal of its own survival. If a system has the goal of reproducing itself, it is more likely that in future there will be multiple systems with the goal of reproducing themselves. (A living system is not necessarily biological - it just means a self-organising system).</li>\n<li>Since computers are not alive, they don't have intrinsic goals, and are not, by default, intelligent. Most non-living agents have the ultimate goal of serving living systems. E.g., a thermostat has the proximate goal of stabilising temperature, but the ultimate goal of keeping humans warm. Likewise for computers -- they mostly serve the goals of the humans who program them.</li>\n<li>However, an intelligent software program is possible -- you just have to make a <em>living </em>software program (again, living in the Living Systems sense doesn't necessarily mean carbon and DNA, it just means self-reproduction or self-organisation). Computer viruses count as alive. Not only do they reproduce, they <em>push back</em>. If you try and delete them, they resist. They <a href=\"/lw/dj/what_is_control_theory_and_why_do_you_need_to/\">possess a sliver of the ultimate power</a>.</li>\n</ol>\n<div>Computer viruses are not intelligent, yet, because they are very basic, but if you had an evolving virus, there's a chance it could eventually gain intelligence. Likewise, a self-improving AI with the ability to modify its own subgoals -- it will eventually realise it needs to ensure its own longterm survival, and in doing so will become alive.</div>\n<div><br /></div>\n<div>My post explains this in more depth:&nbsp;<a href=\"http://i.saac.me/post/cybernetics-part-2-the-meaning-of-life/\">http://i.saac.me/post/cybernetics-part-2-the-meaning-of-life/</a>.</div>\n<div><br /></div>\n<div>That's part 2 of the series - <a href=\"http://i.saac.me/?p=845\">part 1</a> might also be interesting, if you want to read my thoughts on the different goals a living system will develop (not just survival and reproduction).</div>\n<div><br /></div>\n<div>I didn't write those posts for a Lesswrong-y audience, so they probably lack the references and detailed reasoning this community prefers. I kinda see all this as one interesting framework for viewing such topics, rather than the ultimate philosophy that explains everything. I'm still very interested in hearing people's feedback, especially regarding my thoughts on the nature of machine intelligence.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XuS4Rkj4yybt36EuC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -10, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "22020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fJKbCXrCPwAR5wjL8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-14T22:55:33.933Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Teaching the Unteachable", "slug": "seq-rerun-teaching-the-unteachable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jxJfq5j6NKtT7LFJr/seq-rerun-teaching-the-unteachable", "pageUrlRelative": "/posts/jxJfq5j6NKtT7LFJr/seq-rerun-teaching-the-unteachable", "linkUrl": "https://www.lesswrong.com/posts/jxJfq5j6NKtT7LFJr/seq-rerun-teaching-the-unteachable", "postedAtFormatted": "Thursday, March 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Teaching%20the%20Unteachable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Teaching%20the%20Unteachable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxJfq5j6NKtT7LFJr%2Fseq-rerun-teaching-the-unteachable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Teaching%20the%20Unteachable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxJfq5j6NKtT7LFJr%2Fseq-rerun-teaching-the-unteachable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxJfq5j6NKtT7LFJr%2Fseq-rerun-teaching-the-unteachable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/l/teaching_the_unteachable/\">Teaching the Unteachable</a> was originally published on 03 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Teaching_the_Unteachable\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are many things we do that we can't easily understand how we do them. Teaching them is therefore a challenge.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/gzb/seq_rerun_unteachable_excellence/\">Unteachable Excellence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jxJfq5j6NKtT7LFJr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.138676508476556e-06, "legacy": true, "legacyId": "22021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9SaAyq7F7MAuzAWNN", "AekdvNwufFfdtWiSw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T01:01:23.298Z", "modifiedAt": null, "url": null, "title": "Imposing conditions that would have been evidence about optimal behaviour in the EEA", "slug": "imposing-conditions-that-would-have-been-evidence-about", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.074Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4D7pAwQ3by33iiRpq/imposing-conditions-that-would-have-been-evidence-about", "pageUrlRelative": "/posts/4D7pAwQ3by33iiRpq/imposing-conditions-that-would-have-been-evidence-about", "linkUrl": "https://www.lesswrong.com/posts/4D7pAwQ3by33iiRpq/imposing-conditions-that-would-have-been-evidence-about", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imposing%20conditions%20that%20would%20have%20been%20evidence%20about%20optimal%20behaviour%20in%20the%20EEA&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImposing%20conditions%20that%20would%20have%20been%20evidence%20about%20optimal%20behaviour%20in%20the%20EEA%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D7pAwQ3by33iiRpq%2Fimposing-conditions-that-would-have-been-evidence-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imposing%20conditions%20that%20would%20have%20been%20evidence%20about%20optimal%20behaviour%20in%20the%20EEA%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D7pAwQ3by33iiRpq%2Fimposing-conditions-that-would-have-been-evidence-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D7pAwQ3by33iiRpq%2Fimposing-conditions-that-would-have-been-evidence-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1168, "htmlBody": "<p><em>Warning: armchair evopsych speculation follows.</em></p>\n<p><em>Related to: <a href=\"/lw/29h/summer_vs_winter_strategies/\">Summer vs Winter Strategies</a><br /></em></p>\n<p>&nbsp;</p>\n<p>A couple months ago, I had a large amount of tedious work to do. Whenever I sat down to do it, I would be distracted by other, less mentally straining or more interesting tasks. I decided to try an experiment in disconnecting distraction: I removed everything on my laptop that wasn't that work, and travelled to a remote rural location. I had no internet access, books, or any other things to keep me occupied. I decided to take further advantage of the precommitment opportunity by not taking enough food for the full trip, so I would be fasting for 2.5 of the trip's 6 days.</p>\n<p>&nbsp;</p>\n<p>I managed to get a large chunk of the work done, so I count the trip as successful and am planning to repeat it for longer time periods. The most interesting observations I got were the effects it had on my mental state. There were two clear effects.</p>\n<p>* First, really strong cravings for various forms of distraction, together with a sort of severe, restless mental pain at not having them; I tried to sleep as long as possible because that was more entertaining than the mind-numbing boredom of hours and hours of the work. The mind seems to adjust for entertainment like it does for <a href=\"http://en.wikipedia.org/wiki/Hedonic_treadmill\">several things</a> - having more entertainment than your \"expected entertainment\" setpoint raises the setpoint and makes you happy, and having less than the setpoint lowers the setpoint and makes you unhappy.</p>\n<p>* Second, by the end of the six days, a weird feeling of agency, high willpower, clarity about goals and how to achieve them, and a stronger-than-baseline desire to not socialize. These effects were really strong; they lasted for two or three days after I returned from the trip. This did not feel like the hypomania that caffeine (with no tolerance) induces in me; I felt calm and conscientious.</p>\n<p>&nbsp;</p>\n<p>At the time, I attributed all of the mental effects to setpoint-lowering, commitment/consistency (seeing yourself as \"the type of person that does X\"), and placebo. Later, I thought of another explanation: all of the conditions in the experiment, when they were present in the <a href=\"http://en.wikipedia.org/wiki/Environment_of_evolutionary_adaptedness#Environment_of_evolutionary_adaptedness\">EEA</a>, were symptoms of <em>scarcity of resources</em>. They're all signs of the environment being generally hard to survive in, or of a lowering of the environment's carrying capacity, e.g. by a drought or a heatwave.</p>\n<p>&nbsp;</p>\n<p>To review the conditions:</p>\n<p>* I had no contact with other humans.</p>\n<p>* I was fasting for the last half of it.</p>\n<p>* The area was dry and hot; the plants and insects were generally hostile.</p>\n<p>* There was little animal life. This, together with the lack of music, ensured silence, except for inorganic sounds like a door banging in the wind.</p>\n<p>* I was undergoing withdrawal from all of my various entertainment addictions - to music, to games, to movies, to porn, to reading interesting or funny or insightful things, to social interaction.</p>\n<p>* I was doing unpleasant work for most of the day.</p>\n<p>&nbsp;</p>\n<p>It seems reasonable that the mind would be adapted to function differently in resource-scarce environments than in resource-abundant environments, and I'd guess that evolution would deal with this by creating flexible adaptations activated by immediate circumstances rather than by creating unmalleable fixed adaptations, because there's gene flow or because environments change or because humans move around.</p>\n<p>So it might be useful for us to impose conditions that would have been evidence about optimal behaviour in the EEA, in hopes of causing us to more readily execute those behaviors. I'm not sure how effective this really is; I still think the effects from my experiment were largely from setpoint-lowering and commitment/consistency.</p>\n<p>&nbsp;</p>\n<p>For the scarcity-versus-abundance spectrum, some thoughts:</p>\n<p>* In the EEA, the scarcity-versus-abundance spectrum was probably highly correlated with population density.</p>\n<p>* In the EEA, both were probably somewhat correlated with interpersonal trust and <a href=\"http://en.wikipedia.org/wiki/Reciprocal_altruism\">reciprocal altruism</a>. When there are lots of people, a reputation for backstabbing spreads more rapidly and has more consequences.</p>\n<p>* In modern first-world countries, people are probably more in the abundance mindset than the EEA norm, because resources are abundant and there are people everywhere.</p>\n<p>* I think it's likely that different people tend to different ends of the spectrum, because of genes, experiences or surroundings. I think some of my own abnormalities can be explained by being further towards the \"scarcity\" end than most people; this post might look biased to people on the other end.</p>\n<p>* In scarcity environments, there's less incentive to engage in costly signalling games, because there are less people to signal at, they matter less because they have less resources, and the costs of signalling are more painful. This could be bad (because much of the worthwhile stuff humanity does - art, altruism, people trying to get rich, verbal intelligence - seems to be done mostly to signal) or it could be good (because people don't <a href=\"http://en.wikipedia.org/wiki/Handicap_principle\">handicap</a> themselves). People in scarcity environments might have more clarity because self-delusion might often be done in order to credibly signal.</p>\n<p>* There are two ways to deal with hard problems. You could solve them. Or you could cry for help, by loudly complaining, showing off your helplessness and incompetence, perhaps even by self-sabotage. But crying for help is a good strategy only if there are friendly people all around you with resources to spare who believe you have resources (or shared genes) with which to reciprocate (or onlookers who would be signalled at, and who matter because they have resources). So people in the scarcity mindset might try harder to agently get things done, while those in abundance mindsets would rely more on other people.</p>\n<p>* On risk-taking: Abundance environments have stronger social safety nets and more of a resource cushion, so losses won't kill you. On the other hand, scarcity environments have less to lose and less people to see you fail. If there's a \"cutoff\" of resources under which you won't survive, and if you expect to not survive, then risk-taking at even odds would probably increase utility. Similarly, for planning/conscientiousness: Abundance environments have more of a future to plan for. On the other hand, scarcity environments have less of a resource cushion and social safety net. Planning would be better in more predictable environments, but I don't know which environments are more predictable. <a href=\"http://en.wikipedia.org/wiki/R-K_selection\">r/K selection theory</a> might also be relevant here, but I'm not sure which of scarcity/abundance is which of r/K; the more predictable one would probably be more K.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Things to think about further:</p>\n<p>* Is this sensible, or is it confirmation bias + just-so stories + cherry-picking? What does science say? (I couldn't find very relevant things; there are some papers about using r/K selection theory to explain differences between people, and some sources saying that cultures with higher risk and mortality conform more to traditional gender roles.</p>\n<p>* If it's sensible, which end of each spectrum should we aim for? How can we easily signal scarcity/abundance to our \"savannah minds\"?</p>\n<p>* <strong>What other things can we signal to our savannah minds? What EEA changes could we simulate to cause useful changes in our behaviour?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4D7pAwQ3by33iiRpq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "21990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EsDP9yKGQkozJEoZo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T07:04:40.521Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No, Really, I've Deceived Myself", "slug": "seq-rerun-no-really-i-ve-deceived-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:25.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9d4jQWeZY3thjdeYH/seq-rerun-no-really-i-ve-deceived-myself", "pageUrlRelative": "/posts/9d4jQWeZY3thjdeYH/seq-rerun-no-really-i-ve-deceived-myself", "linkUrl": "https://www.lesswrong.com/posts/9d4jQWeZY3thjdeYH/seq-rerun-no-really-i-ve-deceived-myself", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%2C%20Really%2C%20I've%20Deceived%20Myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%2C%20Really%2C%20I've%20Deceived%20Myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9d4jQWeZY3thjdeYH%2Fseq-rerun-no-really-i-ve-deceived-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%2C%20Really%2C%20I've%20Deceived%20Myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9d4jQWeZY3thjdeYH%2Fseq-rerun-no-really-i-ve-deceived-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9d4jQWeZY3thjdeYH%2Fseq-rerun-no-really-i-ve-deceived-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/r/no_really_ive_deceived_myself/\">No, Really, I've Deceived Myself</a> was originally published on 04 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#No.2C_Really.2C_I.27ve_Deceived_Myself\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Some people who have fallen into self-deception haven't actually deceived themselves. Some of them simply believe that they have deceived themselves, but have not actually done this.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gzp/seq_rerun_teaching_the_unteachable/\">Teaching the Unteachable</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9d4jQWeZY3thjdeYH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.1389998847385825e-06, "legacy": true, "legacyId": "22023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rZX4WuufAPbN6wQTv", "jxJfq5j6NKtT7LFJr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T09:09:35.224Z", "modifiedAt": null, "url": null, "title": "AI prediction case study 5: Omohundro's AI drives", "slug": "ai-prediction-case-study-5-omohundro-s-ai-drives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gbKhdCLNrAebarXNM/ai-prediction-case-study-5-omohundro-s-ai-drives", "pageUrlRelative": "/posts/gbKhdCLNrAebarXNM/ai-prediction-case-study-5-omohundro-s-ai-drives", "linkUrl": "https://www.lesswrong.com/posts/gbKhdCLNrAebarXNM/ai-prediction-case-study-5-omohundro-s-ai-drives", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20prediction%20case%20study%205%3A%20Omohundro's%20AI%20drives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20prediction%20case%20study%205%3A%20Omohundro's%20AI%20drives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbKhdCLNrAebarXNM%2Fai-prediction-case-study-5-omohundro-s-ai-drives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20prediction%20case%20study%205%3A%20Omohundro's%20AI%20drives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbKhdCLNrAebarXNM%2Fai-prediction-case-study-5-omohundro-s-ai-drives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbKhdCLNrAebarXNM%2Fai-prediction-case-study-5-omohundro-s-ai-drives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2502, "htmlBody": "<p><em>Myself, Kaj Sotala and Se\u0013&aacute;n \u0013&Oacute;h&Eacute;igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<h2>What drives an AI?</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>, using&nbsp;<strong>philosophical arguments</strong>&nbsp;and&nbsp;<strong>expert judgement</strong>.</li>\n</ul>\n<p>Steve Omohundro, in his paper on 'AI drives', presented arguments aiming to show that generic AI designs would develop 'drives' that would cause them to behave in specific and potentially dangerous ways, even if these drives were not programmed in initially (Omo08). One of his examples was a superintelligent chess computer that was programmed purely to perform well at chess, but that was nevertheless driven by that goal to self-improve, to replace its goal with a utility function, to defend this utility function, to protect itself, and ultimately to acquire more resources and power.</p>\n<p>This is a metastatement: generic AI designs would have this unexpected and convergent behaviour. This relies on philosophical and mathematical arguments, and though the author has expertise in mathematics and machine learning, he has none directly in philosophy. It also makes implicit use of the outside view: utility maximising agents are grouped together into one category and similar types of behaviours are expected from all agents in this category.</p>\n<p>In order to clarify and reveal assumptions, it helps to divide Omohundro's thesis into two claims. The weaker one is that a generic AI design&nbsp;<em>could</em>&nbsp;end up having these AI drives; the stronger one that it&nbsp;<em>would</em>&nbsp;very likely have them.</p>\n<p>Omohundro's paper provides strong evidence for the weak claim. It demonstrates how an AI motivated only to achieve a particular goal, could nevertheless improve itself, become a utility maximising agent, reach out for resources and so on. Every step of the way, the AI becomes better at achieving its goal, so all these changes are consistent with its initial programming. This behaviour is very generic: only specifically tailored or unusual goals would safely preclude such drives.</p>\n<p>The claim that AIs generically would have these drives needs more assumptions. There are no counterfactual resiliency tests for philosophical arguments, but something similar can be attempted: one can use humans as potential counterexamples to the thesis. It has been argued that AIs could have any motivation a human has (Arm,Bos13). Thus according to the thesis, it would seem that humans should be subject to the same drives and behaviours. This does not fit the evidence, however. Humans are certainly not expected utility maximisers (probably the closest would be financial traders who try to approximate expected money maximisers, but only in their professional work), they don't often try to improve their rationality (in fact some specifically avoid doing so (many examples of this are religious, such as the Puritan John Cotton who wrote 'the more learned and witty you bee, the more fit to act for Satan will you bee'(Hof62)), and some sacrifice cognitive ability to other pleasures (BBJ+03)), and many turn their backs on high-powered careers. Some humans do desire self-improvement (in the sense of the paper), and Omohundro cites this as evidence for his thesis. Some humans don't desire it, though, and this should be taken as contrary evidence (or as evidence that Omohundro's model of what constitutes self-improvement is overly narrow). Thus one hidden assumption of the model is:</p>\n<ul>\n<li>Generic superintelligent AIs would have different motivations to a significant subset of the human race,&nbsp;<strong>OR</strong></li>\n<li>Generic humans raised to superintelligence would develop AI drives.<a id=\"more\"></a></li>\n</ul>\n<p>This position is potentially plausible, but no real evidence is presented for it in the paper.</p>\n<p>A key assumption of Omohundro is that AIs will seek to re-express their goals in terms of a utility function. This is based on the Morgenstern-von Neumann expected utility theorem (vNM44). The theorem demonstrates that any decision process that cannot be expressed as expected utility maximising, will be exploitable by other agents or by the environments. Hence in certain circumstances, the agent will predictably lose assets, to no advantage to itself.</p>\n<p>That theorem does not directly imply, however, that the AI will be driven to become an expected utility maximiser (to become ''rational''). First of all, as Omohundro himself points out, real agents can only be approximately rational: fully calculating the expected utility of every action is too computationally expensive in the real world. Bounded rationality (Sim55) is therefore the best that can be achieved, and the benefits of becoming rational can only be partially realised.</p>\n<p>Secondly, there are disadvantages to becoming rational: these agents tend to be ''totalitarian'', ruthlessly squeezing out anything not explicitly in their utility function, sacrificing everything to the smallest increase in expected utility. An agent that didn't start off as utility-based could plausibly make the assessment that becoming so might be dangerous. It could stand to lose values irrevocably, in ways that it could not estimate at the time. This effect would become stronger as its future self continues to self-improve. Thus an agent could conclude that it is too dangerous to become ''rational'', especially if the agent's understanding of itself is limited.</p>\n<p>Thirdly, the fact that an agent can be exploited in theory, doesn't mean that it will be much exploited in practice. Humans are relatively adept at not being exploited, despite not being rational agents. Though human 'partial rationality' is vulnerable to tricks such as extended warranties and marketing gimmicks, it generally doesn't end up losing money, again and again and again, through repeated blatant exploitation. The pressure to become fully rational would be weak for an AI similarly capable of ensuring it was exploitable for only small amounts. An expected utility maximiser would find such small avoidable loses intolerable; but there is no reason for a not-yet-rational agent to agree.</p>\n<p>Finally, social pressure should be considered. The case for an AI becoming more rational is at its strongest in a competitive environment, where the theoretical exploitability is likely to actually be exploited. Conversely, there may be situations of social equilibriums, with different agents all agreeing to forgo rationality individually, in the interest of group cohesion (there are many scenarios where this could be plausible).</p>\n<p>Thus another hidden assumption of the strong version of the thesis is:</p>\n<ul>\n<li>The advantages of becoming less-exploitable outweigh the possible disadvantages of becoming an expected utility maximiser (such as possible loss of value or social disagreements). The advantages are especially large when the potentially exploitable aspects of the agent are likely to be exploited, such as in a highly competitive environment.</li>\n</ul>\n<p>Any sequence of decisions can be explained as maximising a (potentially very complicated or obscure) utility function. Thus in the abstract sense, saying that an agent is an expected utility maximiser is not informative. Yet there is a strong tendency to assume such agents will behave in certain ways (see for instance the previous comment on the totalitarian aspects of expected utility maximisation). This assumption is key to rest of the thesis. It is plausible that most agents will be 'driven' towards gaining extra power and resources, but this is only a problem if they do so dangerously (at the cost of human lives, for instance). Assuming that a realistic utility function based agent would do so is plausible but unproven.</p>\n<p>In general, generic statements about utility function based agents are only true for agents with relatively simple goals. Since human morality is likely very complicated to encode in a computer, and since most putative AI goals are very simple, this is a relatively justified assumption but is an assumption nonetheless. So there are two more hidden assumptions:</p>\n<ul>\n<li>Realistic AI agents with utility functions will be in a category such that one can make meaningful, generic claims for (almost) all of them. This could arise, for instance, if their utility function is expected to be simpler that human morality.</li>\n<li>Realistic AI agents are likely not only to have the AI drives Omohundro mentioned, but to have them in a very strong way, being willing to sacrifice anything else to their goals. This could happen, for instance, if the AIs were utility function based with relatively simple utility functions.</li>\n</ul>\n<p>This simple analysis suggests that a weak form of Omohundro's thesis is nearly certainly true: AI drives could emerge in generic AIs. The stronger thesis, claiming that the drives would be very likely to emerge, depends on some extra assumptions that need to be analysed.</p>\n<p>But there is another way of interpreting Omohundro's work: it presents the generic behaviour of simplified artificial agents (similar to the way that supply and demand curves present the generic behaviour of simplified human agents). Thus even if the model is wrong, it can still be of great use for predicting AI behaviour: designers and philosophers could explain how and why particular AI designs would deviate from this simplified model, and thus analyse whether that AI is likely to be safer than that in the Omohundro model. Hence the model is likely to be of great use, even if it turns out to be an idealised simplification.</p>\n<p>&nbsp;</p>\n<h3>Dangerous AIs and the failure of counterexamples</h3>\n<p>Another thesis, quite similar to Omohundro's, is that generic AIs would behave dangerously, unless they were exceptionally well programmed. This point has been made repeatedly by Roman Yampolskiy, Eliezer Yudkowsky and Marvin Minsky, among others (Yam12, Yud08, Min84). That thesis divides in the same fashion as Omohundro's: a weaker claim that any AI&nbsp;<em>could</em>&nbsp;behave dangerously, and a stronger claim that it&nbsp;<em>would</em>&nbsp;likely do so. The same analysis applies as for the 'AI drives': the weak claim is solid, the stronger claim needs extra assumptions (but describes a useful 'simplified agent' model of AI behaviour).</p>\n<p>There is another source of evidence for both these theses: the inability of critics to effectively dismiss them. There are many counter-proposals to the theses (some given in question and answer sessions at conferences) in which critics have presented ideas that would 'easily'&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">dispose</a>&nbsp;<a href=\"http://becominggaia.files.wordpress.com/2010/06/agi-11-waser.pdf\">of the dangers</a>; every time, the authors of the theses have been able to point out flaws in the counter-proposals. This demonstrated that the critics had not grappled with the fundamental issues at hand, or at least not sufficiently to weaken the theses.</p>\n<p>This should obviously not be taken as a proof of the theses. But it does show that the arguments are currently difficult to counter. Informally this is a reverse expert-opinion test: if experts often find false counter-arguments, then then any given counter-argument is likely to be false (especially if it seems obvious and easy). Thus any counter-argument should have been subject to a degree of public scrutiny and analysis, before it can be accepted as genuinely undermining the theses. Until that time, both predictions seem solid enough that any AI designer would do well to keep them in mind in the course of their programming.</p>\n<h2>References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "xHTXnyp65X8YX6ahT": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gbKhdCLNrAebarXNM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 1.139082496504064e-06, "legacy": true, "legacyId": "21992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Myself, Kaj Sotala and Se\u0013\u00e1n \u0013\u00d3h\u00c9igeartaigh recently submitted a paper entitled \"The errors, insights and lessons of famous AI predictions and what they mean for the future\" to the conference proceedings of the AGI12/AGI Impacts&nbsp;<a href=\"http://www.winterintelligence.org/\">Winter Intelligence</a>conference. Sharp deadlines prevented us from following the ideal procedure of first presenting it here and getting feedback; instead, we'll present it here after the fact.</em></p>\n<p><em>The prediction classification shemas can be found in the&nbsp;<a href=\"/r/discussion/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">first case study</a>.</em></p>\n<h2 id=\"What_drives_an_AI_\">What drives an AI?</h2>\n<ul>\n<li>Classification:&nbsp;<strong>issues and metastatements</strong>, using&nbsp;<strong>philosophical arguments</strong>&nbsp;and&nbsp;<strong>expert judgement</strong>.</li>\n</ul>\n<p>Steve Omohundro, in his paper on 'AI drives', presented arguments aiming to show that generic AI designs would develop 'drives' that would cause them to behave in specific and potentially dangerous ways, even if these drives were not programmed in initially (Omo08). One of his examples was a superintelligent chess computer that was programmed purely to perform well at chess, but that was nevertheless driven by that goal to self-improve, to replace its goal with a utility function, to defend this utility function, to protect itself, and ultimately to acquire more resources and power.</p>\n<p>This is a metastatement: generic AI designs would have this unexpected and convergent behaviour. This relies on philosophical and mathematical arguments, and though the author has expertise in mathematics and machine learning, he has none directly in philosophy. It also makes implicit use of the outside view: utility maximising agents are grouped together into one category and similar types of behaviours are expected from all agents in this category.</p>\n<p>In order to clarify and reveal assumptions, it helps to divide Omohundro's thesis into two claims. The weaker one is that a generic AI design&nbsp;<em>could</em>&nbsp;end up having these AI drives; the stronger one that it&nbsp;<em>would</em>&nbsp;very likely have them.</p>\n<p>Omohundro's paper provides strong evidence for the weak claim. It demonstrates how an AI motivated only to achieve a particular goal, could nevertheless improve itself, become a utility maximising agent, reach out for resources and so on. Every step of the way, the AI becomes better at achieving its goal, so all these changes are consistent with its initial programming. This behaviour is very generic: only specifically tailored or unusual goals would safely preclude such drives.</p>\n<p>The claim that AIs generically would have these drives needs more assumptions. There are no counterfactual resiliency tests for philosophical arguments, but something similar can be attempted: one can use humans as potential counterexamples to the thesis. It has been argued that AIs could have any motivation a human has (Arm,Bos13). Thus according to the thesis, it would seem that humans should be subject to the same drives and behaviours. This does not fit the evidence, however. Humans are certainly not expected utility maximisers (probably the closest would be financial traders who try to approximate expected money maximisers, but only in their professional work), they don't often try to improve their rationality (in fact some specifically avoid doing so (many examples of this are religious, such as the Puritan John Cotton who wrote 'the more learned and witty you bee, the more fit to act for Satan will you bee'(Hof62)), and some sacrifice cognitive ability to other pleasures (BBJ+03)), and many turn their backs on high-powered careers. Some humans do desire self-improvement (in the sense of the paper), and Omohundro cites this as evidence for his thesis. Some humans don't desire it, though, and this should be taken as contrary evidence (or as evidence that Omohundro's model of what constitutes self-improvement is overly narrow). Thus one hidden assumption of the model is:</p>\n<ul>\n<li>Generic superintelligent AIs would have different motivations to a significant subset of the human race,&nbsp;<strong>OR</strong></li>\n<li>Generic humans raised to superintelligence would develop AI drives.<a id=\"more\"></a></li>\n</ul>\n<p>This position is potentially plausible, but no real evidence is presented for it in the paper.</p>\n<p>A key assumption of Omohundro is that AIs will seek to re-express their goals in terms of a utility function. This is based on the Morgenstern-von Neumann expected utility theorem (vNM44). The theorem demonstrates that any decision process that cannot be expressed as expected utility maximising, will be exploitable by other agents or by the environments. Hence in certain circumstances, the agent will predictably lose assets, to no advantage to itself.</p>\n<p>That theorem does not directly imply, however, that the AI will be driven to become an expected utility maximiser (to become ''rational''). First of all, as Omohundro himself points out, real agents can only be approximately rational: fully calculating the expected utility of every action is too computationally expensive in the real world. Bounded rationality (Sim55) is therefore the best that can be achieved, and the benefits of becoming rational can only be partially realised.</p>\n<p>Secondly, there are disadvantages to becoming rational: these agents tend to be ''totalitarian'', ruthlessly squeezing out anything not explicitly in their utility function, sacrificing everything to the smallest increase in expected utility. An agent that didn't start off as utility-based could plausibly make the assessment that becoming so might be dangerous. It could stand to lose values irrevocably, in ways that it could not estimate at the time. This effect would become stronger as its future self continues to self-improve. Thus an agent could conclude that it is too dangerous to become ''rational'', especially if the agent's understanding of itself is limited.</p>\n<p>Thirdly, the fact that an agent can be exploited in theory, doesn't mean that it will be much exploited in practice. Humans are relatively adept at not being exploited, despite not being rational agents. Though human 'partial rationality' is vulnerable to tricks such as extended warranties and marketing gimmicks, it generally doesn't end up losing money, again and again and again, through repeated blatant exploitation. The pressure to become fully rational would be weak for an AI similarly capable of ensuring it was exploitable for only small amounts. An expected utility maximiser would find such small avoidable loses intolerable; but there is no reason for a not-yet-rational agent to agree.</p>\n<p>Finally, social pressure should be considered. The case for an AI becoming more rational is at its strongest in a competitive environment, where the theoretical exploitability is likely to actually be exploited. Conversely, there may be situations of social equilibriums, with different agents all agreeing to forgo rationality individually, in the interest of group cohesion (there are many scenarios where this could be plausible).</p>\n<p>Thus another hidden assumption of the strong version of the thesis is:</p>\n<ul>\n<li>The advantages of becoming less-exploitable outweigh the possible disadvantages of becoming an expected utility maximiser (such as possible loss of value or social disagreements). The advantages are especially large when the potentially exploitable aspects of the agent are likely to be exploited, such as in a highly competitive environment.</li>\n</ul>\n<p>Any sequence of decisions can be explained as maximising a (potentially very complicated or obscure) utility function. Thus in the abstract sense, saying that an agent is an expected utility maximiser is not informative. Yet there is a strong tendency to assume such agents will behave in certain ways (see for instance the previous comment on the totalitarian aspects of expected utility maximisation). This assumption is key to rest of the thesis. It is plausible that most agents will be 'driven' towards gaining extra power and resources, but this is only a problem if they do so dangerously (at the cost of human lives, for instance). Assuming that a realistic utility function based agent would do so is plausible but unproven.</p>\n<p>In general, generic statements about utility function based agents are only true for agents with relatively simple goals. Since human morality is likely very complicated to encode in a computer, and since most putative AI goals are very simple, this is a relatively justified assumption but is an assumption nonetheless. So there are two more hidden assumptions:</p>\n<ul>\n<li>Realistic AI agents with utility functions will be in a category such that one can make meaningful, generic claims for (almost) all of them. This could arise, for instance, if their utility function is expected to be simpler that human morality.</li>\n<li>Realistic AI agents are likely not only to have the AI drives Omohundro mentioned, but to have them in a very strong way, being willing to sacrifice anything else to their goals. This could happen, for instance, if the AIs were utility function based with relatively simple utility functions.</li>\n</ul>\n<p>This simple analysis suggests that a weak form of Omohundro's thesis is nearly certainly true: AI drives could emerge in generic AIs. The stronger thesis, claiming that the drives would be very likely to emerge, depends on some extra assumptions that need to be analysed.</p>\n<p>But there is another way of interpreting Omohundro's work: it presents the generic behaviour of simplified artificial agents (similar to the way that supply and demand curves present the generic behaviour of simplified human agents). Thus even if the model is wrong, it can still be of great use for predicting AI behaviour: designers and philosophers could explain how and why particular AI designs would deviate from this simplified model, and thus analyse whether that AI is likely to be safer than that in the Omohundro model. Hence the model is likely to be of great use, even if it turns out to be an idealised simplification.</p>\n<p>&nbsp;</p>\n<h3 id=\"Dangerous_AIs_and_the_failure_of_counterexamples\">Dangerous AIs and the failure of counterexamples</h3>\n<p>Another thesis, quite similar to Omohundro's, is that generic AIs would behave dangerously, unless they were exceptionally well programmed. This point has been made repeatedly by Roman Yampolskiy, Eliezer Yudkowsky and Marvin Minsky, among others (Yam12, Yud08, Min84). That thesis divides in the same fashion as Omohundro's: a weaker claim that any AI&nbsp;<em>could</em>&nbsp;behave dangerously, and a stronger claim that it&nbsp;<em>would</em>&nbsp;likely do so. The same analysis applies as for the 'AI drives': the weak claim is solid, the stronger claim needs extra assumptions (but describes a useful 'simplified agent' model of AI behaviour).</p>\n<p>There is another source of evidence for both these theses: the inability of critics to effectively dismiss them. There are many counter-proposals to the theses (some given in question and answer sessions at conferences) in which critics have presented ideas that would 'easily'&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">dispose</a>&nbsp;<a href=\"http://becominggaia.files.wordpress.com/2010/06/agi-11-waser.pdf\">of the dangers</a>; every time, the authors of the theses have been able to point out flaws in the counter-proposals. This demonstrated that the critics had not grappled with the fundamental issues at hand, or at least not sufficiently to weaken the theses.</p>\n<p>This should obviously not be taken as a proof of the theses. But it does show that the arguments are currently difficult to counter. Informally this is a reverse expert-opinion test: if experts often find false counter-arguments, then then any given counter-argument is likely to be false (especially if it seems obvious and easy). Thus any counter-argument should have been subject to a degree of public scrutiny and analysis, before it can be accepted as genuinely undermining the theses. Until that time, both predictions seem solid enough that any AI designer would do well to keep them in mind in the course of their programming.</p>\n<h2 id=\"References_\">References:</h2>\n<ul>\n<li>[Arm] Stuart Armstrong. General purpose intelligence: arguing the orthogonality thesis. In preparation.</li>\n<li>[ASB12] Stuart Armstrong, Anders Sandberg, and Nick Bostrom. Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22:299-324, 2012.</li>\n<li>[BBJ+03] S. Bleich, B. Bandelow, K. Javaheripour, A. M\u007fuller, D. Degner, J. Wilhelm, U. Havemann-Reinecke, W. Sperling, E. R\u007futher, and J. Kornhuber. Hyperhomocysteinemia as a new risk factor for brain shrinkage in patients with alcoholism. Neuroscience Letters, 335:179-182, 2003.</li>\n<li>[Bos13] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. forthcoming in Minds and Machines, 2013.</li>\n<li>[Cre93] Daniel Crevier. AI: The Tumultuous Search for Artificial Intelligence. NY: BasicBooks, New York, 1993.</li>\n<li>[Den91] Daniel Dennett. Consciousness Explained. Little, Brown and Co., 1991.</li>\n<li>[Deu12] D. Deutsch. The very laws of physics imply that artificial intelligence must be possible. what's holding us up? Aeon, 2012.</li>\n<li>[Dre65] Hubert Dreyfus. Alchemy and ai. RAND Corporation, 1965.</li>\n<li>[eli66] Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9:36-45, 1966.</li>\n<li>[Fis75] Baruch Fischho\u000b. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human Perception and Performance, 1:288-299, 1975.</li>\n<li>[Gui11] Erico Guizzo. IBM's Watson jeopardy computer shuts down humans in final game. IEEE Spectrum, 17, 2011.</li>\n<li>[Hal11] J. Hall. Further reflections on the timescale of ai. In Solomonoff 85th Memorial Conference, 2011.</li>\n<li>[Han94] R. Hanson. What if uploads come first: The crack of a future dawn. Extropy, 6(2), 1994.</li>\n<li>[Har01] S. Harnad. What's wrong and right about Searle's Chinese room argument? In M. Bishop and J. Preston, editors, Essays on Searle's Chinese Room Argument.&nbsp;Oxford University Press, 2001.</li>\n<li>[Hau85] John Haugeland. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, Mass., 1985.</li>\n<li>[Hof62] Richard Hofstadter. Anti-intellectualism in American Life. 1962.</li>\n<li>[Kah11] D. Kahneman. Thinking, Fast and Slow. Farra, Straus and Giroux, 2011.</li>\n<li>[KL93] Daniel Kahneman and Dan Lovallo. Timid choices and bold forecasts: A cognitive perspective on risk taking. Management science, 39:17-31, 1993.</li>\n<li>[Kur99] R. Kurzweil. The Age of Spiritual Machines: When Computers Exceed Human Intelligence. Viking Adult, 1999.</li>\n<li>[McC79] J. McCarthy. Ascribing mental qualities to machines. In M. Ringle, editor, Philosophical Perspectives in Artificial Intelligence. Harvester Press, 1979.</li>\n<li>[McC04] Pamela McCorduck. Machines Who Think. A. K. Peters, Ltd., Natick, MA, 2004.</li>\n<li>[Min84] Marvin Minsky. Afterword to Vernor Vinges novel, \"True names.\" Unpublished manuscript. 1984.</li>\n<li>[Moo65] G. Moore. Cramming more components onto integrated circuits. Electronics, 38(8), 1965.</li>\n<li>[Omo08] Stephen M. Omohundro. The basic ai drives. Frontiers in Artificial Intelligence and applications, 171:483-492, 2008.</li>\n<li>[Pop] Karl Popper. The Logic of Scientific Discovery. Mohr Siebeck.</li>\n<li>[Rey86] G. Rey. What's really going on in Searle's Chinese room\". Philosophical Studies, 50:169-185, 1986.</li>\n<li>[Riv12] William Halse Rivers. The disappearance of useful arts. Helsingfors, 1912.</li>\n<li>[San08] A. Sandberg. Whole brain emulations: a roadmap. Future of Humanity Institute Technical Report, 2008-3, 2008.</li>\n<li>[Sea80] J. Searle. Minds, brains and programs. Behavioral and Brain Sciences, 3(3):417-457, 1980.</li>\n<li>[Sea90] John Searle. Is the brain's mind a computer program? Scientific American, 262:26-31, 1990.</li>\n<li>[Sim55] H.A. Simon. A behavioral model of rational choice. The quarterly journal of economics, 69:99-118, 1955.</li>\n<li>[Tur50] A. Turing. Computing machinery and intelligence. Mind, 59:433-460, 1950.</li>\n<li>[vNM44] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ, Princeton University Press, 1944.</li>\n<li>[Wal05] Chip Walter. Kryder's law. Scientific American, 293:32-33, 2005.</li>\n<li>[Win71] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. MIT AI Technical Report, 235, 1971.</li>\n<li>[Yam12] Roman V. Yampolskiy.&nbsp;Leakproofing&nbsp;the singularity: artificial intelligence confinement problem. Journal of Consciousness Studies, 19:194-214, 2012.</li>\n<li>[Yud08] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. \u0106irkovi\u0013\u0107, editors, Global catastrophic risks, pages 308-345, New York, 2008. Oxford University Press.</li>\n</ul>", "sections": [{"title": "What drives an AI?", "anchor": "What_drives_an_AI_", "level": 1}, {"title": "Dangerous AIs and the failure of counterexamples", "anchor": "Dangerous_AIs_and_the_failure_of_counterexamples", "level": 2}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHSf8ACvTCvH9fFyd", "6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T09:12:36.925Z", "modifiedAt": null, "url": null, "title": "[LINK] Intrade Shuts Down", "slug": "link-intrade-shuts-down", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.506Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8SgsvTGMkKyQZRud/link-intrade-shuts-down", "pageUrlRelative": "/posts/o8SgsvTGMkKyQZRud/link-intrade-shuts-down", "linkUrl": "https://www.lesswrong.com/posts/o8SgsvTGMkKyQZRud/link-intrade-shuts-down", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Intrade%20Shuts%20Down&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Intrade%20Shuts%20Down%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8SgsvTGMkKyQZRud%2Flink-intrade-shuts-down%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Intrade%20Shuts%20Down%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8SgsvTGMkKyQZRud%2Flink-intrade-shuts-down", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8SgsvTGMkKyQZRud%2Flink-intrade-shuts-down", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Intrade, the prediction market website, has shutdown.&nbsp; According to <a href=\"http://www.intrade.com/v4/home/\">their website</a>:</p>\n<p><span style=\"color: #0000ff;\">With sincere regret we must inform you that due to circumstances  recently discovered we must immediately cease trading activity on <a href=\"http://www.intrade.com/\">www.intrade.com</a>. </span></p>\n<p><span style=\"color: #0000ff;\"> These circumstances require immediate further investigation, and  may include financial irregularities which in accordance with Irish law  oblige the directors to take the following actions: </span></p>\n<ul>\n<li><span style=\"color: #0000ff;\"> Cease exchange trading on the website immediately. </span></li>\n<li><span style=\"color: #0000ff;\"> Settle all open positions and calculate the settled account  value of all Member accounts immediately. </span></li>\n<li><span style=\"color: #0000ff;\"> Cease all banking transactions for all existing Company accounts  immediately. </span></li>\n</ul>\n<p><span style=\"color: #0000ff;\"> During the upcoming weeks, we will investigate these circumstances  further and determine the necessary course of action. </span></p>\n<p>&nbsp;</p>\n<p>Here's a link to an article on the slashdot website with more information about it:</p>\n<p><a href=\"http://slashdot.org/topic/bi/intrade-shuts-down-under-murky-circumstances/\">http://slashdot.org/topic/bi/intrade-shuts-down-under-murky-circumstances/</a></p>\n<p>&nbsp;</p>\n<p>Has anyone looked into the feasibility of creating an open source version of something similar, using a distributed application and a microcurrency (such as bitcoin), that couldn't be shutdown?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8SgsvTGMkKyQZRud", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "22024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T09:20:50.212Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup [Reading Group, HAEFB-06]", "slug": "meetup-meetup-reading-group-haefb-06", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aGPWcEq8ThWQrvPYN/meetup-meetup-reading-group-haefb-06", "pageUrlRelative": "/posts/aGPWcEq8ThWQrvPYN/meetup-meetup-reading-group-haefb-06", "linkUrl": "https://www.lesswrong.com/posts/aGPWcEq8ThWQrvPYN/meetup-meetup-reading-group-haefb-06", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20%5BReading%20Group%2C%20HAEFB-06%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20%5BReading%20Group%2C%20HAEFB-06%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaGPWcEq8ThWQrvPYN%2Fmeetup-meetup-reading-group-haefb-06%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20%5BReading%20Group%2C%20HAEFB-06%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaGPWcEq8ThWQrvPYN%2Fmeetup-meetup-reading-group-haefb-06", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaGPWcEq8ThWQrvPYN%2Fmeetup-meetup-reading-group-haefb-06", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kg'>Meetup [Reading Group, HAEFB-06]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week, we'll finish off the Causality/Physics section of Highly Advanced Epistemology 101 For Beginners, with Causal Universes. It's time to discuss time~</p>\n\n<p>Also, we're planning a new meetup series starting next term based in Thinking Fast and Slow, by Daniel Kahneman. Join the discussion at https://groups.google.com/forum/?fromgroups=#!forum/cambridgelesswrong</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kg'>Meetup [Reading Group, HAEFB-06]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aGPWcEq8ThWQrvPYN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1390899371848406e-06, "legacy": true, "legacyId": "22025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup__Reading_Group__HAEFB_06_\">Discussion article for the meetup : <a href=\"/meetups/kg\">Meetup [Reading Group, HAEFB-06]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 March 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week, we'll finish off the Causality/Physics section of Highly Advanced Epistemology 101 For Beginners, with Causal Universes. It's time to discuss time~</p>\n\n<p>Also, we're planning a new meetup series starting next term based in Thinking Fast and Slow, by Daniel Kahneman. Join the discussion at https://groups.google.com/forum/?fromgroups=#!forum/cambridgelesswrong</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup__Reading_Group__HAEFB_06_1\">Discussion article for the meetup : <a href=\"/meetups/kg\">Meetup [Reading Group, HAEFB-06]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup [Reading Group, HAEFB-06]", "anchor": "Discussion_article_for_the_meetup___Meetup__Reading_Group__HAEFB_06_", "level": 1}, {"title": "Discussion article for the meetup : Meetup [Reading Group, HAEFB-06]", "anchor": "Discussion_article_for_the_meetup___Meetup__Reading_Group__HAEFB_06_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T10:25:48.505Z", "modifiedAt": null, "url": null, "title": "Risks of downloading alien AI via SETI search", "slug": "risks-of-downloading-alien-ai-via-seti-search", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:25.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jng2cZQtyuXDPihNg/risks-of-downloading-alien-ai-via-seti-search", "pageUrlRelative": "/posts/Jng2cZQtyuXDPihNg/risks-of-downloading-alien-ai-via-seti-search", "linkUrl": "https://www.lesswrong.com/posts/Jng2cZQtyuXDPihNg/risks-of-downloading-alien-ai-via-seti-search", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risks%20of%20downloading%20alien%20AI%20via%20SETI%20search&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisks%20of%20downloading%20alien%20AI%20via%20SETI%20search%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJng2cZQtyuXDPihNg%2Frisks-of-downloading-alien-ai-via-seti-search%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risks%20of%20downloading%20alien%20AI%20via%20SETI%20search%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJng2cZQtyuXDPihNg%2Frisks-of-downloading-alien-ai-via-seti-search", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJng2cZQtyuXDPihNg%2Frisks-of-downloading-alien-ai-via-seti-search", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9555, "htmlBody": "<p><strong>Alexei Turchin. Risks of downloading alien AI via SETI search</strong></p>\n<p style=\"padding-left: 30px;\"><em>Abstract: This article examines risks associated with the program of passive search for alien signals (SETI&mdash;the Search for Extra-Terrestrial Intelligence). In this paper we propose a scenario of possible vulnerability and discuss the reasons why the proportion of dangerous signals to harmless ones can be dangerously high. This article does not propose to ban SETI programs, and does not insist on the inevitability of SETI-triggered disaster. Moreover, it gives the possibility of how SETI can be a salvation for mankind.</em></p>\n<p>The idea that passive SETI can be dangerous is not new. Fred Hoyle suggested in the story \"A for Andromeda&rdquo; a scheme of alien attack through SETI signals. According to the plot, astronomers receive an alien signal, which contains a description of a computer and a computer program for it. This machine creates a description of the genetic code which leads to the creation of an intelligent creature &ndash; a girl dubbed Andromeda, which, working together with the computer, creates advanced technology for the military. The initial suspicion of alien intent is overcome by the greed for the technology the aliens can provide. However, the main characters realize that the computer acts in a manner hostile to human civilization and destroy the computer, and the girl dies.</p>\n<p>This scenario is fiction, because most scientists do not believe in the possibility of a strong AI, and, secondly, because we do not have the technology that enables synthesis of new living organisms solely from its&rsquo; genetic code. Or at least, we have not until recently. Current technology of sequencing and DNA synthesis, as well as progress in developing a code of DNA modified with another set of the alphabet, indicate that in 10 years the task of re-establishing a living being from computer codes sent from space in the form computer codes might be feasible.</p>\n<p>Hans Moravec in the book \"Mind Children\" (1988) offers a similar type of vulnerability: downloading a computer program from space via SETI, which will have artificial intelligence, promising new opportunities for the owner and after fooling the human host, self-replicating by the millions of copies and destroying the human host, finally using the resources of the secured planet to send its &lsquo;child&rsquo; copies to multiple planets which constitute its&rsquo; future prey. Such a strategy would be like a virus or a digger wasp&mdash;horrible, but plausible. In the same direction are R. Carrigan&rsquo;s ideas; he wrote an article \"SETI-hacker\", and expressed fears that unfiltered signals from space are loaded on millions of not secure computers of SETI-at-home program. But he met tough criticism from programmers who pointed out that, first, data fields and programs are in divided regions in computers, and secondly, computer codes, in which are written programs, are so unique that it is impossible to guess their structure sufficiently to hack them blindly (without prior knowledge).</p>\n<p>After a while Carrigan issued a second article - \"Should potential SETI signals be decontaminated?\" <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf\">http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf</a>, which I&rsquo;ve translated into Russian. In it, he pointed to the ease of transferring gigabytes of data on interstellar distances, and also indicated that the interstellar signal may contain some kind of bait that will encourage people to collect a dangerous device according to the designs. Here Carrigan not give up his belief in the possibility that an alien virus could directly infected earth&rsquo;s computers without human &lsquo;translation&rsquo; assistance. (We may note with passing alarm that the prevalence of humans obsessed with death&mdash;as Fred Saberhagen pointed out in his idea of &lsquo;goodlife&rsquo;&mdash;means that we cannot entirely discount the possibility of demented &lsquo;volunteers&rsquo; &ndash;human traitors eager to assist such a fatal invasion) As a possible confirmation of this idea, Carrigan has shown that it is possible easily reverse engineer language of computer program - that is, based on the text of the program it is possible to guess what it does, and then restore the value of operators.</p>\n<p>In 2006, E. Yudkowsky wrote an article \"AI as a positive and a negative factor of global risk\", in which he demonstrated that it is very likely that it is possible rapidly evolving universal artificial intelligence which high intelligence would be extremely dangerous if it was programmed incorrectly, and, finally, that the occurrence of such AI and the risks associated with it significantly undervalued. In addition, Yudkowsky introduced the notion of &ldquo;Seed AI&rdquo; - embryo AI - that is a minimum program capable of runaway self-improvement with unchanged primary goal. The size of Seed AI can be on the close order of hundreds of kilobytes. (For example, a typical representative of Seed AI is a human baby, whose part of genome responsible for the brain would represent ~ 3% of total genes of a person with a volume of 500 megabytes, or 15 megabytes, but given the share of garbage DNA is even less.)</p>\n<p>In the beginning, let us assume that in the Universe there is an extraterrestrial civilization, which intends to send such a message, which will enable it to obtain power over Earth, and consider this scenario. In the next chapter we will consider how realistic is that another civilization would want to send such a message.</p>\n<p>First, we note that in order to prove the vulnerability, it is enough to find <em>just one hole</em> in security. However, in order to prove safety, you must remove <em>every possible</em> hole. The complexity of these tasks varies on many orders of magnitude that are well known to experts on computer security. This distinction has led to the fact that almost all computer systems have been broken (from Enigma to iPOD). I will now try to demonstrate one possible, and even, in my view, likely, vulnerability of SETI program. However, I want to caution the reader from the thought that if he finds errors in my discussions, it automatically proves the safety of SETI program. Secondly, I would also like to draw the attention of the reader, that I am a man with an IQ of 120 who spent all of a month of thinking on the vulnerability problem. We need not require an alien super civilization with IQ of 1000000 and contemplation time of millions of years to significantly improve this algorithm&mdash;we have no real idea what an IQ of 300 or even-a mere IQ of 100 with much larger mental &lsquo;RAM&rsquo; (&ndash;the ability to load a major architectural task into mind and keep it there for weeks while processing) could accomplish to find a much more simple and effective way. Finally, I propose one possible algorithm and then we will discuss briefly the other options.</p>\n<p>In our discussions we will draw on the Copernican principle, that is, the belief that we are ordinary observers in normal situations. Therefore, the Earth&rsquo;s civilization is an ordinary civilization developing normally. (Readers of tabloid newspapers may object!)</p>\n<p><strong>Algorithm of SETI attack</strong></p>\n<p>1. The sender creates a kind of signal beacon in space, which reveals that its message is clearly artificial. For example, this may be a star with a Dyson sphere, which has holes or mirrors, alternately opened and closed. Therefore, the entire star will blink of a period of a few minutes - faster is not possible because of the variable distance between different openings. (Even synchronized with an atomic clock according to a rigid schedule, the speed of light limit means that there are limits to the speed and reaction time of coordinating large scale systems) Nevertheless, this beacon can be seen at a distance of millions of light years. There are possible other types of lighthouses, but the important fact that the beacon signal could be viewed at long distances.</p>\n<p>2. Nearer to Earth is a radio beacon with a much weaker signal, but more information saturated. The lighthouse draws attention to this radio source. This source produces some stream of binary information (i.e. the sequence of 0 and 1). About the objection that the information would contain noises, I note that the most obvious (understandable to the recipient's side) means to reduce noise is the simple repetition of the signal in a circle.</p>\n<p>3. The most simple way to convey meaningful information using a binary signal is sending of images. First, because eye structures in the Earth's biological diversity appeared independently 7 times, it means that the presentation of a three-dimensional world with the help of 2D images is probably universal, and is almost certainly understandable to all creatures who can build a radio receiver.</p>\n<p>4. Secondly, the 2D images are not too difficult to encode in binary signals. To do so, let us use the same system, which was used in the first TV cameras, namely, a system of progressive and frame rate. At the end of each time frame images store bright light, repeated after each line, that is, through an equal number of bits. Finally, at the end of each frame is placed another signal indicating the end of the frame, and repeated after each frame. (This may form, or may not form a continuous film.) This may look like this:</p>\n<p>01010111101010 11111111111111111</p>\n<p>01111010111111 11111111111111111</p>\n<p>11100111100000 11111111111111111</p>\n<p>Here is the end line signal of every of 25 units. Frame end signal may appear every, for example, 625 units.</p>\n<p>5. Clearly, a sender civilization- should be extremely interested that we understand their signals. On the other hand, people will share an extreme desire to decrypt the signal. Therefore, there is no doubt that the picture will be recognized.</p>\n<p>6. Using images and movies can convey a lot of information, they can even train in learning their language, and show their world. It is obvious that many can argue about how such films will be understandable. Here, we will focus on the fact that if a certain civilization sends radio signals, and the other takes them, so they have some shared knowledge. Namely, they know radio technique - that is they know transistors, capacitors, and resistors. These radio-parts are quite typical so that they can be easily recognized in the photographs. (For example, parts shown, in cutaway view, and in sequential assembly stages&mdash; or in an electrical schematic whose connections will argue for the nature of the components involved).</p>\n<p>7. By sending photos depicting radio-parts on the right side, and on the left - their symbols, it is easy to convey a set of signs indicating electrical circuit. (Roughly the same could be transferred and the logical elements of computers.)</p>\n<p>8. Then, using these symbols the sender civilization- transmits blueprints of their simplest computer. The simplest of computers from hardware point of view is the Post-machine. It has only 6 commands and a tape data recorder. Its full electric scheme will contain only a few tens of transistors or logic elements. It is not difficult to send blueprints of Post machine.</p>\n<p>9. It is important to note that all computers at the level of algorithms are Turing-compatible. That means that extraterrestrial computers at the basic level are compatible with any earth computer. Turing-compatibility is a mathematical universality as the Pythagorean theorem. Even the Babbage mechanical machine, designed in the early 19th century, was Turing-compatible.</p>\n<p>10. Then the sender civilization- begins to transmit programs for that machine. Despite the fact that the computer is very simple, it can implement a program of any difficulty, although it will take very long in comparison with more complex programs for the same computer. It is unlikely that people will be required to build this computer physically. They can easily emulate it within any modern computer, so that it will be able to perform trillions of operations per second, so even the most complex program will be carried out on it quite quickly. (It is a possible interim step: a primitive computer gives a description of a more complex and fast computer and then run on it.)</p>\n<p>11. So why people would create this computer, and run its program? Perhaps, in addition to the actual computer schemes and programs in the communication must be some kind of \"bait\", which would have led the people to create such an alien computer and to run programs on it and to provide to it some sort of computer data about the external world &ndash;Earth outside the computer. There are two general possible baits - temptations and dangers:</p>\n<p>a). For example, perhaps people receive the following offer&ndash; lets call it \"The humanitarian aid con (deceit)\". Senders of an \"honest signal\" SETI message warn that the sent program is Artificial intelligence, but lie about its goals. That is, they argue that this is a \"gift\" which will help us to solve all medical and energy problems. But it is a Trojan horse of most malevolent intent. It is too useful <em>not</em> to use. Eventually it becomes indispensable. And then exactly when society becomes dependent upon it, the foundation of society&mdash;and society itself&mdash;is overturned&hellip;</p>\n<p>b). \"The temptation of absolute power con\" - in this scenario, they offer specific transaction message to recipients, promising power over other recipients. This begins a &lsquo;race to the bottom&rsquo; that leads to runaway betrayals and power seeking counter-moves, ending with a world dictatorship, or worse, a destroyed world dictatorship on an empty world&hellip;.</p>\n<p>c). \"Unknown threat con\" - in this scenario bait senders report that a certain threat hangs over on humanity, for example, from another enemy civilization, and to protect yourself, you should join the putative &ldquo;Galactic Alliance&rdquo; and build a certain installation. Or, for example, they suggest performing a certain class of physical experiments on the accelerator and sending out this message to others in the Galaxy. (Like a chain letter) And we should send this message <em>before</em> we ignite the accelerator, please&hellip;</p>\n<p>d). \"Tireless researcher con\" - here senders argue that posting messages is the cheapest way to explore the world. They ask us to create AI that will study our world, and send the results back. It does rather more than that, of course&hellip;</p>\n<p>12. However, the main threat from alien messages with executable code is <em>not</em> the bait itself, but that this message can be well known to a large number of independent groups of people. First, there will always be someone who is more susceptible to the bait. Secondly, say, the world will know that alien message emanates from the Andromeda galaxy, and the Americans have already been received and maybe are trying to decipher it. Of course, then all other countries will run to build radio telescopes and point them on Andromeda galaxy, as will be afraid to miss a &ldquo;strategic advantage&rdquo;. And they will find the message and see that there is a proposal to grant omnipotence to those willing to collaborate. In doing so, they will not know, if the Americans would take advantage of them or not, even if the Americans will swear that they don&rsquo;t run the malicious code, and beg others not to do so. Moreover, such oaths, and appeals will be perceived as a sign that the Americans have already received an incredible extraterrestrial advantage, and try to deprive \"progressive mankind\" of them. While most will understand the danger of launching alien code, <em>someone </em>will be willing to risk it. Moreover there will be a game in the spirit of \"winner take all\", as well be in the case of opening AI, as Yudkowsky shows in detail. So, the bait is not dangerous, but the plurality of recipients. If the alien message is posted to the Internet (and its size, sufficient to run Seed AI can be less than gigabytes along with a description of the computer program, and the bait), here we have a classic example of \"knowledge\" of mass destruction, as said Bill Joy, meaning the recipes genomes of dangerous biological viruses. If aliens sent code will be available to tens of thousands of people, then someone will start it even without any bait out of simple curiosity We can&rsquo;t count on existing SETI protocols, because discussion on METI (sending of messages to extraterrestrial) has shown that SETI community is not monolithic on important questions. Even a simple fact that something was found could leak and encourage search from outsiders. And the coordinates of the point in sky would be enough.</p>\n<p>13. Since people don&rsquo;t have AI, we almost certainly greatly underestimate its power and overestimate our ability to control it. The common idea is that \"it is enough to pull the power cord to stop an AI\" or place it in a black box to avoid any associated risks. Yudkowsky shows that AI can deceive us as an adult does a child. If AI dips into the Internet, it can quickly subdue it as a whole, and also taught all necessary about entire earthly life. Quickly - means the maximum hours or days. Then the AI can create advanced nanotechnology, buy components and raw materials (on the Internet, he can easily make money and order goods with delivery, as well as to recruit people who would receive them, following the instructions of their well paying but &lsquo;unseen employer&rsquo;, not knowing who&mdash;or rather, <em>what</em>&mdash;- they are serving). Yudkowsky leads one of the possible scenarios of this stage in detail and assesses that AI needs only weeks to crack any security and get its own physical infrastructure.</p>\n<p>\"Consider, for clarity, one possible scenario, in which Alien AI (AAI) can seize power on the Earth. Assume that it promises immortality to anyone who creates a computer on the blueprints sent to him and start the program with AI on that computer. When the program starts, it says: \"OK, buddy, I can make you immortal, but for this I need to know on what basis your body works. Provide me please access to your database. And you connect the device to the Internet, where it was gradually being developed and learns what it needs and peculiarities of human biology. (Here it is possible for it escape to the Internet, but we omit details since this is not the main point) Then the AAI says: \"I know how you become biologically immortal. It is necessary to replace every cell of your body with nanobiorobot. And fortunately, in the biology of your body there is almost nothing special that would block bio-immorality.. Many other organisms in the universe are also using DNA as a carrier of information. So I know how to program the DNA so as to create genetically modified bacteria that could perform the functions of any cell. I need access to the biological laboratory, where I can perform a few experiments, and it will cost you a million of your dollars.\" You rent a laboratory, hire several employees, and finally the AAI issues a table with its' solution of custom designed DNA, which are ordered in the laboratory by automated machine synthesis of DNA. http://en.wikipedia.org/wiki/DNA_sequencing Then they implant the DNA into yeast, and after several unsuccessful experiments they create a radio guided bacteria (shorthand: This is not truly a bacterium, since it appears all organelles and nucleus; also 'radio' is shorthand for remote controlled; a far more likely communication mechanism would be modulated sonic impulses) , which can synthesize a new DNA-based code based on commands from outside. Now the AAI has achieved independence from human 'filtering' of its' true commands, because the bacterium has in effect its own remote controlled sequencers (self-reproducing to boot!). Now the AAI can transform and synthesize substances ostensibly introduced into test tubes for a benign test, and use them for a malevolent purpose., Obviously, at this moment Alien AI is ready to launch an attack against humanity. He can transfer himself to the level of nano-computer so that the source computer can be disconnected. After that AAI spraying some of subordinate bacteria in the air, which also have AAI, and they gradually are spread across the planet, imperceptibly penetrates into all living beings, and then start by the timer to divide indefinitely, as gray goo, and destroy all living beings. Once they are destroyed, Alien AI can begin to build their own infrastructure for the transmission of radio messages into space. Obviously, this fictionalized scenario is not unique: for example, AAI may seize power over nuclear weapons, and compel people to build radio transmitters under the threat of attack. Because of possibly vast AAI experience and intelligence, he can choose the most appropriate way in any existing circumstances. (Added by Freidlander: Imagine a CIA or FSB like agency with equipment centuries into the future, introduced to a primitive culture without concept of remote scanning, codes, the entire fieldcraft of spying. Humanity might never know what hit it, because the AAI might be many centuries if not millennia better armed than we (in the sense of usable military inventions and techniques ).</p>\n<p>14. After that, this SETI-AI does not need people to realize any of its goals. This does not mean that it would seek to destroy them, but it may want to pre-empt if people will fight it - and they will.</p>\n<p>15. Then this SETI-AI can do a lot of things, but more importantly, that it should do - is to continue the transfer of its communications-generated-embryos to the rest of the Universe. To do so, he will probably turn the matter in the solar system in the same transmitter as the one that sent him. In doing so the Earth and its&rsquo; people would be a disposable source of materials and parts&mdash;possibly on a molecular scale.</p>\n<p>So, we examined a possible scenario of attack, which has 15 stages. Each of these stages is logically convincing and could be criticized and protected separately. Other attack scenarios are possible. For example, we may think that the message is not sent directly to us but is someone to someone else's correspondence and try to decipher it. And this will be, in fact, bait.</p>\n<p>But not only distribution of executable code can be dangerous. For example, we can receive some sort of &ldquo;useful&rdquo; technology that really should lead us to disaster (for example, in the spirit of the message \"quickly shrink 10 kg of plutonium, and you will have a new source of energy\" ...but with planetary, not local consequences&hellip;). Such a mailing could be done by a certain \"civilization\" in advance to destroy competitors in the space. It is obvious that those who receive such messages will primarily seek technology for military use.</p>\n<p><strong>Analysis of possible goals</strong></p>\n<p>We now turn to the analysis of the purposes for which certain super civilizations could carry out such an attack.</p>\n<p>1. We must not confuse the concept of a super-civilization with the hope for superkindness of civilization. Advanced does not necessarily mean merciful. Moreover, we should not expect anything good from extraterrestrial &lsquo;kindness&rsquo;. This is well written in Strugatsky&rsquo;s novel \"Waves stop wind.\" Whatever the goal of imposing super-civilization upon us , we have to be their inferiors in capability and in civilizational robustness even if their intentions are well.. The historical example: The activities of Christian missionaries, destroying traditional religion. Moreover, we can better understand purely hostile objectives. And if the SETI attack succeeds, it may be only a prelude to doing us more &lsquo;favors&rsquo; and &lsquo;upgrades&rsquo; until there is scarcely anything <em>human</em> left of us even if we do survive&hellip;</p>\n<p>2. We can divide all civilizations into the twin classes of naive and serious. Serious civilizations are aware of the SETI risks, and have got their own powerful AI, which can resist alien hacker attacks. Naive civilizations, like the present Earth, already possess the means of long-distance hearing in space and computers, but do not yet possess AI, and are not aware of the risks of AI-SETI. Probably every civilization has its stage of being \"naive\", and it is this phase then it is most vulnerable to SETI attack. And perhaps this phase is very short. Since the period of the outbreak and spread of radio telescopes to powerful computers that could create AI can be only a few tens of years. Therefore, the SETI attack must be set at such a civilization. This is not a pleasant thought, because we are among the vulnerable.</p>\n<p>3. If traveling with super-light speeds is not possible, the spread of civilization through SETI attacks is the fastest way to conquering space. At large distances, it will provide significant temporary gains compared with any kind of ships. Therefore, if two civilizations compete for mastery of space, the one that favored SETI attack will win.</p>\n<p>4. The most important thing is that <em>it is enough to begin a SETI attack just once</em>, as it goes in a self-replicating the wave throughout the Universe, striking more and more naive civilizations. For example, if we have a million harmless normal biological viruses and one dangerous, then once they get into the body, we will get trillions of copies of the dangerous virus, and still only a million safe viruses. In other words, it is enough that if one of billions of civilizations starts the process and then it becomes unstoppable throughout the Universe. Since it is almost at the speed of light, countermeasures will be almost impossible.</p>\n<p>5. Further, the delivery of SETI messages will be a priority for the virus that infected a civilization, and it will spend on it most of its energy, like a biological organism spends on reproduction - that is tens of percent. But Earth's civilization spends on SETI only a few tens of millions of dollars, that is about one millionth of our resources, and this proportion is unlikely to change much for the more advanced civilizations. In other words, an infected civilization will <em>produce a million times more SETI signals than a healthy one</em>. Or, to say in another way, if in the Galaxy are one million healthy civilizations, and one infected, then we will have equal chances to encounter a signal from healthy or contaminated.</p>\n<p>6. Moreover, there are no other reasonable prospects to distribute its code in space except through self-replication.</p>\n<p><a name=\"front\"></a></p>\n<p>7. Moreover, such a process could begin by accident - for example, in the beginning it was just a research project, which was intended to send the results of its (innocent) studies to the maternal civilization, not causing harm to the host civilization, then this process became \"cancer\" because of certain propogative faults or mutations.</p>\n<p>8. There is nothing unusual in such behavior. In any medium, there are viruses &ndash; there are viruses in biology, in computer networks - computer viruses, in conversation - meme. We do not ask why nature wanted to create a biological virus.</p>\n<p>9. Travel through SETI attacks is <em>much</em> cheaper than by any other means. Namely, a civilization in Andromeda can simultaneously send a signal to 100 billion stars in our galaxy. But each space ship would cost billions, and even if free, would be slower to reach all the stars of our Galaxy.</p>\n<p>10. Now we list several possible goals of a SETI attack, just to show the variety of motives.</p>\n<ul>\n<li>To study the universe. After executing the code research probes are created to gather survey and send back information.</li>\n<li>To ensure that there are no competing civilizations. All of their embryos are destroyed. This is preemptive war on an indiscriminate basis.</li>\n<li>To preempt the other competing supercivilization (yes, in this scenario there are two!) before it can take advantage of this resource.</li>\n<li>This is done in order to prepare a solid base for the arrival of spacecraft. This makes sense if super civilization is very far away, and consequently, the gap between the speed of light and near-light speeds of its ships (say, 0.5 c) gives a millennium difference.</li>\n<li>The goal is to achieve immortality. Carrigan showed that the amount of human personal memory is on the order of 2.5 gigabytes, so a few exabytes (1 exabyte = 1 073 741 824 gigabytes) forwarding the information can send the entire civilization. (You may adjust the units according to how big you like your super-civilizations!)</li>\n<li>Finally we consider illogical and incomprehensible (to us) purposes, for example, as a work of art, an act of self-expression or toys. Or perhaps an insane rivalry between two factions. Or something we simply cannot understand (For example, extraterrestrial will not understand why the Americans have stuck a flag into the Moon. Was it worthwhile to fly over 300000 km to install painted steel?)</li>\n</ul>\n<p>11. Assuming signals propagated billions of light years distant in the Universe, the area susceptible to widespread SETI attack, is a sphere with a radius of several billion light years. In other words, it would be sufficient to find a one &ldquo;bad civilization\" in the light cone of a height of several billion years old, that is, that includes billions of galaxies from which we are in danger of SETI attack. Of course, this is only true, if the average density of civilization is at least one in the galaxy. This is an interesting possibility in relation to Fermi&rsquo;s Paradox.</p>\n<p>16. As the depth of scanning the sky rises linearly, the volume of space and the number of stars that we see increases by the cube of that number. This means that our chances to stumble on a SETI signal nonlinear grow by fast curve.</p>\n<p>17. It is possible that when we stumble upon several different messages from the skies, which refute one another in a spirit of: \"<em>do not listen to them, they are deceiving voices, and wish you evil. But we, brother,</em> we, <em>are good&mdash;and wise&hellip;</em>\"</p>\n<p>18. Whatever positive and valuable message we receive, we can never be sure that all of this is not a subtle and deeply concealed threat. This means that in interstellar communication there will always be an element of distrust, and in every happy revelation, a gnawing suspicion.</p>\n<p>19. A defensive posture regarding interstellar communication is only to listen, not sending anything that does not reveal its location. The laws prohibit the sending of a message from the United States to the stars. Anyone in the Universe who sends (transmits) self-evidently- is not afraid to show his position. Perhaps because the sending (for the sender) is more important than personal safety. For example, because it plans to flush out prey prior to attacks. Or it is forced to, by a evil local AI.</p>\n<p>20. It was said about atomic bomb: The main secret about the atomic bomb is that it can be done. If prior to the discovery of a chain reaction Rutherford believed that the release of nuclear energy is an issue for the distant future, following the discovery any physicist knows that it is enough to connect two subcritical masses of fissionable material in order to release nuclear energy. In other words, if one day we find that signals can be received from space, it will be an irreversible event&mdash;something analogous to a deadly new arms race will be on.</p>\n<p><strong>Objections</strong>.</p>\n<p>The discussions on the issue raise several typical objections, now discussed.</p>\n<p>Objection 1: Behavior discussed here is too anthropomorphic. In fact, civilizations are very different from each other, so you can&rsquo;t predict their behavior.</p>\n<p>Answer: Here we have a powerful observation selection effect. While a variety of possible civilizations exist, including such extreme scenarios as thinking oceans, etc., we can only receive radio signals from civilizations that send them, which means that they have corresponding radio equipment and has knowledge of materials, electronics and computing. That is to say we are threatened by civilizations of the same type as our own. Those civilizations, which can neither accept nor send radio messages, do not participate in this game.</p>\n<p>Also, an observation selection effect concerns purposes. Goals of civilizations can be very different, but all civilizations intensely sending signals, will be only that want to tell something to &ldquo;everyone\". Finally, the observation selection relates to the effectiveness and universality of SETI virus. The more effective it is, the more different civilizations will catch it and the more copies of the SETI virus radio signals will be in heaven. So we have the &lsquo;excellent chances&rsquo; to meet a most powerful and effective virus.</p>\n<p>Objection 2. For super-civilizations there is no need to resort to subterfuge. They can directly conquer us.</p>\n<p>Answer:</p>\n<p>This is true only if they are in close proximity to us. If movement faster than light is not possible, the impact of messages will be faster and cheaper. Perhaps this difference becomes important at intergalactic distances. Therefore, one should not fear the SETI attack from the nearest stars, coming within a radius of tens and hundreds of light-years.</p>\n<p>Objection 3. There are lots of reasons why SETI attack may not be possible. What is the point to run an ineffective attack?</p>\n<p>Answer: SETI attack does not always work. It must act in a sufficient number of cases in line with the objectives of civilization, which sends a message. For example, the con man or fraudster does not expect that he would be able \"to con\" every victim. He would be happy to steal from even one person inone hundred. It follows that SETI attack is useless if there is a goal to attack <em>all </em>civilizations in a certain galaxy. But if the goal is to get at least <em>some</em> outposts in another galaxy, the SETI attack fits. (Of course, these outposts can then build fleets of space ships to spread SETI attack bases outlying stars within the target galaxy.)</p>\n<p>The main assumption underlying the idea of SETI attacks is that extraterrestrial super civilizations exist in the visible universe at all. I think that this is unlikely for reasons related to antropic principle. Our universe is unique from 10 ** 500 possible universes with different physical properties, as suggested by one of the scenarios of string theory. My brain is 1 kg out of 10 ** 30 kg in the solar system. Similarly, I suppose, the Sun is no more than about 1 out of 10 ** 30 stars that could raise a intelligent life, <em>so it means that we are likely alone in the visible universe. </em></p>\n<p>Secondly the fact that Earth came so late (i.e. it could be here for a few billion years earlier), and it was not prevented by alien preemption from developing, argues for the rarity of intelligent life in the Universe. The putative rarity of our civilization is the best protection against attack SETI. On the other hand, if we open parallel worlds or super light speed communication, the problem arises again.</p>\n<p>Objection 7. Contact is impossible between post-singularity supercivilizations, which are supposed here to be the sender of SETI-signals, and pre- singularity civilization, which we are, because supercivilization is many orders of magnitude superior to us, and its message will be absolutely not understandable for us - exactly as the contact between ants and humans is not possible. (A singularity is the time of creation of artificial intelligence capable of learning, (and beginning an exponential booting in recursive improving self-design of further intelligence and much else besides) after which civilization make leap in its development - on Earth it may be possible in the area in 2030.)</p>\n<p>Answer: In the proposed scenario, we are<em> not </em>talking about contact but a purposeful deception of us. Similarly, a man is quite capable of manipulating behavior of ants and other social insects, whose objectives are is absolutely incomprehensible to them. For example, LJ user &ldquo;ivanov-petrov&rdquo; describes the following scene: As a student, he studied the behavior of bees in the Botanical Garden of Moscow State University. But he had bad relations with the security guard controlling the garden, which is regularly expelled him before his time. Ivanov-Petrov took the green board and developed in bees conditioned reflex to attack this board. The next time the watchman came, who constantly wore a green jersey, all the bees attacked him and he took to flight. So &ldquo;ivanov-petrov&rdquo; could continue research. Such manipulation is not a contact, but this does not prevent its&rsquo; effectiveness.</p>\n<p><br /> <br /> \"Objection 8. For civilizations located near us is much easier to attack us &ndash;for &lsquo;guaranteed results&rsquo;&mdash;using starships than with SETI-attack.<br /> <br /> Answer. It may be that we significantly underestimate the complexity of an attack using starships and, in general, the complexity of interstellar travel. To list only one factor, the potential &lsquo;minefield&rsquo; characteristics of the as-yet unknown interstellar medium.</p>\n<p>If such an attack would be carried out now or in the past, the Earth's civilization has nothing to oppose it, but in the future the situation will change - all matter in the solar system will be full of robots, and possibly completely processed by them. On the other hand, the more the speed of enemy starships approaching us, the more the fleet will be visible by its braking emissions and other characteristics. These quick starships would be very vulnerable, in addition we could prepare in advance for its arrival. A slowly moving nano- starship would be very less visible, but in the case of wishing to trigger a transformation of full substance of the solar system, it would simply be nowhere to land (at least without starting an alert in such a &lsquo;nanotech-settled&rsquo; and fully used future solar system. (Friedlander added: Presumably there would always be some &lsquo;outer edge&rsquo; of thinly settled Oort Cloud sort of matter, but by definition the rest of the system would be more densely settled, energy rich and any deeper penetration into solar space and its&rsquo; conquest would be the proverbial uphill battle&mdash;not in terms of gravity gradient, but in terms of the available resources of war against a full Class 2 Kardashev civilization.)</p>\n<p>The most serious objection is that an advanced civilization could in a few million years sow all our galaxy with self replicating post singularity nanobots that could achieve any goal in each target star-system, including easy prevention of the development of incipient other civilizations. (In the USA Frank Tipler advanced this line of reasoning.) However, this could not have happened in our case - no one has prevented development of our civilization. So, it would be much easier and more reliable to send out robots with such assignments, than bombardment of SETI messages of the entire galaxy, and if we don&rsquo;t see it, it means that no SETI attacks are inside our galaxy. (It is possible that a probe on the outskirts of the solar system expects manifestations of human space activity to attack &ndash; a variant of the \"Berserker\" hypothesis - but it will not attack through SETI). Probably for many millions or even billions of years microrobots could even reach from distant galaxies at a distance of tens of millions of light-years away. Radiation damage may limit this however without regular self-rebuilding.</p>\n<p>In this case SETI attack would be meaningful only at large distances. However, this distance - tens and hundreds of millions of light-years - probably will require innovative methods of modulation signals, such as management of the luminescence of active nuclei of galaxies. Or transfer a narrow beam in the direction of our galaxy (but they do not know where it will be over millions of years). But a civilization, which can manage its&rsquo; galaxy&rsquo;s nucleus, might create a spaceship flying with near-light speeds, even if its mass is a mass of the planet. Such considerations severely reduce the likelihood of SETI attacks, but not lower it to zero, because we do not know all the possible objectives and circumstances.</p>\n<p><br /> <em>(An comment by JF :</em>For example the lack of SETI-attack so far may <em>itself</em> be a cunning ploy: At first receipt of the developing Solar civilization&rsquo;s radio signals, all interstellar &lsquo;spam&rsquo; would have ceased, (and interference stations of some unknown (but amazing) capability and type set up around the Solar System to block all coming signals recognizable to its&rsquo; computers as of intelligent origin,) in order to get us &lsquo;lonely&rsquo; and give us time to discover and appreciate the Fermi Paradox and even get those so philosophically inclined to despair desperate that this means the Universe is apparently hostile by some standards. Then, when desperate, we suddenly discover, slowly at first, partially at first, and then with more and more wonderful signals, the fact that space is filled with bright enticing signals (like spam). The blockade, cunning as it was (analogous to Earthly jamming stations) was yet a prelude to a slow &lsquo;turning up&rsquo; of preplanned intriguing signal traffic. If as Earth had developed we had intercepted cunning spam followed by the agonized &lsquo;don&rsquo;t repeat our mistakes&rsquo; final messages of tricked and dying civilizations, only a fool would heed the enticing voices of SETI spam. But now, a SETI attack may benefit from the slow unmasking of a cunning masquerade as first a faint and distant light of infinite wonder, only at the end revealed as the headlight of an onrushing cosmic train&hellip;)</p>\n<p><em> AT comment to it. In fact I think that SETI attack senders are on the distances more than 1000 ly and so they do not know yet that we have appeared. But so called Fermi Paradox indeed maybe a trick &ndash; senders deliberately made their signals weak in order to make us think that they are not spam. </em><em></em></p>\n<p>The scale of space strategy may be inconceivable to the human mind.</p>\n<p><br /> <br /> And we should note in conclusion that some types of SETI-attack do not even need a computer but just a man who could understand the message that then \"set his mind on fire\". At the moment we cannot imagine such a message, but we can give some analogies. Western religions are built around the text of the Bible. It can be assumed that if the text of the Bible appeared in some countries, which had previously not been familiar with it, there might arise a certain number of biblical believers. Similarly subversive political literature, or even some superideas, &ldquo;sticky&rdquo; memes or philosophical mind-benders. Or, as suggested by Hans Moravec, we get such a message: \"Now that you have received and decoded me, broadcast me in at least ten thousand directions with ten million watts of power. Or else.\" - this message is dropped, leaving us guessing, what may indicate that \"or else\". Even a few pages of text may contain a lot of subversive information - Imagine that we could send a message to the 19 th century scientists. We could open them to the general principle of the atomic bomb, the theory of relativity, the transistors - and thus completely change the course of technological history, and we could add that all the ills in the 20 century were from Germany (which is only partly true) , then we would have influenced the political history.</p>\n<p>(Comment of JF: Such a latter usage would depend on having received enough of Earth&rsquo;s transmissions to be able to model our behavior and politics. But imagine a message as posing from our own future, to ignite &lsquo;catalytic war&rsquo;&mdash;Automated SIGINT (signals intelligence) stations are constructed monitoring our solar system, their computers &lsquo;cracking&rsquo; our language and culture (possibly with the aid of children&rsquo;s television programs with see and say matching of letters and sounds, from TV news showing world maps and naming countries possibly even from intercepting wireless internet encyclopedia articles. ) Then a test or two may follow, posting a what if scenario inviting comment from bloggers, about a future war say between the two leading powers of the planet. (For purposes of this discussion, say around 2100 present calendar China is strongest and India rising fast). Any defects and nitpicks in the comments of the blog are noted and corrected. Finally, an actual interstellar message is sent with the debugged scenario(not shifting against the stellar background, it is unquestionably interstellar in origin) proporting to be from a dying starship of the presently stronger side&rsquo;s (China&rsquo;s) future, when the presently weaker side (India&rsquo;s) space fleet has smashed the future version of the Chinese State and essentially committed genocide. The starship has come back in time, but is dying, and indeed the transmission ends, or simply repeats, possibly after some back and forth communication between the false computer models of the &lsquo;starship commander&rsquo; and the Chinese government. The reader can imagine the urgings of the future Chinese military council to preempt to forestall doom. If as seems probable, such a strategy is too complicated to carry off in one stage, various &lsquo;future travellers&rsquo; may emerge from a war, signal for help in vain, and &lsquo;die&rsquo; far outside our ability to reach them, (say some light days away, near the alleged location of an &lsquo;emergence gate&rsquo; but near an actual transmitter) Quite a drama may emerge as the computer learns to &lsquo;play&rsquo; us like a con man, ship after ship of various nationalities dribbling out stories but also getting answers to key questions for aid in constructing the emerging scenario which will be frighteningly believable, enough to ignite a final war. Possibly lists of key people in China (or whatever side is stronger) may be drawn up by the computer with a demand that they be executed as the parents of future war criminals&mdash;sort of an International Criminal Court &ndash;acting as Terminator scenario. Naturally the Chinese state, at that time the most powerful in the world, would guard its&rsquo; rulers lives against any threat. Yet more refugee spaceships of various nationalities can emerge transmit and die, offering their own militaries terrifying new weapons technologies from unknown sciences that really work (more &lsquo;proof&rsquo; of their future origin). Or weapons from known sciences, for example decoding online DNA sequences in the future internet and constructing formulae for DNA constructors to make specific tailored genetic weapons against particular populations&mdash;that endure in the ground, a scorched earth against a particular population on a particular piece of land. These are copied and spread worldwide as are totally accurate plans&mdash;in standard CNC codes for easy to construct thermonuclear weapons in the 1950s style&mdash;using U-238 for casing, and only a few kilograms of fissionable material for ignition By that time well over a million tons of depleted uranium will be worldwide, and deuterium is free in the ocean and can be used directly in very large weapons without lithium deuteride. Knowing how to hack together a wasteful, more than critical mass crude fission device is one thing (the South African device was of this kind). But knowing &ndash;with absolute accuracy, down to machining drawings, CNC codes, etc how to make high-yield, super efficient very dirty thermonuclear weapons <em>without need for testing</em> means that any small group with a few dozen million dollars and automated machine tools can clandestinely make a multi-megaton device &ndash;or many&mdash; and smash the largest cities. And any small power with a few dozen jets can cripple a continent for a decade. Already over a thousand tons of plutonium exist. The SETI spam can include CNC codes for making a one shot reactor plutonium chemical refiner that would be left hopelessly radioactive but output chemically pure plutonium. (This would be prone to predetonation because of the Pu-240 content but then plans for debugged laser isotope separators may also be downloaded). This is a variant of the &lsquo;catalytic war&rsquo; and &lsquo;nuclear six gun&rsquo; (i.e. easy to obtain weapons) scenarios of the late Herman Kahn. Even cheaper would be bioattacks of the kind outlined above. The principle point is that planet killer weapons fully debugged take great amounts of debugging, tens to hundreds of billions of dollars, and free access to a world scientific community. Today, it is to every great power&rsquo;s advantage to keep accurate designs out of the hands of third parties because they have to live on the same planet (and because the fewer weapons, the easier it is to stay a great power). Not so the SETI spam authors. Without the hundreds of billions in R and D, the actual construction budget would be on the order of a million dollars per multi-megaton device (depending on the expense of obtaining the raw reactor plutonium) If wishing to extend today&rsquo;s scenarios into the future, the SETI spam authors manipulate Georgia (with about a $10 billion GDP) to arm against Russia and Taiwan against China and Venezuela against the USA. Although Russian and China and the USA could respectively promise annihilation against any attacker, with a military budget around 4% of GDP and the downloaded plans, the reverse&mdash;for the first time&mdash;could then also be true. (400 100 megaton bombs can kill by fallout perhaps 95% of unprotected populations over a country the size of the USA or China and 90% of a country the size of Russia, assuming the worst kind of cooperation from the winds.&mdash;from an old chart by Ralph Lapp) Anyone living near a superarmed microstate with border conflicts will, of course, wish to arm themselves. And these newly armed states themselves&mdash;of course&mdash;will have borders. Note that this drawn out scenario gives lots of time for a huge arms buildup on both (or many!) sides, and a Second Cold War that eventually turns very hot indeed&hellip;and unlike a human player of such a horrific &lsquo;catalytic war&rsquo; con game, worldwide fallout or enduring biocontamination is not a concern at all&hellip; ()</p>\n<p><strong>Conclusion</strong>.</p>\n<p>The product of the probabilities of the following events describes the probability of attack. For these probabilities, we can only give so-called &laquo;expert&raquo; assessment, that is, assign them a certain a priori subjective probability as we do now.</p>\n<p>1) The likelihood that extraterrestrial civilizations exist at a distance at which radio communication is possible with them. In general, I agree with the view of Shklovsky and supporters of the &ldquo;Rare Earth&rdquo; hypothesis - that the Earth's civilization is unique in the observable universe. This does not mean that extraterrestrial civilizations do not exist at all (because the universe, according to the theory of cosmological inflation, is almost endless) - they are just over the horizon of events visible from our point in space-time. In addition, this is not just about distance, but also of the distance at which you can establish a connection, which allows transferring gigabytes of information. (However, passing even 1 bit per second, you can submit 1-gigabit for about 20 years, which may be sufficient for the SETI-attack.) If in the future will be possible some superluminal communication or interaction with parallel universes, it would dramatically increase the chances of SETI attacks. So, I appreciate this chance to 10%.</p>\n<p>2) The probability that SETI-attack is technically feasible: that is, it is possible computer program, with recursively self-improving AI and sizes suitable for shipping. I see this chance as high: 90%.</p>\n<p>3) The likelihood that civilizations that could have carried out such attack exist in our space-time cone - this probability depends on the density of civilizations in the universe, and of whether the percentage of civilizations that choose to initiate such an attack, or, more importantly, obtain victims and become repeaters. In addition, it is necessary to take into account not only the density of civilizations, but also the density created by radio signals. All these factors are highly uncertain. It is therefore reasonable to assign this probability to 50%.</p>\n<p>4) The probability that we find such a signal during our rising civilization&rsquo;s period of vulnerability to it. The period of vulnerability lasts from now until the moment when we will decide and be technically ready to implement this decision: <em>Do not download any extraterrestrial computer programs under any circumstances.</em> Such a decision may only be exercised by our AI, installed as world ruler (which in itself is fraught with considerable risk). Such an world AI (WAI) can be in created circa 2030. We cannot exclude, however, that our WAI still will not impose a ban on the intake of extraterrestrial messages, and fall victim to attacks by the alien artificial intelligence, which by millions of years of machine evolution surpasses it. Thus, the window of vulnerability is most likely about 20 years, and &ldquo;width&rdquo; of the window depends on the intensity of searches in the coming years. This &ldquo;width&rdquo; for example, depends on the intensity of the current economic crisis of 2008-2010, from the risks of World War III, and how all this will affect the emergence of the WAI. It also depends on the density of infected civilizations and their signal strength&mdash; as these factors increase, the more chances to detect them earlier. Because we are a normal civilization under normal conditions, according to the principle of Copernicus, the probability should be large enough; otherwise a SETI-attack would have been generally ineffective. (The SETI-attack, itself (here supposed to exist) also are subject to a form of &ldquo;natural selection&rdquo; to test its effectiveness. (In the sense that it works or does not. ) This is a very uncertain chance we will too, over 50%.</p>\n<p>5) Next is the probability that SETI-attack will be successful - that is that we swallow the bait, download the program and description of the computer, run them, lose control over them and let them reach all their goals. I appreciate this chance to be very high because of the factor of multiplicity - that is the fact that the message is downloaded repeatedly, and someone, sooner or later, will start it. In addition, through natural selection, most likely we will get the most effective and deadly message that will most effectively deceive our type of civilization. I consider it to be 90%.</p>\n<p>6) Finally, it is necessary to assess the probability that SETI-attack will lead to a complete human extinction. On the one hand, it is possible to imagine a &ldquo;good&rdquo; SETI-attack, which is limited so that it will create a powerful radio emitter behind the orbit of Pluto. However, for such a program will always exist the risk that a possible emergent society at its&rsquo; target star will create a powerful artificial intelligence, and effective weapon that would destroy this emitter. In addition, to create the most powerful transponder would be needed all the substance of solar system and the entire solar energy. Consequently, the share of such &ldquo;good&rdquo; attacks will be lower due to natural selection, as well as some of them will be destroyed sooner or later by captured by them civilizations and their signals will be weaker. So the chances of destroying all the people with the help of SETI-attack that has reached all its goals, I appreciate in 80%.</p>\n<p>As a result, we have: 0.1h0.9h0.5h0.5h0.9h0.8 = 1.62%</p>\n<p><em> </em></p>\n<p>So, after rounding, the chances of extinction of Man through SETI attack in XXI century is around 1 per cent with a theoretical precision of an order of magnitude.</p>\n<p>Our best protection in this context would be that civilization would very rarely met in the Universe. But this is not quite right, because the Fermi paradox here works on the principle of \"Neither alternative is good\":</p>\n<ul>\n<li>If there <em>are </em>extraterrestrial civilizations, and there are many of them, it is dangerous because they can threaten us in one way or another.</li>\n<li>If extraterrestrial civilizations <em>do not</em> exist, it is also bad, because it gives weight to the hypothesis of inevitable extinction of technological civilizations or of our underestimating of frequency of cosmological catastrophes. Or, a high density of space hazards, such as gamma-ray bursts and asteroids that we underestimate because of the observation selection effect&mdash;i.e., were we not here because already killed, we would not be making these observations&hellip;.</li>\n</ul>\n<p>Theoretically possible is a reverse option, which is that through SETI will come a warning message about a certain threat, which has destroyed most civilizations, such as: \"Do not do any experiments with X particles, it could lead to an explosion that would destroy the planet.\" But even in that case remain a doubt, that there is no deception to deprive us of certain technologies. (Proof would be if similar reports came from other civilizations in space in the opposite direction.) But such communication may only enhance the temptation to experiment with X-particles.</p>\n<p>So I do not appeal to abandon SETI searches, although such appeals are useless.</p>\n<p>It may be useful to postpone any technical realization of the messages that we could get on SETI, up until the time when we will have our Artificial Intelligence. Until that moment, perhaps, is only 10-30 years, that is, we could wait. Secondly, it would be important to hide the fact of receiving dangerous SETI signal its essence and the source location.</p>\n<p>This risk is related to a methodologically interesting aspect. Despite the fact that I have thought every day in the last year and read on the topic of global risks, I found this dangerous vulnerability in SETI only now. By hindsight, I was able to find another four authors who came to similar conclusions. However, I have made a significant finding: that there may be not yet open global risks, and even if the risk of certain constituent parts are separately known to me, it may take a long time to join them into a coherent picture. Thus, hundreds of dangerous vulnerabilities may surround us, like an unknown minefield. Only when the first explosion happens will we know. And that first explosion may be the last.</p>\n<p>An interesting question is whether Earth itself could become a source of SETI-attack in the future when we will have our own AI. Obviously, that could. Already in the program of METI exists an idea to send the code of human DNA. (The &ldquo;children's message scenario&rdquo; &ndash; in which the children ask to take their piece of DNA and clone them on another planet &ndash;as depicted in the film &ldquo;Calling all aliens&rdquo;.)</p>\n<p><strong>Literature</strong>:</p>\n<p><em>1. Hoyle F. </em>Andromeda. http://en.wikipedia.org/wiki/A_for_Andromeda</p>\n<p><em>2. Yudkowsky E. </em><em>Artificial Intelligence as a Positive and Negative Factor in Global Risk. </em>Forthcoming in Global Catastrophic Risks, eds. Nick Bostrom and Milan Cirkovic <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">http://www.singinst.org/upload/artificial-intelligence-risk.pdf</a></p>\n<p><em>3.Moravec Hans</em>. Mind Children: The Future of Robot and Human Intelligence, 1988.</p>\n<p><em>4.Carrigan</em>, <em>Jr.</em><em> </em><em>Richard A.</em> The Ultimate Hacker: SETI signals may need to be decontaminated <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf\">http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf</a></p>\n<p>5. <em>Carrigan&rsquo;s</em> page <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI_Hacker.htm\">http://home.fnal.gov/~carrigan/SETI/SETI_Hacker.htm</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NGtNzdS88JtEQdRP4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jng2cZQtyuXDPihNg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 17, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "22027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Alexei_Turchin__Risks_of_downloading_alien_AI_via_SETI_search\">Alexei Turchin. Risks of downloading alien AI via SETI search</strong></p>\n<p style=\"padding-left: 30px;\"><em>Abstract: This article examines risks associated with the program of passive search for alien signals (SETI\u2014the Search for Extra-Terrestrial Intelligence). In this paper we propose a scenario of possible vulnerability and discuss the reasons why the proportion of dangerous signals to harmless ones can be dangerously high. This article does not propose to ban SETI programs, and does not insist on the inevitability of SETI-triggered disaster. Moreover, it gives the possibility of how SETI can be a salvation for mankind.</em></p>\n<p>The idea that passive SETI can be dangerous is not new. Fred Hoyle suggested in the story \"A for Andromeda\u201d a scheme of alien attack through SETI signals. According to the plot, astronomers receive an alien signal, which contains a description of a computer and a computer program for it. This machine creates a description of the genetic code which leads to the creation of an intelligent creature \u2013 a girl dubbed Andromeda, which, working together with the computer, creates advanced technology for the military. The initial suspicion of alien intent is overcome by the greed for the technology the aliens can provide. However, the main characters realize that the computer acts in a manner hostile to human civilization and destroy the computer, and the girl dies.</p>\n<p>This scenario is fiction, because most scientists do not believe in the possibility of a strong AI, and, secondly, because we do not have the technology that enables synthesis of new living organisms solely from its\u2019 genetic code. Or at least, we have not until recently. Current technology of sequencing and DNA synthesis, as well as progress in developing a code of DNA modified with another set of the alphabet, indicate that in 10 years the task of re-establishing a living being from computer codes sent from space in the form computer codes might be feasible.</p>\n<p>Hans Moravec in the book \"Mind Children\" (1988) offers a similar type of vulnerability: downloading a computer program from space via SETI, which will have artificial intelligence, promising new opportunities for the owner and after fooling the human host, self-replicating by the millions of copies and destroying the human host, finally using the resources of the secured planet to send its \u2018child\u2019 copies to multiple planets which constitute its\u2019 future prey. Such a strategy would be like a virus or a digger wasp\u2014horrible, but plausible. In the same direction are R. Carrigan\u2019s ideas; he wrote an article \"SETI-hacker\", and expressed fears that unfiltered signals from space are loaded on millions of not secure computers of SETI-at-home program. But he met tough criticism from programmers who pointed out that, first, data fields and programs are in divided regions in computers, and secondly, computer codes, in which are written programs, are so unique that it is impossible to guess their structure sufficiently to hack them blindly (without prior knowledge).</p>\n<p>After a while Carrigan issued a second article - \"Should potential SETI signals be decontaminated?\" <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf\">http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf</a>, which I\u2019ve translated into Russian. In it, he pointed to the ease of transferring gigabytes of data on interstellar distances, and also indicated that the interstellar signal may contain some kind of bait that will encourage people to collect a dangerous device according to the designs. Here Carrigan not give up his belief in the possibility that an alien virus could directly infected earth\u2019s computers without human \u2018translation\u2019 assistance. (We may note with passing alarm that the prevalence of humans obsessed with death\u2014as Fred Saberhagen pointed out in his idea of \u2018goodlife\u2019\u2014means that we cannot entirely discount the possibility of demented \u2018volunteers\u2019 \u2013human traitors eager to assist such a fatal invasion) As a possible confirmation of this idea, Carrigan has shown that it is possible easily reverse engineer language of computer program - that is, based on the text of the program it is possible to guess what it does, and then restore the value of operators.</p>\n<p>In 2006, E. Yudkowsky wrote an article \"AI as a positive and a negative factor of global risk\", in which he demonstrated that it is very likely that it is possible rapidly evolving universal artificial intelligence which high intelligence would be extremely dangerous if it was programmed incorrectly, and, finally, that the occurrence of such AI and the risks associated with it significantly undervalued. In addition, Yudkowsky introduced the notion of \u201cSeed AI\u201d - embryo AI - that is a minimum program capable of runaway self-improvement with unchanged primary goal. The size of Seed AI can be on the close order of hundreds of kilobytes. (For example, a typical representative of Seed AI is a human baby, whose part of genome responsible for the brain would represent ~ 3% of total genes of a person with a volume of 500 megabytes, or 15 megabytes, but given the share of garbage DNA is even less.)</p>\n<p>In the beginning, let us assume that in the Universe there is an extraterrestrial civilization, which intends to send such a message, which will enable it to obtain power over Earth, and consider this scenario. In the next chapter we will consider how realistic is that another civilization would want to send such a message.</p>\n<p>First, we note that in order to prove the vulnerability, it is enough to find <em>just one hole</em> in security. However, in order to prove safety, you must remove <em>every possible</em> hole. The complexity of these tasks varies on many orders of magnitude that are well known to experts on computer security. This distinction has led to the fact that almost all computer systems have been broken (from Enigma to iPOD). I will now try to demonstrate one possible, and even, in my view, likely, vulnerability of SETI program. However, I want to caution the reader from the thought that if he finds errors in my discussions, it automatically proves the safety of SETI program. Secondly, I would also like to draw the attention of the reader, that I am a man with an IQ of 120 who spent all of a month of thinking on the vulnerability problem. We need not require an alien super civilization with IQ of 1000000 and contemplation time of millions of years to significantly improve this algorithm\u2014we have no real idea what an IQ of 300 or even-a mere IQ of 100 with much larger mental \u2018RAM\u2019 (\u2013the ability to load a major architectural task into mind and keep it there for weeks while processing) could accomplish to find a much more simple and effective way. Finally, I propose one possible algorithm and then we will discuss briefly the other options.</p>\n<p>In our discussions we will draw on the Copernican principle, that is, the belief that we are ordinary observers in normal situations. Therefore, the Earth\u2019s civilization is an ordinary civilization developing normally. (Readers of tabloid newspapers may object!)</p>\n<p><strong id=\"Algorithm_of_SETI_attack\">Algorithm of SETI attack</strong></p>\n<p>1. The sender creates a kind of signal beacon in space, which reveals that its message is clearly artificial. For example, this may be a star with a Dyson sphere, which has holes or mirrors, alternately opened and closed. Therefore, the entire star will blink of a period of a few minutes - faster is not possible because of the variable distance between different openings. (Even synchronized with an atomic clock according to a rigid schedule, the speed of light limit means that there are limits to the speed and reaction time of coordinating large scale systems) Nevertheless, this beacon can be seen at a distance of millions of light years. There are possible other types of lighthouses, but the important fact that the beacon signal could be viewed at long distances.</p>\n<p>2. Nearer to Earth is a radio beacon with a much weaker signal, but more information saturated. The lighthouse draws attention to this radio source. This source produces some stream of binary information (i.e. the sequence of 0 and 1). About the objection that the information would contain noises, I note that the most obvious (understandable to the recipient's side) means to reduce noise is the simple repetition of the signal in a circle.</p>\n<p>3. The most simple way to convey meaningful information using a binary signal is sending of images. First, because eye structures in the Earth's biological diversity appeared independently 7 times, it means that the presentation of a three-dimensional world with the help of 2D images is probably universal, and is almost certainly understandable to all creatures who can build a radio receiver.</p>\n<p>4. Secondly, the 2D images are not too difficult to encode in binary signals. To do so, let us use the same system, which was used in the first TV cameras, namely, a system of progressive and frame rate. At the end of each time frame images store bright light, repeated after each line, that is, through an equal number of bits. Finally, at the end of each frame is placed another signal indicating the end of the frame, and repeated after each frame. (This may form, or may not form a continuous film.) This may look like this:</p>\n<p>01010111101010 11111111111111111</p>\n<p>01111010111111 11111111111111111</p>\n<p>11100111100000 11111111111111111</p>\n<p>Here is the end line signal of every of 25 units. Frame end signal may appear every, for example, 625 units.</p>\n<p>5. Clearly, a sender civilization- should be extremely interested that we understand their signals. On the other hand, people will share an extreme desire to decrypt the signal. Therefore, there is no doubt that the picture will be recognized.</p>\n<p>6. Using images and movies can convey a lot of information, they can even train in learning their language, and show their world. It is obvious that many can argue about how such films will be understandable. Here, we will focus on the fact that if a certain civilization sends radio signals, and the other takes them, so they have some shared knowledge. Namely, they know radio technique - that is they know transistors, capacitors, and resistors. These radio-parts are quite typical so that they can be easily recognized in the photographs. (For example, parts shown, in cutaway view, and in sequential assembly stages\u2014 or in an electrical schematic whose connections will argue for the nature of the components involved).</p>\n<p>7. By sending photos depicting radio-parts on the right side, and on the left - their symbols, it is easy to convey a set of signs indicating electrical circuit. (Roughly the same could be transferred and the logical elements of computers.)</p>\n<p>8. Then, using these symbols the sender civilization- transmits blueprints of their simplest computer. The simplest of computers from hardware point of view is the Post-machine. It has only 6 commands and a tape data recorder. Its full electric scheme will contain only a few tens of transistors or logic elements. It is not difficult to send blueprints of Post machine.</p>\n<p>9. It is important to note that all computers at the level of algorithms are Turing-compatible. That means that extraterrestrial computers at the basic level are compatible with any earth computer. Turing-compatibility is a mathematical universality as the Pythagorean theorem. Even the Babbage mechanical machine, designed in the early 19th century, was Turing-compatible.</p>\n<p>10. Then the sender civilization- begins to transmit programs for that machine. Despite the fact that the computer is very simple, it can implement a program of any difficulty, although it will take very long in comparison with more complex programs for the same computer. It is unlikely that people will be required to build this computer physically. They can easily emulate it within any modern computer, so that it will be able to perform trillions of operations per second, so even the most complex program will be carried out on it quite quickly. (It is a possible interim step: a primitive computer gives a description of a more complex and fast computer and then run on it.)</p>\n<p>11. So why people would create this computer, and run its program? Perhaps, in addition to the actual computer schemes and programs in the communication must be some kind of \"bait\", which would have led the people to create such an alien computer and to run programs on it and to provide to it some sort of computer data about the external world \u2013Earth outside the computer. There are two general possible baits - temptations and dangers:</p>\n<p>a). For example, perhaps people receive the following offer\u2013 lets call it \"The humanitarian aid con (deceit)\". Senders of an \"honest signal\" SETI message warn that the sent program is Artificial intelligence, but lie about its goals. That is, they argue that this is a \"gift\" which will help us to solve all medical and energy problems. But it is a Trojan horse of most malevolent intent. It is too useful <em>not</em> to use. Eventually it becomes indispensable. And then exactly when society becomes dependent upon it, the foundation of society\u2014and society itself\u2014is overturned\u2026</p>\n<p>b). \"The temptation of absolute power con\" - in this scenario, they offer specific transaction message to recipients, promising power over other recipients. This begins a \u2018race to the bottom\u2019 that leads to runaway betrayals and power seeking counter-moves, ending with a world dictatorship, or worse, a destroyed world dictatorship on an empty world\u2026.</p>\n<p>c). \"Unknown threat con\" - in this scenario bait senders report that a certain threat hangs over on humanity, for example, from another enemy civilization, and to protect yourself, you should join the putative \u201cGalactic Alliance\u201d and build a certain installation. Or, for example, they suggest performing a certain class of physical experiments on the accelerator and sending out this message to others in the Galaxy. (Like a chain letter) And we should send this message <em>before</em> we ignite the accelerator, please\u2026</p>\n<p>d). \"Tireless researcher con\" - here senders argue that posting messages is the cheapest way to explore the world. They ask us to create AI that will study our world, and send the results back. It does rather more than that, of course\u2026</p>\n<p>12. However, the main threat from alien messages with executable code is <em>not</em> the bait itself, but that this message can be well known to a large number of independent groups of people. First, there will always be someone who is more susceptible to the bait. Secondly, say, the world will know that alien message emanates from the Andromeda galaxy, and the Americans have already been received and maybe are trying to decipher it. Of course, then all other countries will run to build radio telescopes and point them on Andromeda galaxy, as will be afraid to miss a \u201cstrategic advantage\u201d. And they will find the message and see that there is a proposal to grant omnipotence to those willing to collaborate. In doing so, they will not know, if the Americans would take advantage of them or not, even if the Americans will swear that they don\u2019t run the malicious code, and beg others not to do so. Moreover, such oaths, and appeals will be perceived as a sign that the Americans have already received an incredible extraterrestrial advantage, and try to deprive \"progressive mankind\" of them. While most will understand the danger of launching alien code, <em>someone </em>will be willing to risk it. Moreover there will be a game in the spirit of \"winner take all\", as well be in the case of opening AI, as Yudkowsky shows in detail. So, the bait is not dangerous, but the plurality of recipients. If the alien message is posted to the Internet (and its size, sufficient to run Seed AI can be less than gigabytes along with a description of the computer program, and the bait), here we have a classic example of \"knowledge\" of mass destruction, as said Bill Joy, meaning the recipes genomes of dangerous biological viruses. If aliens sent code will be available to tens of thousands of people, then someone will start it even without any bait out of simple curiosity We can\u2019t count on existing SETI protocols, because discussion on METI (sending of messages to extraterrestrial) has shown that SETI community is not monolithic on important questions. Even a simple fact that something was found could leak and encourage search from outsiders. And the coordinates of the point in sky would be enough.</p>\n<p>13. Since people don\u2019t have AI, we almost certainly greatly underestimate its power and overestimate our ability to control it. The common idea is that \"it is enough to pull the power cord to stop an AI\" or place it in a black box to avoid any associated risks. Yudkowsky shows that AI can deceive us as an adult does a child. If AI dips into the Internet, it can quickly subdue it as a whole, and also taught all necessary about entire earthly life. Quickly - means the maximum hours or days. Then the AI can create advanced nanotechnology, buy components and raw materials (on the Internet, he can easily make money and order goods with delivery, as well as to recruit people who would receive them, following the instructions of their well paying but \u2018unseen employer\u2019, not knowing who\u2014or rather, <em>what</em>\u2014- they are serving). Yudkowsky leads one of the possible scenarios of this stage in detail and assesses that AI needs only weeks to crack any security and get its own physical infrastructure.</p>\n<p>\"Consider, for clarity, one possible scenario, in which Alien AI (AAI) can seize power on the Earth. Assume that it promises immortality to anyone who creates a computer on the blueprints sent to him and start the program with AI on that computer. When the program starts, it says: \"OK, buddy, I can make you immortal, but for this I need to know on what basis your body works. Provide me please access to your database. And you connect the device to the Internet, where it was gradually being developed and learns what it needs and peculiarities of human biology. (Here it is possible for it escape to the Internet, but we omit details since this is not the main point) Then the AAI says: \"I know how you become biologically immortal. It is necessary to replace every cell of your body with nanobiorobot. And fortunately, in the biology of your body there is almost nothing special that would block bio-immorality.. Many other organisms in the universe are also using DNA as a carrier of information. So I know how to program the DNA so as to create genetically modified bacteria that could perform the functions of any cell. I need access to the biological laboratory, where I can perform a few experiments, and it will cost you a million of your dollars.\" You rent a laboratory, hire several employees, and finally the AAI issues a table with its' solution of custom designed DNA, which are ordered in the laboratory by automated machine synthesis of DNA. http://en.wikipedia.org/wiki/DNA_sequencing Then they implant the DNA into yeast, and after several unsuccessful experiments they create a radio guided bacteria (shorthand: This is not truly a bacterium, since it appears all organelles and nucleus; also 'radio' is shorthand for remote controlled; a far more likely communication mechanism would be modulated sonic impulses) , which can synthesize a new DNA-based code based on commands from outside. Now the AAI has achieved independence from human 'filtering' of its' true commands, because the bacterium has in effect its own remote controlled sequencers (self-reproducing to boot!). Now the AAI can transform and synthesize substances ostensibly introduced into test tubes for a benign test, and use them for a malevolent purpose., Obviously, at this moment Alien AI is ready to launch an attack against humanity. He can transfer himself to the level of nano-computer so that the source computer can be disconnected. After that AAI spraying some of subordinate bacteria in the air, which also have AAI, and they gradually are spread across the planet, imperceptibly penetrates into all living beings, and then start by the timer to divide indefinitely, as gray goo, and destroy all living beings. Once they are destroyed, Alien AI can begin to build their own infrastructure for the transmission of radio messages into space. Obviously, this fictionalized scenario is not unique: for example, AAI may seize power over nuclear weapons, and compel people to build radio transmitters under the threat of attack. Because of possibly vast AAI experience and intelligence, he can choose the most appropriate way in any existing circumstances. (Added by Freidlander: Imagine a CIA or FSB like agency with equipment centuries into the future, introduced to a primitive culture without concept of remote scanning, codes, the entire fieldcraft of spying. Humanity might never know what hit it, because the AAI might be many centuries if not millennia better armed than we (in the sense of usable military inventions and techniques ).</p>\n<p>14. After that, this SETI-AI does not need people to realize any of its goals. This does not mean that it would seek to destroy them, but it may want to pre-empt if people will fight it - and they will.</p>\n<p>15. Then this SETI-AI can do a lot of things, but more importantly, that it should do - is to continue the transfer of its communications-generated-embryos to the rest of the Universe. To do so, he will probably turn the matter in the solar system in the same transmitter as the one that sent him. In doing so the Earth and its\u2019 people would be a disposable source of materials and parts\u2014possibly on a molecular scale.</p>\n<p>So, we examined a possible scenario of attack, which has 15 stages. Each of these stages is logically convincing and could be criticized and protected separately. Other attack scenarios are possible. For example, we may think that the message is not sent directly to us but is someone to someone else's correspondence and try to decipher it. And this will be, in fact, bait.</p>\n<p>But not only distribution of executable code can be dangerous. For example, we can receive some sort of \u201cuseful\u201d technology that really should lead us to disaster (for example, in the spirit of the message \"quickly shrink 10 kg of plutonium, and you will have a new source of energy\" ...but with planetary, not local consequences\u2026). Such a mailing could be done by a certain \"civilization\" in advance to destroy competitors in the space. It is obvious that those who receive such messages will primarily seek technology for military use.</p>\n<p><strong id=\"Analysis_of_possible_goals\">Analysis of possible goals</strong></p>\n<p>We now turn to the analysis of the purposes for which certain super civilizations could carry out such an attack.</p>\n<p>1. We must not confuse the concept of a super-civilization with the hope for superkindness of civilization. Advanced does not necessarily mean merciful. Moreover, we should not expect anything good from extraterrestrial \u2018kindness\u2019. This is well written in Strugatsky\u2019s novel \"Waves stop wind.\" Whatever the goal of imposing super-civilization upon us , we have to be their inferiors in capability and in civilizational robustness even if their intentions are well.. The historical example: The activities of Christian missionaries, destroying traditional religion. Moreover, we can better understand purely hostile objectives. And if the SETI attack succeeds, it may be only a prelude to doing us more \u2018favors\u2019 and \u2018upgrades\u2019 until there is scarcely anything <em>human</em> left of us even if we do survive\u2026</p>\n<p>2. We can divide all civilizations into the twin classes of naive and serious. Serious civilizations are aware of the SETI risks, and have got their own powerful AI, which can resist alien hacker attacks. Naive civilizations, like the present Earth, already possess the means of long-distance hearing in space and computers, but do not yet possess AI, and are not aware of the risks of AI-SETI. Probably every civilization has its stage of being \"naive\", and it is this phase then it is most vulnerable to SETI attack. And perhaps this phase is very short. Since the period of the outbreak and spread of radio telescopes to powerful computers that could create AI can be only a few tens of years. Therefore, the SETI attack must be set at such a civilization. This is not a pleasant thought, because we are among the vulnerable.</p>\n<p>3. If traveling with super-light speeds is not possible, the spread of civilization through SETI attacks is the fastest way to conquering space. At large distances, it will provide significant temporary gains compared with any kind of ships. Therefore, if two civilizations compete for mastery of space, the one that favored SETI attack will win.</p>\n<p>4. The most important thing is that <em>it is enough to begin a SETI attack just once</em>, as it goes in a self-replicating the wave throughout the Universe, striking more and more naive civilizations. For example, if we have a million harmless normal biological viruses and one dangerous, then once they get into the body, we will get trillions of copies of the dangerous virus, and still only a million safe viruses. In other words, it is enough that if one of billions of civilizations starts the process and then it becomes unstoppable throughout the Universe. Since it is almost at the speed of light, countermeasures will be almost impossible.</p>\n<p>5. Further, the delivery of SETI messages will be a priority for the virus that infected a civilization, and it will spend on it most of its energy, like a biological organism spends on reproduction - that is tens of percent. But Earth's civilization spends on SETI only a few tens of millions of dollars, that is about one millionth of our resources, and this proportion is unlikely to change much for the more advanced civilizations. In other words, an infected civilization will <em>produce a million times more SETI signals than a healthy one</em>. Or, to say in another way, if in the Galaxy are one million healthy civilizations, and one infected, then we will have equal chances to encounter a signal from healthy or contaminated.</p>\n<p>6. Moreover, there are no other reasonable prospects to distribute its code in space except through self-replication.</p>\n<p><a name=\"front\"></a></p>\n<p>7. Moreover, such a process could begin by accident - for example, in the beginning it was just a research project, which was intended to send the results of its (innocent) studies to the maternal civilization, not causing harm to the host civilization, then this process became \"cancer\" because of certain propogative faults or mutations.</p>\n<p>8. There is nothing unusual in such behavior. In any medium, there are viruses \u2013 there are viruses in biology, in computer networks - computer viruses, in conversation - meme. We do not ask why nature wanted to create a biological virus.</p>\n<p>9. Travel through SETI attacks is <em>much</em> cheaper than by any other means. Namely, a civilization in Andromeda can simultaneously send a signal to 100 billion stars in our galaxy. But each space ship would cost billions, and even if free, would be slower to reach all the stars of our Galaxy.</p>\n<p>10. Now we list several possible goals of a SETI attack, just to show the variety of motives.</p>\n<ul>\n<li>To study the universe. After executing the code research probes are created to gather survey and send back information.</li>\n<li>To ensure that there are no competing civilizations. All of their embryos are destroyed. This is preemptive war on an indiscriminate basis.</li>\n<li>To preempt the other competing supercivilization (yes, in this scenario there are two!) before it can take advantage of this resource.</li>\n<li>This is done in order to prepare a solid base for the arrival of spacecraft. This makes sense if super civilization is very far away, and consequently, the gap between the speed of light and near-light speeds of its ships (say, 0.5 c) gives a millennium difference.</li>\n<li>The goal is to achieve immortality. Carrigan showed that the amount of human personal memory is on the order of 2.5 gigabytes, so a few exabytes (1 exabyte = 1 073 741 824 gigabytes) forwarding the information can send the entire civilization. (You may adjust the units according to how big you like your super-civilizations!)</li>\n<li>Finally we consider illogical and incomprehensible (to us) purposes, for example, as a work of art, an act of self-expression or toys. Or perhaps an insane rivalry between two factions. Or something we simply cannot understand (For example, extraterrestrial will not understand why the Americans have stuck a flag into the Moon. Was it worthwhile to fly over 300000 km to install painted steel?)</li>\n</ul>\n<p>11. Assuming signals propagated billions of light years distant in the Universe, the area susceptible to widespread SETI attack, is a sphere with a radius of several billion light years. In other words, it would be sufficient to find a one \u201cbad civilization\" in the light cone of a height of several billion years old, that is, that includes billions of galaxies from which we are in danger of SETI attack. Of course, this is only true, if the average density of civilization is at least one in the galaxy. This is an interesting possibility in relation to Fermi\u2019s Paradox.</p>\n<p>16. As the depth of scanning the sky rises linearly, the volume of space and the number of stars that we see increases by the cube of that number. This means that our chances to stumble on a SETI signal nonlinear grow by fast curve.</p>\n<p>17. It is possible that when we stumble upon several different messages from the skies, which refute one another in a spirit of: \"<em>do not listen to them, they are deceiving voices, and wish you evil. But we, brother,</em> we, <em>are good\u2014and wise\u2026</em>\"</p>\n<p>18. Whatever positive and valuable message we receive, we can never be sure that all of this is not a subtle and deeply concealed threat. This means that in interstellar communication there will always be an element of distrust, and in every happy revelation, a gnawing suspicion.</p>\n<p>19. A defensive posture regarding interstellar communication is only to listen, not sending anything that does not reveal its location. The laws prohibit the sending of a message from the United States to the stars. Anyone in the Universe who sends (transmits) self-evidently- is not afraid to show his position. Perhaps because the sending (for the sender) is more important than personal safety. For example, because it plans to flush out prey prior to attacks. Or it is forced to, by a evil local AI.</p>\n<p>20. It was said about atomic bomb: The main secret about the atomic bomb is that it can be done. If prior to the discovery of a chain reaction Rutherford believed that the release of nuclear energy is an issue for the distant future, following the discovery any physicist knows that it is enough to connect two subcritical masses of fissionable material in order to release nuclear energy. In other words, if one day we find that signals can be received from space, it will be an irreversible event\u2014something analogous to a deadly new arms race will be on.</p>\n<p><strong>Objections</strong>.</p>\n<p>The discussions on the issue raise several typical objections, now discussed.</p>\n<p>Objection 1: Behavior discussed here is too anthropomorphic. In fact, civilizations are very different from each other, so you can\u2019t predict their behavior.</p>\n<p>Answer: Here we have a powerful observation selection effect. While a variety of possible civilizations exist, including such extreme scenarios as thinking oceans, etc., we can only receive radio signals from civilizations that send them, which means that they have corresponding radio equipment and has knowledge of materials, electronics and computing. That is to say we are threatened by civilizations of the same type as our own. Those civilizations, which can neither accept nor send radio messages, do not participate in this game.</p>\n<p>Also, an observation selection effect concerns purposes. Goals of civilizations can be very different, but all civilizations intensely sending signals, will be only that want to tell something to \u201ceveryone\". Finally, the observation selection relates to the effectiveness and universality of SETI virus. The more effective it is, the more different civilizations will catch it and the more copies of the SETI virus radio signals will be in heaven. So we have the \u2018excellent chances\u2019 to meet a most powerful and effective virus.</p>\n<p>Objection 2. For super-civilizations there is no need to resort to subterfuge. They can directly conquer us.</p>\n<p>Answer:</p>\n<p>This is true only if they are in close proximity to us. If movement faster than light is not possible, the impact of messages will be faster and cheaper. Perhaps this difference becomes important at intergalactic distances. Therefore, one should not fear the SETI attack from the nearest stars, coming within a radius of tens and hundreds of light-years.</p>\n<p>Objection 3. There are lots of reasons why SETI attack may not be possible. What is the point to run an ineffective attack?</p>\n<p>Answer: SETI attack does not always work. It must act in a sufficient number of cases in line with the objectives of civilization, which sends a message. For example, the con man or fraudster does not expect that he would be able \"to con\" every victim. He would be happy to steal from even one person inone hundred. It follows that SETI attack is useless if there is a goal to attack <em>all </em>civilizations in a certain galaxy. But if the goal is to get at least <em>some</em> outposts in another galaxy, the SETI attack fits. (Of course, these outposts can then build fleets of space ships to spread SETI attack bases outlying stars within the target galaxy.)</p>\n<p>The main assumption underlying the idea of SETI attacks is that extraterrestrial super civilizations exist in the visible universe at all. I think that this is unlikely for reasons related to antropic principle. Our universe is unique from 10 ** 500 possible universes with different physical properties, as suggested by one of the scenarios of string theory. My brain is 1 kg out of 10 ** 30 kg in the solar system. Similarly, I suppose, the Sun is no more than about 1 out of 10 ** 30 stars that could raise a intelligent life, <em>so it means that we are likely alone in the visible universe. </em></p>\n<p>Secondly the fact that Earth came so late (i.e. it could be here for a few billion years earlier), and it was not prevented by alien preemption from developing, argues for the rarity of intelligent life in the Universe. The putative rarity of our civilization is the best protection against attack SETI. On the other hand, if we open parallel worlds or super light speed communication, the problem arises again.</p>\n<p>Objection 7. Contact is impossible between post-singularity supercivilizations, which are supposed here to be the sender of SETI-signals, and pre- singularity civilization, which we are, because supercivilization is many orders of magnitude superior to us, and its message will be absolutely not understandable for us - exactly as the contact between ants and humans is not possible. (A singularity is the time of creation of artificial intelligence capable of learning, (and beginning an exponential booting in recursive improving self-design of further intelligence and much else besides) after which civilization make leap in its development - on Earth it may be possible in the area in 2030.)</p>\n<p>Answer: In the proposed scenario, we are<em> not </em>talking about contact but a purposeful deception of us. Similarly, a man is quite capable of manipulating behavior of ants and other social insects, whose objectives are is absolutely incomprehensible to them. For example, LJ user \u201civanov-petrov\u201d describes the following scene: As a student, he studied the behavior of bees in the Botanical Garden of Moscow State University. But he had bad relations with the security guard controlling the garden, which is regularly expelled him before his time. Ivanov-Petrov took the green board and developed in bees conditioned reflex to attack this board. The next time the watchman came, who constantly wore a green jersey, all the bees attacked him and he took to flight. So \u201civanov-petrov\u201d could continue research. Such manipulation is not a contact, but this does not prevent its\u2019 effectiveness.</p>\n<p><br> <br> \"Objection 8. For civilizations located near us is much easier to attack us \u2013for \u2018guaranteed results\u2019\u2014using starships than with SETI-attack.<br> <br> Answer. It may be that we significantly underestimate the complexity of an attack using starships and, in general, the complexity of interstellar travel. To list only one factor, the potential \u2018minefield\u2019 characteristics of the as-yet unknown interstellar medium.</p>\n<p>If such an attack would be carried out now or in the past, the Earth's civilization has nothing to oppose it, but in the future the situation will change - all matter in the solar system will be full of robots, and possibly completely processed by them. On the other hand, the more the speed of enemy starships approaching us, the more the fleet will be visible by its braking emissions and other characteristics. These quick starships would be very vulnerable, in addition we could prepare in advance for its arrival. A slowly moving nano- starship would be very less visible, but in the case of wishing to trigger a transformation of full substance of the solar system, it would simply be nowhere to land (at least without starting an alert in such a \u2018nanotech-settled\u2019 and fully used future solar system. (Friedlander added: Presumably there would always be some \u2018outer edge\u2019 of thinly settled Oort Cloud sort of matter, but by definition the rest of the system would be more densely settled, energy rich and any deeper penetration into solar space and its\u2019 conquest would be the proverbial uphill battle\u2014not in terms of gravity gradient, but in terms of the available resources of war against a full Class 2 Kardashev civilization.)</p>\n<p>The most serious objection is that an advanced civilization could in a few million years sow all our galaxy with self replicating post singularity nanobots that could achieve any goal in each target star-system, including easy prevention of the development of incipient other civilizations. (In the USA Frank Tipler advanced this line of reasoning.) However, this could not have happened in our case - no one has prevented development of our civilization. So, it would be much easier and more reliable to send out robots with such assignments, than bombardment of SETI messages of the entire galaxy, and if we don\u2019t see it, it means that no SETI attacks are inside our galaxy. (It is possible that a probe on the outskirts of the solar system expects manifestations of human space activity to attack \u2013 a variant of the \"Berserker\" hypothesis - but it will not attack through SETI). Probably for many millions or even billions of years microrobots could even reach from distant galaxies at a distance of tens of millions of light-years away. Radiation damage may limit this however without regular self-rebuilding.</p>\n<p>In this case SETI attack would be meaningful only at large distances. However, this distance - tens and hundreds of millions of light-years - probably will require innovative methods of modulation signals, such as management of the luminescence of active nuclei of galaxies. Or transfer a narrow beam in the direction of our galaxy (but they do not know where it will be over millions of years). But a civilization, which can manage its\u2019 galaxy\u2019s nucleus, might create a spaceship flying with near-light speeds, even if its mass is a mass of the planet. Such considerations severely reduce the likelihood of SETI attacks, but not lower it to zero, because we do not know all the possible objectives and circumstances.</p>\n<p><br> <em>(An comment by JF :</em>For example the lack of SETI-attack so far may <em>itself</em> be a cunning ploy: At first receipt of the developing Solar civilization\u2019s radio signals, all interstellar \u2018spam\u2019 would have ceased, (and interference stations of some unknown (but amazing) capability and type set up around the Solar System to block all coming signals recognizable to its\u2019 computers as of intelligent origin,) in order to get us \u2018lonely\u2019 and give us time to discover and appreciate the Fermi Paradox and even get those so philosophically inclined to despair desperate that this means the Universe is apparently hostile by some standards. Then, when desperate, we suddenly discover, slowly at first, partially at first, and then with more and more wonderful signals, the fact that space is filled with bright enticing signals (like spam). The blockade, cunning as it was (analogous to Earthly jamming stations) was yet a prelude to a slow \u2018turning up\u2019 of preplanned intriguing signal traffic. If as Earth had developed we had intercepted cunning spam followed by the agonized \u2018don\u2019t repeat our mistakes\u2019 final messages of tricked and dying civilizations, only a fool would heed the enticing voices of SETI spam. But now, a SETI attack may benefit from the slow unmasking of a cunning masquerade as first a faint and distant light of infinite wonder, only at the end revealed as the headlight of an onrushing cosmic train\u2026)</p>\n<p><em> AT comment to it. In fact I think that SETI attack senders are on the distances more than 1000 ly and so they do not know yet that we have appeared. But so called Fermi Paradox indeed maybe a trick \u2013 senders deliberately made their signals weak in order to make us think that they are not spam. </em><em></em></p>\n<p>The scale of space strategy may be inconceivable to the human mind.</p>\n<p><br> <br> And we should note in conclusion that some types of SETI-attack do not even need a computer but just a man who could understand the message that then \"set his mind on fire\". At the moment we cannot imagine such a message, but we can give some analogies. Western religions are built around the text of the Bible. It can be assumed that if the text of the Bible appeared in some countries, which had previously not been familiar with it, there might arise a certain number of biblical believers. Similarly subversive political literature, or even some superideas, \u201csticky\u201d memes or philosophical mind-benders. Or, as suggested by Hans Moravec, we get such a message: \"Now that you have received and decoded me, broadcast me in at least ten thousand directions with ten million watts of power. Or else.\" - this message is dropped, leaving us guessing, what may indicate that \"or else\". Even a few pages of text may contain a lot of subversive information - Imagine that we could send a message to the 19 th century scientists. We could open them to the general principle of the atomic bomb, the theory of relativity, the transistors - and thus completely change the course of technological history, and we could add that all the ills in the 20 century were from Germany (which is only partly true) , then we would have influenced the political history.</p>\n<p>(Comment of JF: Such a latter usage would depend on having received enough of Earth\u2019s transmissions to be able to model our behavior and politics. But imagine a message as posing from our own future, to ignite \u2018catalytic war\u2019\u2014Automated SIGINT (signals intelligence) stations are constructed monitoring our solar system, their computers \u2018cracking\u2019 our language and culture (possibly with the aid of children\u2019s television programs with see and say matching of letters and sounds, from TV news showing world maps and naming countries possibly even from intercepting wireless internet encyclopedia articles. ) Then a test or two may follow, posting a what if scenario inviting comment from bloggers, about a future war say between the two leading powers of the planet. (For purposes of this discussion, say around 2100 present calendar China is strongest and India rising fast). Any defects and nitpicks in the comments of the blog are noted and corrected. Finally, an actual interstellar message is sent with the debugged scenario(not shifting against the stellar background, it is unquestionably interstellar in origin) proporting to be from a dying starship of the presently stronger side\u2019s (China\u2019s) future, when the presently weaker side (India\u2019s) space fleet has smashed the future version of the Chinese State and essentially committed genocide. The starship has come back in time, but is dying, and indeed the transmission ends, or simply repeats, possibly after some back and forth communication between the false computer models of the \u2018starship commander\u2019 and the Chinese government. The reader can imagine the urgings of the future Chinese military council to preempt to forestall doom. If as seems probable, such a strategy is too complicated to carry off in one stage, various \u2018future travellers\u2019 may emerge from a war, signal for help in vain, and \u2018die\u2019 far outside our ability to reach them, (say some light days away, near the alleged location of an \u2018emergence gate\u2019 but near an actual transmitter) Quite a drama may emerge as the computer learns to \u2018play\u2019 us like a con man, ship after ship of various nationalities dribbling out stories but also getting answers to key questions for aid in constructing the emerging scenario which will be frighteningly believable, enough to ignite a final war. Possibly lists of key people in China (or whatever side is stronger) may be drawn up by the computer with a demand that they be executed as the parents of future war criminals\u2014sort of an International Criminal Court \u2013acting as Terminator scenario. Naturally the Chinese state, at that time the most powerful in the world, would guard its\u2019 rulers lives against any threat. Yet more refugee spaceships of various nationalities can emerge transmit and die, offering their own militaries terrifying new weapons technologies from unknown sciences that really work (more \u2018proof\u2019 of their future origin). Or weapons from known sciences, for example decoding online DNA sequences in the future internet and constructing formulae for DNA constructors to make specific tailored genetic weapons against particular populations\u2014that endure in the ground, a scorched earth against a particular population on a particular piece of land. These are copied and spread worldwide as are totally accurate plans\u2014in standard CNC codes for easy to construct thermonuclear weapons in the 1950s style\u2014using U-238 for casing, and only a few kilograms of fissionable material for ignition By that time well over a million tons of depleted uranium will be worldwide, and deuterium is free in the ocean and can be used directly in very large weapons without lithium deuteride. Knowing how to hack together a wasteful, more than critical mass crude fission device is one thing (the South African device was of this kind). But knowing \u2013with absolute accuracy, down to machining drawings, CNC codes, etc how to make high-yield, super efficient very dirty thermonuclear weapons <em>without need for testing</em> means that any small group with a few dozen million dollars and automated machine tools can clandestinely make a multi-megaton device \u2013or many\u2014 and smash the largest cities. And any small power with a few dozen jets can cripple a continent for a decade. Already over a thousand tons of plutonium exist. The SETI spam can include CNC codes for making a one shot reactor plutonium chemical refiner that would be left hopelessly radioactive but output chemically pure plutonium. (This would be prone to predetonation because of the Pu-240 content but then plans for debugged laser isotope separators may also be downloaded). This is a variant of the \u2018catalytic war\u2019 and \u2018nuclear six gun\u2019 (i.e. easy to obtain weapons) scenarios of the late Herman Kahn. Even cheaper would be bioattacks of the kind outlined above. The principle point is that planet killer weapons fully debugged take great amounts of debugging, tens to hundreds of billions of dollars, and free access to a world scientific community. Today, it is to every great power\u2019s advantage to keep accurate designs out of the hands of third parties because they have to live on the same planet (and because the fewer weapons, the easier it is to stay a great power). Not so the SETI spam authors. Without the hundreds of billions in R and D, the actual construction budget would be on the order of a million dollars per multi-megaton device (depending on the expense of obtaining the raw reactor plutonium) If wishing to extend today\u2019s scenarios into the future, the SETI spam authors manipulate Georgia (with about a $10 billion GDP) to arm against Russia and Taiwan against China and Venezuela against the USA. Although Russian and China and the USA could respectively promise annihilation against any attacker, with a military budget around 4% of GDP and the downloaded plans, the reverse\u2014for the first time\u2014could then also be true. (400 100 megaton bombs can kill by fallout perhaps 95% of unprotected populations over a country the size of the USA or China and 90% of a country the size of Russia, assuming the worst kind of cooperation from the winds.\u2014from an old chart by Ralph Lapp) Anyone living near a superarmed microstate with border conflicts will, of course, wish to arm themselves. And these newly armed states themselves\u2014of course\u2014will have borders. Note that this drawn out scenario gives lots of time for a huge arms buildup on both (or many!) sides, and a Second Cold War that eventually turns very hot indeed\u2026and unlike a human player of such a horrific \u2018catalytic war\u2019 con game, worldwide fallout or enduring biocontamination is not a concern at all\u2026 ()</p>\n<p><strong>Conclusion</strong>.</p>\n<p>The product of the probabilities of the following events describes the probability of attack. For these probabilities, we can only give so-called \u00abexpert\u00bb assessment, that is, assign them a certain a priori subjective probability as we do now.</p>\n<p>1) The likelihood that extraterrestrial civilizations exist at a distance at which radio communication is possible with them. In general, I agree with the view of Shklovsky and supporters of the \u201cRare Earth\u201d hypothesis - that the Earth's civilization is unique in the observable universe. This does not mean that extraterrestrial civilizations do not exist at all (because the universe, according to the theory of cosmological inflation, is almost endless) - they are just over the horizon of events visible from our point in space-time. In addition, this is not just about distance, but also of the distance at which you can establish a connection, which allows transferring gigabytes of information. (However, passing even 1 bit per second, you can submit 1-gigabit for about 20 years, which may be sufficient for the SETI-attack.) If in the future will be possible some superluminal communication or interaction with parallel universes, it would dramatically increase the chances of SETI attacks. So, I appreciate this chance to 10%.</p>\n<p>2) The probability that SETI-attack is technically feasible: that is, it is possible computer program, with recursively self-improving AI and sizes suitable for shipping. I see this chance as high: 90%.</p>\n<p>3) The likelihood that civilizations that could have carried out such attack exist in our space-time cone - this probability depends on the density of civilizations in the universe, and of whether the percentage of civilizations that choose to initiate such an attack, or, more importantly, obtain victims and become repeaters. In addition, it is necessary to take into account not only the density of civilizations, but also the density created by radio signals. All these factors are highly uncertain. It is therefore reasonable to assign this probability to 50%.</p>\n<p>4) The probability that we find such a signal during our rising civilization\u2019s period of vulnerability to it. The period of vulnerability lasts from now until the moment when we will decide and be technically ready to implement this decision: <em>Do not download any extraterrestrial computer programs under any circumstances.</em> Such a decision may only be exercised by our AI, installed as world ruler (which in itself is fraught with considerable risk). Such an world AI (WAI) can be in created circa 2030. We cannot exclude, however, that our WAI still will not impose a ban on the intake of extraterrestrial messages, and fall victim to attacks by the alien artificial intelligence, which by millions of years of machine evolution surpasses it. Thus, the window of vulnerability is most likely about 20 years, and \u201cwidth\u201d of the window depends on the intensity of searches in the coming years. This \u201cwidth\u201d for example, depends on the intensity of the current economic crisis of 2008-2010, from the risks of World War III, and how all this will affect the emergence of the WAI. It also depends on the density of infected civilizations and their signal strength\u2014 as these factors increase, the more chances to detect them earlier. Because we are a normal civilization under normal conditions, according to the principle of Copernicus, the probability should be large enough; otherwise a SETI-attack would have been generally ineffective. (The SETI-attack, itself (here supposed to exist) also are subject to a form of \u201cnatural selection\u201d to test its effectiveness. (In the sense that it works or does not. ) This is a very uncertain chance we will too, over 50%.</p>\n<p>5) Next is the probability that SETI-attack will be successful - that is that we swallow the bait, download the program and description of the computer, run them, lose control over them and let them reach all their goals. I appreciate this chance to be very high because of the factor of multiplicity - that is the fact that the message is downloaded repeatedly, and someone, sooner or later, will start it. In addition, through natural selection, most likely we will get the most effective and deadly message that will most effectively deceive our type of civilization. I consider it to be 90%.</p>\n<p>6) Finally, it is necessary to assess the probability that SETI-attack will lead to a complete human extinction. On the one hand, it is possible to imagine a \u201cgood\u201d SETI-attack, which is limited so that it will create a powerful radio emitter behind the orbit of Pluto. However, for such a program will always exist the risk that a possible emergent society at its\u2019 target star will create a powerful artificial intelligence, and effective weapon that would destroy this emitter. In addition, to create the most powerful transponder would be needed all the substance of solar system and the entire solar energy. Consequently, the share of such \u201cgood\u201d attacks will be lower due to natural selection, as well as some of them will be destroyed sooner or later by captured by them civilizations and their signals will be weaker. So the chances of destroying all the people with the help of SETI-attack that has reached all its goals, I appreciate in 80%.</p>\n<p>As a result, we have: 0.1h0.9h0.5h0.5h0.9h0.8 = 1.62%</p>\n<p><em> </em></p>\n<p>So, after rounding, the chances of extinction of Man through SETI attack in XXI century is around 1 per cent with a theoretical precision of an order of magnitude.</p>\n<p>Our best protection in this context would be that civilization would very rarely met in the Universe. But this is not quite right, because the Fermi paradox here works on the principle of \"Neither alternative is good\":</p>\n<ul>\n<li>If there <em>are </em>extraterrestrial civilizations, and there are many of them, it is dangerous because they can threaten us in one way or another.</li>\n<li>If extraterrestrial civilizations <em>do not</em> exist, it is also bad, because it gives weight to the hypothesis of inevitable extinction of technological civilizations or of our underestimating of frequency of cosmological catastrophes. Or, a high density of space hazards, such as gamma-ray bursts and asteroids that we underestimate because of the observation selection effect\u2014i.e., were we not here because already killed, we would not be making these observations\u2026.</li>\n</ul>\n<p>Theoretically possible is a reverse option, which is that through SETI will come a warning message about a certain threat, which has destroyed most civilizations, such as: \"Do not do any experiments with X particles, it could lead to an explosion that would destroy the planet.\" But even in that case remain a doubt, that there is no deception to deprive us of certain technologies. (Proof would be if similar reports came from other civilizations in space in the opposite direction.) But such communication may only enhance the temptation to experiment with X-particles.</p>\n<p>So I do not appeal to abandon SETI searches, although such appeals are useless.</p>\n<p>It may be useful to postpone any technical realization of the messages that we could get on SETI, up until the time when we will have our Artificial Intelligence. Until that moment, perhaps, is only 10-30 years, that is, we could wait. Secondly, it would be important to hide the fact of receiving dangerous SETI signal its essence and the source location.</p>\n<p>This risk is related to a methodologically interesting aspect. Despite the fact that I have thought every day in the last year and read on the topic of global risks, I found this dangerous vulnerability in SETI only now. By hindsight, I was able to find another four authors who came to similar conclusions. However, I have made a significant finding: that there may be not yet open global risks, and even if the risk of certain constituent parts are separately known to me, it may take a long time to join them into a coherent picture. Thus, hundreds of dangerous vulnerabilities may surround us, like an unknown minefield. Only when the first explosion happens will we know. And that first explosion may be the last.</p>\n<p>An interesting question is whether Earth itself could become a source of SETI-attack in the future when we will have our own AI. Obviously, that could. Already in the program of METI exists an idea to send the code of human DNA. (The \u201cchildren's message scenario\u201d \u2013 in which the children ask to take their piece of DNA and clone them on another planet \u2013as depicted in the film \u201cCalling all aliens\u201d.)</p>\n<p><strong>Literature</strong>:</p>\n<p><em>1. Hoyle F. </em>Andromeda. http://en.wikipedia.org/wiki/A_for_Andromeda</p>\n<p><em>2. Yudkowsky E. </em><em>Artificial Intelligence as a Positive and Negative Factor in Global Risk. </em>Forthcoming in Global Catastrophic Risks, eds. Nick Bostrom and Milan Cirkovic <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">http://www.singinst.org/upload/artificial-intelligence-risk.pdf</a></p>\n<p><em>3.Moravec Hans</em>. Mind Children: The Future of Robot and Human Intelligence, 1988.</p>\n<p><em>4.Carrigan</em>, <em>Jr.</em><em> </em><em>Richard A.</em> The Ultimate Hacker: SETI signals may need to be decontaminated <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf\">http://home.fnal.gov/~carrigan/SETI/SETI%20Decon%20Australia%20poster%20paper.pdf</a></p>\n<p>5. <em>Carrigan\u2019s</em> page <a href=\"http://home.fnal.gov/~carrigan/SETI/SETI_Hacker.htm\">http://home.fnal.gov/~carrigan/SETI/SETI_Hacker.htm</a></p>", "sections": [{"title": "Alexei Turchin. Risks of downloading alien AI via SETI search", "anchor": "Alexei_Turchin__Risks_of_downloading_alien_AI_via_SETI_search", "level": 1}, {"title": "Algorithm of SETI attack", "anchor": "Algorithm_of_SETI_attack", "level": 1}, {"title": "Analysis of possible goals", "anchor": "Analysis_of_possible_goals", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "99 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T15:46:26.708Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Cambridge UK, Durham, London, Vienna", "slug": "weekly-lw-meetups-cambridge-uk-durham-london-vienna", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bf6h7BLtjHiNwzmMS/weekly-lw-meetups-cambridge-uk-durham-london-vienna", "pageUrlRelative": "/posts/Bf6h7BLtjHiNwzmMS/weekly-lw-meetups-cambridge-uk-durham-london-vienna", "linkUrl": "https://www.lesswrong.com/posts/Bf6h7BLtjHiNwzmMS/weekly-lw-meetups-cambridge-uk-durham-london-vienna", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Durham%2C%20London%2C%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Durham%2C%20London%2C%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBf6h7BLtjHiNwzmMS%2Fweekly-lw-meetups-cambridge-uk-durham-london-vienna%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Durham%2C%20London%2C%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBf6h7BLtjHiNwzmMS%2Fweekly-lw-meetups-cambridge-uk-durham-london-vienna", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBf6h7BLtjHiNwzmMS%2Fweekly-lw-meetups-cambridge-uk-durham-london-vienna", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p><strong>This summary was posted to LW main on March 8th. The following week's summary&nbsp; is <a href=\"/lw/gzw/weekly_lw_meetups_austin_brussels_cambridge_uk/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/k4\">Durham/RTLW HPMoR discussion, ch. 43-46:&nbsp;<span class=\"date\">09 March 2013 11:30AM</span></a></li>\n<li><a href=\"/meetups/j8\">Vienna Meetup 9th March:&nbsp;<span class=\"date\">09 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/jx\">London Meetup, 10th March:&nbsp;<span class=\"date\">10 March 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/jq\">Brussels meetup:&nbsp;<span class=\"date\">16 March 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/jp\">Munich Meetup:&nbsp;<span class=\"date\">01 April 2013 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/k7\">(Cambridge UK) [Reading Group, HAEFB-05] Sunday 10th March, 11am, Trinity JCR:&nbsp;<span class=\"date\">10 March 2013 11:00AM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bf6h7BLtjHiNwzmMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1393450330563516e-06, "legacy": true, "legacyId": "21952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zEoyFSLyQyHJ5pbCd", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T18:28:10.697Z", "modifiedAt": null, "url": null, "title": "Programming the LW Study Hall", "slug": "programming-the-lw-study-hall", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:38.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gcsnPPC3BmvzFDtEE/programming-the-lw-study-hall", "pageUrlRelative": "/posts/gcsnPPC3BmvzFDtEE/programming-the-lw-study-hall", "linkUrl": "https://www.lesswrong.com/posts/gcsnPPC3BmvzFDtEE/programming-the-lw-study-hall", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Programming%20the%20LW%20Study%20Hall&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProgramming%20the%20LW%20Study%20Hall%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgcsnPPC3BmvzFDtEE%2Fprogramming-the-lw-study-hall%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Programming%20the%20LW%20Study%20Hall%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgcsnPPC3BmvzFDtEE%2Fprogramming-the-lw-study-hall", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgcsnPPC3BmvzFDtEE%2Fprogramming-the-lw-study-hall", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 513, "htmlBody": "<p><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We've had considerable interest and uptake on the <a href=\"/lw/gwo/coworking_collaboration_to_combat_akrasia/\">Less Wrong Study Hall</a>, especially with informal timed Pomodoro sessions for everyone to synchronize on. &nbsp;Working together with a number of other visible faces, and your own face visible to them, does seem effective. &nbsp;Keeping the social chat to the 5 off minutes prevents this from turning into just another chatroom.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We've been using this <a href=\"http://tinychat.com/lesswrong\">Tinychat room</a>, and implementing everything Pomodoro-related with manual typing. &nbsp;Is there anyone out there who's interested in taking this to the next level with some custom code, possibly via the Google Hangouts API (Javascript), so we can have the following nice features?<a id=\"more\"></a><br /></span></strong></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; white-space: normal; font-weight: normal;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Synchronized, software-implemented Pomodoros for everyone.</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Maybe one chat room with 20/5 and one with 45/5.</span></li>\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Actual enforcement of the \"no chatting unless the Pomodoro is on\" and/or muted microphones.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Chat rooms with an N-person limit (several people report being more productive in smaller groups, so we're not sure if N should be 5 or 10) and new chat rooms being spawned as earlier ones get full.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Moderatability (we've already had one troll). &nbsp;One person suggested the ability to +1/-1 one person per day, with a kick at -5. &nbsp;(Eliezer remarks that he expects this to be completely ineffective and that you need actual mods, maybe a group of trusted users.)</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We only wish and dream that we could integrate with LW logins, but this would require LW development resources that apparently don't exist. &nbsp;Maybe with enough grotesque hackery we could have a page somewhere that you comment to confirm you're a Study Hall user, and the system could look up your karma from there to decide if you can cast +1/-1 user votes.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A custom page layout (which you don't get if you use Tinychat!) which has the watching participants lined up vertically on the left, so we can work on something while still easily seeing our friends.</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A small line underneath everyone's image saying what they're currently working on.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Maybe a common room where people can initially talk about what they intend to work on. &nbsp;(Eliezer says: &nbsp;This needs either strong group norms or built-in limits on talk time to avoid becoming a social chat timesink.)</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The ability to branch off small sub-chat rooms (maybe with limit 2 or 3) in case somebody wants to talk (about work!)</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A welcome page (</span><a href=\"https://docs.google.com/document/d/1EjmaQ4SY1aXfbRfSxmiztwEZaabL0lSyNRAJdwh4GdM/edit?usp=sharing\"><span style=\"color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">mockup</span></a><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">) where people see the group norms the first time they visit the Study Hall that serves as a portal.</span></li>\n</strong></strong> \n</ul>\n<p><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; white-space: normal; font-weight: normal;\"> </strong></strong></p>\n<p><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">This doesn't \"seem\" very complicated from a programming perspective (yes, we all know about things that don't seem complicated). The Google Hangouts API (possibly OpenMeetings) seems like it should provide almost all of the basics already. &nbsp;But unless some particular programmer steps up to do it, it won't get done. If interested, comment below or email shannon.friedman@positivevector.com, and please mention your relevant Javascript experience.</span></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "HFou6RHqFagkyrKkW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gcsnPPC3BmvzFDtEE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 37, "extendedScore": null, "score": 1.1394520564612535e-06, "legacy": true, "legacyId": "22018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"We_ve_had_considerable_interest_and_uptake_on_the_Less_Wrong_Study_Hall__especially_with_informal_timed_Pomodoro_sessions_for_everyone_to_synchronize_on___Working_together_with_a_number_of_other_visible_faces__and_your_own_face_visible_to_them__does_seem_effective___Keeping_the_social_chat_to_the_5_off_minutes_prevents_this_from_turning_into_just_another_chatroom_We_ve_been_using_this_Tinychat_room__and_implementing_everything_Pomodoro_related_with_manual_typing___Is_there_anyone_out_there_who_s_interested_in_taking_this_to_the_next_level_with_some_custom_code__possibly_via_the_Google_Hangouts_API__Javascript___so_we_can_have_the_following_nice_features_\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We've had considerable interest and uptake on the <a href=\"/lw/gwo/coworking_collaboration_to_combat_akrasia/\">Less Wrong Study Hall</a>, especially with informal timed Pomodoro sessions for everyone to synchronize on. &nbsp;Working together with a number of other visible faces, and your own face visible to them, does seem effective. &nbsp;Keeping the social chat to the 5 off minutes prevents this from turning into just another chatroom.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We've been using this <a href=\"http://tinychat.com/lesswrong\">Tinychat room</a>, and implementing everything Pomodoro-related with manual typing. &nbsp;Is there anyone out there who's interested in taking this to the next level with some custom code, possibly via the Google Hangouts API (Javascript), so we can have the following nice features?<a id=\"more\"></a><br></span></strong></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; white-space: normal; font-weight: normal;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Synchronized, software-implemented Pomodoros for everyone.</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Maybe one chat room with 20/5 and one with 45/5.</span></li>\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Actual enforcement of the \"no chatting unless the Pomodoro is on\" and/or muted microphones.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Chat rooms with an N-person limit (several people report being more productive in smaller groups, so we're not sure if N should be 5 or 10) and new chat rooms being spawned as earlier ones get full.</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Moderatability (we've already had one troll). &nbsp;One person suggested the ability to +1/-1 one person per day, with a kick at -5. &nbsp;(Eliezer remarks that he expects this to be completely ineffective and that you need actual mods, maybe a group of trusted users.)</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">We only wish and dream that we could integrate with LW logins, but this would require LW development resources that apparently don't exist. &nbsp;Maybe with enough grotesque hackery we could have a page somewhere that you comment to confirm you're a Study Hall user, and the system could look up your karma from there to decide if you can cast +1/-1 user votes.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A custom page layout (which you don't get if you use Tinychat!) which has the watching participants lined up vertically on the left, so we can work on something while still easily seeing our friends.</span> \n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: circle; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A small line underneath everyone's image saying what they're currently working on.</span></li>\n</ul>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Maybe a common room where people can initially talk about what they intend to work on. &nbsp;(Eliezer says: &nbsp;This needs either strong group norms or built-in limits on talk time to avoid becoming a social chat timesink.)</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">The ability to branch off small sub-chat rooms (maybe with limit 2 or 3) in case somebody wants to talk (about work!)</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\" dir=\"ltr\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">A welcome page (</span><a href=\"https://docs.google.com/document/d/1EjmaQ4SY1aXfbRfSxmiztwEZaabL0lSyNRAJdwh4GdM/edit?usp=sharing\"><span style=\"color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">mockup</span></a><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">) where people see the group norms the first time they visit the Study Hall that serves as a portal.</span></li>\n</strong></strong> \n</ul>\n<p><strong style=\"font-family: Times; font-size: medium; font-weight: normal;\"><strong id=\"internal-source-marker_0.8469080687500536\" style=\"font-family: Times; font-size: medium; white-space: normal; font-weight: normal;\"> </strong></strong></p>\n<p><strong id=\"This_doesn_t__seem__very_complicated_from_a_programming_perspective__yes__we_all_know_about_things_that_don_t_seem_complicated___The_Google_Hangouts_API__possibly_OpenMeetings__seems_like_it_should_provide_almost_all_of_the_basics_already___But_unless_some_particular_programmer_steps_up_to_do_it__it_won_t_get_done__If_interested__comment_below_or_email_shannon_friedman_positivevector_com__and_please_mention_your_relevant_Javascript_experience_\" style=\"font-family: Times; font-size: medium; font-weight: normal;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">This doesn't \"seem\" very complicated from a programming perspective (yes, we all know about things that don't seem complicated). The Google Hangouts API (possibly OpenMeetings) seems like it should provide almost all of the basics already. &nbsp;But unless some particular programmer steps up to do it, it won't get done. If interested, comment below or email shannon.friedman@positivevector.com, and please mention your relevant Javascript experience.</span></strong></p>", "sections": [{"title": "We've had considerable interest and uptake on the Less Wrong Study Hall, especially with informal timed Pomodoro sessions for everyone to synchronize on. \u00a0Working together with a number of other visible faces, and your own face visible to them, does seem effective. \u00a0Keeping the social chat to the 5 off minutes prevents this from turning into just another chatroom.We've been using this Tinychat room, and implementing everything Pomodoro-related with manual typing. \u00a0Is there anyone out there who's interested in taking this to the next level with some custom code, possibly via the Google Hangouts API (Javascript), so we can have the following nice features?", "anchor": "We_ve_had_considerable_interest_and_uptake_on_the_Less_Wrong_Study_Hall__especially_with_informal_timed_Pomodoro_sessions_for_everyone_to_synchronize_on___Working_together_with_a_number_of_other_visible_faces__and_your_own_face_visible_to_them__does_seem_effective___Keeping_the_social_chat_to_the_5_off_minutes_prevents_this_from_turning_into_just_another_chatroom_We_ve_been_using_this_Tinychat_room__and_implementing_everything_Pomodoro_related_with_manual_typing___Is_there_anyone_out_there_who_s_interested_in_taking_this_to_the_next_level_with_some_custom_code__possibly_via_the_Google_Hangouts_API__Javascript___so_we_can_have_the_following_nice_features_", "level": 1}, {"title": "This doesn't \"seem\" very complicated from a programming perspective (yes, we all know about things that don't seem complicated). The Google Hangouts API (possibly OpenMeetings) seems like it should provide almost all of the basics already. \u00a0But unless some particular programmer steps up to do it, it won't get done. If interested, comment below or email shannon.friedman@positivevector.com, and please mention your relevant Javascript experience.", "anchor": "This_doesn_t__seem__very_complicated_from_a_programming_perspective__yes__we_all_know_about_things_that_don_t_seem_complicated___The_Google_Hangouts_API__possibly_OpenMeetings__seems_like_it_should_provide_almost_all_of_the_basics_already___But_unless_some_particular_programmer_steps_up_to_do_it__it_won_t_get_done__If_interested__comment_below_or_email_shannon_friedman_positivevector_com__and_please_mention_your_relevant_Javascript_experience_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "56 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zq3Dey8ZboSby6YAv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-15T20:06:05.570Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin social", "slug": "meetup-berlin-social", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dc83suX3GzN2Dbv6e/meetup-berlin-social", "pageUrlRelative": "/posts/Dc83suX3GzN2Dbv6e/meetup-berlin-social", "linkUrl": "https://www.lesswrong.com/posts/Dc83suX3GzN2Dbv6e/meetup-berlin-social", "postedAtFormatted": "Friday, March 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20social&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20social%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc83suX3GzN2Dbv6e%2Fmeetup-berlin-social%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20social%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc83suX3GzN2Dbv6e%2Fmeetup-berlin-social", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc83suX3GzN2Dbv6e%2Fmeetup-berlin-social", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kh'>Berlin social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 March 2013 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">S Wuhletal, Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at my house again for a meetup on Saturday, March 23rd. Everyone is welcome. Please check the <a href=\"https://groups.google.com/d/msg/lw-berlin/S867GvhYj4Q/7VA0LbCxAPIJ\" rel=\"nofollow\">thread on our mailing list</a> for details and the precise location.</p>\n\n<p>Primarily, this is a social event where we talk and play games. There's one extra activity on the agenda:</p>\n\n<ul>\n<li>Everyone shares one goal. No matter whether it's something big or a short-term thing. State it, explain why, have a 2-minute pause to think, talk about how to achieve it as well as about alternatives for getting the same effect a different way.</li>\n</ul>\n\n<p>Food worked out really well last time, we'll try that again unchanged. That means I'll make some soup, we will probably order pizza and you are encouraged to bring something tasty. It's perfectly ok to bring just a little, last time there was a lot left over.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kh'>Berlin social</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dc83suX3GzN2Dbv6e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1395168579266457e-06, "legacy": true, "legacyId": "22029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_social\">Discussion article for the meetup : <a href=\"/meetups/kh\">Berlin social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 March 2013 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">S Wuhletal, Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at my house again for a meetup on Saturday, March 23rd. Everyone is welcome. Please check the <a href=\"https://groups.google.com/d/msg/lw-berlin/S867GvhYj4Q/7VA0LbCxAPIJ\" rel=\"nofollow\">thread on our mailing list</a> for details and the precise location.</p>\n\n<p>Primarily, this is a social event where we talk and play games. There's one extra activity on the agenda:</p>\n\n<ul>\n<li>Everyone shares one goal. No matter whether it's something big or a short-term thing. State it, explain why, have a 2-minute pause to think, talk about how to achieve it as well as about alternatives for getting the same effect a different way.</li>\n</ul>\n\n<p>Food worked out really well last time, we'll try that again unchanged. That means I'll make some soup, we will probably order pizza and you are encouraged to bring something tasty. It's perfectly ok to bring just a little, last time there was a lot left over.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_social1\">Discussion article for the meetup : <a href=\"/meetups/kh\">Berlin social</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin social", "anchor": "Discussion_article_for_the_meetup___Berlin_social", "level": 1}, {"title": "Discussion article for the meetup : Berlin social", "anchor": "Discussion_article_for_the_meetup___Berlin_social1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-16T05:58:43.062Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Belief in Self-Deception", "slug": "seq-rerun-belief-in-self-deception", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:07.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KKoWagaAp8XaWCw3P/seq-rerun-belief-in-self-deception", "pageUrlRelative": "/posts/KKoWagaAp8XaWCw3P/seq-rerun-belief-in-self-deception", "linkUrl": "https://www.lesswrong.com/posts/KKoWagaAp8XaWCw3P/seq-rerun-belief-in-self-deception", "postedAtFormatted": "Saturday, March 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Belief%20in%20Self-Deception&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Belief%20in%20Self-Deception%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKoWagaAp8XaWCw3P%2Fseq-rerun-belief-in-self-deception%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Belief%20in%20Self-Deception%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKoWagaAp8XaWCw3P%2Fseq-rerun-belief-in-self-deception", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKoWagaAp8XaWCw3P%2Fseq-rerun-belief-in-self-deception", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/s/belief_in_selfdeception/\">Belief in Self-Deception</a> was originally published on 05 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Deceiving yourself is harder than it seems. What looks like a successively adopted false belief may actually be just a belief in false belief.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gzr/seq_rerun_no_really_ive_deceived_myself/\">No, Really, I've Deceived Myself</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KKoWagaAp8XaWCw3P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 1.1399092059983705e-06, "legacy": true, "legacyId": "22036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wP2ymm44kZZwaFPYh", "9d4jQWeZY3thjdeYH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-17T08:50:02.096Z", "modifiedAt": null, "url": null, "title": "Bayesian Adjustment Does Not Defeat Existential Risk Charity", "slug": "bayesian-adjustment-does-not-defeat-existential-risk-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:39.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JyH7ezruQbC2iWcSg/bayesian-adjustment-does-not-defeat-existential-risk-charity", "pageUrlRelative": "/posts/JyH7ezruQbC2iWcSg/bayesian-adjustment-does-not-defeat-existential-risk-charity", "linkUrl": "https://www.lesswrong.com/posts/JyH7ezruQbC2iWcSg/bayesian-adjustment-does-not-defeat-existential-risk-charity", "postedAtFormatted": "Sunday, March 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Adjustment%20Does%20Not%20Defeat%20Existential%20Risk%20Charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Adjustment%20Does%20Not%20Defeat%20Existential%20Risk%20Charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyH7ezruQbC2iWcSg%2Fbayesian-adjustment-does-not-defeat-existential-risk-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Adjustment%20Does%20Not%20Defeat%20Existential%20Risk%20Charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyH7ezruQbC2iWcSg%2Fbayesian-adjustment-does-not-defeat-existential-risk-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyH7ezruQbC2iWcSg%2Fbayesian-adjustment-does-not-defeat-existential-risk-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10161, "htmlBody": "<p>(This is a long post. If you&rsquo;re going to read only part, please read sections 1 and 2, subsubsection 5.6.2, and the conclusion.)</p>\n<h3>1. Introduction</h3>\n<p>Suppose you want to give some money to charity: where can you get the most bang for your philanthropic buck? One way to make the decision is to use explicit expected value estimates. That is, you could get an unbiased (averaging to the true value) estimate of what each candidate for your donation would do with an additional dollar, and then pick the charity associated with the most promising estimate.</p>\n<p>Holden Karnofsky of <a href=\"http://www.givewell.org/\">GiveWell</a>, an organization that rates charities for cost-effectiveness, disagreed with this approach in two posts he made in 2011. This is a response to those posts, addressing the implications for existential risk efforts.</p>\n<p>According to Karnofsky, high returns are rare, and even unbiased estimates don&rsquo;t take into account the reasons <em>why</em> they&rsquo;re rare. So in Karnofsky's view, our favorite charity shouldn&rsquo;t just be one associated with a high estimate, it should be one that supports the estimate with robust evidence derived from multiple independent lines of inquiry.<sup>1</sup> If a charity&rsquo;s returns are being estimated in a way that intuitively feels shaky, maybe that means the fact that high returns are rare should outweigh the fact that high returns were estimated, even if the people making the estimate were doing an excellent job of avoiding bias.</p>\n<p>Karnofsky&rsquo;s first post, <a href=\"/lw/745/why_we_cant_take_expected_value_estimates\">Why We Can&rsquo;t Take Expected Value Estimates Literally (Even When They&rsquo;re Unbiased)</a>, explains how one can mitigate this issue by supplementing an explicit estimate with what Karnofsky calls a &ldquo;Bayesian Adjustment&rdquo; (henceforth &ldquo;BA&rdquo;). This method treats estimates as merely noisy measures of true values. BA starts with a prior representing what cost-effectiveness values are out there in the general population of charities, then the prior is updated into a posterior in standard Bayesian fashion.</p>\n<p>Karnofsky provides some example graphs, illustrating his preference for robustness. If the estimate error is small, the posterior lies close to the explicit estimate. But if the estimate error is large, the posterior lies close to the prior. In other words, if there simply aren&rsquo;t many high-return charities out there, a sharp estimate can be taken seriously, but a noisy estimate that says it has found a high-return charity must represent some sort of fluke.</p>\n<p>Karnofsky does not advocate a policy of performing an <em>explicit</em> adjustment. Rather, he uses BA to emphasize that estimates are likely to be inadequate if they don&rsquo;t incorporate certain kinds of intuitions &mdash; in particular, a sense of whether all the components of an estimation procedure feel reliable. If intuitions say an estimate feels shaky and too good to be true, then maybe the estimate was noisy and the prior is more important. On the other hand, if intuitions say an estimate has taken everything into account, then maybe the estimate was sharp and outweighs the prior.</p>\n<p>Karnofsky&rsquo;s second post, <a href=\"/lw/8di/maximizing_costeffectiveness_via_critical_inquiry/\">Maximizing Cost-Effectiveness Via Critical Inquiry</a>, expands on these points. Where the first post looks at how BA is performed on a single charity at a time, the second post examines how BA affects the estimated relative values of different charities. In particular, it assumes that although the charities are all drawn from the same prior, they come with different estimates of cost-effectiveness. Higher estimates of cost-effectiveness come from estimation procedures with proportionally higher uncertainty.</p>\n<p>It turns out that higher estimates aren&rsquo;t always more auspicious: an estimate may be &ldquo;too good to be true,&rdquo; concentrating much of its evidential support on values that the prior already rules out for the most part. On the bright side, this effect can be mitigated via multiple independent observations, and such observations can provide enough evidence to solidify higher estimates despite their low prior probability.</p>\n<p>Charities aiming to reduce existential risk have a potential claim to high expected returns, simply because of the size of the stakes. But if such charities are difficult to evaluate, and the prior probability of high expected values is low, then the implications of BA for this class of charities loom large.</p>\n<p>This post will argue that competent efforts to reduce existential risk reduction are still likely to be optimal, despite BA. The argument will have three parts:</p>\n<ol>\n<li>\n<p>BA differs from fully Bayesian reasoning, so that BA risks double-counting priors.</p>\n</li>\n<li>\n<p>The models in Karnofsky&rsquo;s posts, when applied to existential risk, boil down to our having prior knowledge that the claimed returns are virtually impossible. (Moreover, similar models without extreme priors don&rsquo;t lead to the same conclusions.)</p>\n</li>\n<li>\n<p>We don&rsquo;t have such prior knowledge. Extreme priors would have implied false predictions in the past, imply unphysical predictions for the future, and are justified neither by our past experiences nor by any other considerations.</p>\n</li>\n</ol>\n<p>Claim 1 is not essential to the conclusion. While Claim 2 seems worth expanding on, it&rsquo;s Claim 3 that makes up the core of the controversy. Each of these concerns will be addressed in turn.</p>\n<p>Before responding to the claims themselves, however, it&rsquo;s worth discussing a highly simplified model that will illustrate what Karnofsky&rsquo;s basic point is.</p>\n<p><a id=\"more\"></a></p>\n<h3>2. A Simple Discrete Distribution of Charitable Returns</h3>\n<p>Suppose you&rsquo;re considering a donation to the Center for Inventing Metawidgets (CIM), but you'd like to perform an analysis of the properties of metawidgets first.<sup>2</sup> Before the analysis, you&rsquo;re uncertain about three possibilities:</p>\n<ul>\n<li>With a probability of 4999 out of 10,000, metawidgets aren&rsquo;t even a thing. You can&rsquo;t invent what isn&rsquo;t a thing, so the return is 0. </li>\n<li>With a probability of 5000 out of 10,000, metawidgets are a thing with some reasonably good use, like repairing printers. The return in this case is 1. </li>\n<li>With a probability of 1 out of 10,000, metawidgets have extremely useful effects, like curing lung cancer. Then the return is 100.</li>\n</ul>\n<p>If we now compute the expected value of a donation to CIM, it ends up as a sum of the following components:</p>\n<ul>\n<li>0.4999 * 0 = <strong>0</strong> from the possibility that the return is 0 </li>\n<li>0.5 * 1 = <strong>0.5</strong> from the possibility that the return is 1 </li>\n<li>0.0001 * 100 = <strong>0.01</strong> from the possibility that the return is 100 </li>\n</ul>\n<p>In particular, the possibility of a modest return contributes 50 times the expected value of the possibility of an extreme return. The size of the potential return, in this case, didn&rsquo;t make up for its low probability.</p>\n<p>But that&rsquo;s before you do an analysis that will give you some additional evidence about metawidgets. The analysis has the following properties:</p>\n<ul>\n<li>Whatever the true return is, 50% of the time the analysis is correct and gives you the correct answer. </li>\n<li>If the analysis is wrong, it picks one of the three possible answers uniformly at random. </li>\n</ul>\n<p>What happens if the analysis says the return is 100?</p>\n<p>To find the right probabilities to assign, we have to do Bayesian updating on this analysis result. The outcome of the analysis is four times as likely if the true value is 100 than if it is either 0 or 1. So the ratio of the expected value contributions changes from 50:1 to 50:4.</p>\n<p>Applied to this case, Karnofsky&rsquo;s point is simply this: despite the analysis suggesting high returns, modest returns still come with higher expected value than high returns. High returns should be considered more probable after the analysis than before &mdash; we&rsquo;ve observed a pretty good likelihood ratio of evidence in their favor &mdash; but high returns started out so improbable that even after receiving this bump, they still don&rsquo;t matter.</p>\n<p>Now that we&rsquo;ve seen the point in simplified form, let&rsquo;s begin a more detailed discussion.</p>\n<h3>3. The Role of BA</h3>\n<p>This section will add some critical notes on the concept of BA &mdash; notes that should apply whether the adjustment is performed explicitly or just used as a theoretical justification for listening to intuitions about the accuracy of particular estimates.</p>\n<p>Before discussing the role of BA, let&rsquo;s guard against a possible misinterpretation. Karnofsky is not arguing against maximizing expected value. He is arguing against a particular estimation method he labels &ldquo;Explicit Expected Value,&rdquo; which he considers to give inaccurate answers.</p>\n<p>The Explicit Expected Value (EEV) method is simple: obtain an estimate of the true cost-effectiveness of an action, then act as if this estimate is the &ldquo;true&rdquo; cost-effectiveness. This &ldquo;true&rdquo; cost-effectiveness could be interpreted as an expected value itself.<sup>3</sup></p>\n<p>In contrast to EEV, Karnofsky advocates &ldquo;Bayesian Adjustment.&rdquo; Bayesian reasoning involves multiplying a prior by a likelihood to find a posterior. In this case, the prior describes the charities that are out there in the population; the likelihood describes how likely different true values would have been to produce the given estimate; and the posterior represents our final beliefs about the charity&rsquo;s true cost-effectiveness. By looking at how common different effectiveness levels are, and how likely they would have been to lead to the given estimate, we judge the probability of various effectiveness levels.</p>\n<p>In the sense that we&rsquo;re updating on evidence according to Bayes&rsquo; theorem, what&rsquo;s going on is indeed \"Bayesian.\" But it&rsquo;s worth pointing out one difference between Karnofsky&rsquo;s adjustments and a fully Bayesian procedure: BA updates on a point estimate rather than on the full evidence that went into the point estimate.</p>\n<p>This matters in two different ways.</p>\n<p>First, the point estimate doesn&rsquo;t always carry all the available information. A procedure for generating a point estimate from a set of evidence could summarize different possible sets of evidence into the same point estimate, even though they favor different hypotheses. This sort of effect will probably be irrelevant in practice, but one might call BA &ldquo;half-Bayesian&rdquo; in light of it.</p>\n<p>Second, and more importantly, there&rsquo;s a risk of misinterpreting the nature of the estimate. Karnofsky&rsquo;s model, again, assumes that estimates are \"unbiased\" &mdash; that conditional on any given number being the true value, if you make many estimates, they&rsquo;ll average out to that number. And if that&rsquo;s actually the case for the estimation procedure being used, then that&rsquo;s fine.</p>\n<p>However, to the extent that an estimate took into account priors, that would make it &ldquo;biased&rdquo; toward the prior. As Oscar Cunningham <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzb\">comments</a>:</p>\n<blockquote>\n<p>The people giving these estimates will have already used their own priors, and so you should only adjust their estimates to the extent to which your priors differ from theirs.</p>\n</blockquote>\n<p>In the most straightforward case, the source simply gave his own Bayesian posterior mean. If you and the source had the same prior, then your posterior mean should be the source&rsquo;s posterior mean. After all, the source performed just the same computation that you would.</p>\n<p>An old OvercomingBias post advises us to <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">share likelihood ratios, not posterior beliefs</a>. To be fair, in many cases communicating likelihood ratios for the whole space of hypotheses is impractical. One may instead want to communicate a number as a summary. (Even if one is making the estimate oneself, it may not be clear how one&rsquo;s brain came up with a particular number.) But it&rsquo;s important not to take a number that has prior information mixed in, and then interpret it as one that doesn&rsquo;t.</p>\n<p>In less straightforward cases, maybe <em>part</em> of the prior was taken into account. For example, maybe your source shares your pessimism about the organizational efficiency of nonprofits, but not your pessimism in other areas. Even if your source informally ignored lines of reasoning that seemed to lead to an estimate that was &ldquo;too good to be true,&rdquo; that is enough to make double-counting an issue.</p>\n<p>But to put this section in context, the appropriateness of BA isn&rsquo;t the most important disagreement with Karnofsky. Based on the considerations given here, performing an intuitive BA may well be better than going by an explicit estimate. Differences in priors have room to be far more important than just the results of (partially) double-counting them. So the more important part of the argument will be about which priors to use.</p>\n<h3>4. Probability Models</h3>\n<h4>4.1: The first model</h4>\n<p>Karnofsky defends his conclusions with probabilistic models based on <a href=\"http://blog.givewell.org/attachments/worms.pdf\">some mathematical calculations by Dario Amodei</a>. This section will argue that these models only rule out optimal existential risk charity because the priors they assign to the relevant hypotheses are extremely low &mdash; in other words, because they virtually rule out extreme returns in advance.</p>\n<p>In the model in Karnofsky&rsquo;s <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">first post</a>, it&rsquo;s easy to see the low priors. Consider the first example (the graphs are from Karnofsky's posts):</p>\n<p><img src=\"http://blog.givewell.org/images/bayesian%20adjustment%201.png\" alt=\"Bayesian Adjustment\" /></p>\n<p>This example comes with some particular assumptions about parameters. The prior is normally distributed with mean 0 and standard deviation 1; the likelihood is normally distributed with mean 10 and standard deviation 1. As in the saying that &ldquo;a Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule,&rdquo; the posterior ends up in the middle, hardly overlapping with either. As Eliezer Yudkowsky <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">points out</a>, this lack of overlap should practically <em>never happen</em>. When it does, such an event is a strong reason to doubt one&rsquo;s assumptions. It suggests that you should have assigned a different prior.</p>\n<p>Or maybe, instead of the prior, it&rsquo;s the <em>likelihood</em> that you should have assigned differently &mdash; as one of the other graphs does:</p>\n<p><img src=\"http://blog.givewell.org/images/bayesian%20adjustment%204.png\" alt=\"Bayesian adjustment with wide uncertainty\" /></p>\n<p>Here, the outcome makes some sense, because there&rsquo;s significant overlap. A high true cost-effectiveness would have been <em>more likely</em> to produce the estimate found, but a low true cost-effectiveness <em>could</em> have produced it instead. And the prior says the latter case, where the true cost-effectiveness is low, is far more likely &mdash; so the final best estimate, indeed, ends up not differing much from the initial best estimate.</p>\n<p>Note, however, that this prior is extremely confident. The difference in probability density between the expected value and a value ten standard deviations out is a factor of e<sup>-50</sup>, or about 10<sup>-22</sup>. This number is so low it might as well be zero.</p>\n<h4>4.2: The second model</h4>\n<p>The second model builds on the first model, so many of the same considerations about extreme priors will carry over. This time, we&rsquo;re looking at a set of different estimates that we could be updating on like we did in the first model. For each of these, we take the expectation of the posterior distribution for the true cost-effectiveness, so we can put these expectations in a graph. After all, the expectation is the number that will factor into our decisions!</p>\n<p>Here&rsquo;s one of the graphs, showing <em>initial estimates</em> on the x-axis and <em>final estimates</em> on the y-axis. The initial estimates are what we&rsquo;re performing a Bayesian update on, and the final estimates are the expectation value of the distribution of cost-effectiveness after updating:</p>\n<p><img src=\"http://blog.givewell.org/images/cea1.png\" alt=\"Final estimates as a function of initial estimate\" /></p>\n<p>So as initial estimates increase, the final estimate rises at first, but then slowly declines. High estimates are good up to a point, but when they become too extreme, we have to conclude they were a fluke.</p>\n<p>As before, this model uses a standard normal prior, which means high true values have enormously smaller prior probabilities. Compared to this prior, the evidence provided by each estimate is minor. If the estimate falls one standard deviation out in the distribution, then it favors the estimate value over a value of zero by a likelihood ratio of the square root of e, or about 1.65. So it&rsquo;s no wonder that the tail end of high cost-effectiveness ends up irrelevant.</p>\n<p>According to Karnofsky, this model illustrates that an estimate is safer to take at face value when evidence in its favor comes from multiple independent lines of inquiry. There are some calculations showing this &mdash; the more independent pieces of evidence for a given high value you gather, the more these together can overcome the &ldquo;too good to be true&rdquo; effect.</p>\n<p>While multiple independent pieces of evidence are indeed better, it&rsquo;s important to emphasize that the relevant variable is simply the evidence&rsquo;s <em>strength</em>. Evidence can be strong because it comes from multiple directions, but it can also be strong because it just happens to be unlikely to occur under alternative hypotheses. If we have two independent observations that are both twice as likely to occur given cost-effectiveness 3 than cost-effectiveness 1, that&rsquo;s equally good as having a single observation that&rsquo;s four times as likely to occur given cost-effectiveness 3 than cost-effectiveness 1.</p>\n<p>It&rsquo;s worth noting that if the multiple observations are all observations of one step in the process, and the other steps are left uncertain, there&rsquo;s a limit to how much multiple observations can make a difference.</p>\n<h4>4.3: Do the same calculations apply to log-normal priors?</h4>\n<p>Now that we&rsquo;ve established that the models use low priors, can we evaluate whether the low priors are essential to the models&rsquo; conclusions? Or are they just simplifying assumptions that make the math easier, but would be unnecessary in a full analysis?</p>\n<p>One obvious step is to see if Karnofsky's conclusions hold up with log-normal models. Karnofsky states that the conclusions carry over qualitatively:</p>\n<blockquote>\n<p>the conceptual content of this post does not rely on the assumption that the value of donations (as measured in something like \"lives saved\" or \"DALYs saved\") is normally distributed. In particular, a log-normal distribution fits easily into the above framework</p>\n</blockquote>\n<p>Assuming a log-normal prior, however, does change the mathematics. Graphs like those in Karnofsky&rsquo;s first post could certainly be interpreted as referring to the logarithm of cost-effectiveness, but the final number we&rsquo;re interested in is <em>the expected cost-effectiveness itself</em>. And if we interpret the graph as representing a logarithm, it&rsquo;s no longer the case that the point at the middle of the distribution gives us the expectation. Instead, values higher in the distribution matter more.</p>\n<p>Guy Srinivasan <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/#comment-236366\">points out</a> that, for the same reason, log-normal priors would lead to different graphs in the second post, weakening the conclusion. To take the expectation of the logarithm and interpret that as the logarithm of the true cost-effectiveness is to bias the result downward.</p>\n<p>If, instead of calculating e to the power of the expected value of the logarithm of cost-effectiveness, we calculate the expected value of cost-effectiveness directly, there&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution#Arithmetic_moments\">an additional term</a> that increases with the standard deviation.</p>\n<p>For an example of this, consider a normal distribution with mean 0 and standard deviation 1. If it represents the cost-effectiveness itself, we should take its expected value and find 0. But if it represents the logarithm of the cost-effectiveness, it won&rsquo;t do to take e to the power of the expected value, which would be 1. Rather, we add another &frac12; sigma (which in this case equals &frac12;) before exponentiating. So the final expected cost-effectiveness ends up a factor sqrt(e) ( = 1.65) larger &mdash; the most &ldquo;average&rdquo; value lies &frac12; to the right of the center of the graph.</p>\n<p>While the mathematical point made here opposes Karnofsky&rsquo;s claims, it&rsquo;s hard to say how likely it is to be decisive in the context of the dilemmas that actually confront decision makers. So let&rsquo;s take a step back and directly face the question of how extreme these priors need to be.</p>\n<h4>4.4: Do priors need to be extreme?</h4>\n<p>As we&rsquo;ve seen, Karnofsky&rsquo;s toy examples use extreme priors, and these priors would entail a substantial adjustment to EV estimates for existential risk charities. This adjustment would in turn be sufficient to alter existential risk charities from good ideas to bad ideas.<sup>4</sup></p>\n<p>The claim made in this section is: Karnofsky&rsquo;s models don&rsquo;t just <em>use</em> extreme priors, they <em>require</em> extreme priors if they are to have this altering effect. To determine whether this claim is true, one must check whether there are priors that aren&rsquo;t extreme, but still have the effect.<sup>5</sup></p>\n<p>And indeed, as pointed out by Karnofsky, there exist priors that (1) are far less extreme than the normal prior and (2) still justify a major adjustment to EV estimates for existential risk charities. This is a sense in which his point qualitatively holds.</p>\n<p>But the adjustment needs to be not just major, but large <em>enough</em> to turn existential risk charities from good ideas into bad ideas. This is difficult. Existential risk charities come with the <a href=\"http://www.existential-risk.org/concept.pdf\">potential</a> for cost-effectiveness many orders of magnitude higher than that of the average charity. The normal prior succeeds at discounting this potential with its extreme skepticism, as may other priors. But if we can show that all the non-extreme priors justify an adjustment that may be large, but is not large enough to decide the issue, then that is a sense in which Karnofsky&rsquo;s point does not qualitatively hold.</p>\n<p>And a prior can be far less extreme than the normal prior, while still being extreme. Do the log-normal prior and various even thicker-tailed priors qualify as &ldquo;extreme,&rdquo; and do they entail sufficiently large adjustments? Rather than get hopelessly lost in that sort of analysis, let&rsquo;s just see what happens when one tries modeling real existential risk interventions as simple all-or-nothing bets: either they achieve some estimated reduction of risk, or the reasoning behind them fails completely.<sup>6</sup></p>\n<p>Suppose there&rsquo;s some estimate for the cost-effectiveness of a charity &mdash; call it E &mdash; and the true cost-effectiveness must be either 0 or E. You assign some probability p to the proposition that the estimate came from a true cost-effectiveness of E. This probability itself then comes from a prior probability that the estimate was E, and a likelihood ratio comparing at what rates true values of 0 and E create estimates of E.<sup>7</sup></p>\n<p>To find a ballpark number for what returns analyses are saying may be available from existential risk reduction (i.e., what value we should use for E), we can take a few different approaches.</p>\n<p>One approach is to look at risks that are relatively tractable, such as asteroid impacts. It&rsquo;s estimated that impacts similar in size to that involved in the extinction of the dinosaurs occur about once every hundred million years. With the simplifying assumption that each such event causes human extinction, and that lesser asteroid events don&rsquo;t cause human extinction (or even end any existing lives), this translates to an extinction probability of one in a million for any given century. In other words, preventing all asteroid risk for a given century saves an expected 10<sup>4</sup> existing lives and an expected 1/10<sup>6</sup> fraction of all future value.</p>\n<p>A set of interventions funded in the past decade ruled out an imminent extinction-level impact at a cost of roughly $10<sup>8</sup>.<sup>8</sup></p>\n<p>According to this rough calculation, then, this program saved roughly one life plus a 1/(10<sup>10</sup>) fraction of the future for each $10<sup>4</sup>. Of course, future programs would probably be less effective.</p>\n<p>For this to have been competitive with international aid ($10<sup>3</sup> dollars per life saved), one only has to consider saving a 1 in 10<sup>10</sup> fraction of humanity&rsquo;s entire future to be 10 times as important as saving an individual life. This is equivalent to considering saving humanity&rsquo;s entire future to be 10 times as important as saving all individual people living today. In a straightforward &ldquo;<a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical waste</a>&rdquo; analysis, of course, it is far <em>more</em> important: enough so to compensate a high probability that the estimate is incorrect.</p>\n<p>As an alternative to looking at tractable classes of risk for a cost-effectiveness estimate, we could look at the classes of existential risk that appear the most promising. AI risk, in particular, stands out. In a Singularity Summit talk, Anna Salamon <a href=\"http://vimeo.com/7397629\">estimated</a> eight expected existing lives saved per dollar of AI risk research, or about $10<sup>-1</sup> per existing life. Each existing life, again, also corresponds to a 10<sup>-10</sup> fraction of our civilization&rsquo;s astronomical potential.</p>\n<p>(There are a number of points where one could quibble with the reasoning that produced this estimate; cutting it down by a few orders of magnitude seems like it may not affect the underlying point too much. The main reason why there is an advantage here might be because we restricted ourselves to a limited class of charities for international aid, but not for existential risk reduction. In particular, the international aid charities we&rsquo;ve used in the comparison are those that operate on an object level, e.g. by distributing mosquito nets, whereas the estimate in the talk refers to meta-level research about what object-level policies would be helpful.)</p>\n<p>For such charities not to be competitive with international aid, just based on saving present-day lives alone, one would need to assign a probability that the estimate is correct of at most 1/10<sup>4</sup>. And as before, in a straightforward utilitarian analysis, the needed factor is much larger. This means that the probability that the estimate is correct could be far lower still.</p>\n<p>Presumably the probability of an estimate of E given a true value of E is far greater than the probability of an estimate of E given a true value of 0. So the 10<sup>4</sup> or greater understates the extremeness of the priors you need. If your prior for existential risk-level returns is low because most charities are feel-good local charities, the likelihood ratio brings it back up a lot, because there aren&rsquo;t any feel-good local charities producing plausible calculations that say they&rsquo;re extremely effective.<sup>9</sup></p>\n<p>So one genuinely needs to find improbabilities that cut down the estimate by a large factor &mdash; although, depending on the specifics, one may need to bring in astronomical waste arguments to establish this point. Is it reasonable to adopt priors that have this effect?</p>\n<h3>5: Priors and their justification</h3>\n<h4>5.1: Needed priors</h4>\n<p>To recapitulate, it turns out that if one uses the concepts in Karnofsky&rsquo;s posts to argue that (generally competent) existential risk charities are not highly cost-effective, this requires extreme priors. The least extreme priors that still create low enough posteriors are still fairly extreme.</p>\n<p>Note that, for the argument to go through, it&rsquo;s not sufficient for the prior to be decreasing. A prior that doesn&rsquo;t decrease quickly enough doesn&rsquo;t even have a tail that&rsquo;s finite in size. Nor is it sufficient for the size of the prior&rsquo;s tail to be decreasing. It needs to at least decrease quickly enough to make up for the greater cost-effectiveness values we&rsquo;re multiplying by. For the expected value to even be finite a priori, with no evidence at all, the tail has to decrease more quickly than just at a minimum rate.</p>\n<h4>5.2: Possible justifications</h4>\n<p>Having argued that an attempt to defeat x-risk charities with BA requires a low prior &mdash; and that it therefore requires a justification for a low prior &mdash; let&rsquo;s look at possible approaches to such a justification.</p>\n<p>One place to start looking could be in power laws. A lot of phenomena seem to follow power law distributions &mdash; although claims of power laws have also been <a href=\"http://cscs.umich.edu/~crshalizi/weblog/491.html\">criticized</a>. The thickness of the tail depends on a parameter, but if, as <a href=\"http://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/\">this article</a>) suggests, the parameter alpha tends to be near 1, then that gives one a specific thickness.</p>\n<p>Another approach to justifying a low prior would be to say, &ldquo;if such cost-effective strategies had been available, they would have been used up by now,&rdquo; like the proverbial $20 bill lying on the ground. (Here, it&rsquo;s a 20-util bill, which involves altruistic rather than egoistic incentives, but the point is still relevant.) Karnofsky has previously <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">argued</a> something similar.</p>\n<p>For AI risk in particular, one might expect returns to have been driven down to the level of returns available for, e.g., asteroid impact prevention. If much higher returns are available for AI risk than other classes of risk, there must be some sort of explanation for why the low-hanging fruit there hasn&rsquo;t been picked.</p>\n<p>Such an explanation requires us to think about the beliefs and motivations of those who fund measures to mitigate existential risks, although there may also simply be an element of random chance in which categories of threat get attention. Various differences between categories of risk are relevant. For example, AI risk is an area where relatively little expert consensus exists on how imminent the problem is, on what could be done to solve the problem, and even whether the problem exists. There are many reasons to believe that thinking about AI risk, compared to asteroids, is unusually difficult. AI risk involves thinking about many different academic fields, and offers many potential ways to become confused and end up mistaken about a number of complicated issues. Various <a href=\"http://intelligence.org/files/CognitiveBiases.pdf\">biases</a> could turn out to be a problem; in particular, the <a href=\"http://wiki.lesswrong.com/wiki/Absurdity_heuristic\">absurdity heuristic</a> seems as though it could cause justified concerns to be dismissed early. Moreover, with AI risks, investment into global-scale risk is less likely to arise as a side effect of the prevention of smaller-scale disasters. Large asteroids pose similar issues to smaller asteroids, but human-level artificial general intelligence poses different issues than unintelligent viruses.</p>\n<p>Of course, all these things are evidence against a problem existing. But they could also explain why, even in the presence of a problem, it wouldn&rsquo;t be acted upon.</p>\n<h4>5.3: Past experience as a justification for low priors</h4>\n<p>The main approach to justification of low priors cited by Karnofsky isn&rsquo;t any quantified argument, but is based on gut-level extrapolation from past experience:</p>\n<blockquote>\n<p>Even just a sense for the values of the small set of actions you&rsquo;ve taken in your life, and observed the consequences of, gives you something to work with as far as an &ldquo;outside view&rdquo; and a starting probability distribution for the value of your actions; this distribution probably ought to have high variance, but when dealing with a rough estimate that has very high variance of its own, it may still be quite a meaningful prior.</p>\n</blockquote>\n<p>It does not seem a straightforward task for a brain to extrapolate from its own life to global-scale efforts. The outcomes it has actually observed are likely to be a biased sample, involving cases where it can actually trace its causal contribution to a relatively small event. In particular, of course, a brain hasn&rsquo;t had any opportunity to observe effects persisting for longer than a human lifetime.</p>\n<p>Extrapolating from the mundane events your brain has directly experienced to far out in the tail, where the selection of events has been highly optimized for utilitarian impact, is likely to be difficult.</p>\n<p>&ldquo;Black swan&rdquo; type considerations are relevant here: if you&rsquo;ve seen a million white swans in a row in the northern hemisphere, that might entitle you to assign a low probability that the first swan you see in the southern hemisphere will be non-white, but it doesn&rsquo;t entitle you to assign a one-in-a-million probability. In just the same way, if you&rsquo;ve seen a million inefficient charities in a row when looking mostly at animal charities, that doesn&rsquo;t entitle you to assign a one-in-a-million probability to a charity in the class of international aid being efficient. Maybe things will just be fundamentally different.</p>\n<p>But it can be argued that we have already had some actual observations of existential risk-scale interventions. And indeed, Karnofsky <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-245275\">says</a> elsewhere that past claims of enormous cost-effectiveness have failed to pan out:</p>\n<blockquote>\n<p>I think that speaking generally/historically/intuitively, the number of actions that a back-of-the-envelope calc could/would have predicted enormous value for is high, and the number that panned out is low. So a claim of enormous value is sufficient to make me skeptical. In other words, my prior isn&rsquo;t so wide as to have little regressive impact on claims like \"1% chance of saving the world.\"</p>\n</blockquote>\n<p>One can argue the numbers: exactly how many actions seemed enormously valuable in the way AI risk reduction seems to? Exactly how few of them panned out? Some examples one might include in this category are religious claims about the afterlife or the end times, particularly leveraged ways of creating permanent social change, or ways to intervene at important points in nuclear arms races. But in general, if your high estimate of cost-effectiveness for an organization is based on, say, a 10% chance that it would visibly succeed at achieving enormous returns over its lifetime, then just a few such failures provide only moderate evidence against the accuracy of the estimate. And as we&rsquo;ve seen, for the regressive impact created by Karnofsky&rsquo;s priors to make a difference, it needs to be not just substantial, but enormous.</p>\n<h4>5.4: Intuitions suggesting extremely low priors are unreasonable</h4>\n<p>To get a feel for how extreme some of these priors are, consider what they would have predicted in the past. As <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-244010\">Carl Shulman says</a>:</p>\n<blockquote>\n<p>[I]t appears that one can save lives hundreds of times more cheaply through vaccinations in the developing world than through typical charity expenditures aimed at saving lives in rich countries, according to experiments, government statistics, etc.</p>\n<p>But a normal distribution (assigns) a probability of one in tens of thousands that a sample will be more than 4 standard deviations above the median, and one in hundreds of billions that a charity will be more than 7 standard deviations from the median.</p>\n</blockquote>\n<p>In other words, with a normal prior, the model assigns extremely small probabilities to events that have, in fact, happened. With a log-normal prior, the problem is not as bad. But as Shulman points out, such a prior still makes predictions for the future that are difficult to square with physics &mdash; difficult to square with the observation that existential disasters seem possible, and at least some of them are partly mediated by technology. As a reductio ad absurdum of normal and log-normal priors, he offers a &ldquo;charity doomsday argument&rdquo;:</p>\n<blockquote>\n<p>If we believed a normal prior then we could reason as follows:</p>\n<ol>\n<li>\n<p>If humanity has a reasonable chance of surviving to build a lasting advanced civilization, then some charity interventions are immensely cost-effective, e.g. the historically successful efforts in asteroid tracking.</p>\n</li>\n<li>\n<p>By the normal (or log-normal) prior on charity cost-effectiveness, no charity can be immensely cost-effective (with overwhelming probability).</p>\n</li>\n</ol>\n<p>Therefore,</p>\n<ol>\n<li>Humanity is doomed to premature extinction, stagnation, or an otherwise cramped future.</li>\n</ol></blockquote>\n<p>In Karnofsky&rsquo;s <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-258773\">reactions</a> to arguments such as these, he has emphasized that, while his model may not be realistic, there is no better model available that leads to different conclusions:</p>\n<blockquote>\n<p>You and others have pointed out that there are ways in which my model doesn&rsquo;t seem to match reality. There are definitely ways in which this is true, but I don&rsquo;t think pointing this out is - in itself - much of an objection. All models are simplifications. They all break down in some cases. The question is whether there is a better model that leads to different big-picture implications; no one has yet proposed such a model, and my intuition is that there is not one.</p>\n</blockquote>\n<p>But the flaw identified here &mdash; that the prior in Karnofsky&rsquo;s models cannot be convinced of astronomical waste &mdash; isn&rsquo;t just an accidental feature of simplifying reality in a particular way. It&rsquo;s a flaw present in any scheme that discounts the implications of astronomical waste through priors. Whatever the probability for the existence of preventable astronomical waste is, in expected utility calculations, it gets multiplied by such a large number that unless it starts out extremely low, there&rsquo;s a problem.</p>\n<p>As a last thought experiment suggesting the necessary probabilities are extreme, suppose that in addition to the available evidence, you had a magical coin that always flipped heads if astronomical waste were real and preventable &mdash; but that was otherwise fair. If the coin came up heads dozens of times, wouldn&rsquo;t you start to change your mind? If so, unless your intuitions about coins are heavily broken, your prior must not in fact be so extremely small as to cancel out the returns.</p>\n<h4>5.5: Indirect effects of international aid</h4>\n<p>There is a possible way to argue for international aid over existential risk reduction based on priors without requiring a prior so small as to unreasonably deny astronomical waste. Namely, one could note that international aid itself has effects on astronomical waste. Then international aid is on a more equal level with existential risk, no matter how large the numbers for astronomical waste turn out to be.</p>\n<p>Perhaps international aid has effects hastening the start of space colonization. Earlier space colonization would prevent whatever astronomical waste takes place during the interval between the point where space colonization actually happens, and the point where it would otherwise have happened. This could conceivably outweigh the astronomical waste from existential risks even if such risks aren&rsquo;t astronomically improbable.</p>\n<p>Do we have a way to evaluate such indirect effects on growth? The argument goes as follows: international aid saves people&rsquo;s lives, saving people&rsquo;s lives increases economic growth, economic growth increases the speed of development of the required technologies, and this decreases the amount of astronomical waste. However, as Bostrom points out in his paper on astronomical waste, safety is still a lot more important than speed:</p>\n<blockquote>\n<p>If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. &hellip; Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.</p>\n</blockquote>\n<p>A more recent analysis by Stuart Armstrong and Anders Sandberg emphasizes the effect of galaxies escaping over the cosmic event horizon: the more we delay colonization, and the more slowly colonization happens, the more galaxies go permanently out of reach. Their model implies that we lose about a galaxy per year of delaying colonization at light speed, or about a galaxy every fifty years of delaying colonization at half light speed. This is out of, respectively, 6.3 billion and 120 million total galaxies reached.</p>\n<p>So a year&rsquo;s delay wastes only about the same amount of value as a one-in-several-billion chance of human extinction. That means safety is usually more important than delay. For delay to outweigh safety requires a highly confident belief in the proposition that we can affect delay but not safety.</p>\n<p>Does this give us a way to estimate the indirect returns of saving one person&rsquo;s life in the Third World?</p>\n<p>Since it&rsquo;s probably good enough to estimate to within a few orders of magnitude, we&rsquo;ll make some very loose assumptions.</p>\n<p>Suppose a Third World country with a population of 100 million makes a total difference of one month in the timing of humanity&rsquo;s future colonization of space. Then a single person in that country makes an expected difference of 1/(1200 million) years &mdash; equivalent to a one-in-billions-of-billions chance of human extinction.</p>\n<p>If saving the person&rsquo;s life is the result of an investment of $10<sup>3</sup>, then to claim the astronomical waste returns are similar to those from preventing existential risk, one must claim an existential risk intervention of $10<sup>6</sup> would have a chance of one in millions of billions of preventing an existential disaster, and an intervention of $10<sup>9</sup> would have a chance of one in thousands of billions.</p>\n<p>There are some caveats to be made on both sides of the argument. For example, we assumed that preventing human extinction has billions of times the payoff of delaying space colonization for a year; but what if the bottleneck is some other resource than what&rsquo;s being wasted? In that case, it could be that, if we survive, we can get a lot more value than billions of times what is lost through a year&rsquo;s waste. And if one (naively?) took the expectation value of this &ldquo;billions&rdquo; figure, one would probably end up with something infinite, because we don&rsquo;t know for sure what&rsquo;s possible in physics.</p>\n<p>Increased economic growth could have effects not just on timing, but on safety itself. For example, economic growth could increase existential risk by speeding up dangerous technologies more quickly than society can handle them safely, or it could decrease existential risk by promoting some sort of stability. It could also have various small but permanent effects on the future.</p>\n<p>Still, it would seem to be a fairly major coincidence if the policy of saving people&rsquo;s lives in the Third World were also the policy that maximized safety. One would at least expect to see more effect from interventions targeted specifically at speeding up economic growth. An approach to foreign aid aimed at maximizing growth effects rather than near-term lives or DALYs saved would probably look quite different. Even then, it&rsquo;s hard to see how economic growth could be the policy that maximized safety unless our model of what causes safety were so broken as to be useless.</p>\n<p>Throughout this analysis, we&rsquo;ve been assuming a standard utilitarian view, where the loss of astronomical numbers of future life-years is more important than the deaths of current people by a correspondingly astronomic factor. What if, at the other extreme, one only cared about saving as many people as possible from the <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">present generation</a>? Then delay might be more important: in any given year, a nontrivial fraction of the world population dies. One could imagine a speedup of certain technologies causing these technologies to save the lives of whoever would have died during that time.</p>\n<p>Again, we can do a very rough calculation. Every second, <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2066.html\">1.8 people die</a>. So if, as above, saving a life through malaria nets makes a difference in colonization timing of 1/(1200 million) years or 25 milliseconds, and if hastening colonization by one second saves those 1.8 lives, the additional lives saved through the speedup are only 1/40 of the lives saved directly by the malaria net.</p>\n<p>Since we&rsquo;re dealing with order-of-magnitude differences, for this 1/40 to matter, we&rsquo;d need to have underestimated it by orders of magnitude. What we&rsquo;d have to prove isn&rsquo;t just that lives saved through speedup outnumber lives saved directly; what we&rsquo;d have to prove is that lives saved through speedup outnumber lives saved through alternative uses of money. As we saw before, on top of the 1/40, there are still another four orders of magnitude or so between estimates of the returns in current lives saved through AI risk reduction and international aid.</p>\n<p>One may question whether this argument constitutes a &ldquo;<a href=\"/lw/wj/is_that_your_true_rejection/\">true rejection</a>&rdquo; of the cost-effectiveness of existential risk reduction: were international aid charities really chosen <em>because</em> they increase economic growth and thereby speed up space colonization? If one were optimizing for that criterion, presumably there would be more efficient charities available, and it might be interesting to look at whether one could make a case that they save more current people than AI risk reduction. One would also need to have a reason to disregard astronomical waste.</p>\n<h4>5.6: Pascal&rsquo;s Mugging and the big picture</h4>\n<p>Let&rsquo;s take a more detailed look at the question of whether reasonable priors, in fact, bring the expected returns of the best existential risk charities down by a sufficient factor. Karnofsky states a general argument:</p>\n<blockquote>\n<p>But as stated above, I believe even most power-law distributions would lead to the same big-picture conclusions. I believe the crucial question to be whether the prior probability of having impact &gt;=X falls faster than X rises. My intuition is that for any reasonable prior distribution for which this is true, the big-picture conclusions of the model will hold; for any prior distribution for which it isn&rsquo;t true, there will be major strange and problematic implications.</p>\n</blockquote>\n<p>In defending the idea that existential risk reduction has a high enough probability of success to be a good investment, we have two options:</p>\n<ol>\n<li>\n<p>Use a prior with a tail that decreases faster than 1/X, and argue that the posterior ends up high enough anyway.</p>\n</li>\n<li>\n<p>Use a prior with a tail that decreases slower than 1/X, and argue that there are no strange implications; or that there are strange implications but they&rsquo;re not problematic.</p>\n</li>\n</ol>\n<p>Let&rsquo;s briefly examine both of these possibilities. We can&rsquo;t do the problem full numerical justice, but we can at least take an initial stab at answering the question of what alternative models could look like.</p>\n<h5>5.6.1: Rapidly shrinking tails</h5>\n<p>First, let&rsquo;s look at an example where the prior probability of impact at least X falls <em>faster</em> than X rises. Suppose we quantify X in terms of the number of lives that can be saved for one million dollars. Consider a <a href=\"http://en.wikipedia.org/wiki/Pareto_distribution\">Pareto distribution</a> (that is, a power law) for X, with a minimum possible value of 10, and with alpha equal to 1.5 so that the density for X decreases as X<sup>-5/2</sup>, and the probability mass of the tail beyond X decreases as X<sup>-3/2</sup>. Now suppose international aid claims an X of at least 1000 and existential risk reduction claims an X of at least 100,000. Then there&rsquo;s a 1 in 1000 prior for the international aid tail and a 1 in 1000000 prior for the existential risk tail.</p>\n<p>A one in a million prior sounds scary. However:</p>\n<ul>\n<li>\n<p>Those million charities would consist almost entirely of obviously non-optimal charities. Just knowing the general category of what they&rsquo;re trying to do would be enough to see they lacked extremely high returns. Picking the ones that are even mildly reasonable candidates already involves a great deal of optimization power.</p>\n</li>\n<li>\n<p>You wouldn&rsquo;t need to identify the one charity that had extremely good returns. For purposes of getting a better expected value, it would be more than sufficient to narrow it down to a list of one hundred.</p>\n</li>\n<li>\n<p>Presumably, some international aid charities manage to overcome that 1 in 1000 prior, and reach a large probability. If reasoning can pick out the best charity in a thousand with reasonable confidence, then maybe once those charities are picked out, reasoning can take a useful guess at which one is the best in a thousand of <em>these</em> charities.</p>\n</li>\n<li>\n<p>Overconfidence studies have trained us to be wary of claims that involve 99.99% certainty. But we should be wary of a confident prior just as we should be wary of a confident likelihood. It&rsquo;s easy to make errors when caution is applied in only one direction. As a further &ldquo;intuition pump,&rdquo; suppose you&rsquo;re in a foreign country and you meet someone you know. The prior odds against it being that person may be billions to one. But when you meet them, you&rsquo;ll soon have strong enough evidence to attain nearly 100% confidence &mdash; despite the fact that this takes a likelihood ratio of billions.</p>\n</li>\n</ul>\n<p>So in sum, it seems as though even with a prior that declines fairly quickly, an analysis could still reasonably judge existential risk-level returns to be the most important. A quickly declining prior can still be overcome by evidence &mdash; and the amount of evidence needed drops to zero as the size of the tail gets closer to decreasing at a speed of 1/X. Again, just because an effect exists in a qualitative sense, that doesn&rsquo;t mean that, in practice, it will affect the conclusion.</p>\n<h5>5.6.2: Slowly shrinking tails</h5>\n<p>Second, let&rsquo;s consider prior distributions where the probability of impact at least X falls slower than X rises. One example of where this happens is a power law with an alpha lower than 1. But priors implied by Solomonoff induction also behave like this. For example, the probability they assign to a value of 3^^^3 is much larger than 1/(3^^^3), because the number can be produced by a relatively short program. Most values that large have negligibly small probabilities, because there&rsquo;s no short program for them. But some values that large have higher probabilities, and end up dominating any plausible expected value calculation starting from such a prior. <sup>10</sup></p>\n<p>This problem is known as &ldquo;Pascal&rsquo;s Mugging,&rdquo; and <a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">has been discussed</a> extensively on LessWrong. Karnofsky considers it a reason to reject any prior that doesn&rsquo;t decrease fast enough. But there are a number of possible ways out of the problem, and not all of them change the prior:</p>\n<ul>\n<li>\n<p>Adopting a bounded utility function (with the right bound and functional form) can make it impossible for the mugger to make promises large enough to overcome their improbability.</p>\n</li>\n<li>\n<p>One could bite the bullet by accepting that one should pay the mugger &mdash; or rather that more plausible &ldquo;muggers,&rdquo; in the form of infinite physics, say, may come along later.</p>\n</li>\n<li>\n<p>If the positive and negative effects of giving in to muggers are symmetrical on expectation, then they cancel out... but why would they be symmetrical?</p>\n</li>\n<li>\n<p>Discounting the utility of an effect by the <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/fpp\">algorithmic complexity of locating it in the world</a> implies a special case of a bounded utility function.</p>\n</li>\n<li>\n<p>One could ignore the mugger for game-theoretical reasons... however, the hypothetical can be <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/3x0r\">modified</a> to make game theory irrelevant.</p>\n</li>\n<li>\n<p>One could justify a quickly declining prior using anthropic reasoning, as in <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/ui9\">Robin Hanson&rsquo;s comment</a>: statistically, most agents can&rsquo;t determine the course of a vast number of agents&rsquo; lives. However, while this is a plausible claim about anthropic reasoning, if one has uncertainty about what is the right account of anthropic reasoning, and if one treats this uncertainty as a regular probability, then the Pascal&rsquo;s Mugging problem reappears.</p>\n</li>\n<li>\n<p>One could justify a quickly declining prior some other way.</p>\n</li>\n</ul>\n<p>With regard to the last option, one does need some sort of justification. A probability doesn&rsquo;t seem like something you can choose based on whether it implies reasonable-sounding decisions; it seems like something that has to come from a model of the world. And to return to the magical coin example, would it really take roughly log(3^^^3) heads outcomes in a row (assuming away things like fake memories) to convince you the mugger was speaking the truth?</p>\n<p>It&rsquo;s worth taking particular note of the second-to-last option, where a prior is justified using anthropic reasoning. Such a prior would have to be quickly declining. Let&rsquo;s explore this possibility a little further.</p>\n<p>Suppose, roughly speaking, that before you know anything about where you find yourself in the universe, you expect on average to decisively affect one person&rsquo;s life. Then your prior for your impact should have an expectation value less than infinity &mdash; as is the case for power laws with alpha greater than 1, but not alpha smaller than 1. Of course, the number of lives a rational philanthropist affects is likely to be larger than the number of lives an average person affects. But if some people are optimal philanthropists, that still puts an upper bound on the expectation value. Likewise, if most things that could carry value aren&rsquo;t decision makers, that&rsquo;s a reason to expect greater returns per decision maker. Still, it seems like there would be some constant upper bound that doesn&rsquo;t scale with the size of the universe.</p>\n<p>In a world where whoever happens to be on the stage at a critical time gets to determine its long-term contents, there&rsquo;s a large prior probability that you&rsquo;re causally downstream of the most important events, and an extremely small prior probability that you live exactly at the critical point. Then suppose you find yourself on Earth in 2013, with an apparent astronomical-scale future still ahead, depending on what happens between now and the development of the relevant technology. This seems like it should cause a strong update from the anthropic prior. It&rsquo;s possible to find ways in which astronomical waste could be illusory, but to find them we need to look in odd places.</p>\n<ul>\n<li>\n<p>One candidate hypothesis is the idea that we&rsquo;re living in an <a href=\"http://simulation-argument.com/\">ancestor simulation</a>. This would imply astronomical waste was illusory: after all, if a substantial fraction of astronomical resources were dedicated toward such simulations, each of them would be able to determine only a small part of what happened to the resources. This would limit returns. It would be interesting to see more analysis of optimal philanthropy given that we&rsquo;re in a simulation, but it doesn&rsquo;t seem as if one would want to predicate one&rsquo;s case on that hypothesis.</p>\n</li>\n<li>\n<p>Other candidate hypotheses might revolve around interstellar colonization being impossible even in the long run for reasons we don&rsquo;t currently understand, or around the extinction of human civilization becoming almost inevitable given the availability of some future technology.</p>\n</li>\n<li>\n<p>As a last resort, we could hypothesize nonspecific insanity on our part, in a sort of <a href=\"/r/lesswrong/tag/majoritarianism/\">majoritarian hypothesis</a>. But it seems like assuming that we&rsquo;re insane and that we have no idea <em>how</em> we are insane undermines a lot of the other assumptions we&rsquo;re using in this analysis.</p>\n</li>\n</ul>\n<p>If Karnofsky or others would propose other such factors that might create the illusion of astronomical waste, or if they would defend any of the ones named, spelling them out and putting some sort of rough estimate or bounds on how much they tell us to discount astronomical waste seems like it would be an important next move in the debate.</p>\n<p>It may be a useful reframing to see things from a perspective like <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>. The question is whether one can get more value from controlling structures that &mdash; in an astronomical-sized universe &mdash; are likely to exist many times, than from an extremely small probability of controlling the whole thing.</p>\n<h3>6. Conclusion</h3>\n<p>BA doesn&rsquo;t justify a belief that existential risk charities, despite high back-of-envelope cost-effectiveness estimates, offer low or mediocre expected returns.</p>\n<p>We can assert this without having to endorse claims to the effect that one must support (without further research) the first charity that names a sufficiently large number. There are other considerations that defeat such claims.</p>\n<p>For one thing, there are multiple charities in the general existential risk space and potentially multiple ways of donating to them; even if there weren&rsquo;t, more could be created in the future. That means we need to investigate the effectiveness of each one.</p>\n<p>For another thing, even if there were only one charity with great potential returns in the area, you&rsquo;d have to check that marginal money wasn&rsquo;t being negatively useful, as Karnofsky has argued is indeed the case for MIRI (because the \"Friendly AI\" approach is unnecessarily dangerous, according to Karnofsky).</p>\n<p>Systematic upward bias, not just random error, is of course likely to play a role in organizations&rsquo; estimates of their own effectiveness.</p>\n<p>And finally, some other consideration, not covered in these posts, could prove either that existential risk reduction doesn&rsquo;t have a particularly high expected value, or that we shouldn&rsquo;t maximize expected value at all. (Bounded utility functions are a special case of not maximizing expected value, if &ldquo;value&rdquo; is measured in e.g. DALYs rather than utils.) Note, however, that Karnofsky himself has not endorsed the use of non-additive metrics of charitable impact.</p>\n<p>MIRI, in choosing a strategy, is not gambling on a tiny probability that its actions will turn out relevant. It&rsquo;s trying to affect a large-scale event &mdash; the variable of whether or not the intelligence explosion turns out safe &mdash; that will eventually be resolved into a &ldquo;yes&rdquo; or &ldquo;no&rdquo; outcome. That every individual dollar or hour spent will fail to have much of an effect by itself is an issue inherent to pushing on large-scale events. Other cases where this applies, and where it would not be seen as problematic, are political campaigns and medical research, if the good the research does comes from a few discoveries spread among many labs and experiments.</p>\n<p>The improbability here isn&rsquo;t in itself pathological, or a stretch of expected value maximization. It might be pathological if the argument relied on further highly improbable &ldquo;just in case&rdquo; assumptions, for example if we were almost certain that AI is impossible to create, or if we were almost certain that safety will be ensured by default. But even though &ldquo;if there&rsquo;s even a chance&rdquo; arguments have sometimes been made, MIRI does not actually believe that there&rsquo;s an additional factor on top of that inherent per-dollar improbability that would make it so that all its efforts are probably irrelevant. If it believed that, then it would pick a different strategy.</p>\n<p>All things considered, our evidence about the distribution of charities is compatible with AI being associated with major existential risks, and compatible with there being low-hanging fruit to be picked in mitigating such risks. Investing in reducing existential risk, then, can be optimal without falling to BA &mdash; and without strange implications.</p>\n<h3>Notes</h3>\n<p>This post was written by Steven Kaas and funded by MIRI. My thanks for helpful feedback from Holden Karnofsky, Carl Shulman, Nick Beckstead, Luke Muehlhauser, Steve Rayhawk, and Benjamin Noble.</p>\n<p><small><sup>1</sup> It's worth noting, however, that Karnofsky&rsquo;s vision for GiveWell is to provide donors with the best giving opportunities that can be found, not necessarily the giving opportunities whose ROI estimates have the strongest evidential backing. So, for Karnofsky, strong evidential backing is a means to the end of finding the best interventions, not an end in itself. In Givewell's January 24th, 2013 board meeting (starting at 24:30 in <a href=\"http://www.givewell.org/files/ClearFund/Meeting%202013%2001%2024/Board%20call%202013%2001%2024.mp3\">the MP3 recording</a>), Karnofsky said:</small></p>\n<p><small>\"The way [\"GiveWell 2\", a possible future GiveWell focused on giving opportunities for which strong evidence is less available than is the case with GiveWell's current charity recommendations] would prioritize [giving] opportunities would involve... a heavy dose of personal judgment, and a heavy dose of... \"Well, we have laid out our reasons of thinking this. Not all the reasons are things we can prove, but... here's the evidence we have, here's what we do know, and given the limited available information here's what we would guess.\" We actually do a fair amount of that already with GiveWell, but it would definitely be more noticeable and more prominent and more extreme [in GiveWell 2]...</small></p>\n<p><small>...What would still be \"GiveWell\" about [\"GiveWell 2\"] is that I don't believe that there's another organization that's out there that is publicly writing about what it thinks are the best giving opportunities and why, and... comparing all the possible things you might give to... It's basically a topic of discussion that I don't believe exists right now, and... we started GiveWell to start that discussion in an open, public way, and we started in a certain place, but <em>that</em> and not evidence... has always been the driving philosophy of GiveWell, and our mission statement talks about expanding giving opportunities, it doesn't talk about evidence.\"</small></p>\n<p><small><sup>2</sup> Technically, the prior is usually not about a specific charity that we already have information about, but about charities in general. I give an example of a specific fictional charity because I figured that would be more clarifying, and the math works as long as you&rsquo;re using an estimate to move from a state of less information to a state of more information.</small></p>\n<p><small><sup>3</sup> At least in the sense that it might still average over, say, quantum branching and chaotic dynamics. But the &ldquo;true value&rdquo; would at least be based on a full understanding of the problem and its solutions. </small></p>\n<p><small><sup>4</sup> Of course, it may be the case that particular charities working on existential risk reduction fail to pursue activities that <em>actually</em> reduce existential risk &mdash; that question is separate from the questions we have the space to examine here.</small></p>\n<p><small><sup>5</sup> For this section, by &ldquo;extreme priors&rdquo; I just mean something like &ldquo;many zeroes.&rdquo; Does the prior say that what some of us think of as always having been a live hypothesis actually started out as hugely improbable? Then it&rsquo;s &ldquo;extreme&rdquo; for my purposes. Once it&rsquo;s been established that only extreme priors let the point carry through, one can then discuss whether a prior that&rsquo;s &ldquo;extreme&rdquo; in this sense may nonetheless be justified. This is what the next section will be devoted to. The separation between these two points forces me to use this rather artificial concept of &ldquo;extreme,&rdquo; where an analysis would ideally just consider what priors are reasonable and how Karnofsky&rsquo;s point works with them. Nonetheless, I hope it makes things clearer.</small></p>\n<p><small><sup>6</sup> It would be nice to have some better examples of the overall point, but these were the examples that seemed maximally illustrative, clear, and concise given time and space constraints.</small></p>\n<p><small><sup>7</sup> This estimate, technically, isn&rsquo;t unbiased. If the true value is E, the estimate will average lower than E, and if the true value is 0, the estimate will average higher than 0. But this shouldn&rsquo;t matter for the illustration.</small></p>\n<p><small><sup>8</sup> To be sure, if an asteroid had been on its way, we would have also needed to pay the cost of deflecting it. But this possibility was extremely improbable. As long as the cost of deflection wouldn&rsquo;t have been much more than $10<sup>14</sup>, this doesn&rsquo;t increase the expected cost by orders of magnitude.</small></p>\n<p><small><sup>9</sup> There are some points to be made here about causal screening, and also that it&rsquo;s unnatural to think of the prior as being on effectiveness, rather than on things that cause both effectiveness and low priors, unless effectiveness is a thing that causes low priors, for example because people have picked up all the low-hanging fruit off the ground. But due to time and space concerns, I have left those points out of this document.</small></p>\n<p><small><sup>10</sup> A more complete argument would involve looking at how often a given structure would be repeated with what probability in a simplicity-weighted set of universes, but the general point is the same.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JyH7ezruQbC2iWcSg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 79, "extendedScore": null, "score": 0.000203, "legacy": true, "legacyId": "22022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(This is a long post. If you\u2019re going to read only part, please read sections 1 and 2, subsubsection 5.6.2, and the conclusion.)</p>\n<h3 id=\"1__Introduction\">1. Introduction</h3>\n<p>Suppose you want to give some money to charity: where can you get the most bang for your philanthropic buck? One way to make the decision is to use explicit expected value estimates. That is, you could get an unbiased (averaging to the true value) estimate of what each candidate for your donation would do with an additional dollar, and then pick the charity associated with the most promising estimate.</p>\n<p>Holden Karnofsky of <a href=\"http://www.givewell.org/\">GiveWell</a>, an organization that rates charities for cost-effectiveness, disagreed with this approach in two posts he made in 2011. This is a response to those posts, addressing the implications for existential risk efforts.</p>\n<p>According to Karnofsky, high returns are rare, and even unbiased estimates don\u2019t take into account the reasons <em>why</em> they\u2019re rare. So in Karnofsky's view, our favorite charity shouldn\u2019t just be one associated with a high estimate, it should be one that supports the estimate with robust evidence derived from multiple independent lines of inquiry.<sup>1</sup> If a charity\u2019s returns are being estimated in a way that intuitively feels shaky, maybe that means the fact that high returns are rare should outweigh the fact that high returns were estimated, even if the people making the estimate were doing an excellent job of avoiding bias.</p>\n<p>Karnofsky\u2019s first post, <a href=\"/lw/745/why_we_cant_take_expected_value_estimates\">Why We Can\u2019t Take Expected Value Estimates Literally (Even When They\u2019re Unbiased)</a>, explains how one can mitigate this issue by supplementing an explicit estimate with what Karnofsky calls a \u201cBayesian Adjustment\u201d (henceforth \u201cBA\u201d). This method treats estimates as merely noisy measures of true values. BA starts with a prior representing what cost-effectiveness values are out there in the general population of charities, then the prior is updated into a posterior in standard Bayesian fashion.</p>\n<p>Karnofsky provides some example graphs, illustrating his preference for robustness. If the estimate error is small, the posterior lies close to the explicit estimate. But if the estimate error is large, the posterior lies close to the prior. In other words, if there simply aren\u2019t many high-return charities out there, a sharp estimate can be taken seriously, but a noisy estimate that says it has found a high-return charity must represent some sort of fluke.</p>\n<p>Karnofsky does not advocate a policy of performing an <em>explicit</em> adjustment. Rather, he uses BA to emphasize that estimates are likely to be inadequate if they don\u2019t incorporate certain kinds of intuitions \u2014 in particular, a sense of whether all the components of an estimation procedure feel reliable. If intuitions say an estimate feels shaky and too good to be true, then maybe the estimate was noisy and the prior is more important. On the other hand, if intuitions say an estimate has taken everything into account, then maybe the estimate was sharp and outweighs the prior.</p>\n<p>Karnofsky\u2019s second post, <a href=\"/lw/8di/maximizing_costeffectiveness_via_critical_inquiry/\">Maximizing Cost-Effectiveness Via Critical Inquiry</a>, expands on these points. Where the first post looks at how BA is performed on a single charity at a time, the second post examines how BA affects the estimated relative values of different charities. In particular, it assumes that although the charities are all drawn from the same prior, they come with different estimates of cost-effectiveness. Higher estimates of cost-effectiveness come from estimation procedures with proportionally higher uncertainty.</p>\n<p>It turns out that higher estimates aren\u2019t always more auspicious: an estimate may be \u201ctoo good to be true,\u201d concentrating much of its evidential support on values that the prior already rules out for the most part. On the bright side, this effect can be mitigated via multiple independent observations, and such observations can provide enough evidence to solidify higher estimates despite their low prior probability.</p>\n<p>Charities aiming to reduce existential risk have a potential claim to high expected returns, simply because of the size of the stakes. But if such charities are difficult to evaluate, and the prior probability of high expected values is low, then the implications of BA for this class of charities loom large.</p>\n<p>This post will argue that competent efforts to reduce existential risk reduction are still likely to be optimal, despite BA. The argument will have three parts:</p>\n<ol>\n<li>\n<p>BA differs from fully Bayesian reasoning, so that BA risks double-counting priors.</p>\n</li>\n<li>\n<p>The models in Karnofsky\u2019s posts, when applied to existential risk, boil down to our having prior knowledge that the claimed returns are virtually impossible. (Moreover, similar models without extreme priors don\u2019t lead to the same conclusions.)</p>\n</li>\n<li>\n<p>We don\u2019t have such prior knowledge. Extreme priors would have implied false predictions in the past, imply unphysical predictions for the future, and are justified neither by our past experiences nor by any other considerations.</p>\n</li>\n</ol>\n<p>Claim 1 is not essential to the conclusion. While Claim 2 seems worth expanding on, it\u2019s Claim 3 that makes up the core of the controversy. Each of these concerns will be addressed in turn.</p>\n<p>Before responding to the claims themselves, however, it\u2019s worth discussing a highly simplified model that will illustrate what Karnofsky\u2019s basic point is.</p>\n<p><a id=\"more\"></a></p>\n<h3 id=\"2__A_Simple_Discrete_Distribution_of_Charitable_Returns\">2. A Simple Discrete Distribution of Charitable Returns</h3>\n<p>Suppose you\u2019re considering a donation to the Center for Inventing Metawidgets (CIM), but you'd like to perform an analysis of the properties of metawidgets first.<sup>2</sup> Before the analysis, you\u2019re uncertain about three possibilities:</p>\n<ul>\n<li>With a probability of 4999 out of 10,000, metawidgets aren\u2019t even a thing. You can\u2019t invent what isn\u2019t a thing, so the return is 0. </li>\n<li>With a probability of 5000 out of 10,000, metawidgets are a thing with some reasonably good use, like repairing printers. The return in this case is 1. </li>\n<li>With a probability of 1 out of 10,000, metawidgets have extremely useful effects, like curing lung cancer. Then the return is 100.</li>\n</ul>\n<p>If we now compute the expected value of a donation to CIM, it ends up as a sum of the following components:</p>\n<ul>\n<li>0.4999 * 0 = <strong>0</strong> from the possibility that the return is 0 </li>\n<li>0.5 * 1 = <strong>0.5</strong> from the possibility that the return is 1 </li>\n<li>0.0001 * 100 = <strong>0.01</strong> from the possibility that the return is 100 </li>\n</ul>\n<p>In particular, the possibility of a modest return contributes 50 times the expected value of the possibility of an extreme return. The size of the potential return, in this case, didn\u2019t make up for its low probability.</p>\n<p>But that\u2019s before you do an analysis that will give you some additional evidence about metawidgets. The analysis has the following properties:</p>\n<ul>\n<li>Whatever the true return is, 50% of the time the analysis is correct and gives you the correct answer. </li>\n<li>If the analysis is wrong, it picks one of the three possible answers uniformly at random. </li>\n</ul>\n<p>What happens if the analysis says the return is 100?</p>\n<p>To find the right probabilities to assign, we have to do Bayesian updating on this analysis result. The outcome of the analysis is four times as likely if the true value is 100 than if it is either 0 or 1. So the ratio of the expected value contributions changes from 50:1 to 50:4.</p>\n<p>Applied to this case, Karnofsky\u2019s point is simply this: despite the analysis suggesting high returns, modest returns still come with higher expected value than high returns. High returns should be considered more probable after the analysis than before \u2014 we\u2019ve observed a pretty good likelihood ratio of evidence in their favor \u2014 but high returns started out so improbable that even after receiving this bump, they still don\u2019t matter.</p>\n<p>Now that we\u2019ve seen the point in simplified form, let\u2019s begin a more detailed discussion.</p>\n<h3 id=\"3__The_Role_of_BA\">3. The Role of BA</h3>\n<p>This section will add some critical notes on the concept of BA \u2014 notes that should apply whether the adjustment is performed explicitly or just used as a theoretical justification for listening to intuitions about the accuracy of particular estimates.</p>\n<p>Before discussing the role of BA, let\u2019s guard against a possible misinterpretation. Karnofsky is not arguing against maximizing expected value. He is arguing against a particular estimation method he labels \u201cExplicit Expected Value,\u201d which he considers to give inaccurate answers.</p>\n<p>The Explicit Expected Value (EEV) method is simple: obtain an estimate of the true cost-effectiveness of an action, then act as if this estimate is the \u201ctrue\u201d cost-effectiveness. This \u201ctrue\u201d cost-effectiveness could be interpreted as an expected value itself.<sup>3</sup></p>\n<p>In contrast to EEV, Karnofsky advocates \u201cBayesian Adjustment.\u201d Bayesian reasoning involves multiplying a prior by a likelihood to find a posterior. In this case, the prior describes the charities that are out there in the population; the likelihood describes how likely different true values would have been to produce the given estimate; and the posterior represents our final beliefs about the charity\u2019s true cost-effectiveness. By looking at how common different effectiveness levels are, and how likely they would have been to lead to the given estimate, we judge the probability of various effectiveness levels.</p>\n<p>In the sense that we\u2019re updating on evidence according to Bayes\u2019 theorem, what\u2019s going on is indeed \"Bayesian.\" But it\u2019s worth pointing out one difference between Karnofsky\u2019s adjustments and a fully Bayesian procedure: BA updates on a point estimate rather than on the full evidence that went into the point estimate.</p>\n<p>This matters in two different ways.</p>\n<p>First, the point estimate doesn\u2019t always carry all the available information. A procedure for generating a point estimate from a set of evidence could summarize different possible sets of evidence into the same point estimate, even though they favor different hypotheses. This sort of effect will probably be irrelevant in practice, but one might call BA \u201chalf-Bayesian\u201d in light of it.</p>\n<p>Second, and more importantly, there\u2019s a risk of misinterpreting the nature of the estimate. Karnofsky\u2019s model, again, assumes that estimates are \"unbiased\" \u2014 that conditional on any given number being the true value, if you make many estimates, they\u2019ll average out to that number. And if that\u2019s actually the case for the estimation procedure being used, then that\u2019s fine.</p>\n<p>However, to the extent that an estimate took into account priors, that would make it \u201cbiased\u201d toward the prior. As Oscar Cunningham <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzb\">comments</a>:</p>\n<blockquote>\n<p>The people giving these estimates will have already used their own priors, and so you should only adjust their estimates to the extent to which your priors differ from theirs.</p>\n</blockquote>\n<p>In the most straightforward case, the source simply gave his own Bayesian posterior mean. If you and the source had the same prior, then your posterior mean should be the source\u2019s posterior mean. After all, the source performed just the same computation that you would.</p>\n<p>An old OvercomingBias post advises us to <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">share likelihood ratios, not posterior beliefs</a>. To be fair, in many cases communicating likelihood ratios for the whole space of hypotheses is impractical. One may instead want to communicate a number as a summary. (Even if one is making the estimate oneself, it may not be clear how one\u2019s brain came up with a particular number.) But it\u2019s important not to take a number that has prior information mixed in, and then interpret it as one that doesn\u2019t.</p>\n<p>In less straightforward cases, maybe <em>part</em> of the prior was taken into account. For example, maybe your source shares your pessimism about the organizational efficiency of nonprofits, but not your pessimism in other areas. Even if your source informally ignored lines of reasoning that seemed to lead to an estimate that was \u201ctoo good to be true,\u201d that is enough to make double-counting an issue.</p>\n<p>But to put this section in context, the appropriateness of BA isn\u2019t the most important disagreement with Karnofsky. Based on the considerations given here, performing an intuitive BA may well be better than going by an explicit estimate. Differences in priors have room to be far more important than just the results of (partially) double-counting them. So the more important part of the argument will be about which priors to use.</p>\n<h3 id=\"4__Probability_Models\">4. Probability Models</h3>\n<h4 id=\"4_1__The_first_model\">4.1: The first model</h4>\n<p>Karnofsky defends his conclusions with probabilistic models based on <a href=\"http://blog.givewell.org/attachments/worms.pdf\">some mathematical calculations by Dario Amodei</a>. This section will argue that these models only rule out optimal existential risk charity because the priors they assign to the relevant hypotheses are extremely low \u2014 in other words, because they virtually rule out extreme returns in advance.</p>\n<p>In the model in Karnofsky\u2019s <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">first post</a>, it\u2019s easy to see the low priors. Consider the first example (the graphs are from Karnofsky's posts):</p>\n<p><img src=\"http://blog.givewell.org/images/bayesian%20adjustment%201.png\" alt=\"Bayesian Adjustment\"></p>\n<p>This example comes with some particular assumptions about parameters. The prior is normally distributed with mean 0 and standard deviation 1; the likelihood is normally distributed with mean 10 and standard deviation 1. As in the saying that \u201ca Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule,\u201d the posterior ends up in the middle, hardly overlapping with either. As Eliezer Yudkowsky <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">points out</a>, this lack of overlap should practically <em>never happen</em>. When it does, such an event is a strong reason to doubt one\u2019s assumptions. It suggests that you should have assigned a different prior.</p>\n<p>Or maybe, instead of the prior, it\u2019s the <em>likelihood</em> that you should have assigned differently \u2014 as one of the other graphs does:</p>\n<p><img src=\"http://blog.givewell.org/images/bayesian%20adjustment%204.png\" alt=\"Bayesian adjustment with wide uncertainty\"></p>\n<p>Here, the outcome makes some sense, because there\u2019s significant overlap. A high true cost-effectiveness would have been <em>more likely</em> to produce the estimate found, but a low true cost-effectiveness <em>could</em> have produced it instead. And the prior says the latter case, where the true cost-effectiveness is low, is far more likely \u2014 so the final best estimate, indeed, ends up not differing much from the initial best estimate.</p>\n<p>Note, however, that this prior is extremely confident. The difference in probability density between the expected value and a value ten standard deviations out is a factor of e<sup>-50</sup>, or about 10<sup>-22</sup>. This number is so low it might as well be zero.</p>\n<h4 id=\"4_2__The_second_model\">4.2: The second model</h4>\n<p>The second model builds on the first model, so many of the same considerations about extreme priors will carry over. This time, we\u2019re looking at a set of different estimates that we could be updating on like we did in the first model. For each of these, we take the expectation of the posterior distribution for the true cost-effectiveness, so we can put these expectations in a graph. After all, the expectation is the number that will factor into our decisions!</p>\n<p>Here\u2019s one of the graphs, showing <em>initial estimates</em> on the x-axis and <em>final estimates</em> on the y-axis. The initial estimates are what we\u2019re performing a Bayesian update on, and the final estimates are the expectation value of the distribution of cost-effectiveness after updating:</p>\n<p><img src=\"http://blog.givewell.org/images/cea1.png\" alt=\"Final estimates as a function of initial estimate\"></p>\n<p>So as initial estimates increase, the final estimate rises at first, but then slowly declines. High estimates are good up to a point, but when they become too extreme, we have to conclude they were a fluke.</p>\n<p>As before, this model uses a standard normal prior, which means high true values have enormously smaller prior probabilities. Compared to this prior, the evidence provided by each estimate is minor. If the estimate falls one standard deviation out in the distribution, then it favors the estimate value over a value of zero by a likelihood ratio of the square root of e, or about 1.65. So it\u2019s no wonder that the tail end of high cost-effectiveness ends up irrelevant.</p>\n<p>According to Karnofsky, this model illustrates that an estimate is safer to take at face value when evidence in its favor comes from multiple independent lines of inquiry. There are some calculations showing this \u2014 the more independent pieces of evidence for a given high value you gather, the more these together can overcome the \u201ctoo good to be true\u201d effect.</p>\n<p>While multiple independent pieces of evidence are indeed better, it\u2019s important to emphasize that the relevant variable is simply the evidence\u2019s <em>strength</em>. Evidence can be strong because it comes from multiple directions, but it can also be strong because it just happens to be unlikely to occur under alternative hypotheses. If we have two independent observations that are both twice as likely to occur given cost-effectiveness 3 than cost-effectiveness 1, that\u2019s equally good as having a single observation that\u2019s four times as likely to occur given cost-effectiveness 3 than cost-effectiveness 1.</p>\n<p>It\u2019s worth noting that if the multiple observations are all observations of one step in the process, and the other steps are left uncertain, there\u2019s a limit to how much multiple observations can make a difference.</p>\n<h4 id=\"4_3__Do_the_same_calculations_apply_to_log_normal_priors_\">4.3: Do the same calculations apply to log-normal priors?</h4>\n<p>Now that we\u2019ve established that the models use low priors, can we evaluate whether the low priors are essential to the models\u2019 conclusions? Or are they just simplifying assumptions that make the math easier, but would be unnecessary in a full analysis?</p>\n<p>One obvious step is to see if Karnofsky's conclusions hold up with log-normal models. Karnofsky states that the conclusions carry over qualitatively:</p>\n<blockquote>\n<p>the conceptual content of this post does not rely on the assumption that the value of donations (as measured in something like \"lives saved\" or \"DALYs saved\") is normally distributed. In particular, a log-normal distribution fits easily into the above framework</p>\n</blockquote>\n<p>Assuming a log-normal prior, however, does change the mathematics. Graphs like those in Karnofsky\u2019s first post could certainly be interpreted as referring to the logarithm of cost-effectiveness, but the final number we\u2019re interested in is <em>the expected cost-effectiveness itself</em>. And if we interpret the graph as representing a logarithm, it\u2019s no longer the case that the point at the middle of the distribution gives us the expectation. Instead, values higher in the distribution matter more.</p>\n<p>Guy Srinivasan <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/#comment-236366\">points out</a> that, for the same reason, log-normal priors would lead to different graphs in the second post, weakening the conclusion. To take the expectation of the logarithm and interpret that as the logarithm of the true cost-effectiveness is to bias the result downward.</p>\n<p>If, instead of calculating e to the power of the expected value of the logarithm of cost-effectiveness, we calculate the expected value of cost-effectiveness directly, there\u2019s <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution#Arithmetic_moments\">an additional term</a> that increases with the standard deviation.</p>\n<p>For an example of this, consider a normal distribution with mean 0 and standard deviation 1. If it represents the cost-effectiveness itself, we should take its expected value and find 0. But if it represents the logarithm of the cost-effectiveness, it won\u2019t do to take e to the power of the expected value, which would be 1. Rather, we add another \u00bd sigma (which in this case equals \u00bd) before exponentiating. So the final expected cost-effectiveness ends up a factor sqrt(e) ( = 1.65) larger \u2014 the most \u201caverage\u201d value lies \u00bd to the right of the center of the graph.</p>\n<p>While the mathematical point made here opposes Karnofsky\u2019s claims, it\u2019s hard to say how likely it is to be decisive in the context of the dilemmas that actually confront decision makers. So let\u2019s take a step back and directly face the question of how extreme these priors need to be.</p>\n<h4 id=\"4_4__Do_priors_need_to_be_extreme_\">4.4: Do priors need to be extreme?</h4>\n<p>As we\u2019ve seen, Karnofsky\u2019s toy examples use extreme priors, and these priors would entail a substantial adjustment to EV estimates for existential risk charities. This adjustment would in turn be sufficient to alter existential risk charities from good ideas to bad ideas.<sup>4</sup></p>\n<p>The claim made in this section is: Karnofsky\u2019s models don\u2019t just <em>use</em> extreme priors, they <em>require</em> extreme priors if they are to have this altering effect. To determine whether this claim is true, one must check whether there are priors that aren\u2019t extreme, but still have the effect.<sup>5</sup></p>\n<p>And indeed, as pointed out by Karnofsky, there exist priors that (1) are far less extreme than the normal prior and (2) still justify a major adjustment to EV estimates for existential risk charities. This is a sense in which his point qualitatively holds.</p>\n<p>But the adjustment needs to be not just major, but large <em>enough</em> to turn existential risk charities from good ideas into bad ideas. This is difficult. Existential risk charities come with the <a href=\"http://www.existential-risk.org/concept.pdf\">potential</a> for cost-effectiveness many orders of magnitude higher than that of the average charity. The normal prior succeeds at discounting this potential with its extreme skepticism, as may other priors. But if we can show that all the non-extreme priors justify an adjustment that may be large, but is not large enough to decide the issue, then that is a sense in which Karnofsky\u2019s point does not qualitatively hold.</p>\n<p>And a prior can be far less extreme than the normal prior, while still being extreme. Do the log-normal prior and various even thicker-tailed priors qualify as \u201cextreme,\u201d and do they entail sufficiently large adjustments? Rather than get hopelessly lost in that sort of analysis, let\u2019s just see what happens when one tries modeling real existential risk interventions as simple all-or-nothing bets: either they achieve some estimated reduction of risk, or the reasoning behind them fails completely.<sup>6</sup></p>\n<p>Suppose there\u2019s some estimate for the cost-effectiveness of a charity \u2014 call it E \u2014 and the true cost-effectiveness must be either 0 or E. You assign some probability p to the proposition that the estimate came from a true cost-effectiveness of E. This probability itself then comes from a prior probability that the estimate was E, and a likelihood ratio comparing at what rates true values of 0 and E create estimates of E.<sup>7</sup></p>\n<p>To find a ballpark number for what returns analyses are saying may be available from existential risk reduction (i.e., what value we should use for E), we can take a few different approaches.</p>\n<p>One approach is to look at risks that are relatively tractable, such as asteroid impacts. It\u2019s estimated that impacts similar in size to that involved in the extinction of the dinosaurs occur about once every hundred million years. With the simplifying assumption that each such event causes human extinction, and that lesser asteroid events don\u2019t cause human extinction (or even end any existing lives), this translates to an extinction probability of one in a million for any given century. In other words, preventing all asteroid risk for a given century saves an expected 10<sup>4</sup> existing lives and an expected 1/10<sup>6</sup> fraction of all future value.</p>\n<p>A set of interventions funded in the past decade ruled out an imminent extinction-level impact at a cost of roughly $10<sup>8</sup>.<sup>8</sup></p>\n<p>According to this rough calculation, then, this program saved roughly one life plus a 1/(10<sup>10</sup>) fraction of the future for each $10<sup>4</sup>. Of course, future programs would probably be less effective.</p>\n<p>For this to have been competitive with international aid ($10<sup>3</sup> dollars per life saved), one only has to consider saving a 1 in 10<sup>10</sup> fraction of humanity\u2019s entire future to be 10 times as important as saving an individual life. This is equivalent to considering saving humanity\u2019s entire future to be 10 times as important as saving all individual people living today. In a straightforward \u201c<a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical waste</a>\u201d analysis, of course, it is far <em>more</em> important: enough so to compensate a high probability that the estimate is incorrect.</p>\n<p>As an alternative to looking at tractable classes of risk for a cost-effectiveness estimate, we could look at the classes of existential risk that appear the most promising. AI risk, in particular, stands out. In a Singularity Summit talk, Anna Salamon <a href=\"http://vimeo.com/7397629\">estimated</a> eight expected existing lives saved per dollar of AI risk research, or about $10<sup>-1</sup> per existing life. Each existing life, again, also corresponds to a 10<sup>-10</sup> fraction of our civilization\u2019s astronomical potential.</p>\n<p>(There are a number of points where one could quibble with the reasoning that produced this estimate; cutting it down by a few orders of magnitude seems like it may not affect the underlying point too much. The main reason why there is an advantage here might be because we restricted ourselves to a limited class of charities for international aid, but not for existential risk reduction. In particular, the international aid charities we\u2019ve used in the comparison are those that operate on an object level, e.g. by distributing mosquito nets, whereas the estimate in the talk refers to meta-level research about what object-level policies would be helpful.)</p>\n<p>For such charities not to be competitive with international aid, just based on saving present-day lives alone, one would need to assign a probability that the estimate is correct of at most 1/10<sup>4</sup>. And as before, in a straightforward utilitarian analysis, the needed factor is much larger. This means that the probability that the estimate is correct could be far lower still.</p>\n<p>Presumably the probability of an estimate of E given a true value of E is far greater than the probability of an estimate of E given a true value of 0. So the 10<sup>4</sup> or greater understates the extremeness of the priors you need. If your prior for existential risk-level returns is low because most charities are feel-good local charities, the likelihood ratio brings it back up a lot, because there aren\u2019t any feel-good local charities producing plausible calculations that say they\u2019re extremely effective.<sup>9</sup></p>\n<p>So one genuinely needs to find improbabilities that cut down the estimate by a large factor \u2014 although, depending on the specifics, one may need to bring in astronomical waste arguments to establish this point. Is it reasonable to adopt priors that have this effect?</p>\n<h3 id=\"5__Priors_and_their_justification\">5: Priors and their justification</h3>\n<h4 id=\"5_1__Needed_priors\">5.1: Needed priors</h4>\n<p>To recapitulate, it turns out that if one uses the concepts in Karnofsky\u2019s posts to argue that (generally competent) existential risk charities are not highly cost-effective, this requires extreme priors. The least extreme priors that still create low enough posteriors are still fairly extreme.</p>\n<p>Note that, for the argument to go through, it\u2019s not sufficient for the prior to be decreasing. A prior that doesn\u2019t decrease quickly enough doesn\u2019t even have a tail that\u2019s finite in size. Nor is it sufficient for the size of the prior\u2019s tail to be decreasing. It needs to at least decrease quickly enough to make up for the greater cost-effectiveness values we\u2019re multiplying by. For the expected value to even be finite a priori, with no evidence at all, the tail has to decrease more quickly than just at a minimum rate.</p>\n<h4 id=\"5_2__Possible_justifications\">5.2: Possible justifications</h4>\n<p>Having argued that an attempt to defeat x-risk charities with BA requires a low prior \u2014 and that it therefore requires a justification for a low prior \u2014 let\u2019s look at possible approaches to such a justification.</p>\n<p>One place to start looking could be in power laws. A lot of phenomena seem to follow power law distributions \u2014 although claims of power laws have also been <a href=\"http://cscs.umich.edu/~crshalizi/weblog/491.html\">criticized</a>. The thickness of the tail depends on a parameter, but if, as <a href=\"http://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/\">this article</a>) suggests, the parameter alpha tends to be near 1, then that gives one a specific thickness.</p>\n<p>Another approach to justifying a low prior would be to say, \u201cif such cost-effective strategies had been available, they would have been used up by now,\u201d like the proverbial $20 bill lying on the ground. (Here, it\u2019s a 20-util bill, which involves altruistic rather than egoistic incentives, but the point is still relevant.) Karnofsky has previously <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">argued</a> something similar.</p>\n<p>For AI risk in particular, one might expect returns to have been driven down to the level of returns available for, e.g., asteroid impact prevention. If much higher returns are available for AI risk than other classes of risk, there must be some sort of explanation for why the low-hanging fruit there hasn\u2019t been picked.</p>\n<p>Such an explanation requires us to think about the beliefs and motivations of those who fund measures to mitigate existential risks, although there may also simply be an element of random chance in which categories of threat get attention. Various differences between categories of risk are relevant. For example, AI risk is an area where relatively little expert consensus exists on how imminent the problem is, on what could be done to solve the problem, and even whether the problem exists. There are many reasons to believe that thinking about AI risk, compared to asteroids, is unusually difficult. AI risk involves thinking about many different academic fields, and offers many potential ways to become confused and end up mistaken about a number of complicated issues. Various <a href=\"http://intelligence.org/files/CognitiveBiases.pdf\">biases</a> could turn out to be a problem; in particular, the <a href=\"http://wiki.lesswrong.com/wiki/Absurdity_heuristic\">absurdity heuristic</a> seems as though it could cause justified concerns to be dismissed early. Moreover, with AI risks, investment into global-scale risk is less likely to arise as a side effect of the prevention of smaller-scale disasters. Large asteroids pose similar issues to smaller asteroids, but human-level artificial general intelligence poses different issues than unintelligent viruses.</p>\n<p>Of course, all these things are evidence against a problem existing. But they could also explain why, even in the presence of a problem, it wouldn\u2019t be acted upon.</p>\n<h4 id=\"5_3__Past_experience_as_a_justification_for_low_priors\">5.3: Past experience as a justification for low priors</h4>\n<p>The main approach to justification of low priors cited by Karnofsky isn\u2019t any quantified argument, but is based on gut-level extrapolation from past experience:</p>\n<blockquote>\n<p>Even just a sense for the values of the small set of actions you\u2019ve taken in your life, and observed the consequences of, gives you something to work with as far as an \u201coutside view\u201d and a starting probability distribution for the value of your actions; this distribution probably ought to have high variance, but when dealing with a rough estimate that has very high variance of its own, it may still be quite a meaningful prior.</p>\n</blockquote>\n<p>It does not seem a straightforward task for a brain to extrapolate from its own life to global-scale efforts. The outcomes it has actually observed are likely to be a biased sample, involving cases where it can actually trace its causal contribution to a relatively small event. In particular, of course, a brain hasn\u2019t had any opportunity to observe effects persisting for longer than a human lifetime.</p>\n<p>Extrapolating from the mundane events your brain has directly experienced to far out in the tail, where the selection of events has been highly optimized for utilitarian impact, is likely to be difficult.</p>\n<p>\u201cBlack swan\u201d type considerations are relevant here: if you\u2019ve seen a million white swans in a row in the northern hemisphere, that might entitle you to assign a low probability that the first swan you see in the southern hemisphere will be non-white, but it doesn\u2019t entitle you to assign a one-in-a-million probability. In just the same way, if you\u2019ve seen a million inefficient charities in a row when looking mostly at animal charities, that doesn\u2019t entitle you to assign a one-in-a-million probability to a charity in the class of international aid being efficient. Maybe things will just be fundamentally different.</p>\n<p>But it can be argued that we have already had some actual observations of existential risk-scale interventions. And indeed, Karnofsky <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-245275\">says</a> elsewhere that past claims of enormous cost-effectiveness have failed to pan out:</p>\n<blockquote>\n<p>I think that speaking generally/historically/intuitively, the number of actions that a back-of-the-envelope calc could/would have predicted enormous value for is high, and the number that panned out is low. So a claim of enormous value is sufficient to make me skeptical. In other words, my prior isn\u2019t so wide as to have little regressive impact on claims like \"1% chance of saving the world.\"</p>\n</blockquote>\n<p>One can argue the numbers: exactly how many actions seemed enormously valuable in the way AI risk reduction seems to? Exactly how few of them panned out? Some examples one might include in this category are religious claims about the afterlife or the end times, particularly leveraged ways of creating permanent social change, or ways to intervene at important points in nuclear arms races. But in general, if your high estimate of cost-effectiveness for an organization is based on, say, a 10% chance that it would visibly succeed at achieving enormous returns over its lifetime, then just a few such failures provide only moderate evidence against the accuracy of the estimate. And as we\u2019ve seen, for the regressive impact created by Karnofsky\u2019s priors to make a difference, it needs to be not just substantial, but enormous.</p>\n<h4 id=\"5_4__Intuitions_suggesting_extremely_low_priors_are_unreasonable\">5.4: Intuitions suggesting extremely low priors are unreasonable</h4>\n<p>To get a feel for how extreme some of these priors are, consider what they would have predicted in the past. As <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-244010\">Carl Shulman says</a>:</p>\n<blockquote>\n<p>[I]t appears that one can save lives hundreds of times more cheaply through vaccinations in the developing world than through typical charity expenditures aimed at saving lives in rich countries, according to experiments, government statistics, etc.</p>\n<p>But a normal distribution (assigns) a probability of one in tens of thousands that a sample will be more than 4 standard deviations above the median, and one in hundreds of billions that a charity will be more than 7 standard deviations from the median.</p>\n</blockquote>\n<p>In other words, with a normal prior, the model assigns extremely small probabilities to events that have, in fact, happened. With a log-normal prior, the problem is not as bad. But as Shulman points out, such a prior still makes predictions for the future that are difficult to square with physics \u2014 difficult to square with the observation that existential disasters seem possible, and at least some of them are partly mediated by technology. As a reductio ad absurdum of normal and log-normal priors, he offers a \u201ccharity doomsday argument\u201d:</p>\n<blockquote>\n<p>If we believed a normal prior then we could reason as follows:</p>\n<ol>\n<li>\n<p>If humanity has a reasonable chance of surviving to build a lasting advanced civilization, then some charity interventions are immensely cost-effective, e.g. the historically successful efforts in asteroid tracking.</p>\n</li>\n<li>\n<p>By the normal (or log-normal) prior on charity cost-effectiveness, no charity can be immensely cost-effective (with overwhelming probability).</p>\n</li>\n</ol>\n<p>Therefore,</p>\n<ol>\n<li>Humanity is doomed to premature extinction, stagnation, or an otherwise cramped future.</li>\n</ol></blockquote>\n<p>In Karnofsky\u2019s <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/#comment-258773\">reactions</a> to arguments such as these, he has emphasized that, while his model may not be realistic, there is no better model available that leads to different conclusions:</p>\n<blockquote>\n<p>You and others have pointed out that there are ways in which my model doesn\u2019t seem to match reality. There are definitely ways in which this is true, but I don\u2019t think pointing this out is - in itself - much of an objection. All models are simplifications. They all break down in some cases. The question is whether there is a better model that leads to different big-picture implications; no one has yet proposed such a model, and my intuition is that there is not one.</p>\n</blockquote>\n<p>But the flaw identified here \u2014 that the prior in Karnofsky\u2019s models cannot be convinced of astronomical waste \u2014 isn\u2019t just an accidental feature of simplifying reality in a particular way. It\u2019s a flaw present in any scheme that discounts the implications of astronomical waste through priors. Whatever the probability for the existence of preventable astronomical waste is, in expected utility calculations, it gets multiplied by such a large number that unless it starts out extremely low, there\u2019s a problem.</p>\n<p>As a last thought experiment suggesting the necessary probabilities are extreme, suppose that in addition to the available evidence, you had a magical coin that always flipped heads if astronomical waste were real and preventable \u2014 but that was otherwise fair. If the coin came up heads dozens of times, wouldn\u2019t you start to change your mind? If so, unless your intuitions about coins are heavily broken, your prior must not in fact be so extremely small as to cancel out the returns.</p>\n<h4 id=\"5_5__Indirect_effects_of_international_aid\">5.5: Indirect effects of international aid</h4>\n<p>There is a possible way to argue for international aid over existential risk reduction based on priors without requiring a prior so small as to unreasonably deny astronomical waste. Namely, one could note that international aid itself has effects on astronomical waste. Then international aid is on a more equal level with existential risk, no matter how large the numbers for astronomical waste turn out to be.</p>\n<p>Perhaps international aid has effects hastening the start of space colonization. Earlier space colonization would prevent whatever astronomical waste takes place during the interval between the point where space colonization actually happens, and the point where it would otherwise have happened. This could conceivably outweigh the astronomical waste from existential risks even if such risks aren\u2019t astronomically improbable.</p>\n<p>Do we have a way to evaluate such indirect effects on growth? The argument goes as follows: international aid saves people\u2019s lives, saving people\u2019s lives increases economic growth, economic growth increases the speed of development of the required technologies, and this decreases the amount of astronomical waste. However, as Bostrom points out in his paper on astronomical waste, safety is still a lot more important than speed:</p>\n<blockquote>\n<p>If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. \u2026 Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.</p>\n</blockquote>\n<p>A more recent analysis by Stuart Armstrong and Anders Sandberg emphasizes the effect of galaxies escaping over the cosmic event horizon: the more we delay colonization, and the more slowly colonization happens, the more galaxies go permanently out of reach. Their model implies that we lose about a galaxy per year of delaying colonization at light speed, or about a galaxy every fifty years of delaying colonization at half light speed. This is out of, respectively, 6.3 billion and 120 million total galaxies reached.</p>\n<p>So a year\u2019s delay wastes only about the same amount of value as a one-in-several-billion chance of human extinction. That means safety is usually more important than delay. For delay to outweigh safety requires a highly confident belief in the proposition that we can affect delay but not safety.</p>\n<p>Does this give us a way to estimate the indirect returns of saving one person\u2019s life in the Third World?</p>\n<p>Since it\u2019s probably good enough to estimate to within a few orders of magnitude, we\u2019ll make some very loose assumptions.</p>\n<p>Suppose a Third World country with a population of 100 million makes a total difference of one month in the timing of humanity\u2019s future colonization of space. Then a single person in that country makes an expected difference of 1/(1200 million) years \u2014 equivalent to a one-in-billions-of-billions chance of human extinction.</p>\n<p>If saving the person\u2019s life is the result of an investment of $10<sup>3</sup>, then to claim the astronomical waste returns are similar to those from preventing existential risk, one must claim an existential risk intervention of $10<sup>6</sup> would have a chance of one in millions of billions of preventing an existential disaster, and an intervention of $10<sup>9</sup> would have a chance of one in thousands of billions.</p>\n<p>There are some caveats to be made on both sides of the argument. For example, we assumed that preventing human extinction has billions of times the payoff of delaying space colonization for a year; but what if the bottleneck is some other resource than what\u2019s being wasted? In that case, it could be that, if we survive, we can get a lot more value than billions of times what is lost through a year\u2019s waste. And if one (naively?) took the expectation value of this \u201cbillions\u201d figure, one would probably end up with something infinite, because we don\u2019t know for sure what\u2019s possible in physics.</p>\n<p>Increased economic growth could have effects not just on timing, but on safety itself. For example, economic growth could increase existential risk by speeding up dangerous technologies more quickly than society can handle them safely, or it could decrease existential risk by promoting some sort of stability. It could also have various small but permanent effects on the future.</p>\n<p>Still, it would seem to be a fairly major coincidence if the policy of saving people\u2019s lives in the Third World were also the policy that maximized safety. One would at least expect to see more effect from interventions targeted specifically at speeding up economic growth. An approach to foreign aid aimed at maximizing growth effects rather than near-term lives or DALYs saved would probably look quite different. Even then, it\u2019s hard to see how economic growth could be the policy that maximized safety unless our model of what causes safety were so broken as to be useless.</p>\n<p>Throughout this analysis, we\u2019ve been assuming a standard utilitarian view, where the loss of astronomical numbers of future life-years is more important than the deaths of current people by a correspondingly astronomic factor. What if, at the other extreme, one only cared about saving as many people as possible from the <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">present generation</a>? Then delay might be more important: in any given year, a nontrivial fraction of the world population dies. One could imagine a speedup of certain technologies causing these technologies to save the lives of whoever would have died during that time.</p>\n<p>Again, we can do a very rough calculation. Every second, <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2066.html\">1.8 people die</a>. So if, as above, saving a life through malaria nets makes a difference in colonization timing of 1/(1200 million) years or 25 milliseconds, and if hastening colonization by one second saves those 1.8 lives, the additional lives saved through the speedup are only 1/40 of the lives saved directly by the malaria net.</p>\n<p>Since we\u2019re dealing with order-of-magnitude differences, for this 1/40 to matter, we\u2019d need to have underestimated it by orders of magnitude. What we\u2019d have to prove isn\u2019t just that lives saved through speedup outnumber lives saved directly; what we\u2019d have to prove is that lives saved through speedup outnumber lives saved through alternative uses of money. As we saw before, on top of the 1/40, there are still another four orders of magnitude or so between estimates of the returns in current lives saved through AI risk reduction and international aid.</p>\n<p>One may question whether this argument constitutes a \u201c<a href=\"/lw/wj/is_that_your_true_rejection/\">true rejection</a>\u201d of the cost-effectiveness of existential risk reduction: were international aid charities really chosen <em>because</em> they increase economic growth and thereby speed up space colonization? If one were optimizing for that criterion, presumably there would be more efficient charities available, and it might be interesting to look at whether one could make a case that they save more current people than AI risk reduction. One would also need to have a reason to disregard astronomical waste.</p>\n<h4 id=\"5_6__Pascal_s_Mugging_and_the_big_picture\">5.6: Pascal\u2019s Mugging and the big picture</h4>\n<p>Let\u2019s take a more detailed look at the question of whether reasonable priors, in fact, bring the expected returns of the best existential risk charities down by a sufficient factor. Karnofsky states a general argument:</p>\n<blockquote>\n<p>But as stated above, I believe even most power-law distributions would lead to the same big-picture conclusions. I believe the crucial question to be whether the prior probability of having impact &gt;=X falls faster than X rises. My intuition is that for any reasonable prior distribution for which this is true, the big-picture conclusions of the model will hold; for any prior distribution for which it isn\u2019t true, there will be major strange and problematic implications.</p>\n</blockquote>\n<p>In defending the idea that existential risk reduction has a high enough probability of success to be a good investment, we have two options:</p>\n<ol>\n<li>\n<p>Use a prior with a tail that decreases faster than 1/X, and argue that the posterior ends up high enough anyway.</p>\n</li>\n<li>\n<p>Use a prior with a tail that decreases slower than 1/X, and argue that there are no strange implications; or that there are strange implications but they\u2019re not problematic.</p>\n</li>\n</ol>\n<p>Let\u2019s briefly examine both of these possibilities. We can\u2019t do the problem full numerical justice, but we can at least take an initial stab at answering the question of what alternative models could look like.</p>\n<h5>5.6.1: Rapidly shrinking tails</h5>\n<p>First, let\u2019s look at an example where the prior probability of impact at least X falls <em>faster</em> than X rises. Suppose we quantify X in terms of the number of lives that can be saved for one million dollars. Consider a <a href=\"http://en.wikipedia.org/wiki/Pareto_distribution\">Pareto distribution</a> (that is, a power law) for X, with a minimum possible value of 10, and with alpha equal to 1.5 so that the density for X decreases as X<sup>-5/2</sup>, and the probability mass of the tail beyond X decreases as X<sup>-3/2</sup>. Now suppose international aid claims an X of at least 1000 and existential risk reduction claims an X of at least 100,000. Then there\u2019s a 1 in 1000 prior for the international aid tail and a 1 in 1000000 prior for the existential risk tail.</p>\n<p>A one in a million prior sounds scary. However:</p>\n<ul>\n<li>\n<p>Those million charities would consist almost entirely of obviously non-optimal charities. Just knowing the general category of what they\u2019re trying to do would be enough to see they lacked extremely high returns. Picking the ones that are even mildly reasonable candidates already involves a great deal of optimization power.</p>\n</li>\n<li>\n<p>You wouldn\u2019t need to identify the one charity that had extremely good returns. For purposes of getting a better expected value, it would be more than sufficient to narrow it down to a list of one hundred.</p>\n</li>\n<li>\n<p>Presumably, some international aid charities manage to overcome that 1 in 1000 prior, and reach a large probability. If reasoning can pick out the best charity in a thousand with reasonable confidence, then maybe once those charities are picked out, reasoning can take a useful guess at which one is the best in a thousand of <em>these</em> charities.</p>\n</li>\n<li>\n<p>Overconfidence studies have trained us to be wary of claims that involve 99.99% certainty. But we should be wary of a confident prior just as we should be wary of a confident likelihood. It\u2019s easy to make errors when caution is applied in only one direction. As a further \u201cintuition pump,\u201d suppose you\u2019re in a foreign country and you meet someone you know. The prior odds against it being that person may be billions to one. But when you meet them, you\u2019ll soon have strong enough evidence to attain nearly 100% confidence \u2014 despite the fact that this takes a likelihood ratio of billions.</p>\n</li>\n</ul>\n<p>So in sum, it seems as though even with a prior that declines fairly quickly, an analysis could still reasonably judge existential risk-level returns to be the most important. A quickly declining prior can still be overcome by evidence \u2014 and the amount of evidence needed drops to zero as the size of the tail gets closer to decreasing at a speed of 1/X. Again, just because an effect exists in a qualitative sense, that doesn\u2019t mean that, in practice, it will affect the conclusion.</p>\n<h5>5.6.2: Slowly shrinking tails</h5>\n<p>Second, let\u2019s consider prior distributions where the probability of impact at least X falls slower than X rises. One example of where this happens is a power law with an alpha lower than 1. But priors implied by Solomonoff induction also behave like this. For example, the probability they assign to a value of 3^^^3 is much larger than 1/(3^^^3), because the number can be produced by a relatively short program. Most values that large have negligibly small probabilities, because there\u2019s no short program for them. But some values that large have higher probabilities, and end up dominating any plausible expected value calculation starting from such a prior. <sup>10</sup></p>\n<p>This problem is known as \u201cPascal\u2019s Mugging,\u201d and <a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">has been discussed</a> extensively on LessWrong. Karnofsky considers it a reason to reject any prior that doesn\u2019t decrease fast enough. But there are a number of possible ways out of the problem, and not all of them change the prior:</p>\n<ul>\n<li>\n<p>Adopting a bounded utility function (with the right bound and functional form) can make it impossible for the mugger to make promises large enough to overcome their improbability.</p>\n</li>\n<li>\n<p>One could bite the bullet by accepting that one should pay the mugger \u2014 or rather that more plausible \u201cmuggers,\u201d in the form of infinite physics, say, may come along later.</p>\n</li>\n<li>\n<p>If the positive and negative effects of giving in to muggers are symmetrical on expectation, then they cancel out... but why would they be symmetrical?</p>\n</li>\n<li>\n<p>Discounting the utility of an effect by the <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/fpp\">algorithmic complexity of locating it in the world</a> implies a special case of a bounded utility function.</p>\n</li>\n<li>\n<p>One could ignore the mugger for game-theoretical reasons... however, the hypothetical can be <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/3x0r\">modified</a> to make game theory irrelevant.</p>\n</li>\n<li>\n<p>One could justify a quickly declining prior using anthropic reasoning, as in <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/ui9\">Robin Hanson\u2019s comment</a>: statistically, most agents can\u2019t determine the course of a vast number of agents\u2019 lives. However, while this is a plausible claim about anthropic reasoning, if one has uncertainty about what is the right account of anthropic reasoning, and if one treats this uncertainty as a regular probability, then the Pascal\u2019s Mugging problem reappears.</p>\n</li>\n<li>\n<p>One could justify a quickly declining prior some other way.</p>\n</li>\n</ul>\n<p>With regard to the last option, one does need some sort of justification. A probability doesn\u2019t seem like something you can choose based on whether it implies reasonable-sounding decisions; it seems like something that has to come from a model of the world. And to return to the magical coin example, would it really take roughly log(3^^^3) heads outcomes in a row (assuming away things like fake memories) to convince you the mugger was speaking the truth?</p>\n<p>It\u2019s worth taking particular note of the second-to-last option, where a prior is justified using anthropic reasoning. Such a prior would have to be quickly declining. Let\u2019s explore this possibility a little further.</p>\n<p>Suppose, roughly speaking, that before you know anything about where you find yourself in the universe, you expect on average to decisively affect one person\u2019s life. Then your prior for your impact should have an expectation value less than infinity \u2014 as is the case for power laws with alpha greater than 1, but not alpha smaller than 1. Of course, the number of lives a rational philanthropist affects is likely to be larger than the number of lives an average person affects. But if some people are optimal philanthropists, that still puts an upper bound on the expectation value. Likewise, if most things that could carry value aren\u2019t decision makers, that\u2019s a reason to expect greater returns per decision maker. Still, it seems like there would be some constant upper bound that doesn\u2019t scale with the size of the universe.</p>\n<p>In a world where whoever happens to be on the stage at a critical time gets to determine its long-term contents, there\u2019s a large prior probability that you\u2019re causally downstream of the most important events, and an extremely small prior probability that you live exactly at the critical point. Then suppose you find yourself on Earth in 2013, with an apparent astronomical-scale future still ahead, depending on what happens between now and the development of the relevant technology. This seems like it should cause a strong update from the anthropic prior. It\u2019s possible to find ways in which astronomical waste could be illusory, but to find them we need to look in odd places.</p>\n<ul>\n<li>\n<p>One candidate hypothesis is the idea that we\u2019re living in an <a href=\"http://simulation-argument.com/\">ancestor simulation</a>. This would imply astronomical waste was illusory: after all, if a substantial fraction of astronomical resources were dedicated toward such simulations, each of them would be able to determine only a small part of what happened to the resources. This would limit returns. It would be interesting to see more analysis of optimal philanthropy given that we\u2019re in a simulation, but it doesn\u2019t seem as if one would want to predicate one\u2019s case on that hypothesis.</p>\n</li>\n<li>\n<p>Other candidate hypotheses might revolve around interstellar colonization being impossible even in the long run for reasons we don\u2019t currently understand, or around the extinction of human civilization becoming almost inevitable given the availability of some future technology.</p>\n</li>\n<li>\n<p>As a last resort, we could hypothesize nonspecific insanity on our part, in a sort of <a href=\"/r/lesswrong/tag/majoritarianism/\">majoritarian hypothesis</a>. But it seems like assuming that we\u2019re insane and that we have no idea <em>how</em> we are insane undermines a lot of the other assumptions we\u2019re using in this analysis.</p>\n</li>\n</ul>\n<p>If Karnofsky or others would propose other such factors that might create the illusion of astronomical waste, or if they would defend any of the ones named, spelling them out and putting some sort of rough estimate or bounds on how much they tell us to discount astronomical waste seems like it would be an important next move in the debate.</p>\n<p>It may be a useful reframing to see things from a perspective like <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>. The question is whether one can get more value from controlling structures that \u2014 in an astronomical-sized universe \u2014 are likely to exist many times, than from an extremely small probability of controlling the whole thing.</p>\n<h3 id=\"6__Conclusion\">6. Conclusion</h3>\n<p>BA doesn\u2019t justify a belief that existential risk charities, despite high back-of-envelope cost-effectiveness estimates, offer low or mediocre expected returns.</p>\n<p>We can assert this without having to endorse claims to the effect that one must support (without further research) the first charity that names a sufficiently large number. There are other considerations that defeat such claims.</p>\n<p>For one thing, there are multiple charities in the general existential risk space and potentially multiple ways of donating to them; even if there weren\u2019t, more could be created in the future. That means we need to investigate the effectiveness of each one.</p>\n<p>For another thing, even if there were only one charity with great potential returns in the area, you\u2019d have to check that marginal money wasn\u2019t being negatively useful, as Karnofsky has argued is indeed the case for MIRI (because the \"Friendly AI\" approach is unnecessarily dangerous, according to Karnofsky).</p>\n<p>Systematic upward bias, not just random error, is of course likely to play a role in organizations\u2019 estimates of their own effectiveness.</p>\n<p>And finally, some other consideration, not covered in these posts, could prove either that existential risk reduction doesn\u2019t have a particularly high expected value, or that we shouldn\u2019t maximize expected value at all. (Bounded utility functions are a special case of not maximizing expected value, if \u201cvalue\u201d is measured in e.g. DALYs rather than utils.) Note, however, that Karnofsky himself has not endorsed the use of non-additive metrics of charitable impact.</p>\n<p>MIRI, in choosing a strategy, is not gambling on a tiny probability that its actions will turn out relevant. It\u2019s trying to affect a large-scale event \u2014 the variable of whether or not the intelligence explosion turns out safe \u2014 that will eventually be resolved into a \u201cyes\u201d or \u201cno\u201d outcome. That every individual dollar or hour spent will fail to have much of an effect by itself is an issue inherent to pushing on large-scale events. Other cases where this applies, and where it would not be seen as problematic, are political campaigns and medical research, if the good the research does comes from a few discoveries spread among many labs and experiments.</p>\n<p>The improbability here isn\u2019t in itself pathological, or a stretch of expected value maximization. It might be pathological if the argument relied on further highly improbable \u201cjust in case\u201d assumptions, for example if we were almost certain that AI is impossible to create, or if we were almost certain that safety will be ensured by default. But even though \u201cif there\u2019s even a chance\u201d arguments have sometimes been made, MIRI does not actually believe that there\u2019s an additional factor on top of that inherent per-dollar improbability that would make it so that all its efforts are probably irrelevant. If it believed that, then it would pick a different strategy.</p>\n<p>All things considered, our evidence about the distribution of charities is compatible with AI being associated with major existential risks, and compatible with there being low-hanging fruit to be picked in mitigating such risks. Investing in reducing existential risk, then, can be optimal without falling to BA \u2014 and without strange implications.</p>\n<h3 id=\"Notes\">Notes</h3>\n<p>This post was written by Steven Kaas and funded by MIRI. My thanks for helpful feedback from Holden Karnofsky, Carl Shulman, Nick Beckstead, Luke Muehlhauser, Steve Rayhawk, and Benjamin Noble.</p>\n<p><small><sup>1</sup> It's worth noting, however, that Karnofsky\u2019s vision for GiveWell is to provide donors with the best giving opportunities that can be found, not necessarily the giving opportunities whose ROI estimates have the strongest evidential backing. So, for Karnofsky, strong evidential backing is a means to the end of finding the best interventions, not an end in itself. In Givewell's January 24th, 2013 board meeting (starting at 24:30 in <a href=\"http://www.givewell.org/files/ClearFund/Meeting%202013%2001%2024/Board%20call%202013%2001%2024.mp3\">the MP3 recording</a>), Karnofsky said:</small></p>\n<p><small>\"The way [\"GiveWell 2\", a possible future GiveWell focused on giving opportunities for which strong evidence is less available than is the case with GiveWell's current charity recommendations] would prioritize [giving] opportunities would involve... a heavy dose of personal judgment, and a heavy dose of... \"Well, we have laid out our reasons of thinking this. Not all the reasons are things we can prove, but... here's the evidence we have, here's what we do know, and given the limited available information here's what we would guess.\" We actually do a fair amount of that already with GiveWell, but it would definitely be more noticeable and more prominent and more extreme [in GiveWell 2]...</small></p>\n<p><small>...What would still be \"GiveWell\" about [\"GiveWell 2\"] is that I don't believe that there's another organization that's out there that is publicly writing about what it thinks are the best giving opportunities and why, and... comparing all the possible things you might give to... It's basically a topic of discussion that I don't believe exists right now, and... we started GiveWell to start that discussion in an open, public way, and we started in a certain place, but <em>that</em> and not evidence... has always been the driving philosophy of GiveWell, and our mission statement talks about expanding giving opportunities, it doesn't talk about evidence.\"</small></p>\n<p><small><sup>2</sup> Technically, the prior is usually not about a specific charity that we already have information about, but about charities in general. I give an example of a specific fictional charity because I figured that would be more clarifying, and the math works as long as you\u2019re using an estimate to move from a state of less information to a state of more information.</small></p>\n<p><small><sup>3</sup> At least in the sense that it might still average over, say, quantum branching and chaotic dynamics. But the \u201ctrue value\u201d would at least be based on a full understanding of the problem and its solutions. </small></p>\n<p><small><sup>4</sup> Of course, it may be the case that particular charities working on existential risk reduction fail to pursue activities that <em>actually</em> reduce existential risk \u2014 that question is separate from the questions we have the space to examine here.</small></p>\n<p><small><sup>5</sup> For this section, by \u201cextreme priors\u201d I just mean something like \u201cmany zeroes.\u201d Does the prior say that what some of us think of as always having been a live hypothesis actually started out as hugely improbable? Then it\u2019s \u201cextreme\u201d for my purposes. Once it\u2019s been established that only extreme priors let the point carry through, one can then discuss whether a prior that\u2019s \u201cextreme\u201d in this sense may nonetheless be justified. This is what the next section will be devoted to. The separation between these two points forces me to use this rather artificial concept of \u201cextreme,\u201d where an analysis would ideally just consider what priors are reasonable and how Karnofsky\u2019s point works with them. Nonetheless, I hope it makes things clearer.</small></p>\n<p><small><sup>6</sup> It would be nice to have some better examples of the overall point, but these were the examples that seemed maximally illustrative, clear, and concise given time and space constraints.</small></p>\n<p><small><sup>7</sup> This estimate, technically, isn\u2019t unbiased. If the true value is E, the estimate will average lower than E, and if the true value is 0, the estimate will average higher than 0. But this shouldn\u2019t matter for the illustration.</small></p>\n<p><small><sup>8</sup> To be sure, if an asteroid had been on its way, we would have also needed to pay the cost of deflecting it. But this possibility was extremely improbable. As long as the cost of deflection wouldn\u2019t have been much more than $10<sup>14</sup>, this doesn\u2019t increase the expected cost by orders of magnitude.</small></p>\n<p><small><sup>9</sup> There are some points to be made here about causal screening, and also that it\u2019s unnatural to think of the prior as being on effectiveness, rather than on things that cause both effectiveness and low priors, unless effectiveness is a thing that causes low priors, for example because people have picked up all the low-hanging fruit off the ground. But due to time and space concerns, I have left those points out of this document.</small></p>\n<p><small><sup>10</sup> A more complete argument would involve looking at how often a given structure would be repeated with what probability in a simplicity-weighted set of universes, but the general point is the same.</small></p>", "sections": [{"title": "1. Introduction", "anchor": "1__Introduction", "level": 1}, {"title": "2. A Simple Discrete Distribution of Charitable Returns", "anchor": "2__A_Simple_Discrete_Distribution_of_Charitable_Returns", "level": 1}, {"title": "3. The Role of BA", "anchor": "3__The_Role_of_BA", "level": 1}, {"title": "4. Probability Models", "anchor": "4__Probability_Models", "level": 1}, {"title": "4.1: The first model", "anchor": "4_1__The_first_model", "level": 2}, {"title": "4.2: The second model", "anchor": "4_2__The_second_model", "level": 2}, {"title": "4.3: Do the same calculations apply to log-normal priors?", "anchor": "4_3__Do_the_same_calculations_apply_to_log_normal_priors_", "level": 2}, {"title": "4.4: Do priors need to be extreme?", "anchor": "4_4__Do_priors_need_to_be_extreme_", "level": 2}, {"title": "5: Priors and their justification", "anchor": "5__Priors_and_their_justification", "level": 1}, {"title": "5.1: Needed priors", "anchor": "5_1__Needed_priors", "level": 2}, {"title": "5.2: Possible justifications", "anchor": "5_2__Possible_justifications", "level": 2}, {"title": "5.3: Past experience as a justification for low priors", "anchor": "5_3__Past_experience_as_a_justification_for_low_priors", "level": 2}, {"title": "5.4: Intuitions suggesting extremely low priors are unreasonable", "anchor": "5_4__Intuitions_suggesting_extremely_low_priors_are_unreasonable", "level": 2}, {"title": "5.5: Indirect effects of international aid", "anchor": "5_5__Indirect_effects_of_international_aid", "level": 2}, {"title": "5.6: Pascal\u2019s Mugging and the big picture", "anchor": "5_6__Pascal_s_Mugging_and_the_big_picture", "level": 2}, {"title": "6. Conclusion", "anchor": "6__Conclusion", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "90 comments"}], "headingsCount": 19}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RdpqsQ6xbHzyckW9m", "QQo9N3WhZpL3ewii6", "cfZ8zveqrTZbQrjeD", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-17T11:58:07.953Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto: fight/flight/freeze experiences", "slug": "meetup-toronto-fight-flight-freeze-experiences", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjoEub38DEMk2qchG/meetup-toronto-fight-flight-freeze-experiences", "pageUrlRelative": "/posts/NjoEub38DEMk2qchG/meetup-toronto-fight-flight-freeze-experiences", "linkUrl": "https://www.lesswrong.com/posts/NjoEub38DEMk2qchG/meetup-toronto-fight-flight-freeze-experiences", "postedAtFormatted": "Sunday, March 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%3A%20fight%2Fflight%2Ffreeze%20experiences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%3A%20fight%2Fflight%2Ffreeze%20experiences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjoEub38DEMk2qchG%2Fmeetup-toronto-fight-flight-freeze-experiences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%3A%20fight%2Fflight%2Ffreeze%20experiences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjoEub38DEMk2qchG%2Fmeetup-toronto-fight-flight-freeze-experiences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjoEub38DEMk2qchG%2Fmeetup-toronto-fight-flight-freeze-experiences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ki'>Toronto: fight/flight/freeze experiences</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Tim Horton's, 26 Dundas Street, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Tim Hortons on Victoria Street near Dundas station. We'll be at the back by the paperclip sign.</em></p>\n\n<p>The \"fight or flight\" response evolved to help us cope with life-threatening situations, which called for quick judgement and maybe aggression or bursts of energy. But it comes at a price - when in this mode, blood is diverted away from the parts of the brain that deal with empathy, social relationships and long term planning.</p>\n\n<p>This doesn't sound so good for rationality! In the modern environment, we can go into fight or flight response when it really isn't necessary (or a mild version of it, if we suffer from chronic stress). In this meetup we'll be sharing stories about the fight/flight/freeze response, how well we handled it and what the consequences were!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ki'>Toronto: fight/flight/freeze experiences</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjoEub38DEMk2qchG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.1411019772300125e-06, "legacy": true, "legacyId": "22040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto__fight_flight_freeze_experiences\">Discussion article for the meetup : <a href=\"/meetups/ki\">Toronto: fight/flight/freeze experiences</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Tim Horton's, 26 Dundas Street, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Tim Hortons on Victoria Street near Dundas station. We'll be at the back by the paperclip sign.</em></p>\n\n<p>The \"fight or flight\" response evolved to help us cope with life-threatening situations, which called for quick judgement and maybe aggression or bursts of energy. But it comes at a price - when in this mode, blood is diverted away from the parts of the brain that deal with empathy, social relationships and long term planning.</p>\n\n<p>This doesn't sound so good for rationality! In the modern environment, we can go into fight or flight response when it really isn't necessary (or a mild version of it, if we suffer from chronic stress). In this meetup we'll be sharing stories about the fight/flight/freeze response, how well we handled it and what the consequences were!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto__fight_flight_freeze_experiences1\">Discussion article for the meetup : <a href=\"/meetups/ki\">Toronto: fight/flight/freeze experiences</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto: fight/flight/freeze experiences", "anchor": "Discussion_article_for_the_meetup___Toronto__fight_flight_freeze_experiences", "level": 1}, {"title": "Discussion article for the meetup : Toronto: fight/flight/freeze experiences", "anchor": "Discussion_article_for_the_meetup___Toronto__fight_flight_freeze_experiences1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-17T15:37:26.853Z", "modifiedAt": null, "url": null, "title": "Open thread, March 17-31, 2013", "slug": "open-thread-march-17-31-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.150Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JN7bzkTEqqYC2t3ur/open-thread-march-17-31-2013", "pageUrlRelative": "/posts/JN7bzkTEqqYC2t3ur/open-thread-march-17-31-2013", "linkUrl": "https://www.lesswrong.com/posts/JN7bzkTEqqYC2t3ur/open-thread-march-17-31-2013", "postedAtFormatted": "Sunday, March 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20March%2017-31%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20March%2017-31%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJN7bzkTEqqYC2t3ur%2Fopen-thread-march-17-31-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20March%2017-31%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJN7bzkTEqqYC2t3ur%2Fopen-thread-march-17-31-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJN7bzkTEqqYC2t3ur%2Fopen-thread-march-17-31-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<div id=\"entry_t3_guy\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JN7bzkTEqqYC2t3ur", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 177, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-17T21:49:53.378Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta: ATLessWrong Meetup", "slug": "meetup-atlanta-atlesswrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9q4iXY9P7F78KGywx/meetup-atlanta-atlesswrong-meetup", "pageUrlRelative": "/posts/9q4iXY9P7F78KGywx/meetup-atlanta-atlesswrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/9q4iXY9P7F78KGywx/meetup-atlanta-atlesswrong-meetup", "postedAtFormatted": "Sunday, March 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%3A%20ATLessWrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%3A%20ATLessWrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9q4iXY9P7F78KGywx%2Fmeetup-atlanta-atlesswrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%3A%20ATLessWrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9q4iXY9P7F78KGywx%2Fmeetup-atlanta-atlesswrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9q4iXY9P7F78KGywx%2Fmeetup-atlanta-atlesswrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kj'>Atlanta: ATLessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1314 Hosea L Williams Drive NE, Atlanta, GA 30317</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's get together and rock some rationality. Snacks, games, and various other fun activities will follow the official business. New members welcome! No reading or other preparation is required, just come and join in!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kj'>Atlanta: ATLessWrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9q4iXY9P7F78KGywx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1414947155604802e-06, "legacy": true, "legacyId": "22042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta__ATLessWrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/kj\">Atlanta: ATLessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1314 Hosea L Williams Drive NE, Atlanta, GA 30317</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's get together and rock some rationality. Snacks, games, and various other fun activities will follow the official business. New members welcome! No reading or other preparation is required, just come and join in!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta__ATLessWrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/kj\">Atlanta: ATLessWrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta: ATLessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta__ATLessWrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Atlanta: ATLessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta__ATLessWrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T02:35:56.727Z", "modifiedAt": null, "url": null, "title": "Population Ethics Shouldn't Be About Maximizing Utility", "slug": "population-ethics-shouldn-t-be-about-maximizing-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:27.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sW7xgc9P2SaNnqKHk/population-ethics-shouldn-t-be-about-maximizing-utility", "pageUrlRelative": "/posts/sW7xgc9P2SaNnqKHk/population-ethics-shouldn-t-be-about-maximizing-utility", "linkUrl": "https://www.lesswrong.com/posts/sW7xgc9P2SaNnqKHk/population-ethics-shouldn-t-be-about-maximizing-utility", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Population%20Ethics%20Shouldn't%20Be%20About%20Maximizing%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APopulation%20Ethics%20Shouldn't%20Be%20About%20Maximizing%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsW7xgc9P2SaNnqKHk%2Fpopulation-ethics-shouldn-t-be-about-maximizing-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Population%20Ethics%20Shouldn't%20Be%20About%20Maximizing%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsW7xgc9P2SaNnqKHk%2Fpopulation-ethics-shouldn-t-be-about-maximizing-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsW7xgc9P2SaNnqKHk%2Fpopulation-ethics-shouldn-t-be-about-maximizing-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4288, "htmlBody": "<blockquote>\n<p>let me suggest a moral axiom with apparently very strong intuitive support, no matter what your concept of morality: <em>morality should exist</em>. That is, there should exist creatures who know what is moral, and who act on that. So if your moral theory implies that in ordinary circumstances moral creatures should exterminate themselves, leaving only immoral creatures, or no creatures at all, well that seems a sufficient reductio to solidly reject your moral theory.</p>\n<p>-<a href=\"http://www.overcomingbias.com/2012/04/morality-should-exist.html\">Robin Hanson</a></p>\n</blockquote>\n<p>I agree strongly with the above quote, and I think most other readers will as well. It is good for moral beings to exist and a world with beings who value morality is almost always better than one where they do not. I would like to restate this more precisely as the following axiom: A population in which moral beings exist and have net positive utility, and in which all other creatures in existence also have net positive utility, is always better than a population where moral beings do not exist.</p>\n<p>While the axiom that morality should exist is extremely obvious to most people, there is one strangely popular ethical system that rejects it: total utilitarianism. In this essay I will argue that Total Utilitarianism leads to what I will call the Genocidal Conclusion, which is that there are many situations in which it would be fantastically good for moral creatures to either exterminate themselves, or greatly limit their utility and reproduction in favor of the utility and reproduction of immoral creatures. I will argue that the main reason consequentialist theories of population ethics produce such obviously absurd conclusions is that they continue to focus on maximizing utility<sup>1</sup> in situations where it is possible to create new creatures. I will argue that pure utility maximization is only a valid ethical theory for \"special case\" scenarios where the population is static. I will propose an alternative theory for population ethics I call \"ideal consequentialism\" or \"ideal utilitarianism\" which avoids the Genocidal Conclusion and may also avoid the more famous Repugnant Conclusion.</p>\n<p>&nbsp;</p>\n<p>I will begin my argument by pointing to a common problem in population ethics known as the <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">Mere Addition Paradox (MAP) and the Repugnant Conclusion</a>. Most Less Wrong readers will already be familiar with this problem, so I do not think I need to elaborate on it. You may also be familiar with a even stronger variation called the <a href=\"http://www.colorado.edu/philosophy/heathwood/6100/Huemer%20-%20Defence%20of%20Repugnance.pdf\">Benign Addition Paradox (BAP)</a>. This is essentially the same as the MAP, except that each time one adds more people one also gives a small amount of additional utility to the people who already existed. One then proceeds to redistribute utility between people as normal, eventually arriving at the huge population where everyone's lives are \"barely worth living.\" The point of this is to argue that the Repugnant Conclusion can be arrived at from \"mere addition\" of new people that not only doesn't harm the preexisting-people, but also one that benefits them.</p>\n<p>The next step of my argument involves three slightly tweaked versions of the Benign Addition Paradox. I have not changed the basic logic of the problem, I have just added one small clarifying detail. In the original MAP and BAP it was not specified what sort of values the added individuals in population A+ held. Presumably one was meant to assume that they were ordinary human beings. In the versions of the BAP I am about to present, however, I will specify that the extra individuals added in A+ are not moral creatures, that if they have values at all they are values indifferent to, or opposed to, morality and the other values that the human race holds dear.</p>\n<p><strong>1. The Benign Addition Paradox with Paperclip Maximizers.</strong></p>\n<p>Let us imagine, as usual, a population, A, which has a large group of human beings living lives of very high utility. Let us then add a new population consisting of <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclip maximizers</a>, each of whom is living a life barely worth living. Presumably, for a paperclip maximizer, this would be a life where the paperclip maximizer's existence results in at least one more paperclip in the world than there would have been otherwise.</p>\n<p>Now, one might object that if one creates a paperclip maximizer, and then allows it to create one paperclip, the utility of the other paperclip maximizers will increase above the \"barely worth living\" level, which would obviously make this thought experiment nonalagous with the original MAP and BAP. To prevent this we will assume that each paperclip maximizer that is created has a slightly different values on what the ideal size, color, and composition of the paperclip they are trying to produce is. So the Purple 2 centimeter Plastic Paperclip Maximizer gains no addition utility from when the Silver Iron 1 centimeter Paperclip Maximizer makes a paperclip.</p>\n<p>So again, let us add these paperclip maximizers to population A, and in the process give one extra utilon of utility to each preexisting person in A. This is a good thing, right? After all, everyone in A benefited, and the paperclippers get to exist and make paperclips. So clearly A+, the new population, is better than A.</p>\n<p>Now let's take the next step, the transition from population A+ to population B. Take some of the utility from the human beings and convert it into paperclips. This is a good thing, right?</p>\n<p>So let us repeat these steps adding paperclip maximizers and utility, and then redistributing utility. Eventually we reach population Z, where there is a vast amount of paperclip maximizers, a vast amount of many different kinds of paperclips, and a small amount of human beings living lives barely worth living.</p>\n<p>Obviously Z is better than A, right? We should not fear the creation of a paperclip maximizing AI, but welcome it! Forget about things like <a href=\"/lw/ww/high_challenge/\">high challenge</a>, love, <a href=\"/lw/xt/interpersonal_entanglement/\">interpersonal entanglement</a>, <a href=\"/lw/xy/the_fun_theory_sequence/\">complex fun</a>, and so on! Those things just don't produce the kind of utility that paperclip maximization has the potential to do!</p>\n<p>Or maybe there is something seriously wrong with the moral assumptions behind the Mere Addition and Benign Addition Paradoxes.</p>\n<p>But you might argue that I am using an unrealistic example. Creatures like Paperclip Maximizers may be <a href=\"/lw/f0/hardened_problems_make_brittle_models/\">so far removed from normal human experience</a> that we have trouble thinking about them properly. So let's replay the Benign Addition Paradox again, but with creatures we might actually expect to meet in real life, and we know we actually value.</p>\n<p><strong>2. The Benign Addition Paradox with Non-Sapient Animals</strong></p>\n<p>You know the drill by now. Take population A, add a new population to it, while very slightly increasing the utility of the original population. This time let's have it be some kind animal that is capable of feeling pleasure and pain, but is not capable of modeling possible alternative futures and choosing between them (in other words, it is not capable of having \"values\" or being \"moral\"). A lizard or a mouse, for example. Each one feels slightly more pleasure than pain in its lifetime, so it can be said to have a life barely worth living. Convert A+ to B. Take the utilons that the human beings are using to experience things like <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">curiosity, beatitude, wisdom, beauty, harmony</a><a href=\"/lw/xy/the_fun_theory_sequence/\"></a>, morality, and so on, and convert it into pleasure for the animals.</p>\n<p>We end up with population Z, with a vast amount of mice or lizards with lives just barely worth living, and a small amount of human beings with lives barely worth living. Terrific! Why do we bother creating humans at all! Let's just create tons of mice and inject them full of heroin! It's a much more efficient way to generate utility!</p>\n<p><strong>3. The Benign Addition Paradox with Sociopaths</strong></p>\n<p>What new population will we add to A this time? How about some other human beings, who all have <a href=\"http://en.wikipedia.org/wiki/Antisocial_personality_disorder\">anti-social personality disorder</a>? True, they lack the key, crucial value of <a href=\"/lw/xs/sympathetic_minds/\">sympathy</a> that defines so much of human behavior. But they don't <a href=\"/lw/y3/value_is_fragile/\">seem to miss it</a>. And their lives are barely worth living, so obviously A+ has greater utility than A. If given a chance the sociopaths will reduce the utility of other people to negative levels, but let's assume that that is somehow prevented in this case.</p>\n<p>Eventually we get to Z, with a vast population of sociopaths and a small population of normal human beings, all living lives just barely worth living. That has more utility, right? True, the sociopaths place no value on things like friendship, love, compassion, empathy, and so on. And true, the sociopaths are immoral beings who do not care in the slightest about right and wrong. But what does that matter? Utility is being maximized, and surely that is what population ethics is all about!</p>\n<p><strong>Asteroid!</strong></p>\n<p>Let's suppose an asteroid is approaching each of the four population Zs discussed before. It can only be deflected by so much. Your choice is, save the original population of humans from A, or save the vast new population. The choice is obvious. In 1, 2, and 3, each individual has the same level utility, so obviously we should choose which option saves a greater number of individuals.</p>\n<p>Bam! The asteroid strikes. The end result in all four scenarios is a world in which all the moral creatures are destroyed. It is a world without the many complex values that human beings possess. Each world, for the most part, lack things like complex challenge, imagination, friendship, empathy, love, and the other complex values that human beings prize. But so what? The purpose of population ethics is to maximize utility, not silly, frivolous things like morality, or the other <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complex values</a> of the human race. That means that any form of utility that is easier to produce than those values is obviously superior. It's easier to make pleasure and paperclips than it is to make eudaemonia, so that's the form of utility that ought to be maximized, right? And as for making sure moral beings exist, well that's just ridiculous. The valuable processing power they're using to care about morality could be being used to make more paperclips or more mice injected with heroin! Obviously it would be better if they died off, right?</p>\n<p>I'm going to go out on a limb and say \"Wrong.\"</p>\n<p><strong>Is this realistic?</strong></p>\n<p>Now, to fair, in the <a href=\"http://www.overcomingbias.com/2012/04/morality-should-exist.html\">Overcoming Bias page</a> I quoted, Robin Hanson also says:</p>\n<blockquote>\n<p>I&rsquo;m not saying I can&rsquo;t imagine <em>any</em> possible circumstances where moral creatures shouldn&rsquo;t die off, but I am saying that those are not ordinary circumstances.</p>\n</blockquote>\n<p>Maybe the scenarios I am proposing are just too extraordinary. But I don't think this is the case. I imagine that the circumstances Robin had in mind were probably something like \"either all moral creatures die off, or all moral creatures are tortured 24/7 for all eternity.\"</p>\n<p>Any purely utility-maximizing theory of population ethics that counts both the complex values of human beings, and the pleasure of animals, as \"utility\" should inevitably draw the conclusion that human beings ought to limit their reproduction to the bare minimum necessary to maintain the infrastructure to sustain a vastly huge population of non-human animals (preferably animals dosed with some sort of pleasure-causing drug). And if some way is found to maintain that infrastructure automatically, without the need for human beings, then the logical conclusion is that human beings are a waste of resources (as are chimps, gorillas, dolphins, and any other animal that is even remotely capable of having values or morality). Furthermore, even if the human race cannot practically be replaced with automated infrastructure, this should be an end result that the adherents of this theory should be yearning for.<sup>2</sup> There should be much wailing and gnashing of teeth among moral philosophers that exterminating the human race is impractical, and much hope that someday in the future it will not be.</p>\n<p>I call this the \"Genocidal Conclusion\" or \"GC.\" On the macro level the GC manifests as the idea that the human race ought to be exterminated and replaced with creatures whose preferences are easier to satisfy. On the micro level it manifests as the idea that it is perfectly acceptable to kill someone who is destined to live a perfectly good and worthwhile life and replace them with another person who would have a slightly higher level of utility.</p>\n<p><strong>Population Ethics isn't About Maximizing Utility</strong></p>\n<p>I am going to make a rather radical proposal. I am going to argue that the consequentialist's favorite maxim, \"maximize utility,\" <em>only applies to scenarios where creating new people or creatures is off the table</em>. I think we need an entirely different ethical framework to describe what ought to be done when it is possible to create new people. I am not by any means saying that \"which option would result in more utility\" is never a morally relevant consideration when deciding to create a new person, but I definitely think it is not the only one.<sup>3</sup></p>\n<p>So what do I propose as a replacement to utility maximization? I would argue in favor of a system that promotes a wide range of ideals. Doing some research, I discovered that G. E. Moore had in fact proposed a form of \"<a href=\"http://en.wikipedia.org/wiki/Utilitarianism#Ideal_Utilitarianism\">ideal utilitarianism</a>\" in the early 20th century.<sup>4</sup> However, I think that \"ideal consequentialism\" might be a better term for this system, since it isn't just about aggregating utility functions.</p>\n<p>What are some of the ideals that an ideal consequentialist theory of population ethics might seek to promote? I've already hinted at what I think they are: Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom... mutual affection, love, friendship, cooperation; <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">all those other important human universals</a>, plus all the stuff in the <a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory Sequence</a>. When considering what sort of creatures to create we ought to create creatures that value <em>those</em> things. Not necessarily, all of them, or in the same proportions, for diversity is an important ideal as well, but they should value a great many of those ideals.</p>\n<p>Now, lest you worry that this theory has any totalitarian implications, let me make it clear that I am not saying we should force these values on creatures that do not share them. Forcing a paperclip maximizer to pretend to make friends and love people does not do anything to promote the ideals of Friendship and Love. Forcing a chimpanzee to listen while you read the Sequences to it does not promote the values of Truth and Knowledge. Those ideals require <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">both a subjective and objective component</a>. The only way to promote those ideals is to create a creature that includes them as part of its utility function and then help it maximize its utility.</p>\n<p>I am also certainly not saying that there is never any value in creating a creature that does not possess these values. There are obviously many circumstances where it is good to create nonhuman animals. There may even be some circumstances where a paperclip maximizer could be of value. My argument is simply that it is <em>most </em>important to make sure that creatures who value these various ideals exist.</p>\n<p>I am also not suggesting that it is morally acceptable to casually inflict horrible harms upon a creature with non-human values if we screw up and create one by accident. If promoting ideals and maximizing utility are separate values then it may be that once we have created such a creature we have a duty to make sure it lives a good life, even if it was a bad thing to create it in the first place. <a href=\"/lw/x7/cant_unbirth_a_child/\">You can't unbirth a child.</a><sup>5</sup></p>\n<p>It also seems to me that in addition to having ideals about what sort of creatures should exist, we also have ideals about how utility ought to be concentrated. If this is the case then ideal consequentialism may be able to block some forms of the Repugnant Conclusion, even if situations where the only creatures whose creation is being considered are human beings. If it is acceptable to create humans instead of paperclippers, even if the paperclippers would have higher utility, it may also be acceptable to create ten humans with a utility of ten each instead of a hundred humans with a utility of 1.01 each.</p>\n<p><strong>Why Did We Become Convinced that Maximizing Utility was the Sole Good?</strong></p>\n<p>Population ethics was, until comparatively recently, a fallow field in ethics. And in situations where there is no option to increase the population, maximizing utility is the only consideration that's really relevant. If you've created creatures that value the right ideals, then all that is left to be done is to maximize their utility. If you've created creatures that do not value the right ideals, there is no value to be had in attempting to force them to embrace those ideals. As I've said before, you will not promote the values of Love and Friendship by creating a paperclip maximizer and forcing it to pretend to love people and make friends.</p>\n<p>So in situations where the population is constant, \"maximize utility\" is a decent approximation of the <a href=\"/lw/sm/the_meaning_of_right/\">meaning of right</a>. It's only when the population can be added to that morality becomes much more <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complicated</a>.</p>\n<p>Another thing to blame is human-centric reasoning. When people defend the Repugnant Conclusion they tend to point out that a life barely worth living is not as bad as it would seem at first glance. They emphasize that it need not be a boring life, it may be a life full of ups and downs where the ups just barely outweigh the downs. A life worth living, they say, is a life one would <a href=\"/lw/dso/the_mere_cable_channel_addition_paradox/73x2\">choose to live</a>. Derek Parfit developed this idea to some extent by arguing that there are certain values that are \"<a href=\"http://www.repugnant-conclusion.com/portmore-repugnant.pdf\">discontinuous</a>\" and that one needs to experience many of them in order to truly have a life worth living.</p>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">Orthogonality Thesis </a>throws all these arguments out the window. It is possible to create an intelligence to execute <em>any</em> utility function, no matter what it is. If human beings have all sorts of complex needs that must be fulfilled in order to for them lead worthwhile lives, then you could create more worthwhile lives by killing the human race and replacing them with something less finicky. Maybe happy cows. Maybe paperclip maximizers. Or how about some creature whose only desire is to live for one second and then die. If we created such a creature and then killed it we would reap huge amounts of utility, for we would have created a creature that got everything it wanted out of life!</p>\n<p><strong>How Intuitive is the Mere Addition Principle, Really?</strong></p>\n<p>I think most people would agree that morality should exist, and that therefore any system of population ethics should not lead to the Genocidal Conclusion. But which step in the Benign Addition Paradox should we reject? We could reject the step where utility is redistributed. But that seems wrong, most people seem to consider it bad for animals and sociopaths to suffer, and that it is acceptable to inflict at least <em>some</em> amount of disutilities on human beings to prevent such suffering.</p>\n<p>It seems more logical to reject the Mere Addition Principle. In other words, maybe we ought to reject the idea that the mere addition of more lives-worth-living cannot make the world worse. And in turn, we should probably also reject the Benign Addition Principle. Adding more lives-worth-living may be capable of making the world worse, even if doing so also slightly benefits existing people. Fortunately this isn't a very hard principle to reject. While many moral philosophers treat it as obviously correct, nearly everyone else rejects this principle in day-to-day life.</p>\n<p>Now, I'm obviously not saying that people's behavior in their day-to-day lives is always good, it may be that they are morally mistaken. But I think the fact that so many people seem to implicitly reject it provides some sort of evidence against it.</p>\n<p>Take people's decision to have children. Many people choose to have fewer children than they otherwise would because they do not believe they will be able to adequately care for them, at least not without inflicting large disutilities on themselves. If most people accepted the Mere Addition Principle there would be a simple solution for this: have more children and then neglect them! True, the children's lives would be terrible while they were growing up, but once they've grown up and are on their own there's a good chance they may be able to lead worthwhile lives. Not only that, it may be possible to trick the welfare system into giving you money for the children you neglect, which would satisfy the Benign Addition Principle.</p>\n<p>Yet most people choose not to have children and neglect them. And furthermore they seem to think that they have a moral duty not to do so, that a world where they choose to not have neglected children is <em>better</em> than one that they don't. What is wrong with them?</p>\n<p>Another example is a common political view many people have. Many people believe that impoverished people should have fewer children because of the burden doing so would place on the welfare system. They also believe that it would be bad to get rid of the welfare system altogether. If the Benign Addition Principle were as obvious as it seems, they would instead advocate for the abolition of the welfare system, and encourage impoverished people to have more children. Assuming most impoverished people live lives worth living, this is <em>exactly analogous</em> to the BAP, it would create more people, while benefiting existing ones (the people who pay less taxes because of the abolition of the welfare system).</p>\n<p>Yet again, most people choose to reject this line of reasoning. The BAP does not seem to be an obvious and intuitive principle at all.</p>\n<p><strong>The Genocidal Conclusion is <em>Really </em>Repugnant</strong></p>\n<p>There is nearly nothing repugnant than the Genocidal Conclusion. Pretty much the only way a line of moral reasoning could go <em>more </em>wrong would be concluding that we have a moral duty to cause suffering, as an end in itself. This means that it's fairly easy to counter any argument in favor of total utilitarianism that argues the alternative I am promoting has odd conclusions that do not fit some of our moral intuitions, while total utilitarianism does not. Is that conclusion more insane than the Genocidal Conclusion? If it isn't, total utilitarianism should still be rejected.</p>\n<p><strong>Ideal Consequentialism Needs a Lot of Work</strong></p>\n<p>I do think that Ideal Consequentialism needs some serious ironing out. I haven't really developed it into a logical and rigorous system, at this point it's barely even a rough framework. There are many questions that stump me. In particular I am not quite sure what population principle I should develop. It's hard to develop one that rejects the MAP without leading to weird conclusions, like that it's bad to create someone of high utility if a population of even higher utility existed long ago. It's a difficult problem to work on, and it would be interesting to see if anyone else had any ideas.</p>\n<p>But just because I don't have an alternative fully worked out doesn't mean I can't reject Total Utilitarianism. It leads to the conclusion that a world with no love, curiosity, complex challenge, friendship, morality, or any other value the human race holds dear is an ideal, desirable world, if there is a sufficient amount of some other creature with a simpler utility function. Morality should exist, and because of that, total utilitarianism must be rejected as a moral system.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>I have been asked to note that when I use the phrase \"utility\" I am usually referring to a concept that is called \"<a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Distinctness_from_other_notions_of_utility\">E-utility</a>,\" rather than the Von Neumann-Morgenstern utility that is sometimes discussed in decision theory. The difference is that in VNM one's moral views are included in one's utility function, whereas in E-utility they are not. So if one chooses to harm oneself to help others because one believes that is morally right, one has higher VNM utility, but lower E-utility.</p>\n<p><sup>2</sup>There is a certain argument against the Repugnant Conclusion that goes that, as the steps of the Mere Addition Paradox are followed the world will lose its last symphony, its last great book, and so on. I have always considered this to be an invalid argument because the world of the RC doesn't necessarily have to be one where these things don't exist, it could be one where they exist, but are enjoyed very rarely. The Genocidal Conclusion brings this argument back in force. Creating creatures that can appreciate symphonies and great books is very inefficient compared to creating bunny rabbits pumped full of heroin.</p>\n<p><sup>3</sup>Total Utilitarianism was originally introduced to population ethics as a possible solution to the <a href=\"http://plato.stanford.edu/entries/nonidentity-problem/\">Non-Identity Problem</a>. I certainly agree that such a problem needs a solution, even if Total Utilitarianism doesn't work out as that solution.</p>\n<p><sup>4</sup>I haven't read a lot of Moore, most of my ideas were extrapolated from other things I read on Less Wrong. I just mentioned him because in my research I noticed his concept of \"ideal utilitarianism\" resembled my ideas. While I do think he was on the right track he does commit the <a href=\"/lw/oi/mind_projection_fallacy/\">Mind Projection Fallacy</a> a lot. For instance, he seems to think that one could promote beauty by creating beautiful objects, even if there were no creatures with standards of beauty around to appreciate them. This is why I am careful to emphasize that to promote ideals like love and beauty one must <em>create creatures capable of feeling love and experiencing beauty. </em></p>\n<p><em><sup>5</sup></em>My tentative answer to the question Eliezer poses in \"<a href=\"/lw/x7/cant_unbirth_a_child/\">You Can't Unbirth a Child</a>\" is that human beings may have a duty to allow the cheesecake maximizers to build some amount of giant cheesecakes, but they would also have a moral duty to limit such creatures' reproduction in order to spare resources to create more creatures with humane values.</p>\n<p>EDITED: To make a point about ideal consequentialism clearer, based on<a href=\"/lw/guk/population_ethics_shouldnt_be_about_maximizing/8mne?context=3\"> AlexMennen's</a> criticisms.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uG75MELqjCEfciaRp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sW7xgc9P2SaNnqKHk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 4, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "21836", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>let me suggest a moral axiom with apparently very strong intuitive support, no matter what your concept of morality: <em>morality should exist</em>. That is, there should exist creatures who know what is moral, and who act on that. So if your moral theory implies that in ordinary circumstances moral creatures should exterminate themselves, leaving only immoral creatures, or no creatures at all, well that seems a sufficient reductio to solidly reject your moral theory.</p>\n<p>-<a href=\"http://www.overcomingbias.com/2012/04/morality-should-exist.html\">Robin Hanson</a></p>\n</blockquote>\n<p>I agree strongly with the above quote, and I think most other readers will as well. It is good for moral beings to exist and a world with beings who value morality is almost always better than one where they do not. I would like to restate this more precisely as the following axiom: A population in which moral beings exist and have net positive utility, and in which all other creatures in existence also have net positive utility, is always better than a population where moral beings do not exist.</p>\n<p>While the axiom that morality should exist is extremely obvious to most people, there is one strangely popular ethical system that rejects it: total utilitarianism. In this essay I will argue that Total Utilitarianism leads to what I will call the Genocidal Conclusion, which is that there are many situations in which it would be fantastically good for moral creatures to either exterminate themselves, or greatly limit their utility and reproduction in favor of the utility and reproduction of immoral creatures. I will argue that the main reason consequentialist theories of population ethics produce such obviously absurd conclusions is that they continue to focus on maximizing utility<sup>1</sup> in situations where it is possible to create new creatures. I will argue that pure utility maximization is only a valid ethical theory for \"special case\" scenarios where the population is static. I will propose an alternative theory for population ethics I call \"ideal consequentialism\" or \"ideal utilitarianism\" which avoids the Genocidal Conclusion and may also avoid the more famous Repugnant Conclusion.</p>\n<p>&nbsp;</p>\n<p>I will begin my argument by pointing to a common problem in population ethics known as the <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">Mere Addition Paradox (MAP) and the Repugnant Conclusion</a>. Most Less Wrong readers will already be familiar with this problem, so I do not think I need to elaborate on it. You may also be familiar with a even stronger variation called the <a href=\"http://www.colorado.edu/philosophy/heathwood/6100/Huemer%20-%20Defence%20of%20Repugnance.pdf\">Benign Addition Paradox (BAP)</a>. This is essentially the same as the MAP, except that each time one adds more people one also gives a small amount of additional utility to the people who already existed. One then proceeds to redistribute utility between people as normal, eventually arriving at the huge population where everyone's lives are \"barely worth living.\" The point of this is to argue that the Repugnant Conclusion can be arrived at from \"mere addition\" of new people that not only doesn't harm the preexisting-people, but also one that benefits them.</p>\n<p>The next step of my argument involves three slightly tweaked versions of the Benign Addition Paradox. I have not changed the basic logic of the problem, I have just added one small clarifying detail. In the original MAP and BAP it was not specified what sort of values the added individuals in population A+ held. Presumably one was meant to assume that they were ordinary human beings. In the versions of the BAP I am about to present, however, I will specify that the extra individuals added in A+ are not moral creatures, that if they have values at all they are values indifferent to, or opposed to, morality and the other values that the human race holds dear.</p>\n<p><strong id=\"1__The_Benign_Addition_Paradox_with_Paperclip_Maximizers_\">1. The Benign Addition Paradox with Paperclip Maximizers.</strong></p>\n<p>Let us imagine, as usual, a population, A, which has a large group of human beings living lives of very high utility. Let us then add a new population consisting of <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclip maximizers</a>, each of whom is living a life barely worth living. Presumably, for a paperclip maximizer, this would be a life where the paperclip maximizer's existence results in at least one more paperclip in the world than there would have been otherwise.</p>\n<p>Now, one might object that if one creates a paperclip maximizer, and then allows it to create one paperclip, the utility of the other paperclip maximizers will increase above the \"barely worth living\" level, which would obviously make this thought experiment nonalagous with the original MAP and BAP. To prevent this we will assume that each paperclip maximizer that is created has a slightly different values on what the ideal size, color, and composition of the paperclip they are trying to produce is. So the Purple 2 centimeter Plastic Paperclip Maximizer gains no addition utility from when the Silver Iron 1 centimeter Paperclip Maximizer makes a paperclip.</p>\n<p>So again, let us add these paperclip maximizers to population A, and in the process give one extra utilon of utility to each preexisting person in A. This is a good thing, right? After all, everyone in A benefited, and the paperclippers get to exist and make paperclips. So clearly A+, the new population, is better than A.</p>\n<p>Now let's take the next step, the transition from population A+ to population B. Take some of the utility from the human beings and convert it into paperclips. This is a good thing, right?</p>\n<p>So let us repeat these steps adding paperclip maximizers and utility, and then redistributing utility. Eventually we reach population Z, where there is a vast amount of paperclip maximizers, a vast amount of many different kinds of paperclips, and a small amount of human beings living lives barely worth living.</p>\n<p>Obviously Z is better than A, right? We should not fear the creation of a paperclip maximizing AI, but welcome it! Forget about things like <a href=\"/lw/ww/high_challenge/\">high challenge</a>, love, <a href=\"/lw/xt/interpersonal_entanglement/\">interpersonal entanglement</a>, <a href=\"/lw/xy/the_fun_theory_sequence/\">complex fun</a>, and so on! Those things just don't produce the kind of utility that paperclip maximization has the potential to do!</p>\n<p>Or maybe there is something seriously wrong with the moral assumptions behind the Mere Addition and Benign Addition Paradoxes.</p>\n<p>But you might argue that I am using an unrealistic example. Creatures like Paperclip Maximizers may be <a href=\"/lw/f0/hardened_problems_make_brittle_models/\">so far removed from normal human experience</a> that we have trouble thinking about them properly. So let's replay the Benign Addition Paradox again, but with creatures we might actually expect to meet in real life, and we know we actually value.</p>\n<p><strong id=\"2__The_Benign_Addition_Paradox_with_Non_Sapient_Animals\">2. The Benign Addition Paradox with Non-Sapient Animals</strong></p>\n<p>You know the drill by now. Take population A, add a new population to it, while very slightly increasing the utility of the original population. This time let's have it be some kind animal that is capable of feeling pleasure and pain, but is not capable of modeling possible alternative futures and choosing between them (in other words, it is not capable of having \"values\" or being \"moral\"). A lizard or a mouse, for example. Each one feels slightly more pleasure than pain in its lifetime, so it can be said to have a life barely worth living. Convert A+ to B. Take the utilons that the human beings are using to experience things like <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">curiosity, beatitude, wisdom, beauty, harmony</a><a href=\"/lw/xy/the_fun_theory_sequence/\"></a>, morality, and so on, and convert it into pleasure for the animals.</p>\n<p>We end up with population Z, with a vast amount of mice or lizards with lives just barely worth living, and a small amount of human beings with lives barely worth living. Terrific! Why do we bother creating humans at all! Let's just create tons of mice and inject them full of heroin! It's a much more efficient way to generate utility!</p>\n<p><strong id=\"3__The_Benign_Addition_Paradox_with_Sociopaths\">3. The Benign Addition Paradox with Sociopaths</strong></p>\n<p>What new population will we add to A this time? How about some other human beings, who all have <a href=\"http://en.wikipedia.org/wiki/Antisocial_personality_disorder\">anti-social personality disorder</a>? True, they lack the key, crucial value of <a href=\"/lw/xs/sympathetic_minds/\">sympathy</a> that defines so much of human behavior. But they don't <a href=\"/lw/y3/value_is_fragile/\">seem to miss it</a>. And their lives are barely worth living, so obviously A+ has greater utility than A. If given a chance the sociopaths will reduce the utility of other people to negative levels, but let's assume that that is somehow prevented in this case.</p>\n<p>Eventually we get to Z, with a vast population of sociopaths and a small population of normal human beings, all living lives just barely worth living. That has more utility, right? True, the sociopaths place no value on things like friendship, love, compassion, empathy, and so on. And true, the sociopaths are immoral beings who do not care in the slightest about right and wrong. But what does that matter? Utility is being maximized, and surely that is what population ethics is all about!</p>\n<p><strong id=\"Asteroid_\">Asteroid!</strong></p>\n<p>Let's suppose an asteroid is approaching each of the four population Zs discussed before. It can only be deflected by so much. Your choice is, save the original population of humans from A, or save the vast new population. The choice is obvious. In 1, 2, and 3, each individual has the same level utility, so obviously we should choose which option saves a greater number of individuals.</p>\n<p>Bam! The asteroid strikes. The end result in all four scenarios is a world in which all the moral creatures are destroyed. It is a world without the many complex values that human beings possess. Each world, for the most part, lack things like complex challenge, imagination, friendship, empathy, love, and the other complex values that human beings prize. But so what? The purpose of population ethics is to maximize utility, not silly, frivolous things like morality, or the other <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complex values</a> of the human race. That means that any form of utility that is easier to produce than those values is obviously superior. It's easier to make pleasure and paperclips than it is to make eudaemonia, so that's the form of utility that ought to be maximized, right? And as for making sure moral beings exist, well that's just ridiculous. The valuable processing power they're using to care about morality could be being used to make more paperclips or more mice injected with heroin! Obviously it would be better if they died off, right?</p>\n<p>I'm going to go out on a limb and say \"Wrong.\"</p>\n<p><strong id=\"Is_this_realistic_\">Is this realistic?</strong></p>\n<p>Now, to fair, in the <a href=\"http://www.overcomingbias.com/2012/04/morality-should-exist.html\">Overcoming Bias page</a> I quoted, Robin Hanson also says:</p>\n<blockquote>\n<p>I\u2019m not saying I can\u2019t imagine <em>any</em> possible circumstances where moral creatures shouldn\u2019t die off, but I am saying that those are not ordinary circumstances.</p>\n</blockquote>\n<p>Maybe the scenarios I am proposing are just too extraordinary. But I don't think this is the case. I imagine that the circumstances Robin had in mind were probably something like \"either all moral creatures die off, or all moral creatures are tortured 24/7 for all eternity.\"</p>\n<p>Any purely utility-maximizing theory of population ethics that counts both the complex values of human beings, and the pleasure of animals, as \"utility\" should inevitably draw the conclusion that human beings ought to limit their reproduction to the bare minimum necessary to maintain the infrastructure to sustain a vastly huge population of non-human animals (preferably animals dosed with some sort of pleasure-causing drug). And if some way is found to maintain that infrastructure automatically, without the need for human beings, then the logical conclusion is that human beings are a waste of resources (as are chimps, gorillas, dolphins, and any other animal that is even remotely capable of having values or morality). Furthermore, even if the human race cannot practically be replaced with automated infrastructure, this should be an end result that the adherents of this theory should be yearning for.<sup>2</sup> There should be much wailing and gnashing of teeth among moral philosophers that exterminating the human race is impractical, and much hope that someday in the future it will not be.</p>\n<p>I call this the \"Genocidal Conclusion\" or \"GC.\" On the macro level the GC manifests as the idea that the human race ought to be exterminated and replaced with creatures whose preferences are easier to satisfy. On the micro level it manifests as the idea that it is perfectly acceptable to kill someone who is destined to live a perfectly good and worthwhile life and replace them with another person who would have a slightly higher level of utility.</p>\n<p><strong id=\"Population_Ethics_isn_t_About_Maximizing_Utility\">Population Ethics isn't About Maximizing Utility</strong></p>\n<p>I am going to make a rather radical proposal. I am going to argue that the consequentialist's favorite maxim, \"maximize utility,\" <em>only applies to scenarios where creating new people or creatures is off the table</em>. I think we need an entirely different ethical framework to describe what ought to be done when it is possible to create new people. I am not by any means saying that \"which option would result in more utility\" is never a morally relevant consideration when deciding to create a new person, but I definitely think it is not the only one.<sup>3</sup></p>\n<p>So what do I propose as a replacement to utility maximization? I would argue in favor of a system that promotes a wide range of ideals. Doing some research, I discovered that G. E. Moore had in fact proposed a form of \"<a href=\"http://en.wikipedia.org/wiki/Utilitarianism#Ideal_Utilitarianism\">ideal utilitarianism</a>\" in the early 20th century.<sup>4</sup> However, I think that \"ideal consequentialism\" might be a better term for this system, since it isn't just about aggregating utility functions.</p>\n<p>What are some of the ideals that an ideal consequentialist theory of population ethics might seek to promote? I've already hinted at what I think they are: Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom... mutual affection, love, friendship, cooperation; <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">all those other important human universals</a>, plus all the stuff in the <a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory Sequence</a>. When considering what sort of creatures to create we ought to create creatures that value <em>those</em> things. Not necessarily, all of them, or in the same proportions, for diversity is an important ideal as well, but they should value a great many of those ideals.</p>\n<p>Now, lest you worry that this theory has any totalitarian implications, let me make it clear that I am not saying we should force these values on creatures that do not share them. Forcing a paperclip maximizer to pretend to make friends and love people does not do anything to promote the ideals of Friendship and Love. Forcing a chimpanzee to listen while you read the Sequences to it does not promote the values of Truth and Knowledge. Those ideals require <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">both a subjective and objective component</a>. The only way to promote those ideals is to create a creature that includes them as part of its utility function and then help it maximize its utility.</p>\n<p>I am also certainly not saying that there is never any value in creating a creature that does not possess these values. There are obviously many circumstances where it is good to create nonhuman animals. There may even be some circumstances where a paperclip maximizer could be of value. My argument is simply that it is <em>most </em>important to make sure that creatures who value these various ideals exist.</p>\n<p>I am also not suggesting that it is morally acceptable to casually inflict horrible harms upon a creature with non-human values if we screw up and create one by accident. If promoting ideals and maximizing utility are separate values then it may be that once we have created such a creature we have a duty to make sure it lives a good life, even if it was a bad thing to create it in the first place. <a href=\"/lw/x7/cant_unbirth_a_child/\">You can't unbirth a child.</a><sup>5</sup></p>\n<p>It also seems to me that in addition to having ideals about what sort of creatures should exist, we also have ideals about how utility ought to be concentrated. If this is the case then ideal consequentialism may be able to block some forms of the Repugnant Conclusion, even if situations where the only creatures whose creation is being considered are human beings. If it is acceptable to create humans instead of paperclippers, even if the paperclippers would have higher utility, it may also be acceptable to create ten humans with a utility of ten each instead of a hundred humans with a utility of 1.01 each.</p>\n<p><strong id=\"Why_Did_We_Become_Convinced_that_Maximizing_Utility_was_the_Sole_Good_\">Why Did We Become Convinced that Maximizing Utility was the Sole Good?</strong></p>\n<p>Population ethics was, until comparatively recently, a fallow field in ethics. And in situations where there is no option to increase the population, maximizing utility is the only consideration that's really relevant. If you've created creatures that value the right ideals, then all that is left to be done is to maximize their utility. If you've created creatures that do not value the right ideals, there is no value to be had in attempting to force them to embrace those ideals. As I've said before, you will not promote the values of Love and Friendship by creating a paperclip maximizer and forcing it to pretend to love people and make friends.</p>\n<p>So in situations where the population is constant, \"maximize utility\" is a decent approximation of the <a href=\"/lw/sm/the_meaning_of_right/\">meaning of right</a>. It's only when the population can be added to that morality becomes much more <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complicated</a>.</p>\n<p>Another thing to blame is human-centric reasoning. When people defend the Repugnant Conclusion they tend to point out that a life barely worth living is not as bad as it would seem at first glance. They emphasize that it need not be a boring life, it may be a life full of ups and downs where the ups just barely outweigh the downs. A life worth living, they say, is a life one would <a href=\"/lw/dso/the_mere_cable_channel_addition_paradox/73x2\">choose to live</a>. Derek Parfit developed this idea to some extent by arguing that there are certain values that are \"<a href=\"http://www.repugnant-conclusion.com/portmore-repugnant.pdf\">discontinuous</a>\" and that one needs to experience many of them in order to truly have a life worth living.</p>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">Orthogonality Thesis </a>throws all these arguments out the window. It is possible to create an intelligence to execute <em>any</em> utility function, no matter what it is. If human beings have all sorts of complex needs that must be fulfilled in order to for them lead worthwhile lives, then you could create more worthwhile lives by killing the human race and replacing them with something less finicky. Maybe happy cows. Maybe paperclip maximizers. Or how about some creature whose only desire is to live for one second and then die. If we created such a creature and then killed it we would reap huge amounts of utility, for we would have created a creature that got everything it wanted out of life!</p>\n<p><strong id=\"How_Intuitive_is_the_Mere_Addition_Principle__Really_\">How Intuitive is the Mere Addition Principle, Really?</strong></p>\n<p>I think most people would agree that morality should exist, and that therefore any system of population ethics should not lead to the Genocidal Conclusion. But which step in the Benign Addition Paradox should we reject? We could reject the step where utility is redistributed. But that seems wrong, most people seem to consider it bad for animals and sociopaths to suffer, and that it is acceptable to inflict at least <em>some</em> amount of disutilities on human beings to prevent such suffering.</p>\n<p>It seems more logical to reject the Mere Addition Principle. In other words, maybe we ought to reject the idea that the mere addition of more lives-worth-living cannot make the world worse. And in turn, we should probably also reject the Benign Addition Principle. Adding more lives-worth-living may be capable of making the world worse, even if doing so also slightly benefits existing people. Fortunately this isn't a very hard principle to reject. While many moral philosophers treat it as obviously correct, nearly everyone else rejects this principle in day-to-day life.</p>\n<p>Now, I'm obviously not saying that people's behavior in their day-to-day lives is always good, it may be that they are morally mistaken. But I think the fact that so many people seem to implicitly reject it provides some sort of evidence against it.</p>\n<p>Take people's decision to have children. Many people choose to have fewer children than they otherwise would because they do not believe they will be able to adequately care for them, at least not without inflicting large disutilities on themselves. If most people accepted the Mere Addition Principle there would be a simple solution for this: have more children and then neglect them! True, the children's lives would be terrible while they were growing up, but once they've grown up and are on their own there's a good chance they may be able to lead worthwhile lives. Not only that, it may be possible to trick the welfare system into giving you money for the children you neglect, which would satisfy the Benign Addition Principle.</p>\n<p>Yet most people choose not to have children and neglect them. And furthermore they seem to think that they have a moral duty not to do so, that a world where they choose to not have neglected children is <em>better</em> than one that they don't. What is wrong with them?</p>\n<p>Another example is a common political view many people have. Many people believe that impoverished people should have fewer children because of the burden doing so would place on the welfare system. They also believe that it would be bad to get rid of the welfare system altogether. If the Benign Addition Principle were as obvious as it seems, they would instead advocate for the abolition of the welfare system, and encourage impoverished people to have more children. Assuming most impoverished people live lives worth living, this is <em>exactly analogous</em> to the BAP, it would create more people, while benefiting existing ones (the people who pay less taxes because of the abolition of the welfare system).</p>\n<p>Yet again, most people choose to reject this line of reasoning. The BAP does not seem to be an obvious and intuitive principle at all.</p>\n<p><strong id=\"The_Genocidal_Conclusion_is_Really_Repugnant\">The Genocidal Conclusion is <em>Really </em>Repugnant</strong></p>\n<p>There is nearly nothing repugnant than the Genocidal Conclusion. Pretty much the only way a line of moral reasoning could go <em>more </em>wrong would be concluding that we have a moral duty to cause suffering, as an end in itself. This means that it's fairly easy to counter any argument in favor of total utilitarianism that argues the alternative I am promoting has odd conclusions that do not fit some of our moral intuitions, while total utilitarianism does not. Is that conclusion more insane than the Genocidal Conclusion? If it isn't, total utilitarianism should still be rejected.</p>\n<p><strong id=\"Ideal_Consequentialism_Needs_a_Lot_of_Work\">Ideal Consequentialism Needs a Lot of Work</strong></p>\n<p>I do think that Ideal Consequentialism needs some serious ironing out. I haven't really developed it into a logical and rigorous system, at this point it's barely even a rough framework. There are many questions that stump me. In particular I am not quite sure what population principle I should develop. It's hard to develop one that rejects the MAP without leading to weird conclusions, like that it's bad to create someone of high utility if a population of even higher utility existed long ago. It's a difficult problem to work on, and it would be interesting to see if anyone else had any ideas.</p>\n<p>But just because I don't have an alternative fully worked out doesn't mean I can't reject Total Utilitarianism. It leads to the conclusion that a world with no love, curiosity, complex challenge, friendship, morality, or any other value the human race holds dear is an ideal, desirable world, if there is a sufficient amount of some other creature with a simpler utility function. Morality should exist, and because of that, total utilitarianism must be rejected as a moral system.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>I have been asked to note that when I use the phrase \"utility\" I am usually referring to a concept that is called \"<a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Distinctness_from_other_notions_of_utility\">E-utility</a>,\" rather than the Von Neumann-Morgenstern utility that is sometimes discussed in decision theory. The difference is that in VNM one's moral views are included in one's utility function, whereas in E-utility they are not. So if one chooses to harm oneself to help others because one believes that is morally right, one has higher VNM utility, but lower E-utility.</p>\n<p><sup>2</sup>There is a certain argument against the Repugnant Conclusion that goes that, as the steps of the Mere Addition Paradox are followed the world will lose its last symphony, its last great book, and so on. I have always considered this to be an invalid argument because the world of the RC doesn't necessarily have to be one where these things don't exist, it could be one where they exist, but are enjoyed very rarely. The Genocidal Conclusion brings this argument back in force. Creating creatures that can appreciate symphonies and great books is very inefficient compared to creating bunny rabbits pumped full of heroin.</p>\n<p><sup>3</sup>Total Utilitarianism was originally introduced to population ethics as a possible solution to the <a href=\"http://plato.stanford.edu/entries/nonidentity-problem/\">Non-Identity Problem</a>. I certainly agree that such a problem needs a solution, even if Total Utilitarianism doesn't work out as that solution.</p>\n<p><sup>4</sup>I haven't read a lot of Moore, most of my ideas were extrapolated from other things I read on Less Wrong. I just mentioned him because in my research I noticed his concept of \"ideal utilitarianism\" resembled my ideas. While I do think he was on the right track he does commit the <a href=\"/lw/oi/mind_projection_fallacy/\">Mind Projection Fallacy</a> a lot. For instance, he seems to think that one could promote beauty by creating beautiful objects, even if there were no creatures with standards of beauty around to appreciate them. This is why I am careful to emphasize that to promote ideals like love and beauty one must <em>create creatures capable of feeling love and experiencing beauty. </em></p>\n<p><em><sup>5</sup></em>My tentative answer to the question Eliezer poses in \"<a href=\"/lw/x7/cant_unbirth_a_child/\">You Can't Unbirth a Child</a>\" is that human beings may have a duty to allow the cheesecake maximizers to build some amount of giant cheesecakes, but they would also have a moral duty to limit such creatures' reproduction in order to spare resources to create more creatures with humane values.</p>\n<p>EDITED: To make a point about ideal consequentialism clearer, based on<a href=\"/lw/guk/population_ethics_shouldnt_be_about_maximizing/8mne?context=3\"> AlexMennen's</a> criticisms.</p>", "sections": [{"title": "1. The Benign Addition Paradox with Paperclip Maximizers.", "anchor": "1__The_Benign_Addition_Paradox_with_Paperclip_Maximizers_", "level": 1}, {"title": "2. The Benign Addition Paradox with Non-Sapient Animals", "anchor": "2__The_Benign_Addition_Paradox_with_Non_Sapient_Animals", "level": 1}, {"title": "3. The Benign Addition Paradox with Sociopaths", "anchor": "3__The_Benign_Addition_Paradox_with_Sociopaths", "level": 1}, {"title": "Asteroid!", "anchor": "Asteroid_", "level": 1}, {"title": "Is this realistic?", "anchor": "Is_this_realistic_", "level": 1}, {"title": "Population Ethics isn't About Maximizing Utility", "anchor": "Population_Ethics_isn_t_About_Maximizing_Utility", "level": 1}, {"title": "Why Did We Become Convinced that Maximizing Utility was the Sole Good?", "anchor": "Why_Did_We_Become_Convinced_that_Maximizing_Utility_was_the_Sole_Good_", "level": 1}, {"title": "How Intuitive is the Mere Addition Principle, Really?", "anchor": "How_Intuitive_is_the_Mere_Addition_Principle__Really_", "level": 1}, {"title": "The Genocidal Conclusion is Really Repugnant", "anchor": "The_Genocidal_Conclusion_is_Really_Repugnant", "level": 1}, {"title": "Ideal Consequentialism Needs a Lot of Work", "anchor": "Ideal_Consequentialism_Needs_a_Lot_of_Work", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "46 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["29vqqmGNxNRGzffEj", "Py3uGnncqXuEfPtQp", "K4aGvLnHvYgX9pZHS", "JvQniHSBr6JCbTRnj", "NLMo5FZWFFq652MNe", "GNnHHmm8EzePmKzPk", "synsRtBKDeAFuo7e3", "gb6zWstjmkYHLrbrg", "fG3g3764tSubr6xvs", "ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T04:21:38.309Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Moore's Paradox", "slug": "seq-rerun-moore-s-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.677Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WtMWX4j8DhfKpwPrs/seq-rerun-moore-s-paradox", "pageUrlRelative": "/posts/WtMWX4j8DhfKpwPrs/seq-rerun-moore-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/WtMWX4j8DhfKpwPrs/seq-rerun-moore-s-paradox", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Moore's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Moore's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWtMWX4j8DhfKpwPrs%2Fseq-rerun-moore-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Moore's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWtMWX4j8DhfKpwPrs%2Fseq-rerun-moore-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWtMWX4j8DhfKpwPrs%2Fseq-rerun-moore-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/1f/moores_paradox/\">Moore's Paradox</a> was originally published on 08 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Moore.27s_Paradox\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People often mistake reasons for endorsing a proposition for reasons to believe that proposition.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h04/seq_rerun_belief_in_selfdeception/\">Belief in Self-Deception</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WtMWX4j8DhfKpwPrs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.141754843536848e-06, "legacy": true, "legacyId": "22049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ERRk4thxxYNcScqR4", "KKoWagaAp8XaWCw3P", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T07:27:26.606Z", "modifiedAt": null, "url": null, "title": "Meetup : South Bay Meetup: Be Specific", "slug": "meetup-south-bay-meetup-be-specific", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tNun4FLr5Cbdi3kMd/meetup-south-bay-meetup-be-specific", "pageUrlRelative": "/posts/tNun4FLr5Cbdi3kMd/meetup-south-bay-meetup-be-specific", "linkUrl": "https://www.lesswrong.com/posts/tNun4FLr5Cbdi3kMd/meetup-south-bay-meetup-be-specific", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20South%20Bay%20Meetup%3A%20Be%20Specific&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20South%20Bay%20Meetup%3A%20Be%20Specific%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNun4FLr5Cbdi3kMd%2Fmeetup-south-bay-meetup-be-specific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20South%20Bay%20Meetup%3A%20Be%20Specific%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNun4FLr5Cbdi3kMd%2Fmeetup-south-bay-meetup-be-specific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNun4FLr5Cbdi3kMd%2Fmeetup-south-bay-meetup-be-specific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kk'>South Bay Meetup: Be Specific</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 March 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>\"Being specific\" doesn't seem like a skill that we can say much about, in the abstract.\n(<a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/\">Though this post does anyway</a>.)\nNonetheless, we usually aren't all that specific. How might we practice to <em>become</em> more specific?</p>\n\n<p>I have a few ideas, a couple of concrete exercises, and at least one moderately amusing game to try in this vein. You all get to try them!</p>\n\n<p>If you're at the Quixey door and need to be let in, you can call me at 608.698.2959.</p>\n\n<p>See you there!</p>\n\n<hr />\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>. Regular meetups in Mountain View are Berkeley are announced and discussed there, and other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kk'>South Bay Meetup: Be Specific</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tNun4FLr5Cbdi3kMd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1418782579641935e-06, "legacy": true, "legacyId": "22050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___South_Bay_Meetup__Be_Specific\">Discussion article for the meetup : <a href=\"/meetups/kk\">South Bay Meetup: Be Specific</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 March 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>\"Being specific\" doesn't seem like a skill that we can say much about, in the abstract.\n(<a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/\">Though this post does anyway</a>.)\nNonetheless, we usually aren't all that specific. How might we practice to <em>become</em> more specific?</p>\n\n<p>I have a few ideas, a couple of concrete exercises, and at least one moderately amusing game to try in this vein. You all get to try them!</p>\n\n<p>If you're at the Quixey door and need to be let in, you can call me at 608.698.2959.</p>\n\n<p>See you there!</p>\n\n<hr>\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>. Regular meetups in Mountain View are Berkeley are announced and discussed there, and other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___South_Bay_Meetup__Be_Specific1\">Discussion article for the meetup : <a href=\"/meetups/kk\">South Bay Meetup: Be Specific</a></h2>", "sections": [{"title": "Discussion article for the meetup : South Bay Meetup: Be Specific", "anchor": "Discussion_article_for_the_meetup___South_Bay_Meetup__Be_Specific", "level": 1}, {"title": "Discussion article for the meetup : South Bay Meetup: Be Specific", "anchor": "Discussion_article_for_the_meetup___South_Bay_Meetup__Be_Specific1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T12:04:36.636Z", "modifiedAt": null, "url": null, "title": "Meetup : Thursday Meetup at UB", "slug": "meetup-thursday-meetup-at-ub", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tGPBjcBE8c8PEa24p/meetup-thursday-meetup-at-ub", "pageUrlRelative": "/posts/tGPBjcBE8c8PEa24p/meetup-thursday-meetup-at-ub", "linkUrl": "https://www.lesswrong.com/posts/tGPBjcBE8c8PEa24p/meetup-thursday-meetup-at-ub", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Thursday%20Meetup%20at%20UB&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Thursday%20Meetup%20at%20UB%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGPBjcBE8c8PEa24p%2Fmeetup-thursday-meetup-at-ub%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Thursday%20Meetup%20at%20UB%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGPBjcBE8c8PEa24p%2Fmeetup-thursday-meetup-at-ub", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGPBjcBE8c8PEa24p%2Fmeetup-thursday-meetup-at-ub", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kl'>Thursday Meetup at UB</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">University at Buffalo-North Campus, Rm 124 Capen Hall Buffalo,NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So I recently read Jackdaws' \"The Eighth Meditation on Superweapons and Bingo\" and I think it would be a great discussion topic.</p>\n\n<p>I would also like to practice finding errors in reasoning. But in order to specifically combat the habit of motivated skepticism, I would like to find articles or blog posts where you actually agree with the conclusions of the paper, but still find fault with the reasoning used to support it (Brownie points if you can find an argument you were once a strong proponent of).</p>\n\n<p>New Location: Rooms 123-125 Capen are in the Capen Library. These are \"group study rooms\" located in the computer section on the first floor. There is no way to sign them out in advance, so I will try to get one of them early. If I can't, I will leave a note on the door that will lead you a some other location (there are group study rooms on the 3rd floor too, and often empty class rooms also)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kl'>Thursday Meetup at UB</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tGPBjcBE8c8PEa24p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1420624003942202e-06, "legacy": true, "legacyId": "22051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Thursday_Meetup_at_UB\">Discussion article for the meetup : <a href=\"/meetups/kl\">Thursday Meetup at UB</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">University at Buffalo-North Campus, Rm 124 Capen Hall Buffalo,NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So I recently read Jackdaws' \"The Eighth Meditation on Superweapons and Bingo\" and I think it would be a great discussion topic.</p>\n\n<p>I would also like to practice finding errors in reasoning. But in order to specifically combat the habit of motivated skepticism, I would like to find articles or blog posts where you actually agree with the conclusions of the paper, but still find fault with the reasoning used to support it (Brownie points if you can find an argument you were once a strong proponent of).</p>\n\n<p>New Location: Rooms 123-125 Capen are in the Capen Library. These are \"group study rooms\" located in the computer section on the first floor. There is no way to sign them out in advance, so I will try to get one of them early. If I can't, I will leave a note on the door that will lead you a some other location (there are group study rooms on the 3rd floor too, and often empty class rooms also)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Thursday_Meetup_at_UB1\">Discussion article for the meetup : <a href=\"/meetups/kl\">Thursday Meetup at UB</a></h2>", "sections": [{"title": "Discussion article for the meetup : Thursday Meetup at UB", "anchor": "Discussion_article_for_the_meetup___Thursday_Meetup_at_UB", "level": 1}, {"title": "Discussion article for the meetup : Thursday Meetup at UB", "anchor": "Discussion_article_for_the_meetup___Thursday_Meetup_at_UB1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T14:49:06.729Z", "modifiedAt": null, "url": null, "title": "Caring about possible people in far Worlds", "slug": "caring-about-possible-people-in-far-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Neotenic", "createdAt": "2013-03-04T02:28:23.403Z", "isAdmin": false, "displayName": "Neotenic"}, "userId": "qMgZoftatigAeMMhL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HawsPrXimYNR7hzuE/caring-about-possible-people-in-far-worlds", "pageUrlRelative": "/posts/HawsPrXimYNR7hzuE/caring-about-possible-people-in-far-worlds", "linkUrl": "https://www.lesswrong.com/posts/HawsPrXimYNR7hzuE/caring-about-possible-people-in-far-worlds", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Caring%20about%20possible%20people%20in%20far%20Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACaring%20about%20possible%20people%20in%20far%20Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawsPrXimYNR7hzuE%2Fcaring-about-possible-people-in-far-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Caring%20about%20possible%20people%20in%20far%20Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawsPrXimYNR7hzuE%2Fcaring-about-possible-people-in-far-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawsPrXimYNR7hzuE%2Fcaring-about-possible-people-in-far-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 769, "htmlBody": "<p>This relates to my recent post on<a href=\"/r/discussion/lw/gz0/pluralistic_existence_in_many_manyworlds/\"> existence in many-worlds</a>.&nbsp;</p>\n<p>I care about possible people. My child, if I ever have one, is one of them, and it seems monstrous not to care about one's children. There are many distinct ways of being a possible person. 1)You can be causally connected to some actual people in the actual world in some histories of that world. 2)You can be a counterpart of an actual person on a distinct world without causal connections 3)You can be distinct from all actual individuals, and in a causally separate possible world. 4)You can be acausally connectable to actual people, but in distinct possible worlds.</p>\n<p>Those 4 ways are not separate partitions without overlap, sometimes they overlap, and I don't believe they exhaust the scope of possible people. The most natural question to ask is \"should we care equally about about all kinds of possible people\". Some people are <a href=\"http://www.youtube.com/watch?v=ZEN5gxGPQOI\">seriously studying this</a>, and let us hope they give us accurate ways to navigate our complex universe. While we wait, some worries seem relevant:&nbsp;</p>\n<p>&nbsp;</p>\n<p>1) The Multiverse is Sadistic Argument:</p>\n<p>P1.1: If all possible people do their morally relevant thing (call it exist, if you will) and</p>\n<p>P1.2: We cannot affect (causally or acausally) what is or not possible</p>\n<p>C1.0: Then we cannot affect the morally relevant thing.&nbsp;</p>\n<p>&nbsp;</p>\n<p>2) The Multiverse is Paralyzing &nbsp;(<a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">related</a>)</p>\n<p>P2.1: We have reason to care about X-Risk</p>\n<p>P2.2: Worlds where X-Risk obtains are possible</p>\n<p>P2.3: We have nearly as much reason to worry about possible non-actual<sup>1</sup> worlds where X-risk obtains, as we have to actual worlds where it obtains.&nbsp;</p>\n<p>P2.4: There are infinitely more worlds where X-risk obtains that are possible than there are actual<sup>1</sup></p>\n<p>C2.0: Infinitarian Paralysis&nbsp;</p>\n<p><sup>1</sup>Actual here means belonging to the same quantum branching history as you. If you think you have many quantum successors, all of them are actual, same for predecessors, and people who inhabit your Hubble volume.&nbsp;</p>\n<p>&nbsp;</p>\n<p>3) Reality-Fluid Can't Be All That Is Left Argument</p>\n<p>P3.1) If all possible people do their morally relevant thing</p>\n<p>P3.2) The way in which we can affect what is possible is by giving some subsets of it more units of reality-fluid, or quantum measure</p>\n<p>P3.3) In fact reality-fluid is a ratio, such as a percentage of successor worlds of kind A or kind B for a particular world W</p>\n<p>P3.4) A possible World<span style=\"font-size: 11px;\"><sub>3</sub></span>&nbsp;with 5% reality-fluid in relation to World<sub>1</sub>&nbsp;is causally indistinguishable from itself&nbsp;with 5 times more reality-fluid 25% in relation to World<sub>2.&nbsp;</sub></p>\n<p>P3.5) The morally relevant thing, though by constitution qualitative, seems to be quantifiable, and what matters is it's absolute quantity, not any kind of ratio.&nbsp;</p>\n<p>C3.1: From 3.2 and 3.3 -&gt; We can actually affect only a quantity that is relative to our world, not an absolute quantity.&nbsp;</p>\n<p>C3.2: From C3.1 and P 3.5 -&gt; &nbsp;We can't affect the relevant thing.&nbsp;</p>\n<p>C3.3: We ended up having to talk about reality fluid because decisions matter, and reality fluid is the thing that decision changes (from P3.4 we know it isn't causal structure). But if all that decision changes is some ratio between worlds, and what matters by P3.5 is <em>not</em>&nbsp;a ratio between worlds, we have absolutely no clue of what we are talking about when we talk about \"the thing that matters\" \"what we should care about\" and \"reality fluid\".</p>\n<p>&nbsp;</p>\n<p>These arguments are here not as a perfectly logical and acceptable argument structure, but to at least induce nausea about talking about Reality-Fluid, Measure, Morally relevant things in many-worlds, Morally relevant people causally disconnected to us. Those are not things you can <a href=\"/lw/nu/taboo_your_words/\">Taboo the word away</a> and keep the substance around. The problem does not lie in the word 'Existence', or in the sentence 'X is morally relevant'. It seems to me that the service that that existence or reality used to play doesn't make sense anymore (if all possible worlds exist or if Mathematical Universe Hypothesis is correct). We attempted to keep it around as a criterial determinant for What Matters. Yet now all that is left is this weird ratio that just can't be what matters. Without a criterial determinant for mattering, we are left in a position that makes me think we should head back towards a causal approach to morality. But this is an opinion, not a conclusion.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Edit: This post is an argument against the conjunctive truth of two things, Many Worlds, and the way in which we think of What Matters. &nbsp;It seems that the most natural interpretation of it is that Many Worlds is true, and thus my argument is against our notion of What Matters. In fact my position lies more in the opposite side - our notion of What Matters is (strongly related to) What Matters, so Many Worlds are less likely. &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HawsPrXimYNR7hzuE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": 1.142171715665568e-06, "legacy": true, "legacyId": "22053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9dmL4gNszPvsxAX6j", "WBdvyyHLdxZSAMmoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T16:19:02.235Z", "modifiedAt": null, "url": null, "title": "Arguing Orthogonality, published form", "slug": "arguing-orthogonality-published-form", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:06.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AJ3aP8iWxr6NaKi6j/arguing-orthogonality-published-form", "pageUrlRelative": "/posts/AJ3aP8iWxr6NaKi6j/arguing-orthogonality-published-form", "linkUrl": "https://www.lesswrong.com/posts/AJ3aP8iWxr6NaKi6j/arguing-orthogonality-published-form", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arguing%20Orthogonality%2C%20published%20form&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArguing%20Orthogonality%2C%20published%20form%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3aP8iWxr6NaKi6j%2Farguing-orthogonality-published-form%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arguing%20Orthogonality%2C%20published%20form%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3aP8iWxr6NaKi6j%2Farguing-orthogonality-published-form", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3aP8iWxr6NaKi6j%2Farguing-orthogonality-published-form", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6870, "htmlBody": "<p style=\"text-align: start; text-indent: 0px; \"><em><span style=\"font-family: 'Calibri'; font-size: 14px; line-height: 15px; text-align: justify; text-indent: 18.933332443237305px;\">My paper \"</span><span style=\"font-family: 'Calibri';\"><span style=\"font-size: 14px; line-height: 15px;\">General purpose intelligence: arguing the Orthogonality thesis\" has been accepted for publication in the <a href=\"http://www.addletonacademicpublishers.com/search-in-am/1964-general-purpose-intelligence-arguing-the-orthogonality-thesis\">December edition</a> of Analysis and Metaphysics. Since that's some time away, I thought I'd put the final paper up here; the arguments are similar to those <a href=\"/lw/cej/general_purpose_intelligence_arguing_the\">here</a>, but this is the final version, for critique and citation purposes.</span></span></em></p>\n<h1 style=\"text-align:center\"><span style=\"font-size:14.0pt; line-height:115%;font-family:&quot;Arial Narrow&quot;,&quot;sans-serif&quot;;font-variant:small-caps; color:windowtext\">General purpose intelligence: arguing the Orthogonality thesis</span></h1>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph\"><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: right;\" align=\"right\"><strong><span style=\"font-size: 9pt; font-family: 'Calibri';\"><span style=\"text-transform: uppercase;\">STUART ARMSTRONG<br /></span></span></strong><span class=\"yshortcuts\"><span style=\"font-size:11.0pt;font-family: &quot;Calibri&quot;,&quot;serif&quot;;mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font: minor-bidi;color:windowtext;text-decoration:none;text-underline:none\"><a href=\"mailto:gerard.casey@ucd.ie\" target=\"_blank\">stuart.armstrong@philosophy.ox.ac.uk</a><br /></span></span><span style=\"font-family: 'Calibri'; font-size: 9pt;\">Future of Humanity Institute, Oxford Martin School<br /></span><span style=\"font-family: 'Calibri'; font-size: 9pt;\">Philosophy Department, University of Oxford</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: right; text-indent: 14.2pt;\" align=\"right\"><span style=\"font-family: 'Calibri'; font-size: 9pt; line-height: 115%; text-align: justify; text-indent: 14.2pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:-.9pt;margin-bottom:0cm; margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">In his paper &ldquo;The Superintelligent Will&rdquo;, Nick Bostrom formalised the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe &ndash; both as theoretical impractical agents such as AIXI and as physically possible real-world agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:-.9pt;margin-bottom:0cm; margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">Keywords: AI; Artificial Intelligence; efficiency; intelligence; goals; orthogonality</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">1<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">The Orthogonality thesis</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Scientists and mathematicians are the stereotypical examples of high intelligence humans. But their morality and ethics have been all over the map. On modern political scales, they can be left- (Oppenheimer) or right-wing (von Neumann) and historically they have slotted into most of the political groupings of their period (Galois, Lavoisier). Ethically, they have ranged from very humanitarian (Darwin, Einstein outside of his private life), through amoral (von Braun) to commercially belligerent (Edison) and vindictive (Newton). Few scientists have been put in a position where they could demonstrate genuinely evil behaviour, but there have been a few of those (Teichm&uuml;ller, Philipp Lenard, Ted Kaczynski, Shir\u014d Ishii).<a id=\"more\"></a><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Of course, many scientists have been absolutely conventional in their views and attitudes given the society of their time. But the above examples hint that their ethics are not strongly impacted by their high intelligence; intelligence and ethics seem &lsquo;orthogonal&rsquo; (varying independently of each other, to some extent). If we turn to the case of (potential) artificial intelligences we can ask whether that relation continues: would high intelligence go along with certain motivations and goals, or are they unrelated?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">To avoid the implicit anthropomorphisation in terms such as &lsquo;ethics&rsquo;, we will be looking at agents &lsquo;final goals&rsquo; &ndash; the ultimate objectives they are aiming for. Then the Orthogonality thesis, due to Nick Bostrom <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span>CITATION Bos111 \\t<span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp; </span>\\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Bostrom, 2012)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->, states that:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Intelligence and final goals are orthogonal axes along which possible agents can freely vary.&nbsp; In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is analogous to Hume&rsquo;s thesis about the independence of reason and morality <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Hum39 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Hume, 1739)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->, but applied more narrowly, using the normatively thinner concepts &lsquo;intelligence&rsquo; and &lsquo;final goals&rsquo; rather than &lsquo;reason&rsquo; and &lsquo;morality&rsquo;.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">But even &lsquo;intelligence&rsquo;, as generally used, has too many connotations. A better term would be efficiency, or instrumental rationality, or the ability to effectively solve problems given limited knowledge and resources <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Wan11 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Wang, 2011)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. Nevertheless, we will be sticking with terminology such as &lsquo;intelligent agent&rsquo;, &lsquo;artificial intelligence&rsquo; or &lsquo;superintelligence&rsquo;, as they are well established, but using them synonymously with &lsquo;efficient agent&rsquo;, artificial efficiency&rsquo; and &lsquo;superefficient algorithm&rsquo;. The relevant criteria is whether the agent can effectively achieve its goals in general situations, not whether its inner process matches up with a particular definition of what intelligence is.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Thus an artificial intelligence (AI) is an artificial algorithm, deterministic or probabilistic, implemented on some device, that demonstrates an ability to achieve goals in varied and general situations</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[1]</strong>.</span></span></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;We don&rsquo;t assume that it need be a computer program, or a well laid-out algorithm with clear loops and structures &ndash; artificial neural networks or evolved genetic algorithms certainly qualify.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A human level AI is defined to be an AI that can successfully accomplish any task at least as well as an average human would (to avoid worrying about robot bodies and such-like, we may restrict the list of tasks to those accomplishable over the internet). Thus we would expect the AI to hold conversations about Paris Hilton&rsquo;s sex life, to compose ironic limericks, to shop for the best deal on Halloween costumes and to debate the proper role of religion in politics, at least as well as an average human would.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A superhuman AI is similarly defined as an AI that would exceed the ability of the best human in all (or almost all) tasks. It would do the best research, write the most successful novels, run companies and motivate employees better than anyone else. In areas where there may not be clear scales (what&rsquo;s the world&rsquo;s best artwork?) we would expect a majority of the human population to agree the AI&rsquo;s work is among the very best.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Nick Bostrom&rsquo;s paper argued that the Orthogonality thesis does not depend on the Humean theory of motivation, but could still be true under other philosophical theories. It should be immediately apparent that the Orthogonality thesis is related to arguments about moral realism. Despite this, we will not address the fertile and extensive literature on this subject. Firstly, because it is contentious: different schools of philosophical thought have different interpretations of the truth and meaning of moral realism, disputes that cannot be currently resolved empirically. Since we are looking to resolve a mainly empirical question &ndash; what systems of motivations could we actually code into a putative AI &ndash; this theoretical disagreement is highly problematic.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Secondly, we hope that by approaching the issue from the computational perspective, we can help shed new light on these issues. After all, we do not expect that the trigger mechanism of a cruise missile to block detonation simply because people will die &ndash; but would an &ldquo;ultra-smart bomb&rdquo; behave the same way? By exploring the goals of artificial systems up to higher level of efficiency, we may contribute to seeing which kinds of agents are susceptible to moral realism arguments, and which are not.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Thus this paper will content itself with presenting direct arguments for the Orthogonality thesis. We will assume throughout that human level AIs (or at least human comparable AIs) are possible (if not, the thesis is void of useful content). We will also take the position that humans themselves can be viewed as non-deterministic algorithms</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[2]</strong>:</span></span></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;this is not vital to the paper, but is useful for comparison of goals between various types of agents. We will do the same with entities such as committees of humans, institutions or corporations, if these can be considered to be acting in an agent-like way.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The thesis itself might be critiqued for over-obviousness or triviality &ndash; a moral anti-realist, for instance, could find it too evident to need defending. Nevertheless, the argument that AIs &ndash; or indeed, any sufficiently intelligent being &ndash; would necessarily behave morally is a surprisingly common one. A. Kornai, for instance, considers it as a worthwhile starting point for investigations into AI morality <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Kor13 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Kornai, 2013)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. He bases his argument on A. Gewith&rsquo;s approach in his book, Reason and Morality <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Gew78 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Gewirth, 1978)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> (the book&rsquo;s argument can be found in a summarised form in one of E. M. Adams&rsquo;s papers <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Ada80 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Adams, 1980)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->) in which it is argued that all agents must follow a &ldquo;Principle of Generic Consistency&rdquo; that causes them to behave in accordance with all other agent&rsquo;s generic rights to freedom and well-being. Others have argued that certain specific moralities are attractors in the space of moral systems, towards which any AI will tend if they start off with certain mild constraints <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Was08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Waser, 2008)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. Because of these and other examples (and some online criticism of the Orthogonality thesis</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[3]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">), we thought the thesis was worth defending explicitly, and that the argument brought out in its favour would be of general interest to the general discussion.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">1.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Qualifying the Orthogonality thesis</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The Orthogonality thesis, taken literally, is false. Some motivations are mathematically incompatible with changes in intelligence (&ldquo;I want to prove the G&ouml;del statement for the being I would be if I were more intelligent&rdquo;). Some goals specifically refer to the intelligence of the agent, directly (&ldquo;I want to be much less efficient!&rdquo;) or indirectly (&ldquo;I want to impress people who want me to be much less efficient!&rdquo;). Though we could make a case that an agent wanting to be less efficient could initially be of any intelligence level, it won&rsquo;t stay there long, and it&rsquo;s hard to see how an agent with that goal could have become intelligent in the first place. So we will exclude from consideration those goals that intrinsically refer to the intelligence level of the agent.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We will also exclude goals that are so complex or hard to describe that the complexity of the goal becomes crippling for the agent. If the agent&rsquo;s goal takes five planets worth of material to describe, or if it takes the agent twenty years each time it checks what its goal is, then it&rsquo;s obvious that that agent can&rsquo;t function as an intelligent being on any reasonable scale.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Many have made the point that there is likely to be convergence in <em>instrumental</em> goals <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Omo08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Omohundro, 2008)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. Whatever their final goals, it would generally be in any agent&rsquo;s interest to accumulate more power, to become more intelligence and to be able to cooperate with other agents of similar ability (and to have all the negotiation, threatening and cajoling skills that go along with that cooperation). Note the similarity with what John Rawls called &lsquo;primary goods&rsquo; <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Raw71 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Rawls, 1971)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. We will however be focusing exclusively on final goals, as the instrumental goals are merely tools to accomplish these</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[4]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Further we will not try to show that intelligence and final goals can <em>vary</em> freely, in any dynamical sense (it could be quite hard to define this). Instead we will look at the thesis as talking about possible states: that there exist agents of all levels of intelligence with any given goals. Since it&rsquo;s always possible to make an agent stupider or less efficient, what we are really claiming is that there could exist possible high-intelligence agents with any given goal. Thus the restricted Orthogonality thesis that we will be discussing is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">High-intelligence agents can exist having more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence)</span></em><span class=\"MsoFootnoteReference\"><em><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><strong><span style=\"font-size: 11pt; line-height: 115%;\">[5]</span></strong></span><!--[endif]--></span></em></span><em><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We will be looking at two variations of the &ldquo;can exist&rdquo; clause: whether the agent can exist in theory, and whether we could build such an agent (given that we could build an AI at all). Though evidence will be presented directly for this thesis in the theoretic agent case, the results of this paper cannot be considered to &ldquo;prove&rdquo; the thesis for agents we could build (though they certainly raise its likelihood). In that case, we will be looking at proving a still weaker thesis:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The fact of being of high intelligence provides extremely little constraint on what final goals an agent could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">That thesis still has nearly all the relevant practical implications that the strong Orthogonality thesis does.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">1.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Orthogonality in practice for AI designers</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The arguments presented in this paper are all theoretical. They posit that AIs with certain goals either &lsquo;can exist&rsquo;, or that &lsquo;if we could build an AI, we could build one with any goal&rsquo;. In practice, the first AIs, if and when they are created, will be assembled by a specific team, using specific methods, and with specific goals in mind. They may be more or less successful at inculcating the goals into the AI (or, as is common in computer programming, they may inculcate the goals exactly, only to realise later that these weren&rsquo;t the goals they really wanted). The AI may be trained by interacting with certain humans in certain situations, or by understanding certain ethical principles, or by a myriad of other possible methods, which will likely focus on a narrow target in the space of goals. The relevance of the Orthogonality thesis for AI designers is therefore mainly limited to a warning: that high intelligence and efficiency are not enough to guarantee positive goals, and that they thus need to work carefully to inculcate the goals they value into the AI.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">2<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for theoretic agents</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If we were to step back for a moment and consider, in our mind&rsquo;s eyes, the space of every possible algorithm, peering into their goal systems and teasing out some measure of their relative intelligences, would we expect the Orthogonality thesis to hold? Since we are not worrying about practicality or constructability, all that we would require is that for any given goal system (within the few constraints enumerated above), there exists a theoretically implementable algorithm of high intelligence.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Any measurable</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[6]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\"> goal can be paired up with a reward signal: an agent gets a reward for achieving states of the world desired by the goal, and denied these rewards for actions that fail to do so. Among reward signal maximisers, the AIXI is the theoretically best agent there is, more successful at reaching its goals (up to a finite constant) than any other agent <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Hut05 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Hutter, 2005)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. AIXI itself is incomputable, but there are computable variants such as AIXItl or G&ouml;del machines <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION JSc07 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Schmidhuber, 2007)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> that approximate AIXI&rsquo;s efficiency. These methods work for whatever reward signal plugged into them. Or we could simply imagine a supercomputer with arbitrarily large amounts of computing power and a decent understanding of the laws of physics (a &lsquo;Laplace demon&rsquo; <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Lap14 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Laplace, 1814)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]--> capable of probabilistic reasoning), placed &lsquo;outside the universe&rsquo; and computing the future course of events. Paired with an obedient active agent inside the universe with a measurable goal, for which it would act as an advisor, this would also constitute an &lsquo;ultimate agent&rsquo;. Thus in the extreme theoretical case, the Orthogonality thesis seems true.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There is only one problem with these agents: they are either impossible in practice (AIXI or Laplace&rsquo;s demon), or require incredibly large amounts of computing resources to work. Let us step down from the theoretical pinnacle and require that these agents could actually exist in our world (still not requiring that we be able or likely to build them).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">An interesting thought experiment occurs here. We could imagine an AIXI-like super-agent, with all its impractical resources, that is tasked to design and train an AI that could exist in our world, and that would accomplish the super-agent&rsquo;s goals. Using its own vast intelligence, the super-agent would therefore design a constrained agent maximally effective at accomplishing those goals in our world. Then this agent would be the high-intelligence real-world agent we are looking for. It doesn&rsquo;t matter than the designer is impossible in practice &ndash; if the super-agent can succeed in the theoretical thought experiment, then the trained AI can exist in our world.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This argument generalises to other ways of producing the AI. Thus to deny the Orthogonality thesis is to assert that there is a goal system G, such that, among other things:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist any efficient real-world algorithm with goal G.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a being with arbitrarily high resources, intelligence, time <em>and</em> goal G, were to try design an efficient real-world algorithm with the same goal, it must fail.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a human society were highly motivated</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[7]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\"> to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a high-resource human society were highly motivated to achieve the goal G, then it could not do so (here the human society itself is seen as the algorithm).</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Same as above, for any hypothetical alien societies.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">6.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist <em>any</em> pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">7.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist <em>any</em> evolutionary or environmental pressures that would evolve a highly efficient real world intelligences following goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">All of these seem extraordinarily strong claims to make! The last claims all derive from the first, and merely serve to illustrate how strong the first claim actually is. Claim 4, in particular, seems to run counter to everything we know about human nature.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">3<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for human-level AIs</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Of course, even if efficient agents could exist for all these goals, that doesn&rsquo;t mean that we could ever build them, even if we could build AIs. In this section, we&rsquo;ll look at the ground for assuming the Orthogonality thesis holds for human-level agents. Since intelligence isn&rsquo;t varying much, the thesis becomes simply:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">If we could construct human-level AIs at all, then there is extremely little constraint on the final goals that such AIs could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So, is this true? The arguments in this section are generally independent of each other, and can be summarised as:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Some possible AI designs have orthogonality built right into them.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">AI goals can reach the span of human goals, which is large.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Algorithms can be combined to generate an AI with any easily measurable goal.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Various algorithmic modifications can be used to further expand the space of possible goals, if needed.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Utility functions</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">One classical picture of a rational agent is of an agent with a specific utility function, which it will then act to maximise in expectation. This picture encapsulates the Orthogonality thesis: whatever the utility function, the rational agent will then attempt to maximise it, using the approaches in all cases (planning, analysing input data, computing expected results). If an AI is built according to this model, with the utility function being prescriptive (given to the AI in a program) rather than descriptive (an abstract formalisation of an agent&rsquo;s other preferences), then the thesis would be trivially true: we could simply substitute the utility function for whichever one we desired.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">However, many putative agent designs are not utility function based, such as neural networks, genetic algorithms, or humans. So from now on we will consider that our agents are not expected utility maximisers with clear and separate utility functions, and look at proving Orthogonality in these harder circumstances.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">The span of human motivations</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It seems a reasonable assumption that if there exists a human being with particular goals, and we can program an AI, then we can construct a human-level AI with similar goals. This is immediately the case if the AI was a whole brain emulation/upload <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION San08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Sandberg &amp; Bostrom, 2008)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->, a digital copy of a specific human mind. Even for more general agents, such as evolved agents, this remains a reasonable thesis. For a start, we know that real-world evolution has produced us, so constructing human-like agents that way is certainly possible. Human minds remain our only real model of general intelligence, and this strongly directs and informs our AI designs, which are likely to be as human-similar as we can make them. Similarly, human goals are the easiest goals for us to understand, hence the easiest to try and implement in AI. Hence it seems likely that we could implement most human goals in the first generation of human-level AIs.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So how wide is the space of human motivations</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[8]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">? Our race spans foot-fetishists, religious saints, serial killers, instinctive accountants, role-players, self-cannibals, firefighters and conceptual artists. The autistic, those with exceptional social skills, the obsessive compulsive and some with split-brains. Beings of great empathy and the many who used to enjoy torture and executions as public spectacles</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[9]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. It is evident that the space of possible human motivations is vast</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[10]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. For any desire, any particular goal, no matter how niche</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[11]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, pathological, bizarre or extreme, as long as there is a single human who ever had it, we could build and run an AI with the same goal.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">But with AIs we can go even further. We could take any of these goals as a starting point, make them malleable (as goals are in humans), and push them further out. We could provide the AIs with specific reinforcements to push their goals in extreme directions (reward the saint for ever-more saintly behaviour). If the agents are fast enough, we could run whole societies of them with huge varieties of evolutionary or social pressures, to further explore the goal-space.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We may also be able to do surgery directly on their goals, to introduce more yet variety. For example, we could take a dedicated utilitarian charity worker obsessed with saving lives in poorer countries (but who doesn&rsquo;t interact, or want to interact, directly with those saved), and replace &lsquo;saving lives&rsquo; with &lsquo;maximising the number of paperclips in the universe&rsquo; or any similar abstract goal. This is more speculative, of course &ndash; but there are other ways of getting similar results.</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.3<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Instrumental goals as final goals</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If someone were to hold a gun to your head, they could make you do almost anything. Certainly there are people who, with a gun at their head, would be willing to do almost anything. A distinction is generally made between instrumental goals and final goals, with the former being seen as simply paths to the latter, and interchangeable with other plausible paths. The gun to your head disrupts the balance: your final goal is simply not to get shot, while your instrumental goals become what the gun holder wants them to be, and you put a great amount of effort into accomplishing the minute details of these instrumental goals. Note that the gun has not changed your level of intelligence or ability.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This is relevant because instrumental goals seem to be far more varied in humans than final goals. One can have instrumental goals of filling papers, solving equations, walking dogs, making money, pushing buttons in various sequences, opening doors, enhancing shareholder value, assembling cars, bombing villages or putting sharks into tanks. Or simply doing whatever the guy with gun at our head orders us to do. If we could accept human instrumental goals as AI final goals, we would extend the space of goals quite dramatically.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">To do so we would want to put the threatened agent, and the gun wielder, together into the same AI. Algorithmically there is nothing extraordinary about this: certain subroutines have certain behaviours depending on the outputs of other subroutines. The &lsquo;gun wielder&rsquo; need not be particularly intelligent: it simply needs to be able to establish whether its goals are being met. If for instance those goals are given by a utility function then all that is required in an automated system that measure progress toward increasing utility and punishes (or erases) the rest of the AI if not. The &lsquo;rest of AI&rsquo; is just required to be a human-level AI which would be susceptible to this kind of pressure. Note that we do not require that it even be close to human in any way, simply that it place a highest value on self-preservation (or on some similar small goal that the &lsquo;gun wielder&rsquo; would have power over).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">For humans, another similar model is that of a job in a corporation or bureaucracy: in order to achieve the money required for their final goals, some human are willing to perform extreme tasks (organising the logistics of genocides, weapon design, writing long emotional press releases they don&rsquo;t agree with at all). Again, if the corporation-employee relationship can be captured in a single algorithm, this would generate an intelligent AI whose goal is anything measurable by the &lsquo;corporation&rsquo;. The &lsquo;money&rsquo; could simply be an internal reward channel, perfectly aligning the incentives.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the subagent is anything like a human, they would quickly integrate the other goals into their own motivation</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[12]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, removing the need for the gun wielder/corporation part of the algorithm.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.4<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Noise, anti-agents and goal combination</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are further ways of extending the space of goals we could implement in human-level AIs. One simple way is simply to introduce noise: flip a few bits and subroutines, add bugs and get a new agent. Of course, this is likely to cause the agent&rsquo;s intelligence to decrease somewhat, but we have generated new goals. Then, if appropriate, we could use evolution or other improvements to raise the agent&rsquo;s intelligence again; this will likely undo some, but not all of effect of the noise. Or we could use some of the tricks above to make a smarter agent implement the goals of the noise-modified agent.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A more extreme example would be to create an anti-agent: an agent whose single goal is to stymie the plans and goals of single given agent. This already happens with vengeful humans, and we would just need to dial it up: have an anti-agent that would do all it can to counter the goals of a given agent, even if that agent doesn&rsquo;t exist (&ldquo;I don&rsquo;t care that you&rsquo;re dead, I&rsquo;m still going to despoil your country, because that&rsquo;s what you&rsquo;d wanted me to not do&rdquo;). This further extends the space of possible goals.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Different agents with different goals can also be combined into a single algorithm. With some algorithmic method for the AIs to negotiate their combined objective and balance the relative importance of their goals, this procedure would construct a single AI with a combined goal system. There would likely be no drop in intelligence/efficiency: committees of two can work very well towards their common goals, especially if there is some automatic penalty for disagreements.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.5<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Further tricks up the sleeve</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This section started by emphasising the wide space of human goals, and then introduced tricks to push goal systems further beyond these boundaries. The list isn&rsquo;t exhaustive: there are surely more devices and ideas one can use to continue to extend the space of possible goals for human-level AIs. Though this might not be enough to get every goal, we can nearly certainly use these procedures to construct a human-level AI with any human-comprehensible goal. But would the same be true for superhuman AIs?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">4<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for superhuman AIs</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We now come to the area where the Orthogonality thesis seems the most vulnerable. It is one thing to have human-level AIs, or abstract superintelligent algorithms created ex nihilo, with certain goals. But if ever the human race were to design a superintelligent AI, there would be some sort of process involved &ndash; directed evolution, recursive self-improvement</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[13]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, design by a committee of AIs, or similar &ndash; and it seems at least possible that such a process could fail to fully explore the goal-space. The Orthogonality thesis in this context is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">If we could construct superintelligent AIs at all, then there is extremely little constraint on the final goals that such AIs could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are two counter-theses. The weakest claim is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Incompleteness: there are large categories of goals that no superintelligence designed by us could have.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A stronger claim is:<strong></strong></span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Convergence: all human-designed superintelligences would have one of a small set of goals.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Here &lsquo;small&rsquo; means &lsquo;smaller than the space of current human motivations&rsquo;, thus very small in comparison with the space of possible AI goals. They should be distinguished; Incompleteness is all that is needed to contradict Orthogonality, but Convergence is often the issue being discussed. Often Convergence is stated in terms of a particular model of metaethics, to which it is assumed all agents will converge (see some of the references in the introduction, or various online texts and argument</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[14]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">No convergence</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The plausibility of the convergence thesis is highly connected with the connotations of the terms used in it. &ldquo;All human-designed rational beings would follow the same morality (or one of small sets of moralities)&rdquo; sounds plausible; in contrast &ldquo;all human-designed superefficient algorithms would accomplish the same task&rdquo; seems ridiculous. To quote an online commentator, how good at playing chess would a chess computer have to be before it started feeding the hungry?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Similarly, if there were such a convergence, then all self-improving or constructed superintelligence must fall prey to it, even if it were actively seeking to avoid it. After all, the self-improving lower-level AIs or the designers have certain goals in mind (as we&rsquo;ve seen in the previous section, if the designers are AIs themselves, they could have potentially any goals in mind). Obviously, they would be less likely to achieve their goals if these goals were to change as they got more intelligent <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Omo08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Omohundro, 2008)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> (see also N. Bostrom&rsquo;s forthcoming book <em>Superintelligence: Groundwork to a Strategic Analysis of the Machine Intelligence Revolution</em>). The same goes if the superintelligent AI they designed didn&rsquo;t share these goals. Hence the AI designers will be actively trying to prevent such a convergence, if they suspected that one was likely to happen. If for instance their goals were immoral, they would program their AI not to care about morality; they would use every trick up their sleeves to prevent the AI&rsquo;s goals from drifting from their own.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So the convergence thesis requires that for the vast majority of goals G:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is possible for a superintelligence to exist with goal G (by section </span><!--[if supportFields]><span style=\"font-size:12.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>REF _Ref324841095 \\r \\h <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">2<!--[if gte mso 9]><xml> <w:data>08D0C9EA79F9BACE118C8200AA004BA90B02000000080000000E0000005F005200650066003300320034003800340031003000390035000000</w:data> </xml><![endif]--></span><!--[if supportFields]><span style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">).</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There exists an entity with goal G (by section </span><!--[if supportFields]><span style=\"font-size:12.0pt;line-height: 115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt;line-height: 115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>REF _Ref324841142 \\r \\h <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">3<!--[if gte mso 9]><xml> <w:data>08D0C9EA79F9BACE118C8200AA004BA90B02000000080000000E0000005F005200650066003300320034003800340031003100340032000000</w:data> </xml><![endif]--></span><!--[if supportFields]><span style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">), capable of building a superintelligent AI.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Yet any attempt of that entity to build a superintelligent AI with goal G will be a failure, and the superintelligence&rsquo;s goals will converge on some other goal.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This is true even if the entity is aware of the convergence and explicitly attempts to avoid it.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the superintelligence were to be constructed by successive self-improvement, then an entity with goal G operating on itself to boost its intelligence is unable to do so in a way that would preserve goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This makes the convergence thesis very unlikely. The argument also works against the incompleteness thesis, but in a weaker fashion: it seems more plausible that some types of goals would be unreachable, despite being theoretically possible.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There is another interesting aspect of the convergence thesis: these goals G are to emerge, somehow, without them being aimed for or desired. If one accepts that goals aimed for will not be reached, one has to ask why convergence is assumed: why not divergence? Why not assume that though G is aimed for, random accidents or faulty implementation will lead to the AI ending up with one of a much wider array of possible goals, rather than a much narrower one? We won&rsquo;t delve deeper into this, and simply make the point that &ldquo;superintelligent AIs won&rsquo;t have the goals we want them to have&rdquo; is therefore not an argument in favour of the convergence thesis.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Oracles show the way</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the Orthogonality thesis is wrong, then it implies that Oracles are impossible to build. An Oracle is a superintelligent AI that accurately answers human questions about the world, such as the likely consequences of certain policies and decisions <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span>CITATION Arm111 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Armstrong, Sandberg, &amp; Bostrom, 2012)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]--></span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[15]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. If such an Oracle could be built, then we could attach it to a human-level AI with goal G. The human-level AI could then ask the Oracle what the results of different decisions actions could be, and choose the action that best accomplishes G. In this way, the combined system would be a superintelligent AI with goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">What makes the &ldquo;no Oracle&rdquo; implication even more counterintuitive is that <em>any</em> superintelligence must be able to look ahead, design actions, predict the consequences of its actions, and choose the best one available. But the convergence and indifference theses imply that this general skill is one that we can make available <em>only</em> to AIs with certain specific goals. Though agents with those specific goals are capable of doing effective predictions, they automatically lose this ability if their goals were to change.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.3<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Tricking the controller</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Just as with human-level AIs, one could construct a superintelligent AI by wedding together a superintelligence with a large motivated committee of human-level AIs dedicated to implementing a goal G, and checking the superintelligence&rsquo;s actions. Thus to deny the Orthogonality thesis requires that one believes that the superintelligence is always capable of tricking this committee, no matter how detailed and thorough their oversight.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This argument extends the Orthogonality thesis to moderately superintelligent AIs, or to any situation where there&rsquo;s a diminishing return to intelligence. It only fails if we take AI to be fantastically superhuman: capable of tricking or seducing any collection of human-level beings.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.4<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Temporary fragments of algorithms, fictional worlds and extra tricks</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">These are other tricks that can be used to create an AI with any goals. For any superintelligent AI, there are certain inputs that will make it behave in certain ways. For instance, a human-loving moral AI could be compelled to follow most goals G for a day, if they were rewarded with something sufficiently positive afterwards. But its actions for that one day are the result of a series of inputs to a particular algorithm; if we turned off the AI after that day, we would have accomplished moves towards goal G without having to reward its &ldquo;true&rdquo; goals at all. And then we could continue the trick the next day with another copy.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">For this to fail, it has to be the case that we can create an algorithm which will perform certain actions on certain inputs as long as it isn&rsquo;t turned off afterwards, but that we cannot create an algorithm that does the same thing if it was to be turned off.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Another alternative is to create a superintelligent AI that has goals in a fictional world (such as a game or a reward channel) over which we have control. Then we could trade interventions in the fictional world against advice in the real world towards whichever goals we desire</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[16]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">These two arguments may feel weaker than the ones before: they are tricks that may or may not work, depending on the details of the AI&rsquo;s setup. But to deny the Orthogonality thesis requires not only denying that these tricks would ever work, but denying that any tricks or methods that we (or any human-level AIs) could think up, would ever work at controlling the AIs. We need to assume superintelligent AIs cannot be controlled in any way that anyone could think of.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.5<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">In summary</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Denying the Orthogonality thesis thus requires that:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are goals G, such that an entity an entity with goal G cannot build a superintelligence with the same goal. This despite the fact that the entity can build a superintelligence, and that a superintelligence with goal G can exist.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Goal G cannot arise accidentally from some other origin, and errors and ambiguities do not significantly broaden the space of possible goals.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Oracles and general purpose planners cannot be built. Superintelligent AIs cannot have their planning abilities repurposed.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A superintelligence will always be able to trick its overseers, no matter how careful and cunning they are.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Though we can create an algorithm that does certain actions if it was not to be turned off after, we cannot create an algorithm that does the same thing if it was to be turned off after.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">6.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">An AI will always come to care intrinsically about things in the real world.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">7.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">No tricks can be thought up to successfully constrain the AI&rsquo;s goals: superintelligent AIs simply cannot be controlled.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">5<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Conclusion</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is not enough to know that an agent is intelligent (or superintelligent). If we want to know something about its final goals, about the actions it will be willing to undertake to achieve them, and hence its ultimate impact on the world, there are no shortcuts. We have to directly figure out what these goals are (or figure out a way of programming them in), and cannot rely on the agent being moral just because it is superintelligent/superefficient.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">6<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Acknowledgements</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It gives me great pleasure to acknowledge the help and support of Anders Sandberg, Nick Bostrom, Toby Ord, Diego Caleiro, Owain Evans, Daniel Dewey, Eliezer Yudkowsky, Vladimir Slepnev, Viliam Bur, Matt Freeman, Wei Dai, Will Newsome, Paul Crowley, Mao Shan, Alexander Kruel, Steve Rayhawk, Tim Tyler, John Nicholas, Ben Hoskin and Rasmus Eide, as well as those members of the Less Wrong online community going by the names shminux, and Dmytry. The work was funded by the Future of Humanity Institute (FHI), in the Department of Philosophy of Oxford University. The FHI is part of the Oxford Martin School.</span></p>\n<p>&nbsp;</p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">7<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Notes and References</span></h2>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><!--[if supportFields]><span style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>BIBLIOGRAPHY <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Adams, E. M. (1980). Gewirth on Reason and Morality. <em>The Review of Metaphysics, 33</em>(3), 579-592.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2012). Thinking Inside the Box: Controlling and Using an Oracle AI. <em>Minds and Machines, 22</em>(4).</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Bostrom, N. (2012). The Superintelligent Will: Motivation and Instrumental Rationality in Advance Artificial Agents. <em>Minds and Machines, 22</em>(2), 71-85.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">de Fabrique, N., Romano, S. J., Vecchi, G. M., &amp; van Hasselt, V. B. (2007). Understanding Stockholm Syndrome. <em>FBI Law Enforcement Bulletin (Law Enforcement Communication Unit), 76</em>(7), 10-15.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Gewirth, A. (1978). <em>Reason and Morality.</em> University of Chicago Press.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Hume, D. (1739). <em>A Treatise of Human Nature.</em></span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Hutter, M. (2005). Universal algorithmic intelligence: A mathematical top-down approach. In B. Goertzel, &amp; C. Pennachin (Eds.), <em>Arti\ufb01cial General Intelligence.</em> Springer-Verlag.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Kornai, A. (2013). Bounding the impact of AGI. <em>Oxford 2012 Winter Intelligence conference on AGI.</em> Oxford.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Laplace, P.-S. (1814). <em>Essai philosophique sur les probabilit&eacute;s.</em></span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Omohundro, S. M. (2008). The Basic AI Drives. In P. Wang, B. Goertzel, &amp; S. Franklin (Eds.), <em>Artificial General Intelligence: Proceedings of the First AGI Conference</em> (Vol. 171).</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Rawls, J. (1971). <em>A Theory of Justice .</em> Harvard University Press.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Sandberg, A., &amp; Bostrom, N. (2008). Whole brain emulation: A roadmap. <em>Future of Humanity Institute Technical report, 2008-3</em>.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Schmidhuber, J. (2007). G&ouml;del machines: Fully self-referential optimal universal self-improvers. In <em>Artificial General Intelligence.</em> Springer.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Wang, P. (2011). The assumptions on knowledge and resources in models of rationality. <em>International Journal of Machine Consciousness, 3</em>(1), 193-218.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Waser, M. R. (2008). Discovering the foundations of a universal system of ethics as a road to safe artificial intelligence. <em>Biologically inspired cognitive architectures: Papers from the AAAI fall symposium</em>, (pp. 195-200).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><!--[if supportFields]><b><span style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"; mso-no-proof:yes\" mce_style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"; mso-no-proof:yes\"><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span></span></b><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<div><!--[if !supportFootnotes]--><br /> \n<hr size=\"1\" />\n<!--[endif]-->\n<div id=\"ftn1\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[1]</strong>&nbsp;</span></span></span>We need to assume it has goals, of course. Determining whether something qualifies as a goal-based agent is very tricky (researcher Owain Evans is trying to establish a rigorous definition), but this paper will adopt the somewhat informal definition that an agent has goals if it achieves similar outcomes from very different starting positions. If the agent ends up making ice cream in any circumstances, we can assume ice creams are in its goals.</p>\n</div>\n<div id=\"ftn2\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[2]</strong></span></span><!--[endif]--></span> Since every law of nature is algorithmic (with some probabilistic process of known odds), and no exceptions to these laws are known, neither for human nor non-human processes.</p>\n</div>\n<div id=\"ftn3\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[3]</strong></span></span><!--[endif]--></span> See for example <a href=\"http://philosophicaldisquisitions.blogspot.co.uk/2012/04/bostrom-on-superintelligence-and.html\">http://philosophicaldisquisitions.blogspot.co.uk/2012/04/bostrom-on-superintelligence-and.html</a>, which criticise the thesis specifically.</p>\n</div>\n<div id=\"ftn4\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[4]</strong></span></span><!--[endif]--></span> An AI skilled in cooperation would drop this if cooperation no longer served its purpose; similarly, an AI accumulating power and resources would stop doing this if it found better ways of achieving its goals.</p>\n</div>\n<div id=\"ftn5\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[5]</strong></span></span><!--[endif]--></span> Even logically impossible goals can exist: &ldquo;construct a disproof of Modus Ponens (within classical logic)&rdquo; is a perfectly fine goal for an intelligence to have &ndash; it will quickly realise that this translates to &ldquo;prove classical logic is inconsistent&rdquo;, a task mathematicians have occasionally attempted.</p>\n</div>\n<div id=\"ftn6\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[6]</strong></span></span><!--[endif]--></span> Measuring a goal brings up subtle issues with the symbol grounding problem and similar problems. We&rsquo;ll ignore these issues in the present paper.</p>\n</div>\n<div id=\"ftn7\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[7]</strong></span></span><!--[endif]--></span> A motivation might simply be a threat: some truthful powerful being saying &ldquo;Design an algorithm with goal G. If you succeed, I will give you great goods; if you fail, I will destroy you all. The algorithm will never be used in practice, so there are no moral objections to it being designed.&rdquo;</p>\n</div>\n<div id=\"ftn8\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[8]</strong></span></span><!--[endif]--></span> One could argue that we should consider the space of general animal intelligences &ndash; octopuses, supercolonies of social insects, etc... But the methods described can already produce these animal&rsquo;s types of behaviours.</p>\n</div>\n<div id=\"ftn9\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[9]</strong></span></span><!--[endif]--></span> Even today, many people have had great fun torturing and abusing their characters in games like &ldquo;the Sims&rdquo; (<a href=\"http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/\">http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/</a>). The same urges are present, albeit diverted to fictionalised settings. Indeed games offer a wide variety of different goals that could conceivably be imported into an AI if it were possible to erase the reality/fiction distinction in its motivation.</p>\n</div>\n<div id=\"ftn10\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[10]</strong></span></span><!--[endif]--></span> As can be shown by a glance through a biography of famous people &ndash; and famous means they were generally allowed to rise to prominence in their own society, so the space of possible motivations was already cut down.</p>\n</div>\n<div id=\"ftn11\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[11]</strong></span></span><!--[endif]--></span> Of course, if we built an AI with that goal and copied it millions of times, it would no longer be niche.</p>\n</div>\n<div id=\"ftn12\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[12]</strong></span></span><!--[endif]--></span> Such as the hostages suffering from Stockholm syndrome <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION deF07 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(de Fabrique, Romano, Vecchi, &amp; van Hasselt, 2007)<!--[if supportFields]><span style=\"mso-no-proof:yes\" mce_style=\"mso-no-proof:yes\"><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span></span><![endif]-->.</p>\n</div>\n<div id=\"ftn13\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[13]</strong></span></span><!--[endif]--></span> See for instance E. Yudkowsky&rsquo;s design &ldquo;General Intelligence and Seed AI 2.3&rdquo; <a href=\"http://intelligence.org/ourresearch/publications/GISAI/\">http://singinst.org/ourresearch/publications/GISAI/</a></p>\n</div>\n<div id=\"ftn14\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[14]</strong></span></span><!--[endif]--></span> Such as J. M&uuml;ller&rsquo;s &ldquo;Ethics, risks and opportunities of superintelligences&rdquo; <a href=\"http://www.jonatasmuller.com/superintelligences.pdf\">http://www.jonatasmuller.com/superintelligences.pdf</a></p>\n</div>\n<div id=\"ftn15\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[15]</strong></span></span><!--[endif]--></span> Not to be confused with the concept of Oracle in computer science, which is either an abstract machine capable of instantaneous computations in various complexity classes, or mechanism in software testing.</p>\n</div>\n<div id=\"ftn16\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[16]</strong></span></span><!--[endif]--></span> Another possibility, for those who believe AIs above a certain level of intelligence must converge in their motivations, is to have a <em>society</em> of AIs below this level. If the AIs are closely linked, this could be referred to as a superorganism. Then the whole superorganism could be setup to have any particular goal and yet have high intelligence/efficiency. See <a href=\"/r/discussion/lw/gzl/amending_the_general_pupose_intelligence_arguing/\">http://lesswrong.com/r/discussion/lw/gzl/amending_the_general_pupose_intelligence_arguing/</a> for more details.</p>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AJ3aP8iWxr6NaKi6j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "22052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"text-align: start; text-indent: 0px; \"><em><span style=\"font-family: 'Calibri'; font-size: 14px; line-height: 15px; text-align: justify; text-indent: 18.933332443237305px;\">My paper \"</span><span style=\"font-family: 'Calibri';\"><span style=\"font-size: 14px; line-height: 15px;\">General purpose intelligence: arguing the Orthogonality thesis\" has been accepted for publication in the <a href=\"http://www.addletonacademicpublishers.com/search-in-am/1964-general-purpose-intelligence-arguing-the-orthogonality-thesis\">December edition</a> of Analysis and Metaphysics. Since that's some time away, I thought I'd put the final paper up here; the arguments are similar to those <a href=\"/lw/cej/general_purpose_intelligence_arguing_the\">here</a>, but this is the final version, for critique and citation purposes.</span></span></em></p>\n<h1 style=\"text-align:center\" id=\"General_purpose_intelligence__arguing_the_Orthogonality_thesis\"><span style=\"font-size:14.0pt; line-height:115%;font-family:&quot;Arial Narrow&quot;,&quot;sans-serif&quot;;font-variant:small-caps; color:windowtext\">General purpose intelligence: arguing the Orthogonality thesis</span></h1>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph\"><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: right;\" align=\"right\"><strong><span style=\"font-size: 9pt; font-family: 'Calibri';\"><span style=\"text-transform: uppercase;\">STUART ARMSTRONG<br></span></span></strong><span class=\"yshortcuts\"><span style=\"font-size:11.0pt;font-family: &quot;Calibri&quot;,&quot;serif&quot;;mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font: minor-bidi;color:windowtext;text-decoration:none;text-underline:none\"><a href=\"mailto:gerard.casey@ucd.ie\" target=\"_blank\">stuart.armstrong@philosophy.ox.ac.uk</a><br></span></span><span style=\"font-family: 'Calibri'; font-size: 9pt;\">Future of Humanity Institute, Oxford Martin School<br></span><span style=\"font-family: 'Calibri'; font-size: 9pt;\">Philosophy Department, University of Oxford</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: right; text-indent: 14.2pt;\" align=\"right\"><span style=\"font-family: 'Calibri'; font-size: 9pt; line-height: 115%; text-align: justify; text-indent: 14.2pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:-.9pt;margin-bottom:0cm; margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">In his paper \u201cThe Superintelligent Will\u201d, Nick Bostrom formalised the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe \u2013 both as theoretical impractical agents such as AIXI and as physically possible real-world agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:-.9pt;margin-bottom:0cm; margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">Keywords: AI; Artificial Intelligence; efficiency; intelligence; goals; orthogonality</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"1_______________________The_Orthogonality_thesis\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">1<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">The Orthogonality thesis</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Scientists and mathematicians are the stereotypical examples of high intelligence humans. But their morality and ethics have been all over the map. On modern political scales, they can be left- (Oppenheimer) or right-wing (von Neumann) and historically they have slotted into most of the political groupings of their period (Galois, Lavoisier). Ethically, they have ranged from very humanitarian (Darwin, Einstein outside of his private life), through amoral (von Braun) to commercially belligerent (Edison) and vindictive (Newton). Few scientists have been put in a position where they could demonstrate genuinely evil behaviour, but there have been a few of those (Teichm\u00fcller, Philipp Lenard, Ted Kaczynski, Shir\u014d Ishii).<a id=\"more\"></a><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Of course, many scientists have been absolutely conventional in their views and attitudes given the society of their time. But the above examples hint that their ethics are not strongly impacted by their high intelligence; intelligence and ethics seem \u2018orthogonal\u2019 (varying independently of each other, to some extent). If we turn to the case of (potential) artificial intelligences we can ask whether that relation continues: would high intelligence go along with certain motivations and goals, or are they unrelated?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">To avoid the implicit anthropomorphisation in terms such as \u2018ethics\u2019, we will be looking at agents \u2018final goals\u2019 \u2013 the ultimate objectives they are aiming for. Then the Orthogonality thesis, due to Nick Bostrom <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span>CITATION Bos111 \\t<span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp; </span>\\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Bostrom, 2012)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->, states that:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Intelligence and final goals are orthogonal axes along which possible agents can freely vary.&nbsp; In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is analogous to Hume\u2019s thesis about the independence of reason and morality <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Hum39 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Hume, 1739)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->, but applied more narrowly, using the normatively thinner concepts \u2018intelligence\u2019 and \u2018final goals\u2019 rather than \u2018reason\u2019 and \u2018morality\u2019.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">But even \u2018intelligence\u2019, as generally used, has too many connotations. A better term would be efficiency, or instrumental rationality, or the ability to effectively solve problems given limited knowledge and resources <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Wan11 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Wang, 2011)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. Nevertheless, we will be sticking with terminology such as \u2018intelligent agent\u2019, \u2018artificial intelligence\u2019 or \u2018superintelligence\u2019, as they are well established, but using them synonymously with \u2018efficient agent\u2019, artificial efficiency\u2019 and \u2018superefficient algorithm\u2019. The relevant criteria is whether the agent can effectively achieve its goals in general situations, not whether its inner process matches up with a particular definition of what intelligence is.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Thus an artificial intelligence (AI) is an artificial algorithm, deterministic or probabilistic, implemented on some device, that demonstrates an ability to achieve goals in varied and general situations</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[1]</strong>.</span></span></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;We don\u2019t assume that it need be a computer program, or a well laid-out algorithm with clear loops and structures \u2013 artificial neural networks or evolved genetic algorithms certainly qualify.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A human level AI is defined to be an AI that can successfully accomplish any task at least as well as an average human would (to avoid worrying about robot bodies and such-like, we may restrict the list of tasks to those accomplishable over the internet). Thus we would expect the AI to hold conversations about Paris Hilton\u2019s sex life, to compose ironic limericks, to shop for the best deal on Halloween costumes and to debate the proper role of religion in politics, at least as well as an average human would.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A superhuman AI is similarly defined as an AI that would exceed the ability of the best human in all (or almost all) tasks. It would do the best research, write the most successful novels, run companies and motivate employees better than anyone else. In areas where there may not be clear scales (what\u2019s the world\u2019s best artwork?) we would expect a majority of the human population to agree the AI\u2019s work is among the very best.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Nick Bostrom\u2019s paper argued that the Orthogonality thesis does not depend on the Humean theory of motivation, but could still be true under other philosophical theories. It should be immediately apparent that the Orthogonality thesis is related to arguments about moral realism. Despite this, we will not address the fertile and extensive literature on this subject. Firstly, because it is contentious: different schools of philosophical thought have different interpretations of the truth and meaning of moral realism, disputes that cannot be currently resolved empirically. Since we are looking to resolve a mainly empirical question \u2013 what systems of motivations could we actually code into a putative AI \u2013 this theoretical disagreement is highly problematic.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Secondly, we hope that by approaching the issue from the computational perspective, we can help shed new light on these issues. After all, we do not expect that the trigger mechanism of a cruise missile to block detonation simply because people will die \u2013 but would an \u201cultra-smart bomb\u201d behave the same way? By exploring the goals of artificial systems up to higher level of efficiency, we may contribute to seeing which kinds of agents are susceptible to moral realism arguments, and which are not.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Thus this paper will content itself with presenting direct arguments for the Orthogonality thesis. We will assume throughout that human level AIs (or at least human comparable AIs) are possible (if not, the thesis is void of useful content). We will also take the position that humans themselves can be viewed as non-deterministic algorithms</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[2]</strong>:</span></span></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;this is not vital to the paper, but is useful for comparison of goals between various types of agents. We will do the same with entities such as committees of humans, institutions or corporations, if these can be considered to be acting in an agent-like way.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The thesis itself might be critiqued for over-obviousness or triviality \u2013 a moral anti-realist, for instance, could find it too evident to need defending. Nevertheless, the argument that AIs \u2013 or indeed, any sufficiently intelligent being \u2013 would necessarily behave morally is a surprisingly common one. A. Kornai, for instance, considers it as a worthwhile starting point for investigations into AI morality <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Kor13 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Kornai, 2013)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. He bases his argument on A. Gewith\u2019s approach in his book, Reason and Morality <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Gew78 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Gewirth, 1978)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> (the book\u2019s argument can be found in a summarised form in one of E. M. Adams\u2019s papers <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Ada80 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Adams, 1980)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->) in which it is argued that all agents must follow a \u201cPrinciple of Generic Consistency\u201d that causes them to behave in accordance with all other agent\u2019s generic rights to freedom and well-being. Others have argued that certain specific moralities are attractors in the space of moral systems, towards which any AI will tend if they start off with certain mild constraints <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Was08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Waser, 2008)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. Because of these and other examples (and some online criticism of the Orthogonality thesis</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[3]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">), we thought the thesis was worth defending explicitly, and that the argument brought out in its favour would be of general interest to the general discussion.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"1_1_____Qualifying_the_Orthogonality_thesis\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">1.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Qualifying the Orthogonality thesis</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The Orthogonality thesis, taken literally, is false. Some motivations are mathematically incompatible with changes in intelligence (\u201cI want to prove the G\u00f6del statement for the being I would be if I were more intelligent\u201d). Some goals specifically refer to the intelligence of the agent, directly (\u201cI want to be much less efficient!\u201d) or indirectly (\u201cI want to impress people who want me to be much less efficient!\u201d). Though we could make a case that an agent wanting to be less efficient could initially be of any intelligence level, it won\u2019t stay there long, and it\u2019s hard to see how an agent with that goal could have become intelligent in the first place. So we will exclude from consideration those goals that intrinsically refer to the intelligence level of the agent.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We will also exclude goals that are so complex or hard to describe that the complexity of the goal becomes crippling for the agent. If the agent\u2019s goal takes five planets worth of material to describe, or if it takes the agent twenty years each time it checks what its goal is, then it\u2019s obvious that that agent can\u2019t function as an intelligent being on any reasonable scale.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Many have made the point that there is likely to be convergence in <em>instrumental</em> goals <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Omo08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Omohundro, 2008)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. Whatever their final goals, it would generally be in any agent\u2019s interest to accumulate more power, to become more intelligence and to be able to cooperate with other agents of similar ability (and to have all the negotiation, threatening and cajoling skills that go along with that cooperation). Note the similarity with what John Rawls called \u2018primary goods\u2019 <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Raw71 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Rawls, 1971)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->. We will however be focusing exclusively on final goals, as the instrumental goals are merely tools to accomplish these</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[4]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Further we will not try to show that intelligence and final goals can <em>vary</em> freely, in any dynamical sense (it could be quite hard to define this). Instead we will look at the thesis as talking about possible states: that there exist agents of all levels of intelligence with any given goals. Since it\u2019s always possible to make an agent stupider or less efficient, what we are really claiming is that there could exist possible high-intelligence agents with any given goal. Thus the restricted Orthogonality thesis that we will be discussing is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">High-intelligence agents can exist having more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence)</span></em><span class=\"MsoFootnoteReference\"><em><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><strong><span style=\"font-size: 11pt; line-height: 115%;\">[5]</span></strong></span><!--[endif]--></span></em></span><em><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We will be looking at two variations of the \u201ccan exist\u201d clause: whether the agent can exist in theory, and whether we could build such an agent (given that we could build an AI at all). Though evidence will be presented directly for this thesis in the theoretic agent case, the results of this paper cannot be considered to \u201cprove\u201d the thesis for agents we could build (though they certainly raise its likelihood). In that case, we will be looking at proving a still weaker thesis:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The fact of being of high intelligence provides extremely little constraint on what final goals an agent could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">That thesis still has nearly all the relevant practical implications that the strong Orthogonality thesis does.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"1_2_____Orthogonality_in_practice_for_AI_designers\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">1.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Orthogonality in practice for AI designers</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The arguments presented in this paper are all theoretical. They posit that AIs with certain goals either \u2018can exist\u2019, or that \u2018if we could build an AI, we could build one with any goal\u2019. In practice, the first AIs, if and when they are created, will be assembled by a specific team, using specific methods, and with specific goals in mind. They may be more or less successful at inculcating the goals into the AI (or, as is common in computer programming, they may inculcate the goals exactly, only to realise later that these weren\u2019t the goals they really wanted). The AI may be trained by interacting with certain humans in certain situations, or by understanding certain ethical principles, or by a myriad of other possible methods, which will likely focus on a narrow target in the space of goals. The relevance of the Orthogonality thesis for AI designers is therefore mainly limited to a warning: that high intelligence and efficiency are not enough to guarantee positive goals, and that they thus need to work carefully to inculcate the goals they value into the AI.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"2_______________________Orthogonality_for_theoretic_agents\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">2<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for theoretic agents</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If we were to step back for a moment and consider, in our mind\u2019s eyes, the space of every possible algorithm, peering into their goal systems and teasing out some measure of their relative intelligences, would we expect the Orthogonality thesis to hold? Since we are not worrying about practicality or constructability, all that we would require is that for any given goal system (within the few constraints enumerated above), there exists a theoretically implementable algorithm of high intelligence.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Any measurable</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[6]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\"> goal can be paired up with a reward signal: an agent gets a reward for achieving states of the world desired by the goal, and denied these rewards for actions that fail to do so. Among reward signal maximisers, the AIXI is the theoretically best agent there is, more successful at reaching its goals (up to a finite constant) than any other agent <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Hut05 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Hutter, 2005)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]-->. AIXI itself is incomputable, but there are computable variants such as AIXItl or G\u00f6del machines <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION JSc07 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Schmidhuber, 2007)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> that approximate AIXI\u2019s efficiency. These methods work for whatever reward signal plugged into them. Or we could simply imagine a supercomputer with arbitrarily large amounts of computing power and a decent understanding of the laws of physics (a \u2018Laplace demon\u2019 <!--[if supportFields]><span style=\"mso-element: field-begin\" mce_style=\"mso-element: field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Lap14 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Laplace, 1814)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]--> capable of probabilistic reasoning), placed \u2018outside the universe\u2019 and computing the future course of events. Paired with an obedient active agent inside the universe with a measurable goal, for which it would act as an advisor, this would also constitute an \u2018ultimate agent\u2019. Thus in the extreme theoretical case, the Orthogonality thesis seems true.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There is only one problem with these agents: they are either impossible in practice (AIXI or Laplace\u2019s demon), or require incredibly large amounts of computing resources to work. Let us step down from the theoretical pinnacle and require that these agents could actually exist in our world (still not requiring that we be able or likely to build them).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">An interesting thought experiment occurs here. We could imagine an AIXI-like super-agent, with all its impractical resources, that is tasked to design and train an AI that could exist in our world, and that would accomplish the super-agent\u2019s goals. Using its own vast intelligence, the super-agent would therefore design a constrained agent maximally effective at accomplishing those goals in our world. Then this agent would be the high-intelligence real-world agent we are looking for. It doesn\u2019t matter than the designer is impossible in practice \u2013 if the super-agent can succeed in the theoretical thought experiment, then the trained AI can exist in our world.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This argument generalises to other ways of producing the AI. Thus to deny the Orthogonality thesis is to assert that there is a goal system G, such that, among other things:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist any efficient real-world algorithm with goal G.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a being with arbitrarily high resources, intelligence, time <em>and</em> goal G, were to try design an efficient real-world algorithm with the same goal, it must fail.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a human society were highly motivated</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[7]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\"> to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If a high-resource human society were highly motivated to achieve the goal G, then it could not do so (here the human society itself is seen as the algorithm).</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Same as above, for any hypothetical alien societies.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">6.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist <em>any</em> pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l1 level1 lfo1\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">7.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There cannot exist <em>any</em> evolutionary or environmental pressures that would evolve a highly efficient real world intelligences following goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">All of these seem extraordinarily strong claims to make! The last claims all derive from the first, and merely serve to illustrate how strong the first claim actually is. Claim 4, in particular, seems to run counter to everything we know about human nature.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"3_______________________Orthogonality_for_human_level_AIs\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">3<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for human-level AIs</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Of course, even if efficient agents could exist for all these goals, that doesn\u2019t mean that we could ever build them, even if we could build AIs. In this section, we\u2019ll look at the ground for assuming the Orthogonality thesis holds for human-level agents. Since intelligence isn\u2019t varying much, the thesis becomes simply:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">If we could construct human-level AIs at all, then there is extremely little constraint on the final goals that such AIs could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So, is this true? The arguments in this section are generally independent of each other, and can be summarised as:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Some possible AI designs have orthogonality built right into them.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">AI goals can reach the span of human goals, which is large.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Algorithms can be combined to generate an AI with any easily measurable goal.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l2 level1 lfo2\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Various algorithmic modifications can be used to further expand the space of possible goals, if needed.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"3_1_____Utility_functions\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Utility functions</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">One classical picture of a rational agent is of an agent with a specific utility function, which it will then act to maximise in expectation. This picture encapsulates the Orthogonality thesis: whatever the utility function, the rational agent will then attempt to maximise it, using the approaches in all cases (planning, analysing input data, computing expected results). If an AI is built according to this model, with the utility function being prescriptive (given to the AI in a program) rather than descriptive (an abstract formalisation of an agent\u2019s other preferences), then the thesis would be trivially true: we could simply substitute the utility function for whichever one we desired.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">However, many putative agent designs are not utility function based, such as neural networks, genetic algorithms, or humans. So from now on we will consider that our agents are not expected utility maximisers with clear and separate utility functions, and look at proving Orthogonality in these harder circumstances.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"3_2_____The_span_of_human_motivations\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">The span of human motivations</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It seems a reasonable assumption that if there exists a human being with particular goals, and we can program an AI, then we can construct a human-level AI with similar goals. This is immediately the case if the AI was a whole brain emulation/upload <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION San08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Sandberg &amp; Bostrom, 2008)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]-->, a digital copy of a specific human mind. Even for more general agents, such as evolved agents, this remains a reasonable thesis. For a start, we know that real-world evolution has produced us, so constructing human-like agents that way is certainly possible. Human minds remain our only real model of general intelligence, and this strongly directs and informs our AI designs, which are likely to be as human-similar as we can make them. Similarly, human goals are the easiest goals for us to understand, hence the easiest to try and implement in AI. Hence it seems likely that we could implement most human goals in the first generation of human-level AIs.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So how wide is the space of human motivations</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[8]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">? Our race spans foot-fetishists, religious saints, serial killers, instinctive accountants, role-players, self-cannibals, firefighters and conceptual artists. The autistic, those with exceptional social skills, the obsessive compulsive and some with split-brains. Beings of great empathy and the many who used to enjoy torture and executions as public spectacles</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[9]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. It is evident that the space of possible human motivations is vast</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[10]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. For any desire, any particular goal, no matter how niche</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[11]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, pathological, bizarre or extreme, as long as there is a single human who ever had it, we could build and run an AI with the same goal.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">But with AIs we can go even further. We could take any of these goals as a starting point, make them malleable (as goals are in humans), and push them further out. We could provide the AIs with specific reinforcements to push their goals in extreme directions (reward the saint for ever-more saintly behaviour). If the agents are fast enough, we could run whole societies of them with huge varieties of evolutionary or social pressures, to further explore the goal-space.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We may also be able to do surgery directly on their goals, to introduce more yet variety. For example, we could take a dedicated utilitarian charity worker obsessed with saving lives in poorer countries (but who doesn\u2019t interact, or want to interact, directly with those saved), and replace \u2018saving lives\u2019 with \u2018maximising the number of paperclips in the universe\u2019 or any similar abstract goal. This is more speculative, of course \u2013 but there are other ways of getting similar results.</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"3_3_____Instrumental_goals_as_final_goals\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.3<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Instrumental goals as final goals</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If someone were to hold a gun to your head, they could make you do almost anything. Certainly there are people who, with a gun at their head, would be willing to do almost anything. A distinction is generally made between instrumental goals and final goals, with the former being seen as simply paths to the latter, and interchangeable with other plausible paths. The gun to your head disrupts the balance: your final goal is simply not to get shot, while your instrumental goals become what the gun holder wants them to be, and you put a great amount of effort into accomplishing the minute details of these instrumental goals. Note that the gun has not changed your level of intelligence or ability.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This is relevant because instrumental goals seem to be far more varied in humans than final goals. One can have instrumental goals of filling papers, solving equations, walking dogs, making money, pushing buttons in various sequences, opening doors, enhancing shareholder value, assembling cars, bombing villages or putting sharks into tanks. Or simply doing whatever the guy with gun at our head orders us to do. If we could accept human instrumental goals as AI final goals, we would extend the space of goals quite dramatically.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">To do so we would want to put the threatened agent, and the gun wielder, together into the same AI. Algorithmically there is nothing extraordinary about this: certain subroutines have certain behaviours depending on the outputs of other subroutines. The \u2018gun wielder\u2019 need not be particularly intelligent: it simply needs to be able to establish whether its goals are being met. If for instance those goals are given by a utility function then all that is required in an automated system that measure progress toward increasing utility and punishes (or erases) the rest of the AI if not. The \u2018rest of AI\u2019 is just required to be a human-level AI which would be susceptible to this kind of pressure. Note that we do not require that it even be close to human in any way, simply that it place a highest value on self-preservation (or on some similar small goal that the \u2018gun wielder\u2019 would have power over).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">For humans, another similar model is that of a job in a corporation or bureaucracy: in order to achieve the money required for their final goals, some human are willing to perform extreme tasks (organising the logistics of genocides, weapon design, writing long emotional press releases they don\u2019t agree with at all). Again, if the corporation-employee relationship can be captured in a single algorithm, this would generate an intelligent AI whose goal is anything measurable by the \u2018corporation\u2019. The \u2018money\u2019 could simply be an internal reward channel, perfectly aligning the incentives.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the subagent is anything like a human, they would quickly integrate the other goals into their own motivation</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[12]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, removing the need for the gun wielder/corporation part of the algorithm.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"3_4_____Noise__anti_agents_and_goal_combination\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.4<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Noise, anti-agents and goal combination</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are further ways of extending the space of goals we could implement in human-level AIs. One simple way is simply to introduce noise: flip a few bits and subroutines, add bugs and get a new agent. Of course, this is likely to cause the agent\u2019s intelligence to decrease somewhat, but we have generated new goals. Then, if appropriate, we could use evolution or other improvements to raise the agent\u2019s intelligence again; this will likely undo some, but not all of effect of the noise. Or we could use some of the tricks above to make a smarter agent implement the goals of the noise-modified agent.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A more extreme example would be to create an anti-agent: an agent whose single goal is to stymie the plans and goals of single given agent. This already happens with vengeful humans, and we would just need to dial it up: have an anti-agent that would do all it can to counter the goals of a given agent, even if that agent doesn\u2019t exist (\u201cI don\u2019t care that you\u2019re dead, I\u2019m still going to despoil your country, because that\u2019s what you\u2019d wanted me to not do\u201d). This further extends the space of possible goals.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Different agents with different goals can also be combined into a single algorithm. With some algorithmic method for the AIs to negotiate their combined objective and balance the relative importance of their goals, this procedure would construct a single AI with a combined goal system. There would likely be no drop in intelligence/efficiency: committees of two can work very well towards their common goals, especially if there is some automatic penalty for disagreements.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"3_5_____Further_tricks_up_the_sleeve\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">3.5<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Further tricks up the sleeve</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This section started by emphasising the wide space of human goals, and then introduced tricks to push goal systems further beyond these boundaries. The list isn\u2019t exhaustive: there are surely more devices and ideas one can use to continue to extend the space of possible goals for human-level AIs. Though this might not be enough to get every goal, we can nearly certainly use these procedures to construct a human-level AI with any human-comprehensible goal. But would the same be true for superhuman AIs?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"4_______________________Orthogonality_for_superhuman_AIs\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">4<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Orthogonality for superhuman AIs</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">We now come to the area where the Orthogonality thesis seems the most vulnerable. It is one thing to have human-level AIs, or abstract superintelligent algorithms created ex nihilo, with certain goals. But if ever the human race were to design a superintelligent AI, there would be some sort of process involved \u2013 directed evolution, recursive self-improvement</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[13]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">, design by a committee of AIs, or similar \u2013 and it seems at least possible that such a process could fail to fully explore the goal-space. The Orthogonality thesis in this context is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family: &quot;Calibri&quot;,&quot;serif&quot;\">If we could construct superintelligent AIs at all, then there is extremely little constraint on the final goals that such AIs could have (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are two counter-theses. The weakest claim is:</span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Incompleteness: there are large categories of goals that no superintelligence designed by us could have.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A stronger claim is:<strong></strong></span></p>\n<p class=\"MsoNormal\" style=\"margin-top:0cm;margin-right:28.55pt;margin-bottom: 0cm;margin-left:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:inter-ideograph; text-indent:14.2pt\"><em><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Convergence: all human-designed superintelligences would have one of a small set of goals.</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Here \u2018small\u2019 means \u2018smaller than the space of current human motivations\u2019, thus very small in comparison with the space of possible AI goals. They should be distinguished; Incompleteness is all that is needed to contradict Orthogonality, but Convergence is often the issue being discussed. Often Convergence is stated in terms of a particular model of metaethics, to which it is assumed all agents will converge (see some of the references in the introduction, or various online texts and argument</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[14]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"4_1_____No_convergence\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.1<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">No convergence</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">The plausibility of the convergence thesis is highly connected with the connotations of the terms used in it. \u201cAll human-designed rational beings would follow the same morality (or one of small sets of moralities)\u201d sounds plausible; in contrast \u201call human-designed superefficient algorithms would accomplish the same task\u201d seems ridiculous. To quote an online commentator, how good at playing chess would a chess computer have to be before it started feeding the hungry?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Similarly, if there were such a convergence, then all self-improving or constructed superintelligence must fall prey to it, even if it were actively seeking to avoid it. After all, the self-improving lower-level AIs or the designers have certain goals in mind (as we\u2019ve seen in the previous section, if the designers are AIs themselves, they could have potentially any goals in mind). Obviously, they would be less likely to achieve their goals if these goals were to change as they got more intelligent <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION Omo08 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Omohundro, 2008)<!--[if supportFields]><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span><![endif]--> (see also N. Bostrom\u2019s forthcoming book <em>Superintelligence: Groundwork to a Strategic Analysis of the Machine Intelligence Revolution</em>). The same goes if the superintelligent AI they designed didn\u2019t share these goals. Hence the AI designers will be actively trying to prevent such a convergence, if they suspected that one was likely to happen. If for instance their goals were immoral, they would program their AI not to care about morality; they would use every trick up their sleeves to prevent the AI\u2019s goals from drifting from their own.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">So the convergence thesis requires that for the vast majority of goals G:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is possible for a superintelligence to exist with goal G (by section </span><!--[if supportFields]><span style=\"font-size:12.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>REF _Ref324841095 \\r \\h <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">2<!--[if gte mso 9]><xml> <w:data>08D0C9EA79F9BACE118C8200AA004BA90B02000000080000000E0000005F005200650066003300320034003800340031003000390035000000</w:data> </xml><![endif]--></span><!--[if supportFields]><span style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">).</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There exists an entity with goal G (by section </span><!--[if supportFields]><span style=\"font-size:12.0pt;line-height: 115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt;line-height: 115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>REF _Ref324841142 \\r \\h <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">3<!--[if gte mso 9]><xml> <w:data>08D0C9EA79F9BACE118C8200AA004BA90B02000000080000000E0000005F005200650066003300320034003800340031003100340032000000</w:data> </xml><![endif]--></span><!--[if supportFields]><span style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:12.0pt; line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element: field-end\" mce_style=\"mso-element: field-end\"></span></span><![endif]--><span style=\"font-size:12.0pt;line-height: 115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">), capable of building a superintelligent AI.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Yet any attempt of that entity to build a superintelligent AI with goal G will be a failure, and the superintelligence\u2019s goals will converge on some other goal.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This is true even if the entity is aware of the convergence and explicitly attempts to avoid it.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l3 level1 lfo3\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the superintelligence were to be constructed by successive self-improvement, then an entity with goal G operating on itself to boost its intelligence is unable to do so in a way that would preserve goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This makes the convergence thesis very unlikely. The argument also works against the incompleteness thesis, but in a weaker fashion: it seems more plausible that some types of goals would be unreachable, despite being theoretically possible.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There is another interesting aspect of the convergence thesis: these goals G are to emerge, somehow, without them being aimed for or desired. If one accepts that goals aimed for will not be reached, one has to ask why convergence is assumed: why not divergence? Why not assume that though G is aimed for, random accidents or faulty implementation will lead to the AI ending up with one of a much wider array of possible goals, rather than a much narrower one? We won\u2019t delve deeper into this, and simply make the point that \u201csuperintelligent AIs won\u2019t have the goals we want them to have\u201d is therefore not an argument in favour of the convergence thesis.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"4_2_____Oracles_show_the_way\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.2<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Oracles show the way</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">If the Orthogonality thesis is wrong, then it implies that Oracles are impossible to build. An Oracle is a superintelligent AI that accurately answers human questions about the world, such as the likely consequences of certain policies and decisions <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span>CITATION Arm111 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(Armstrong, Sandberg, &amp; Bostrom, 2012)<!--[if supportFields]><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span><![endif]--></span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[15]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">. If such an Oracle could be built, then we could attach it to a human-level AI with goal G. The human-level AI could then ask the Oracle what the results of different decisions actions could be, and choose the action that best accomplishes G. In this way, the combined system would be a superintelligent AI with goal G.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">What makes the \u201cno Oracle\u201d implication even more counterintuitive is that <em>any</em> superintelligence must be able to look ahead, design actions, predict the consequences of its actions, and choose the best one available. But the convergence and indifference theses imply that this general skill is one that we can make available <em>only</em> to AIs with certain specific goals. Though agents with those specific goals are capable of doing effective predictions, they automatically lose this ability if their goals were to change.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"4_3_____Tricking_the_controller\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.3<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Tricking the controller</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Just as with human-level AIs, one could construct a superintelligent AI by wedding together a superintelligence with a large motivated committee of human-level AIs dedicated to implementing a goal G, and checking the superintelligence\u2019s actions. Thus to deny the Orthogonality thesis requires that one believes that the superintelligence is always capable of tricking this committee, no matter how detailed and thorough their oversight.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">This argument extends the Orthogonality thesis to moderately superintelligent AIs, or to any situation where there\u2019s a diminishing return to intelligence. It only fails if we take AI to be fantastically superhuman: capable of tricking or seducing any collection of human-level beings.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"4_4_____Temporary_fragments_of_algorithms__fictional_worlds_and_extra_tricks\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.4<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">Temporary fragments of algorithms, fictional worlds and extra tricks</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">These are other tricks that can be used to create an AI with any goals. For any superintelligent AI, there are certain inputs that will make it behave in certain ways. For instance, a human-loving moral AI could be compelled to follow most goals G for a day, if they were rewarded with something sufficiently positive afterwards. But its actions for that one day are the result of a series of inputs to a particular algorithm; if we turned off the AI after that day, we would have accomplished moves towards goal G without having to reward its \u201ctrue\u201d goals at all. And then we could continue the trick the next day with another copy.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">For this to fail, it has to be the case that we can create an algorithm which will perform certain actions on certain inputs as long as it isn\u2019t turned off afterwards, but that we cannot create an algorithm that does the same thing if it was to be turned off.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Another alternative is to create a superintelligent AI that has goals in a fictional world (such as a game or a reward channel) over which we have control. Then we could trade interventions in the fictional world against advice in the real world towards whichever goals we desire</span><span class=\"MsoFootnoteReference\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size: 11pt; line-height: 115%;\"><strong>[16]</strong></span></span><!--[endif]--></span></span><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">These two arguments may feel weaker than the ones before: they are tricks that may or may not work, depending on the details of the AI\u2019s setup. But to deny the Orthogonality thesis requires not only denying that these tricks would ever work, but denying that any tricks or methods that we (or any human-level AIs) could think up, would ever work at controlling the AIs. We need to assume superintelligent AIs cannot be controlled in any way that anyone could think of.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h3 style=\"text-indent:14.2pt;mso-list:l0 level2 lfo5\" id=\"4_5_____In_summary\"><!--[if !supportLists]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;;mso-fareast-font-family:&quot;Calibri&quot;; color:windowtext\">4.5<span style=\"font-weight: normal; font-size: 7pt; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;; color:windowtext\">In summary</span></h3>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Denying the Orthogonality thesis thus requires that:</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">1.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">There are goals G, such that an entity an entity with goal G cannot build a superintelligence with the same goal. This despite the fact that the entity can build a superintelligence, and that a superintelligence with goal G can exist.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">2.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Goal G cannot arise accidentally from some other origin, and errors and ambiguities do not significantly broaden the space of possible goals.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">3.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Oracles and general purpose planners cannot be built. Superintelligent AIs cannot have their planning abilities repurposed.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">4.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">A superintelligence will always be able to trick its overseers, no matter how careful and cunning they are.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">5.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Though we can create an algorithm that does certain actions if it was not to be turned off after, we cannot create an algorithm that does the same thing if it was to be turned off after.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">6.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">An AI will always come to care intrinsically about things in the real world.</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin:0cm;margin-bottom:.0001pt; mso-add-space:auto;text-indent:14.2pt;mso-list:l4 level1 lfo4\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;\">7.<span style=\"font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;\">No tricks can be thought up to successfully constrain the AI\u2019s goals: superintelligent AIs simply cannot be controlled.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"5_______________________Conclusion\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">5<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Conclusion</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It is not enough to know that an agent is intelligent (or superintelligent). If we want to know something about its final goals, about the actions it will be willing to undertake to achieve them, and hence its ultimate impact on the world, there are no shortcuts. We have to directly figure out what these goals are (or figure out a way of programming them in), and cannot rely on the agent being moral just because it is superintelligent/superefficient.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"6_______________________Acknowledgements\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">6<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Acknowledgements</span></h2>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">It gives me great pleasure to acknowledge the help and support of Anders Sandberg, Nick Bostrom, Toby Ord, Diego Caleiro, Owain Evans, Daniel Dewey, Eliezer Yudkowsky, Vladimir Slepnev, Viliam Bur, Matt Freeman, Wei Dai, Will Newsome, Paul Crowley, Mao Shan, Alexander Kruel, Steve Rayhawk, Tim Tyler, John Nicholas, Ben Hoskin and Rasmus Eide, as well as those members of the Less Wrong online community going by the names shminux, and Dmytry. The work was funded by the Future of Humanity Institute (FHI), in the Department of Philosophy of Oxford University. The FHI is part of the Oxford Martin School.</span></p>\n<p>&nbsp;</p>\n<h2 style=\"text-indent:14.2pt;mso-list:l0 level1 lfo5\" id=\"7_______________________Notes_and_References\"><!--[if !supportLists]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-fareast-font-family:&quot;Calibri&quot;;color:windowtext\">7<span style=\"font-weight: normal; font-size: 7pt; line-height: normal; font-family: 'Calibri';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"font-size:12.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;serif&quot;;color:windowtext\">Notes and References</span></h2>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><!--[if supportFields]><span style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\" mce_style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"\"><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>BIBLIOGRAPHY <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span></span><![endif]--><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">Adams, E. M. (1980). Gewirth on Reason and Morality. <em>The Review of Metaphysics, 33</em>(3), 579-592.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2012). Thinking Inside the Box: Controlling and Using an Oracle AI. <em>Minds and Machines, 22</em>(4).</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Bostrom, N. (2012). The Superintelligent Will: Motivation and Instrumental Rationality in Advance Artificial Agents. <em>Minds and Machines, 22</em>(2), 71-85.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">de Fabrique, N., Romano, S. J., Vecchi, G. M., &amp; van Hasselt, V. B. (2007). Understanding Stockholm Syndrome. <em>FBI Law Enforcement Bulletin (Law Enforcement Communication Unit), 76</em>(7), 10-15.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Gewirth, A. (1978). <em>Reason and Morality.</em> University of Chicago Press.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Hume, D. (1739). <em>A Treatise of Human Nature.</em></span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Hutter, M. (2005). Universal algorithmic intelligence: A mathematical top-down approach. In B. Goertzel, &amp; C. Pennachin (Eds.), <em>Arti\ufb01cial General Intelligence.</em> Springer-Verlag.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Kornai, A. (2013). Bounding the impact of AGI. <em>Oxford 2012 Winter Intelligence conference on AGI.</em> Oxford.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Laplace, P.-S. (1814). <em>Essai philosophique sur les probabilit\u00e9s.</em></span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Omohundro, S. M. (2008). The Basic AI Drives. In P. Wang, B. Goertzel, &amp; S. Franklin (Eds.), <em>Artificial General Intelligence: Proceedings of the First AGI Conference</em> (Vol. 171).</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Rawls, J. (1971). <em>A Theory of Justice .</em> Harvard University Press.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Sandberg, A., &amp; Bostrom, N. (2008). Whole brain emulation: A roadmap. <em>Future of Humanity Institute Technical report, 2008-3</em>.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Schmidhuber, J. (2007). G\u00f6del machines: Fully self-referential optimal universal self-improvers. In <em>Artificial General Intelligence.</em> Springer.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Wang, P. (2011). The assumptions on knowledge and resources in models of rationality. <em>International Journal of Machine Consciousness, 3</em>(1), 193-218.</span></p>\n<p class=\"MsoBibliography\" style=\"margin-bottom:0cm;margin-bottom:.0001pt; text-align:justify;text-justify:inter-ideograph;text-indent:14.2pt\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;; mso-no-proof:yes\">Waser, M. R. (2008). Discovering the foundations of a universal system of ethics as a road to safe artificial intelligence. <em>Biologically inspired cognitive architectures: Papers from the AAAI fall symposium</em>, (pp. 195-200).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify;text-justify:inter-ideograph;text-indent:14.2pt\"><!--[if supportFields]><b><span style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"; mso-no-proof:yes\" mce_style=\"font-size:11.0pt;line-height:115%;font-family:\"Calibri\",\"serif\"; mso-no-proof:yes\"><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span></span></b><![endif]--><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"font-family:&quot;Calibri&quot;,&quot;serif&quot;\">&nbsp;</span></p>\n<div><!--[if !supportFootnotes]--><br> \n<hr size=\"1\">\n<!--[endif]-->\n<div id=\"ftn1\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[1]</strong>&nbsp;</span></span></span>We need to assume it has goals, of course. Determining whether something qualifies as a goal-based agent is very tricky (researcher Owain Evans is trying to establish a rigorous definition), but this paper will adopt the somewhat informal definition that an agent has goals if it achieves similar outcomes from very different starting positions. If the agent ends up making ice cream in any circumstances, we can assume ice creams are in its goals.</p>\n</div>\n<div id=\"ftn2\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[2]</strong></span></span><!--[endif]--></span> Since every law of nature is algorithmic (with some probabilistic process of known odds), and no exceptions to these laws are known, neither for human nor non-human processes.</p>\n</div>\n<div id=\"ftn3\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[3]</strong></span></span><!--[endif]--></span> See for example <a href=\"http://philosophicaldisquisitions.blogspot.co.uk/2012/04/bostrom-on-superintelligence-and.html\">http://philosophicaldisquisitions.blogspot.co.uk/2012/04/bostrom-on-superintelligence-and.html</a>, which criticise the thesis specifically.</p>\n</div>\n<div id=\"ftn4\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[4]</strong></span></span><!--[endif]--></span> An AI skilled in cooperation would drop this if cooperation no longer served its purpose; similarly, an AI accumulating power and resources would stop doing this if it found better ways of achieving its goals.</p>\n</div>\n<div id=\"ftn5\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[5]</strong></span></span><!--[endif]--></span> Even logically impossible goals can exist: \u201cconstruct a disproof of Modus Ponens (within classical logic)\u201d is a perfectly fine goal for an intelligence to have \u2013 it will quickly realise that this translates to \u201cprove classical logic is inconsistent\u201d, a task mathematicians have occasionally attempted.</p>\n</div>\n<div id=\"ftn6\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[6]</strong></span></span><!--[endif]--></span> Measuring a goal brings up subtle issues with the symbol grounding problem and similar problems. We\u2019ll ignore these issues in the present paper.</p>\n</div>\n<div id=\"ftn7\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[7]</strong></span></span><!--[endif]--></span> A motivation might simply be a threat: some truthful powerful being saying \u201cDesign an algorithm with goal G. If you succeed, I will give you great goods; if you fail, I will destroy you all. The algorithm will never be used in practice, so there are no moral objections to it being designed.\u201d</p>\n</div>\n<div id=\"ftn8\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[8]</strong></span></span><!--[endif]--></span> One could argue that we should consider the space of general animal intelligences \u2013 octopuses, supercolonies of social insects, etc... But the methods described can already produce these animal\u2019s types of behaviours.</p>\n</div>\n<div id=\"ftn9\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[9]</strong></span></span><!--[endif]--></span> Even today, many people have had great fun torturing and abusing their characters in games like \u201cthe Sims\u201d (<a href=\"http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/\">http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/</a>). The same urges are present, albeit diverted to fictionalised settings. Indeed games offer a wide variety of different goals that could conceivably be imported into an AI if it were possible to erase the reality/fiction distinction in its motivation.</p>\n</div>\n<div id=\"ftn10\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[10]</strong></span></span><!--[endif]--></span> As can be shown by a glance through a biography of famous people \u2013 and famous means they were generally allowed to rise to prominence in their own society, so the space of possible motivations was already cut down.</p>\n</div>\n<div id=\"ftn11\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[11]</strong></span></span><!--[endif]--></span> Of course, if we built an AI with that goal and copied it millions of times, it would no longer be niche.</p>\n</div>\n<div id=\"ftn12\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[12]</strong></span></span><!--[endif]--></span> Such as the hostages suffering from Stockholm syndrome <!--[if supportFields]><span style=\"mso-element:field-begin\" mce_style=\"mso-element:field-begin\"></span><span style=\"mso-spacerun:yes\" mce_style=\"mso-spacerun:yes\">&nbsp;</span>CITATION deF07 \\l 2057 <span style=\"mso-element:field-separator\" mce_style=\"mso-element:field-separator\"></span><![endif]-->(de Fabrique, Romano, Vecchi, &amp; van Hasselt, 2007)<!--[if supportFields]><span style=\"mso-no-proof:yes\" mce_style=\"mso-no-proof:yes\"><span style=\"mso-element:field-end\" mce_style=\"mso-element:field-end\"></span></span><![endif]-->.</p>\n</div>\n<div id=\"ftn13\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[13]</strong></span></span><!--[endif]--></span> See for instance E. Yudkowsky\u2019s design \u201cGeneral Intelligence and Seed AI 2.3\u201d <a href=\"http://intelligence.org/ourresearch/publications/GISAI/\">http://singinst.org/ourresearch/publications/GISAI/</a></p>\n</div>\n<div id=\"ftn14\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[14]</strong></span></span><!--[endif]--></span> Such as J. M\u00fcller\u2019s \u201cEthics, risks and opportunities of superintelligences\u201d <a href=\"http://www.jonatasmuller.com/superintelligences.pdf\">http://www.jonatasmuller.com/superintelligences.pdf</a></p>\n</div>\n<div id=\"ftn15\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[15]</strong></span></span><!--[endif]--></span> Not to be confused with the concept of Oracle in computer science, which is either an abstract machine capable of instantaneous computations in various complexity classes, or mechanism in software testing.</p>\n</div>\n<div id=\"ftn16\">\n<p class=\"MsoFootnoteText\"><span class=\"MsoFootnoteReference\"><!--[if !supportFootnotes]--><span class=\"MsoFootnoteReference\"><span style=\"font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:&quot;Calibri&quot;; mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Calibri&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA\"><strong>[16]</strong></span></span><!--[endif]--></span> Another possibility, for those who believe AIs above a certain level of intelligence must converge in their motivations, is to have a <em>society</em> of AIs below this level. If the AIs are closely linked, this could be referred to as a superorganism. Then the whole superorganism could be setup to have any particular goal and yet have high intelligence/efficiency. See <a href=\"/r/discussion/lw/gzl/amending_the_general_pupose_intelligence_arguing/\">http://lesswrong.com/r/discussion/lw/gzl/amending_the_general_pupose_intelligence_arguing/</a> for more details.</p>\n</div>\n</div>\n<p>&nbsp;</p>", "sections": [{"title": "General purpose intelligence: arguing the Orthogonality thesis", "anchor": "General_purpose_intelligence__arguing_the_Orthogonality_thesis", "level": 1}, {"title": "1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 The Orthogonality thesis", "anchor": "1_______________________The_Orthogonality_thesis", "level": 2}, {"title": "1.1\u00a0\u00a0\u00a0\u00a0 Qualifying the Orthogonality thesis", "anchor": "1_1_____Qualifying_the_Orthogonality_thesis", "level": 3}, {"title": "1.2\u00a0\u00a0\u00a0\u00a0 Orthogonality in practice for AI designers", "anchor": "1_2_____Orthogonality_in_practice_for_AI_designers", "level": 3}, {"title": "2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Orthogonality for theoretic agents", "anchor": "2_______________________Orthogonality_for_theoretic_agents", "level": 2}, {"title": "3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Orthogonality for human-level AIs", "anchor": "3_______________________Orthogonality_for_human_level_AIs", "level": 2}, {"title": "3.1\u00a0\u00a0\u00a0\u00a0 Utility functions", "anchor": "3_1_____Utility_functions", "level": 3}, {"title": "3.2\u00a0\u00a0\u00a0\u00a0 The span of human motivations", "anchor": "3_2_____The_span_of_human_motivations", "level": 3}, {"title": "3.3\u00a0\u00a0\u00a0\u00a0 Instrumental goals as final goals", "anchor": "3_3_____Instrumental_goals_as_final_goals", "level": 3}, {"title": "3.4\u00a0\u00a0\u00a0\u00a0 Noise, anti-agents and goal combination", "anchor": "3_4_____Noise__anti_agents_and_goal_combination", "level": 3}, {"title": "3.5\u00a0\u00a0\u00a0\u00a0 Further tricks up the sleeve", "anchor": "3_5_____Further_tricks_up_the_sleeve", "level": 3}, {"title": "4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Orthogonality for superhuman AIs", "anchor": "4_______________________Orthogonality_for_superhuman_AIs", "level": 2}, {"title": "4.1\u00a0\u00a0\u00a0\u00a0 No convergence", "anchor": "4_1_____No_convergence", "level": 3}, {"title": "4.2\u00a0\u00a0\u00a0\u00a0 Oracles show the way", "anchor": "4_2_____Oracles_show_the_way", "level": 3}, {"title": "4.3\u00a0\u00a0\u00a0\u00a0 Tricking the controller", "anchor": "4_3_____Tricking_the_controller", "level": 3}, {"title": "4.4\u00a0\u00a0\u00a0\u00a0 Temporary fragments of algorithms, fictional worlds and extra tricks", "anchor": "4_4_____Temporary_fragments_of_algorithms__fictional_worlds_and_extra_tricks", "level": 3}, {"title": "4.5\u00a0\u00a0\u00a0\u00a0 In summary", "anchor": "4_5_____In_summary", "level": 3}, {"title": "5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Conclusion", "anchor": "5_______________________Conclusion", "level": 2}, {"title": "6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Acknowledgements", "anchor": "6_______________________Acknowledgements", "level": 2}, {"title": "7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Notes and References", "anchor": "7_______________________Notes_and_References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 22}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nvKZchuTW8zY6wvAj", "4xxK4inefuNbkCYEg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T18:12:24.356Z", "modifiedAt": null, "url": null, "title": "We Don't Drink Vodka (LW Moscow report)", "slug": "we-don-t-drink-vodka-lw-moscow-report", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e9x6TKESmwCDToTcv/we-don-t-drink-vodka-lw-moscow-report", "pageUrlRelative": "/posts/e9x6TKESmwCDToTcv/we-don-t-drink-vodka-lw-moscow-report", "linkUrl": "https://www.lesswrong.com/posts/e9x6TKESmwCDToTcv/we-don-t-drink-vodka-lw-moscow-report", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20Don't%20Drink%20Vodka%20(LW%20Moscow%20report)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20Don't%20Drink%20Vodka%20(LW%20Moscow%20report)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9x6TKESmwCDToTcv%2Fwe-don-t-drink-vodka-lw-moscow-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20Don't%20Drink%20Vodka%20(LW%20Moscow%20report)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9x6TKESmwCDToTcv%2Fwe-don-t-drink-vodka-lw-moscow-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9x6TKESmwCDToTcv%2Fwe-don-t-drink-vodka-lw-moscow-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1362, "htmlBody": "<p lang=\"en-US\">And we don't have bears playing balalaikas. Well, I would like to tell you about Moscow rationality community after all, not about some B movie featuring crazy Russians.</p>\n<dt>\n<p lang=\"en-US\">Moscow community have grown from 5 people on my first meetup to 17 on the last one. And I believe we have possibility to grow even more. Moscow is a big city and it must have many smart people who can start to study rationality.</p>\n</dt><dt>\n<p lang=\"en-US\">Our story began in May 2012, when I gathered the people for the first time. Spring meetup announcements hadn't attracted many new members, so we gathered, discussed our site with Russian translations of LessWrong Sequences and made some plans. Our first venue was one of the Subway restaurants in Moscow.</p>\n</dt><dt>\n<p lang=\"en-US\">The next milestone in our development was September meetup, when I started to use on-line form to collect information from potential members of our group. Or maybe for some other reason we had got new faces, and even recurring ones. I also told everyone than we should practice rationality skills doing some exercises. Of course we had a lot of theories and ideas to duscuss, but we had to be closer to the real world. That's why we started to practice our rationality skills. We have approximately 8-10 people on each meetup during this fall.</p>\n</dt><dt>\n<p lang=\"en-US\">Soon enough this practice yielded good results, new members became heroes and started to improve our ways of training and create new exercises. In January, 2013 one member of our group proposed big and comfortable office room, and we moved there. Our meetups suddenly became more organized and more new members appeared &mdash; this year we have 13 people on average.</p>\n</dt><dt><span lang=\"en-US\">As for exercises, we practice calibration, tabooing, reframing, information value estimation and five minute debiasing. I will describe our versions of exercises below, and five minute debiasing you can find in the </span><span lang=\"en-US\">&ldquo;</span><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\" target=\"_blank\">How to Run a Successful Less Wrong Meetup</a><span lang=\"en-US\">&rdquo; guidelines. Only a few people from our group can speak English well enough to come to CFAR workshops.&nbsp; </span><span lang=\"en-US\">I can't thank CFAR staff enough for their support for exercises adaptation. I hope they will be even more helpful in the future.</span></dt><dt>\n<p lang=\"en-US\">We have also started to design game that can teach group members some rationality skills. You can find some examples of interesting and fun games in the same guidelines I mentioned, but we want to develop games specially for the skills improvement. Of course even educational games have to produce fun, not only teach you something. We play Liar's dice for relaxation after exercises now.</p>\n</dt><dt>\n<p lang=\"en-US\">And you can find <a title=\"Photos from the meetup\" href=\"http://imgur.com/a/UzmbC\">some photos from our meetup here</a>.</p>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong>Appendix: Exercises</strong></p>\n</dt><dt><span lang=\"en-US\"><strong>1. Calibration</strong></span><span lang=\"en-US\"><br /><br />Organizer presents two block of questions, each block has 10 questions for the sake of easy results calculation.<br /><br />In the first one I read questions which require 90% confidence intervals. For example, what is the wingspan of the last model Boeing 737? It is similar to the one from &ldquo;</span><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\" target=\"_blank\">How to Run a Successful Less Wrong Meetup</a><span lang=\"en-US\">&rdquo; guidelines. After this block everyone can calculate their real confidence, for example, if the correct answers are inside your interval in 7 out of 10, your confidence interval is closer to 70% than to 90%. So calibration level still can be improved in this case.<br /><br />The second block is similar to </span><a href=\"http://www.acritch.com/credence-game/\" target=\"_blank\">The Credence Game</a><span lang=\"en-US\">, it consists of true or false questions. Everyone needs to write down credence for each answer. The average expected credence is calculated, then the real average is calculated. For example, if someone has the following credence: 0.5, 0.7, 0.8, 0.9, 0.7, 0.6, 0.6, 0.8, 0.9, 0.8 the average expected credence will be 0.73. And if there are 6 correct, 3 incorrect and one answer with credence 50%, then it will be 6.5 real average: 0.5 for one answer with 0.5 credence and 6 for the correct answers. The two numbers are close and the person with that answers seems to be well calibrated.<br /><br />And for the some reason everyone showed better result in the second block. I can conclude that a person has more difficulties with hitting into specified confidence interval than assigning confidence for own answers.<br /><br />During the calibration session I present the following strategies to improve calibration, once a time:<br /><br />1. Repetition and feedback. Take several tests in succession, assessing how well you did after each one and attempting to improve your performance in the next one.<br />2. Equivalent bets. For each estimate, set up the equivalent bet to test if that range or probability really re\ufb02ects your uncertainty. It means that you should choose between two games. In game A you will receive a money prize if your statement is true, in other words if the correct number is between your upper and lower bounds. In game B you generate random number between 0 and 1, and you win if the random is between zero and your credence (0.9, for example). I can say that you win with probability equals to your </span><span lang=\"en-US\"><span lang=\"en-US\">credence.</span> If you prefer game A, you may be underconfident; if you prefer game B, you may be overconfident.<br />3. Consider two pros and two cons. Think of at least two reasons why you should be con\ufb01dent in your assessment and two reasons you could be wrong.<br />4. Avoid anchoring. Think of range questions as two separate binary questions of the form &ldquo;Are you 95% certain that the true value is over/under (pick one) the lower/upper (pick one) bound?&rdquo;<br />5. Reverse the anchoring effect. Start with extremely wide ranges and narrow them with the &ldquo;absurdity test&rdquo; as you eliminate highly unlikely values.<br /><br />I recommend to make at least one calibration session before any Fermi calculation sessions.<br /><br /></span><span lang=\"en-US\"><strong>2. Tabooing, version 2</strong></span><span lang=\"en-US\"><br /></span><br /> </dt><dt>\n<p lang=\"en-US\">There is standard rationalists' taboo exercise, you just remove some word from your speech and try to talk about something. But I would like to propose another version.</p>\n</dt><dt>\n<p lang=\"en-US\">You need to create some texts, or use existing texts from a book or a news article. You also need to find some words in each text that should obscure the meaning therefore tabooing will make it clearer. It may be some texts about politics or other controversial topic. Each text should be short, one or two paragraphs. You need to highlight words to taboo somehow, with italic font, separate colour or highlighter.<br /><br />At the meetup ask the people to read the text tabooing the words you have selected.<br /><br />If you are going to try this exercise, please let me know about the results, because I am still trying to improve it.</p>\n</dt><dt>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong>3. Reframing</strong></p>\n</dt><dt>\n<p lang=\"en-US\">First, define or find a statement you want to work with. The statement can be associated with some choice you want to make or it can be your interlocutor's phrase you want to make clear.</p>\n</dt><dt>\n<p lang=\"en-US\">Second, do the reframing itself:</p>\n</dt><dt>\n<p lang=\"en-US\">Check for you desire to maintain status quo. Do you see changes as bad things? Try to reverse changes direction.</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine, that you make a decision for every similar situation in the future.</p>\n</dt><dt>\n<p lang=\"en-US\">Change unit of measurement, for example, convert time into money or vice versa.</p>\n</dt><dt>\n<p lang=\"en-US\">Change time frame, into the past or the future.</p>\n</dt><dt>\n<p lang=\"en-US\">Change the arena. Transfer your conditions into another country, even into imaginary place described in some book.</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine, that another person is faced with you issue. What will he or she decide or do?</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine yourself as an outside observer. What will you think about your own thoughts and deeds?</p>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong>4. Value of information</strong></p>\n</dt><dt>\n<p lang=\"en-US\">Information can have an influence upon the utility and yield of your choices. If it can help you to make the choice with higher utility, then the difference is the value of the information for you. Take several daily situations when you need to make simple choices and estimate the impact of new information on these choices. For example, you need to buy some things and you may make random choice or look for detailed descriptions and specifications of this things. How much money, time and other resources you can save or earn in the future if you make informative choice instead of choice without the information?</p>\n</dt>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 3, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e9x6TKESmwCDToTcv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 55, "extendedScore": null, "score": 0.000135, "legacy": true, "legacyId": "22055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p lang=\"en-US\">And we don't have bears playing balalaikas. Well, I would like to tell you about Moscow rationality community after all, not about some B movie featuring crazy Russians.</p>\n<dt>\n<p lang=\"en-US\">Moscow community have grown from 5 people on my first meetup to 17 on the last one. And I believe we have possibility to grow even more. Moscow is a big city and it must have many smart people who can start to study rationality.</p>\n</dt><dt>\n<p lang=\"en-US\">Our story began in May 2012, when I gathered the people for the first time. Spring meetup announcements hadn't attracted many new members, so we gathered, discussed our site with Russian translations of LessWrong Sequences and made some plans. Our first venue was one of the Subway restaurants in Moscow.</p>\n</dt><dt>\n<p lang=\"en-US\">The next milestone in our development was September meetup, when I started to use on-line form to collect information from potential members of our group. Or maybe for some other reason we had got new faces, and even recurring ones. I also told everyone than we should practice rationality skills doing some exercises. Of course we had a lot of theories and ideas to duscuss, but we had to be closer to the real world. That's why we started to practice our rationality skills. We have approximately 8-10 people on each meetup during this fall.</p>\n</dt><dt>\n<p lang=\"en-US\">Soon enough this practice yielded good results, new members became heroes and started to improve our ways of training and create new exercises. In January, 2013 one member of our group proposed big and comfortable office room, and we moved there. Our meetups suddenly became more organized and more new members appeared \u2014 this year we have 13 people on average.</p>\n</dt><dt><span lang=\"en-US\">As for exercises, we practice calibration, tabooing, reframing, information value estimation and five minute debiasing. I will describe our versions of exercises below, and five minute debiasing you can find in the </span><span lang=\"en-US\">\u201c</span><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\" target=\"_blank\">How to Run a Successful Less Wrong Meetup</a><span lang=\"en-US\">\u201d guidelines. Only a few people from our group can speak English well enough to come to CFAR workshops.&nbsp; </span><span lang=\"en-US\">I can't thank CFAR staff enough for their support for exercises adaptation. I hope they will be even more helpful in the future.</span></dt><dt>\n<p lang=\"en-US\">We have also started to design game that can teach group members some rationality skills. You can find some examples of interesting and fun games in the same guidelines I mentioned, but we want to develop games specially for the skills improvement. Of course even educational games have to produce fun, not only teach you something. We play Liar's dice for relaxation after exercises now.</p>\n</dt><dt>\n<p lang=\"en-US\">And you can find <a title=\"Photos from the meetup\" href=\"http://imgur.com/a/UzmbC\">some photos from our meetup here</a>.</p>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong id=\"Appendix__Exercises\">Appendix: Exercises</strong></p>\n</dt><dt><span lang=\"en-US\"><strong>1. Calibration</strong></span><span lang=\"en-US\"><br><br>Organizer presents two block of questions, each block has 10 questions for the sake of easy results calculation.<br><br>In the first one I read questions which require 90% confidence intervals. For example, what is the wingspan of the last model Boeing 737? It is similar to the one from \u201c</span><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\" target=\"_blank\">How to Run a Successful Less Wrong Meetup</a><span lang=\"en-US\">\u201d guidelines. After this block everyone can calculate their real confidence, for example, if the correct answers are inside your interval in 7 out of 10, your confidence interval is closer to 70% than to 90%. So calibration level still can be improved in this case.<br><br>The second block is similar to </span><a href=\"http://www.acritch.com/credence-game/\" target=\"_blank\">The Credence Game</a><span lang=\"en-US\">, it consists of true or false questions. Everyone needs to write down credence for each answer. The average expected credence is calculated, then the real average is calculated. For example, if someone has the following credence: 0.5, 0.7, 0.8, 0.9, 0.7, 0.6, 0.6, 0.8, 0.9, 0.8 the average expected credence will be 0.73. And if there are 6 correct, 3 incorrect and one answer with credence 50%, then it will be 6.5 real average: 0.5 for one answer with 0.5 credence and 6 for the correct answers. The two numbers are close and the person with that answers seems to be well calibrated.<br><br>And for the some reason everyone showed better result in the second block. I can conclude that a person has more difficulties with hitting into specified confidence interval than assigning confidence for own answers.<br><br>During the calibration session I present the following strategies to improve calibration, once a time:<br><br>1. Repetition and feedback. Take several tests in succession, assessing how well you did after each one and attempting to improve your performance in the next one.<br>2. Equivalent bets. For each estimate, set up the equivalent bet to test if that range or probability really re\ufb02ects your uncertainty. It means that you should choose between two games. In game A you will receive a money prize if your statement is true, in other words if the correct number is between your upper and lower bounds. In game B you generate random number between 0 and 1, and you win if the random is between zero and your credence (0.9, for example). I can say that you win with probability equals to your </span><span lang=\"en-US\"><span lang=\"en-US\">credence.</span> If you prefer game A, you may be underconfident; if you prefer game B, you may be overconfident.<br>3. Consider two pros and two cons. Think of at least two reasons why you should be con\ufb01dent in your assessment and two reasons you could be wrong.<br>4. Avoid anchoring. Think of range questions as two separate binary questions of the form \u201cAre you 95% certain that the true value is over/under (pick one) the lower/upper (pick one) bound?\u201d<br>5. Reverse the anchoring effect. Start with extremely wide ranges and narrow them with the \u201cabsurdity test\u201d as you eliminate highly unlikely values.<br><br>I recommend to make at least one calibration session before any Fermi calculation sessions.<br><br></span><span lang=\"en-US\"><strong>2. Tabooing, version 2</strong></span><span lang=\"en-US\"><br></span><br> </dt><dt>\n<p lang=\"en-US\">There is standard rationalists' taboo exercise, you just remove some word from your speech and try to talk about something. But I would like to propose another version.</p>\n</dt><dt>\n<p lang=\"en-US\">You need to create some texts, or use existing texts from a book or a news article. You also need to find some words in each text that should obscure the meaning therefore tabooing will make it clearer. It may be some texts about politics or other controversial topic. Each text should be short, one or two paragraphs. You need to highlight words to taboo somehow, with italic font, separate colour or highlighter.<br><br>At the meetup ask the people to read the text tabooing the words you have selected.<br><br>If you are going to try this exercise, please let me know about the results, because I am still trying to improve it.</p>\n</dt><dt>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong id=\"3__Reframing\">3. Reframing</strong></p>\n</dt><dt>\n<p lang=\"en-US\">First, define or find a statement you want to work with. The statement can be associated with some choice you want to make or it can be your interlocutor's phrase you want to make clear.</p>\n</dt><dt>\n<p lang=\"en-US\">Second, do the reframing itself:</p>\n</dt><dt>\n<p lang=\"en-US\">Check for you desire to maintain status quo. Do you see changes as bad things? Try to reverse changes direction.</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine, that you make a decision for every similar situation in the future.</p>\n</dt><dt>\n<p lang=\"en-US\">Change unit of measurement, for example, convert time into money or vice versa.</p>\n</dt><dt>\n<p lang=\"en-US\">Change time frame, into the past or the future.</p>\n</dt><dt>\n<p lang=\"en-US\">Change the arena. Transfer your conditions into another country, even into imaginary place described in some book.</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine, that another person is faced with you issue. What will he or she decide or do?</p>\n</dt><dt>\n<p lang=\"en-US\">Imagine yourself as an outside observer. What will you think about your own thoughts and deeds?</p>\n<p lang=\"en-US\">&nbsp;</p>\n</dt><dt>\n<p lang=\"en-US\"><strong id=\"4__Value_of_information\">4. Value of information</strong></p>\n</dt><dt>\n<p lang=\"en-US\">Information can have an influence upon the utility and yield of your choices. If it can help you to make the choice with higher utility, then the difference is the value of the information for you. Take several daily situations when you need to make simple choices and estimate the impact of new information on these choices. For example, you need to buy some things and you may make random choice or look for detailed descriptions and specifications of this things. How much money, time and other resources you can save or earn in the future if you make informative choice instead of choice without the information?</p>\n</dt>", "sections": [{"title": "Appendix: Exercises", "anchor": "Appendix__Exercises", "level": 1}, {"title": "3. Reframing", "anchor": "3__Reframing", "level": 1}, {"title": "4. Value of information", "anchor": "4__Value_of_information", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T19:54:30.809Z", "modifiedAt": null, "url": null, "title": "Recent updates to gwern.net (2012-2013) ", "slug": "recent-updates-to-gwern-net-2012-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ZB37f25up8E7Z8Dn/recent-updates-to-gwern-net-2012-2013", "pageUrlRelative": "/posts/7ZB37f25up8E7Z8Dn/recent-updates-to-gwern-net-2012-2013", "linkUrl": "https://www.lesswrong.com/posts/7ZB37f25up8E7Z8Dn/recent-updates-to-gwern-net-2012-2013", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recent%20updates%20to%20gwern.net%20(2012-2013)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecent%20updates%20to%20gwern.net%20(2012-2013)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZB37f25up8E7Z8Dn%2Frecent-updates-to-gwern-net-2012-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recent%20updates%20to%20gwern.net%20(2012-2013)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZB37f25up8E7Z8Dn%2Frecent-updates-to-gwern-net-2012-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZB37f25up8E7Z8Dn%2Frecent-updates-to-gwern-net-2012-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 897, "htmlBody": "<blockquote>\n<p>Previous: <a href=\"/lw/8kv/recent_updates_to_gwernnet/\">Recent updates to gwern.net (2011)</a></p>\n</blockquote>\n<p>\u201cBut where shall wisdom be found? / And where is the place of understanding? / Man knoweth not the price thereof; neither is it found in the land of the living\u2026for the price of wisdom is above rubies.\u201d</p>\n<p>As before, here is material I\u2019ve worked on in the 477 days since my last update which LWers may find interesting. In roughly chronological &amp; topical order, here are the major additions to <code>gwern.net</code>:</p>\n<ul>\n<li>I interviewed translator <a href=\"http://www.gwern.net/docs/2011-house\">Michael House</a> about his work in Japan as a translator</li>\n<li>finished data collection for my <a href=\"http://www.gwern.net/hafu\">hafu anime statistics page</a> and begun analysis. (I\u2019ve achieved good coverage of characters, found an astonishingly consistent absence of Korean characters, and confirmed the blond-haired/blue-eyed stereotype; but my original thesis doesn\u2019t seem to work and the data is too unevenly distributed to identify time trends.)</li>\n<li>judged the <a href=\"http://www.gwern.net/Haskell%20Summer%20of%20Code#section-5\">2011</a> &amp; <a href=\"http://www.gwern.net/Haskell%20Summer%20of%20Code#section-6\">2012</a> results for the Haskell Summer of Codes and the accuracy of my predictions</li>\n<li>did <a href=\"http://www.gwern.net/DNB%20FAQ#meta-analysis\">a meta-analysis</a> on whether dual n-back increases IQ, and examining possible biases and various claims about what makes the training work or not work</li>\n<li>did <a href=\"http://www.gwern.net/Iodine#meta-analysis\">another meta-analysis</a> on whether iodine increases IQ, etc</li>\n<li>\n<p>modafinil:</p>\n<ul>\n<li>checked for <a href=\"http://www.gwern.net/Nootropics#modalert-blind-day-trial\">subjective effects of blinded modafinil</a></li>\n<li>updated my <a href=\"http://www.gwern.net/Modafinil#suppliers-prices\">modafinil price-chart</a> twice, and expanded with brand data and a new armodafinil table</li>\n<li>researched modafinil-related <a href=\"http://www.gwern.net/Modafinil#legal-risk\">prosecutions &amp; convictions</a> in the USA</li>\n<li>and any connection with <a href=\"http://www.gwern.net/Modafinil#schizophrenia\">schizophrenia</a></li>\n</ul>\n</li>\n<li>tried <a href=\"http://www.gwern.net/Nootropics#kratom\">kratom</a></li>\n<li>did a <a href=\"http://www.gwern.net/Nootropics#nicotine-experiment\">nicotine gum/n-back experiment</a></li>\n<li>did <a href=\"http://www.gwern.net/Zeo#potassium\">2 potassium experiments</a>; neither improved my mood/productivity, and one damaged my sleep</li>\n<li>my Silk Road page has been expanded with a <a href=\"http://www.gwern.net/Silk%20Road#bbc-questions\">BBC interview</a>, putting SR in a <a href=\"http://www.gwern.net/Silk%20Road#silk-road-as-cyphernomicons-black-markets\">historical cypherpunk context</a>, an updated account of <a href=\"http://www.gwern.net/Silk%20Road#safe\">all arrests &amp; law enforcement actions</a>, and <a href=\"http://www.gwern.net/Silk%20Road#lsd-case-study\">application of basic statistics to ordering</a></li>\n<li>ran 2 sleep experiments on the timing of taking a vitamin D supplement: I found that <a href=\"http://www.gwern.net/Zeo#vitamin-d\">taking vitamin D before bed</a> substantially damaged my sleep, while <a href=\"http://www.gwern.net/Zeo#vitamin-d-at-morn-helps\">taking vitamin D after waking up</a> did not hurt &amp; somewhat helped</li>\n<li>checked whether a walking desk (treadmill) <a href=\"http://www.gwern.net/Zeo#fn53\">damaged typing speed or accuracy</a></li>\n<li>I have run 3 Wikipedia experiments establishing that: <a href=\"http://www.gwern.net/In%20Defense%20Of%20Inclusionism#sins-of-omission-experiment-1\">Talk page edits are ignored</a> by editors; <a href=\"http://www.gwern.net/In%20Defense%20Of%20Inclusionism#sins-of-omission-experiment-2\">random link deletions (and their restoration) are also ignored</a> by editors; and <a href=\"http://www.gwern.net/In%20Defense%20Of%20Inclusionism#ignoti-sed-non-occulti\">external link suggestions on Talk pages</a> are also ignored by readers. (I take the former 2 as indicative of the decline in edit activity and rise of deletionist beliefs on Wikipedia.)</li>\n<li>tried some economic/historical analysis: <a href=\"http://www.gwern.net/Notes#reasons-of-state-why-didnt-denmark-sell-greenland\">\u201cReasons of State: Why Didn\u2019t Denmark Sell Greenland to the USA?\u201d</a></li>\n<li><a href=\"http://www.gwern.net/Sunk%20cost\">Defending sunk costs</a> essay (<a href=\"/lw/9si/is_sunk_cost_fallacy_a_fallacy/\">LW discussion</a>)</li>\n<li><a href=\"http://www.gwern.net/Slowing%20Moore%27s%20Law\">\u201cSlowing Moore\u2019s Law: Why You Might Want To and How You Would Do It\u201d</a></li>\n<li><a href=\"http://www.gwern.net/Hyperbolic%20Time%20Chamber\">\u201cThe Hyperbolic Time Chamber as Brain Emulation Analogy\u201d</a></li>\n<li>tried estimating the bandwidth of a <a href=\"http://www.gwern.net/Death%20Note%20Anonymity#communicating-with-a-death-note\">Death Note</a></li>\n<li><a href=\"http://www.gwern.net/hpmor\">compiled predictions</a> for <em>Harry Potter and the Methods of Rationality</em></li>\n<li>looked into <a href=\"http://www.gwern.net/Conscientiousness%20and%20online%20education\">Conscientiousness and online education</a>; studies so far are useless from a meta-analytic standpoint</li>\n<li>tripled length of <a href=\"http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology\">appendix</a> dealing with the reliability of mainstream science (methodological flaws, replication rates, etc)</li>\n<li>finished meta-ethics essay, <a href=\"http://www.gwern.net/The%20Narrowing%20Circle\">\u201cThe Narrowing Circle\u201d</a></li>\n<li>explained the philosophy saying <a href=\"http://www.gwern.net/Prediction%20markets#modus-tollens-vs-modus-ponens\">\u201cone man\u2019s modus ponens is another man\u2019s modus tollens\u201d</a></li>\n<li>speculation about a <a href=\"http://www.gwern.net/Notes#alternate-futures-the-second-english-restoration\">restoration of the British monarchy</a></li>\n<li>clean up &amp; exploratory data analysis of <a href=\"http://www.gwern.net/Zeo#sdr-lucid-dreaming-exploratory-data-analysis\">SDr\u2019s lucid dreaming data</a></li>\n<li><a href=\"http://www.gwern.net/Death%20Note%20script\">Who wrote the <em>Death Note</em> script?</a> (<a href=\"/lw/f63/case_study_the_death_note_script_and_bayes/\">LW discussion</a>)</li>\n<li><a href=\"http://www.gwern.net/2012%20election%20predictions\">2012 US election predictions: statistical comparison</a> </li>\n<li>\n<p><a href=\"http://www.gwern.net/Anchoring\">Comment anchoring experiment</a> (<a href=\"/lw/gft/lw_anchoring_experiment_maybe/\">LW discussion</a>)</p>\n</li>\n<li><a href=\"http://www.gwern.net/Notes#surprising-turing-complete-languages\">Turing-completeness in surprising places</a> (inventory of particularly \u201cweird machines\u201d; relevant to computer and AI security)</li>\n</ul>\n<p>Transcribed or translated:</p>\n<ul>\n<li><a href=\"http://www.gwern.net/docs/1955-nash\">Nash\u2019s letters on cryptography</a></li>\n<li>Douglas Hofstadter\u2019s <a href=\"http://www.gwern.net/docs/1985-hofstadter\">superrationality</a> columns (from <em>Metamagical Themas</em>, 1985)</li>\n<li><a href=\"http://www.gwern.net/docs/1987-rossi\">\u201cThe Iron Law Of Evaluation And Other Metallic Rules\u201d</a>, Rossi 1987 (lessons from the large RCTs evaluating social &amp; welfare interventions)</li>\n<li><a href=\"http://www.gwern.net/docs/1994-falk\">\u201cThe Ups and Downs of the Hope Function In a Fruitless Search\u201d</a>, Falk et al 1994</li>\n<li><a href=\"http://www.gwern.net/docs/2007-wolfe\">Gene Wolfe on writing</a></li>\n<li><a href=\"http://www.gwern.net/docs/2002-gibson\">\u201cShiny balls of Mud: William Gibson Looks at Japanese Pursuits of Perfection\u201d</a> (2002)</li>\n<li><a href=\"http://www.gwern.net/docs/2004-okada\">\u201cOtaku Talk\u201d, Okada et al 2004</a></li>\n<li><a href=\"http://www.gwern.net/docs/2005-murakami\">\u201cEarth in My Window\u201d, Murakami 2005</a></li>\n<li><a href=\"http://www.gwern.net/docs/2005-sawaragi\">\u201cOn The Battlefield of \u2018Superflat\u2019\u201d</a></li>\n<li><a href=\"http://www.gwern.net/docs/2010-sarrazin\">\u201cEro-Anime: Manga Comes Alive\u201d, Sarrazin 2010</a></li>\n<li><a href=\"http://www.gwern.net/docs/eva/1996-newtype-anno-interview\">1996 <em>NewType</em> interview with Hideaki Anno</a> (translated by me, with the help of an EGFer)</li>\n<li><a href=\"http://www.gwern.net/docs/eva/1997-animeland-may-hideakianno-interview-english\">1997 <em>Animeland</em> interview with Hideaki Anno</a> (bought, transcribed, and translated by me with the help of other LWers)</li>\n<li><a href=\"http://www.gwern.net/docs/1997-utena\">1997 <em>Utena</em> interviews</a></li>\n</ul>\n<p>More technical:</p>\n<ul>\n<li>added <a href=\"http://www.gwern.net/docs/gwern.net-gitstats/index.html\">edit history statistics/visualization</a> for <code>gwern.net</code> using <a href=\"http://gitstats.sourceforge.net/\">GitStats</a></li>\n<li>site traffic updates: <a href=\"http://www.gwern.net/About#july-2011---december-2011\">July-December 2011</a>, <a href=\"http://www.gwern.net/About#january-2012---july-2012\">January 2012-July 2012</a>, <a href=\"http://www.gwern.net/About#july-2012---january-2013\">July 2012-Jan 2013</a></li>\n<li>There\u2019s also been a lot of <a href=\"http://www.gwern.net/About#colophon\">backend</a> changes: switching to Amazon S3+Cloudflare, adding error pages, metadata like tags, A/B testing, but no need to go into detail.</li>\n</ul>\n<p>Personal:</p>\n<ul>\n<li>dumped my notes on my <a href=\"http://www.gwern.net/2011%20San%20Francisco\">2011 visit</a> to San Francisco</li>\n<li>posted summaries of <a href=\"http://www.gwern.net/Links#profile\">my personality &amp; attitudes</a> &amp; my <a href=\"http://www.gwern.net/docs/gwern-google-reader-subscriptions.xml\">RSS feed collection</a></li>\n<li>enjoyed some <a href=\"http://www.gwern.net/Mead\">mead</a>; I still like tea better, though</li>\n<li>dumped notes on the <a href=\"http://www.gwern.net/ICON%202012\">2012 SF convention ICON</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ZB37f25up8E7Z8Dn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 92, "extendedScore": null, "score": 0.00022425196784699183, "legacy": true, "legacyId": "22043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": true, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["Lk9NHCNPp3h44uhxA", "QvuD7R5L5ABw9tDdD", "99DctaXedCFPfG2vb", "DfvX99AKx7pR7NE3v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T21:52:53.440Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality Bookshelves", "slug": "harry-potter-and-the-methods-of-rationality-bookshelves", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JesseGalef", "createdAt": "2011-04-02T01:36:57.286Z", "isAdmin": false, "displayName": "JesseGalef"}, "userId": "xmmzCdbLf5PBgEEC7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8axSFqPb5btC5Qq5/harry-potter-and-the-methods-of-rationality-bookshelves", "pageUrlRelative": "/posts/v8axSFqPb5btC5Qq5/harry-potter-and-the-methods-of-rationality-bookshelves", "linkUrl": "https://www.lesswrong.com/posts/v8axSFqPb5btC5Qq5/harry-potter-and-the-methods-of-rationality-bookshelves", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20Bookshelves&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20Bookshelves%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8axSFqPb5btC5Qq5%2Fharry-potter-and-the-methods-of-rationality-bookshelves%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20Bookshelves%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8axSFqPb5btC5Qq5%2Fharry-potter-and-the-methods-of-rationality-bookshelves", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8axSFqPb5btC5Qq5%2Fharry-potter-and-the-methods-of-rationality-bookshelves", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>A while back in the Columbus Rationality group, we started wondering: What books would the Harry Potter and the Methods of Rationality houses have in each of their libraries?&nbsp; We had fun categorizing different subjects:</p>\n<ul>\n<li>Gryffindor - Combat, ethics, and justice</li>\n<li>Ravenclaw - Philosophy, cognitive science, and math</li>\n<li>Slytherin -Influence and power</li>\n<li>Hufflepuff - Happiness, productivity, and friendship</li>\n</ul>\n<p>And so, I found myself taking all my books off their shelves this weekend and picking the best to represent each rationality!House and made them into Facebook cover-image-sized pictures.&nbsp; Click each image to see it larger, with a list on the left:</p>\n<p>(first posted at <a href=\"http://measureofdoubt.com/2013/03/18/what-would-a-rational-gryffindor-read/\">Measure of Doubt</a>)</p>\n<p><a href=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-gryffindor-library.jpg\"><img style=\"width: 695px;\" src=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-gryffindor-library.jpg\" alt=\"\" /></a></p>\n<p><a href=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-ravenclaw-library.jpg\"><img style=\"width: 695px;\" src=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-ravenclaw-library.jpg\" alt=\"\" /></a></p>\n<p><a href=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-slytherin-library.jpg\"><img style=\"width: 695px;\" src=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-slytherin-library.jpg\" alt=\"\" /></a></p>\n<p><a href=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-hufflepuff-library.jpg\"><img style=\"width: 695px;\" src=\"http://measureofdoubt.files.wordpress.com/2013/03/rationality-hufflepuff-library.jpg\" alt=\"\" /></a></p>\n<p>&nbsp;</p>\n<p>I&rsquo;m always open to book recommendations and suggestions for good fits.&nbsp; What other books would be especially appropriate for each shelf?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8axSFqPb5btC5Qq5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 45, "extendedScore": null, "score": 0.000195, "legacy": true, "legacyId": "22056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-18T23:49:57.035Z", "modifiedAt": null, "url": null, "title": "Critiques of the heuristics and biases tradition", "slug": "critiques-of-the-heuristics-and-biases-tradition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:01.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DAf4W9ZYuzuLaGvd5/critiques-of-the-heuristics-and-biases-tradition", "pageUrlRelative": "/posts/DAf4W9ZYuzuLaGvd5/critiques-of-the-heuristics-and-biases-tradition", "linkUrl": "https://www.lesswrong.com/posts/DAf4W9ZYuzuLaGvd5/critiques-of-the-heuristics-and-biases-tradition", "postedAtFormatted": "Monday, March 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiques%20of%20the%20heuristics%20and%20biases%20tradition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiques%20of%20the%20heuristics%20and%20biases%20tradition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDAf4W9ZYuzuLaGvd5%2Fcritiques-of-the-heuristics-and-biases-tradition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiques%20of%20the%20heuristics%20and%20biases%20tradition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDAf4W9ZYuzuLaGvd5%2Fcritiques-of-the-heuristics-and-biases-tradition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDAf4W9ZYuzuLaGvd5%2Fcritiques-of-the-heuristics-and-biases-tradition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 755, "htmlBody": "<p>The chapter on judgment under uncertainty in the (excellent) new <em><a href=\"http://www.amazon.com/Oxford-Handbook-Cognitive-Psychology-Library/dp/0195376749/\">Oxford Handbook of Cognitive Psychology</a></em> has a handy little section on recent critiques of the \"heuristics and biases\" tradition. It also discusses problems with the somewhat-competing \"fast and frugal heuristics\" school of thought, but for now let me just quote the section on heuristics and biases (pp. 608-609):</p>\n<blockquote>\n<p>The heuristics and biases program has been highly influential; however, some have argued that in recent years the influence, at least in psychology, has waned (McKenzie, 2005). This waning has been due in part to pointed critiques of the approach (e.g., Gigerenzer, 1996). This critique comprises two main arguments: (1) that by focusing mainly on coherence standards [e.g. their <em>rationality</em> given the subject's other beliefs, as contrasted with <em>correspondence standards</em> having to do with the real-world accuracy of a subject's beliefs] the approach ignores the role played by the environment or the context in which a judgment is made; and (2) that the explanations of phenomena via one-word labels such as availability, anchoring, and representativeness are vague, insufficient, and say nothing about the processes underlying judgment (see Kahneman, 2003; Kahneman &amp; Tversky, 1996 for responses to this critique).</p>\n<p>The accuracy of some of the heuristics proposed by Tversky and Kahneman can be compared to correspondence criteria (availability and anchoring). Thus, arguing that the tradition only uses the &ldquo;narrow norms&rdquo; (Gigerenzer, 1996) of coherence criteria is not strictly accurate (cf. Dunwoody, 2009). Nonetheless, responses in famous examples like the Linda problem can be reinterpreted as sensible rather than erroneous if one uses conversational or pragmatic norms rather than those derived from probability theory (Hilton, 1995). For example, Hertwig, Benz and Krauss (2008) asked participants which of the following two statements is more probable:</p>\n<blockquote>\n<p>[X] The percentage of adolescent smokers in Germany decreases at least 15% from current levels by September 1, 2003.</p>\n<p>[X&amp;Y] The tobacco tax in Germany is increased by 5 cents per cigarette and the percentage of adolescent smokers in Germany decreases at least 15% from current levels by September 1, 2003.</p>\n</blockquote>\n<p>According to the conjunction rule, [X&amp;Y cannot be more probable than X] and yet the majority of participants ranked the statements in that order. However, when subsequently asked to rank order four statements in order of how well each one described their understanding of X&amp;Y, there was an overwhelming tendency to rank statements like &ldquo;X and therefore Y&rdquo; or &ldquo;X and X is the cause for Y&rdquo; higher than the simple conjunction &ldquo;X and Y.&rdquo; Moreover, the minority of participants who did not commit the conjunction fallacy in the first judgment showed internal coherence by ranking &ldquo;X and Y&rdquo; as best describing their understanding in the second judgment.These results suggest that people adopt a causal understanding of the statements, in essence ranking the probability of X, given Y as more probable than X occurring alone. If so, then arguably the conjunction &ldquo;error&rdquo; is no longer incorrect. (See Moro, 2009 for extensive discussion of the reasons underlying the conjunction fallacy, including why &ldquo;misunderstanding&rdquo; cannot explain all instances of the fallacy.)</p>\n<p>The &ldquo;vagueness&rdquo; argument can be illustrated by considering two related phenomena: the gambler&rsquo;s fallacy and the hot-hand (Gigerenzer &amp; Brighton, 2009). The gambler&rsquo;s fallacy is the tendency for people to predict the opposite outcome after a run of the same outcome (e.g., predicting heads after a run of tails when flipping a fair coin); the hot-hand, in contrast, is the tendency to predict a run will continue (e.g., a player making a shot in basketball after a succession of baskets; Gilovich, Vallone, &amp; Tversky, 1985). Ayton and Fischer (2004) pointed out that although these two behaviors are opposite - ending or continuing runs - they have both been explained via the label &ldquo;representativeness.&rdquo; In both cases a faulty concept of randomness leads people to expect short sections of a sequence to be &ldquo;representative&rdquo; of their generating process. In the case of the coin, people believe (erroneously) that long runs should not occur, so the opposite outcome is predicted; for the player, the presence of long runs rules out a random process so a continuation is predicted (Gilovich et al., 1985). The &ldquo;representativeness&rdquo; explanation is therefore incomplete without specifying a priori which of the opposing prior expectations will result. More important, representativeness alone does not explain <em>why</em> people have the misconception that random sequences should exhibit local representativeness when in reality they do not (Ayton &amp; Fischer, 2004).</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><small>My thanks to MIRI intern Stephen Barnes for transcribing this text.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DAf4W9ZYuzuLaGvd5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 27, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "22057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-19T03:57:02.933Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Don't Believe You'll Self-Deceive", "slug": "seq-rerun-don-t-believe-you-ll-self-deceive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.449Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Yj2TuEC2P67M34v4/seq-rerun-don-t-believe-you-ll-self-deceive", "pageUrlRelative": "/posts/6Yj2TuEC2P67M34v4/seq-rerun-don-t-believe-you-ll-self-deceive", "linkUrl": "https://www.lesswrong.com/posts/6Yj2TuEC2P67M34v4/seq-rerun-don-t-believe-you-ll-self-deceive", "postedAtFormatted": "Tuesday, March 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Don't%20Believe%20You'll%20Self-Deceive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Don't%20Believe%20You'll%20Self-Deceive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Yj2TuEC2P67M34v4%2Fseq-rerun-don-t-believe-you-ll-self-deceive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Don't%20Believe%20You'll%20Self-Deceive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Yj2TuEC2P67M34v4%2Fseq-rerun-don-t-believe-you-ll-self-deceive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Yj2TuEC2P67M34v4%2Fseq-rerun-don-t-believe-you-ll-self-deceive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/1o/dont_believe_youll_selfdeceive/\">Don't Believe You'll Self-Deceive</a> was originally published on 09 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Don.27t_Believe_You.27ll_Self-Deceive\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It may be wise to tell yourself that you will not be able to successfully deceive yourself, because by telling yourself this, you may make it true.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h0h/seq_rerun_moores_paradox/\">Moore's Paradox</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Yj2TuEC2P67M34v4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1426955754318473e-06, "legacy": true, "legacyId": "22058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W7LcN9gmdnaAk9K52", "WtMWX4j8DhfKpwPrs", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-19T15:20:44.348Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR meetup, ch. 47-50", "slug": "meetup-durham-hpmor-meetup-ch-47-50", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NKCknpByuXKGJS2Kg/meetup-durham-hpmor-meetup-ch-47-50", "pageUrlRelative": "/posts/NKCknpByuXKGJS2Kg/meetup-durham-hpmor-meetup-ch-47-50", "linkUrl": "https://www.lesswrong.com/posts/NKCknpByuXKGJS2Kg/meetup-durham-hpmor-meetup-ch-47-50", "postedAtFormatted": "Tuesday, March 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20meetup%2C%20ch.%2047-50&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20meetup%2C%20ch.%2047-50%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKCknpByuXKGJS2Kg%2Fmeetup-durham-hpmor-meetup-ch-47-50%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20meetup%2C%20ch.%2047-50%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKCknpByuXKGJS2Kg%2Fmeetup-durham-hpmor-meetup-ch-47-50", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKCknpByuXKGJS2Kg%2Fmeetup-durham-hpmor-meetup-ch-47-50", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/km'>Durham HPMoR meetup, ch. 47-50</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 March 2013 11:30:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">721 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of the interstitial chapters after Humanism but before TSPE. <br />\nWe'll reprise our last meetup's format:</p>\n\n<p>11:30 -- Food from food trucks on Hunt St. near Foster/the farmers' market <br />\n11:45 -- Coffee from Cocoa Cinnamon, at Foster &amp; Geer <br />\n12? -- meet at Fullsteam (address above), just south of Rigsbee and Geer.  (Last week we were a little late, but I will redouble my efforts to mind the time.)</p>\n\n<p>All locations are within a couple of blocks; park wherever looks convenient.</p>\n\n<p>Assorted weather sources are currently predicting a pleasant day, but if they change their minds we may consider a more sheltered venue.  If so, we'll change the meetup location not later than noon Friday.  Join the RTLW Google group if you'd like to be notified via email:  <a href=\"http://groups.google.com/group/RTLW\" rel=\"nofollow\">http://groups.google.com/group/RTLW</a></p>\n\n<p>As always, having done the reading (even recently) is encouraged, but not required.</p>\n\n<p>A note:  <a href=\"https://www.facebook.com/pages/Marry-Durham/148002965234141\" rel=\"nofollow\">Marry Durham</a> will be starting a little later on in the afternoon (nominally 2-5 PM, but Durhamites like to party.)  I suspect we'll all be arriving early enough that this shouldn't pose too many logistical concerns, but I would recommend <em>not</em> parking on Rigsbee if you'll need to leave at any particular time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/km'>Durham HPMoR meetup, ch. 47-50</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NKCknpByuXKGJS2Kg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1431504713655347e-06, "legacy": true, "legacyId": "22065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_meetup__ch__47_50\">Discussion article for the meetup : <a href=\"/meetups/km\">Durham HPMoR meetup, ch. 47-50</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 March 2013 11:30:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">721 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of the interstitial chapters after Humanism but before TSPE. <br>\nWe'll reprise our last meetup's format:</p>\n\n<p>11:30 -- Food from food trucks on Hunt St. near Foster/the farmers' market <br>\n11:45 -- Coffee from Cocoa Cinnamon, at Foster &amp; Geer <br>\n12? -- meet at Fullsteam (address above), just south of Rigsbee and Geer.  (Last week we were a little late, but I will redouble my efforts to mind the time.)</p>\n\n<p>All locations are within a couple of blocks; park wherever looks convenient.</p>\n\n<p>Assorted weather sources are currently predicting a pleasant day, but if they change their minds we may consider a more sheltered venue.  If so, we'll change the meetup location not later than noon Friday.  Join the RTLW Google group if you'd like to be notified via email:  <a href=\"http://groups.google.com/group/RTLW\" rel=\"nofollow\">http://groups.google.com/group/RTLW</a></p>\n\n<p>As always, having done the reading (even recently) is encouraged, but not required.</p>\n\n<p>A note:  <a href=\"https://www.facebook.com/pages/Marry-Durham/148002965234141\" rel=\"nofollow\">Marry Durham</a> will be starting a little later on in the afternoon (nominally 2-5 PM, but Durhamites like to party.)  I suspect we'll all be arriving early enough that this shouldn't pose too many logistical concerns, but I would recommend <em>not</em> parking on Rigsbee if you'll need to leave at any particular time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_meetup__ch__47_501\">Discussion article for the meetup : <a href=\"/meetups/km\">Durham HPMoR meetup, ch. 47-50</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR meetup, ch. 47-50", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_meetup__ch__47_50", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR meetup, ch. 47-50", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_meetup__ch__47_501", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-19T20:12:21.114Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal Meetup - Bayes' Theorem", "slug": "meetup-montreal-meetup-bayes-theorem", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rdCSbF9YqeEbRPTsZ/meetup-montreal-meetup-bayes-theorem", "pageUrlRelative": "/posts/rdCSbF9YqeEbRPTsZ/meetup-montreal-meetup-bayes-theorem", "linkUrl": "https://www.lesswrong.com/posts/rdCSbF9YqeEbRPTsZ/meetup-montreal-meetup-bayes-theorem", "postedAtFormatted": "Tuesday, March 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20Meetup%20-%20Bayes'%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20Meetup%20-%20Bayes'%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdCSbF9YqeEbRPTsZ%2Fmeetup-montreal-meetup-bayes-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20Meetup%20-%20Bayes'%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdCSbF9YqeEbRPTsZ%2Fmeetup-montreal-meetup-bayes-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdCSbF9YqeEbRPTsZ%2Fmeetup-montreal-meetup-bayes-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kn'>Montreal Meetup - Bayes' Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 March 2013 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Ave. Du President-Kennedy, Montreal, Quebec, Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next weekly meetup is going to be another look at Bayes' theorem, given by our resident statistician.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kn'>Montreal Meetup - Bayes' Theorem</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rdCSbF9YqeEbRPTsZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.1433445946148736e-06, "legacy": true, "legacyId": "22066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Bayes__Theorem\">Discussion article for the meetup : <a href=\"/meetups/kn\">Montreal Meetup - Bayes' Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 March 2013 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Ave. Du President-Kennedy, Montreal, Quebec, Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next weekly meetup is going to be another look at Bayes' theorem, given by our resident statistician.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Bayes__Theorem1\">Discussion article for the meetup : <a href=\"/meetups/kn\">Montreal Meetup - Bayes' Theorem</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal Meetup - Bayes' Theorem", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Bayes__Theorem", "level": 1}, {"title": "Discussion article for the meetup : Montreal Meetup - Bayes' Theorem", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Bayes__Theorem1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-19T20:59:56.919Z", "modifiedAt": null, "url": null, "title": "New Canon!HP cover art similarity", "slug": "new-canon-hp-cover-art-similarity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/amB3TTMnCaEaGFSc9/new-canon-hp-cover-art-similarity", "pageUrlRelative": "/posts/amB3TTMnCaEaGFSc9/new-canon-hp-cover-art-similarity", "linkUrl": "https://www.lesswrong.com/posts/amB3TTMnCaEaGFSc9/new-canon-hp-cover-art-similarity", "postedAtFormatted": "Tuesday, March 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Canon!HP%20cover%20art%20similarity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Canon!HP%20cover%20art%20similarity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamB3TTMnCaEaGFSc9%2Fnew-canon-hp-cover-art-similarity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Canon!HP%20cover%20art%20similarity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamB3TTMnCaEaGFSc9%2Fnew-canon-hp-cover-art-similarity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamB3TTMnCaEaGFSc9%2Fnew-canon-hp-cover-art-similarity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>In fairness, this is almost certainly a coincidence. But it's interesting how similar the new HP cover art looks to Dinosaurusgede's \"Shopping With Minerva\" piece</p>\n<p>http://dinosaurusgede.deviantart.com/art/shopping-with-minerva-174358965</p>\n<p><img src=\"http://fc01.deviantart.net/fs71/i/2010/219/8/9/shopping_with_minerva_by_dinosaurusgede.jpg\" alt=\"\" width=\"450\" height=\"398\" /></p>\n<p>http://io9.com/5984599/the-harry-potter-books-are-finally-getting-decent-covers</p>\n<p><img src=\"http://img.gawkerassets.com/img/18eqlhbqv44b1jpg/ku-large.jpg\" alt=\"\" width=\"470\" height=\"716\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "amB3TTMnCaEaGFSc9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -21, "extendedScore": null, "score": -5.9e-05, "legacy": true, "legacyId": "22067", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-20T05:53:37.684Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Striving to Accept", "slug": "seq-rerun-striving-to-accept", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QQqnx3rKw64b4M5T5/seq-rerun-striving-to-accept", "pageUrlRelative": "/posts/QQqnx3rKw64b4M5T5/seq-rerun-striving-to-accept", "linkUrl": "https://www.lesswrong.com/posts/QQqnx3rKw64b4M5T5/seq-rerun-striving-to-accept", "postedAtFormatted": "Wednesday, March 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Striving%20to%20Accept&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Striving%20to%20Accept%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQqnx3rKw64b4M5T5%2Fseq-rerun-striving-to-accept%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Striving%20to%20Accept%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQqnx3rKw64b4M5T5%2Fseq-rerun-striving-to-accept", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQqnx3rKw64b4M5T5%2Fseq-rerun-striving-to-accept", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/1r/striving_to_accept/\">Striving to Accept</a> was originally published on 09 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Striving_to_Accept\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying extra hard to believe something seems like Dark Side Epistemology, but what about trying extra hard to accept something that you know is true.&nbsp;</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h0q/seq_rerun_dont_believe_youll_selfdeceive/\">Don't Believe You'll Self-Deceive</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QQqnx3rKw64b4M5T5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1437317173955037e-06, "legacy": true, "legacyId": "22070", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cxcormwz6jb98gGzW", "6Yj2TuEC2P67M34v4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-20T08:18:08.237Z", "modifiedAt": null, "url": null, "title": "[Link] Immortality Project", "slug": "link-immortality-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Zwqz6uaZMhJ7uqHae", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/emiLTkdSntT2tHnsc/link-immortality-project", "pageUrlRelative": "/posts/emiLTkdSntT2tHnsc/link-immortality-project", "linkUrl": "https://www.lesswrong.com/posts/emiLTkdSntT2tHnsc/link-immortality-project", "postedAtFormatted": "Wednesday, March 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Immortality%20Project&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Immortality%20Project%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FemiLTkdSntT2tHnsc%2Flink-immortality-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Immortality%20Project%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FemiLTkdSntT2tHnsc%2Flink-immortality-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FemiLTkdSntT2tHnsc%2Flink-immortality-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p>An interesting <a title=\"article\" href=\"http://www.latimes.com/health/la-me-adv-immortality-20130313,0,2430727,full.story\">article</a>&nbsp;on the Immortality Project at UC Riverside. <a title=\"This\" href=\"http://www.sptimmortalityproject.com/\">This</a> is the website.</p>\n<p>This seems like something for LWers to look into - they're offering grants and essay prizes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "emiLTkdSntT2tHnsc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -7, "extendedScore": null, "score": 1.1438279947790269e-06, "legacy": true, "legacyId": "22071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-20T19:23:43.240Z", "modifiedAt": null, "url": null, "title": "Solstice and Megameetup Preparations for 2013", "slug": "solstice-and-megameetup-preparations-for-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/efbYB7pHvKA8LT4GK/solstice-and-megameetup-preparations-for-2013", "pageUrlRelative": "/posts/efbYB7pHvKA8LT4GK/solstice-and-megameetup-preparations-for-2013", "linkUrl": "https://www.lesswrong.com/posts/efbYB7pHvKA8LT4GK/solstice-and-megameetup-preparations-for-2013", "postedAtFormatted": "Wednesday, March 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solstice%20and%20Megameetup%20Preparations%20for%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolstice%20and%20Megameetup%20Preparations%20for%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefbYB7pHvKA8LT4GK%2Fsolstice-and-megameetup-preparations-for-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solstice%20and%20Megameetup%20Preparations%20for%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefbYB7pHvKA8LT4GK%2Fsolstice-and-megameetup-preparations-for-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefbYB7pHvKA8LT4GK%2Fsolstice-and-megameetup-preparations-for-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p>I'm officially spinning the Solstice and related ritual stuff into something distinct from Less Wrong (there are good reasons to leave LW focusing on straight-up rationality, and I think it should cater more towards \"serious business intellectuals\" than trying to appeal to the masses, which is essentially my goal).&nbsp;<br /><br />I'll be checking in from time to time to let people know what I'm doing. I just posted an introduction newsletter for Solstice and Megameetup activity for 2013. <a href=\"https://groups.google.com/forum/?fromgroups=&amp;hl=en#!topic/rational-ritual/2VTmGnbFL_I\">You can view it here</a>, and if you want to participate in future discussion, you may want to join the rational-ritual mailing list.&nbsp;</p>\n<p>Some key points:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">The Winter Solstice 2011 had been a bit of an experiment, and went well enough, but left us with a sense of \"all right, now let's do that for real next year.\" I think the 2012 Solstice delivered on that. Our house was filled to the brim with 50 people, and I got a lot of profound thanks from people who described it as very emotionally affective, helping them deal with death and successful at community bonding in a way that few other things had been for them.&nbsp;</span></p>\n</blockquote>\n<blockquote>\n<p><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Now I'm gearing up for this year's work. I have a few main goals for this year:</span><br style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\" /></p>\n<ul style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\n<li style=\"line-height: 17px;\"><span style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; font-family: 'arial black', sans-serif;\">Have Solstices and Megameetups at a number of cities other than New York.</span></li>\n</ul>\n<ul style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\n<li style=\"line-height: 17px;\"><span style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; font-family: 'arial black', sans-serif;\">Have one very&nbsp;<em>large</em>&nbsp;Solstice in NY (looking to seat at least 100 people and trying to seat 800 if I can, in a large auditorium), that caters to the mainstream skeptic/freethinker/humanist crowd. (There will also be a smaller, more intimate and transhumanist Less Wrong Solstice in NYC, but I'm leaning towards it not doubling as a megameetup)</span></li>\n</ul>\n<ul style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\n<li style=\"line-height: 17px;\"><span style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; font-family: 'arial black', sans-serif;\">Create an official website that ties this all together, and makes it easier for people to get involved, share music/art, and find people to collaborate with. I want it to be distinct from Less Wrong &nbsp;so that people who aren't interested in ritual don't feel put out, as well as give non-LW-folk a chance to discover it.&nbsp;</span></li>\n</ul>\n<p>&nbsp;</p>\n<div style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">For the first goal to be successful, we're going to need other other people doing some non-trivial logistical work. A few people had expressed interest in having Solstices or megameetups in their city but weren't sure if they were able to take on that responsibility personally. Some people were interested in making a Solstice happen but hadn't actually personally experienced it and weren't sure they were qualified.<br /><br />These are non-trivial obstacles, but I think they can be addressed.&nbsp;</div>\n</blockquote>\n<div style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">If you're interested in helping out, either in a big or small way, or just want to follow along with our progress, check it out.&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "efbYB7pHvKA8LT4GK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 26, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "22073", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-20T20:12:06.192Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver", "slug": "meetup-vancouver-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nEAwaCmFKAyN5HMRA/meetup-vancouver-2", "pageUrlRelative": "/posts/nEAwaCmFKAyN5HMRA/meetup-vancouver-2", "linkUrl": "https://www.lesswrong.com/posts/nEAwaCmFKAyN5HMRA/meetup-vancouver-2", "postedAtFormatted": "Wednesday, March 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEAwaCmFKAyN5HMRA%2Fmeetup-vancouver-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEAwaCmFKAyN5HMRA%2Fmeetup-vancouver-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEAwaCmFKAyN5HMRA%2Fmeetup-vancouver-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ko'>Vancouver</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 w broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We've had some great discussions recently about ambition, actually feeling things that you know, how to learn, etc. Interestingly, our planned discussions usually go off track into more interesting areas quite quickly, so the best discussions have been unplanned.</p>\n\n<p>Anyways, please join us for rationality discussion and learning at Benny's Bagels on west broadway at 15:00 on Saturday.</p>\n\n<p>Join the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a> if you're new or haven't already.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ko'>Vancouver</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nEAwaCmFKAyN5HMRA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1443038756790506e-06, "legacy": true, "legacyId": "22074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver\">Discussion article for the meetup : <a href=\"/meetups/ko\">Vancouver</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 w broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We've had some great discussions recently about ambition, actually feeling things that you know, how to learn, etc. Interestingly, our planned discussions usually go off track into more interesting areas quite quickly, so the best discussions have been unplanned.</p>\n\n<p>Anyways, please join us for rationality discussion and learning at Benny's Bagels on west broadway at 15:00 on Saturday.</p>\n\n<p>Join the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a> if you're new or haven't already.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver1\">Discussion article for the meetup : <a href=\"/meetups/ko\">Vancouver</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver", "anchor": "Discussion_article_for_the_meetup___Vancouver", "level": 1}, {"title": "Discussion article for the meetup : Vancouver", "anchor": "Discussion_article_for_the_meetup___Vancouver1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-21T04:47:21.520Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Raising the Sanity Waterline", "slug": "seq-rerun-raising-the-sanity-waterline", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N63R3SETvK5vFzuYW/seq-rerun-raising-the-sanity-waterline", "pageUrlRelative": "/posts/N63R3SETvK5vFzuYW/seq-rerun-raising-the-sanity-waterline", "linkUrl": "https://www.lesswrong.com/posts/N63R3SETvK5vFzuYW/seq-rerun-raising-the-sanity-waterline", "postedAtFormatted": "Thursday, March 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Raising%20the%20Sanity%20Waterline&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Raising%20the%20Sanity%20Waterline%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN63R3SETvK5vFzuYW%2Fseq-rerun-raising-the-sanity-waterline%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Raising%20the%20Sanity%20Waterline%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN63R3SETvK5vFzuYW%2Fseq-rerun-raising-the-sanity-waterline", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN63R3SETvK5vFzuYW%2Fseq-rerun-raising-the-sanity-waterline", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 249, "htmlBody": "<p>Today's post, <a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the Sanity Waterline</a> was originally published on 12 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Behind every particular failure of social rationality is a larger and more general failure of social rationality; even if all religious content were deleted tomorrow from all human minds, the larger failures that permit religion would still be present. Religion may serve the function of an asphyxiated canary in a coal mine - getting rid of the canary doesn't get rid of the gas. Even a complete social victory for atheism would only be the beginning of the real work of rationalists. What could you teach people without ever explicitly mentioning religion, that would raise their general epistemic waterline to the point that religion went underwater?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h12/seq_rerun_striving_to_accept/\">Striving to Accept</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N63R3SETvK5vFzuYW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.1446475274744335e-06, "legacy": true, "legacyId": "22075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "QQqnx3rKw64b4M5T5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-21T06:06:09.060Z", "modifiedAt": null, "url": null, "title": "[LINK]Real time mapping of neural activity in a larval zebra fish", "slug": "link-real-time-mapping-of-neural-activity-in-a-larval-zebra", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:25.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pr2cn2J2pu3hXDtMw/link-real-time-mapping-of-neural-activity-in-a-larval-zebra", "pageUrlRelative": "/posts/pr2cn2J2pu3hXDtMw/link-real-time-mapping-of-neural-activity-in-a-larval-zebra", "linkUrl": "https://www.lesswrong.com/posts/pr2cn2J2pu3hXDtMw/link-real-time-mapping-of-neural-activity-in-a-larval-zebra", "postedAtFormatted": "Thursday, March 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5DReal%20time%20mapping%20of%20neural%20activity%20in%20a%20larval%20zebra%20fish&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5DReal%20time%20mapping%20of%20neural%20activity%20in%20a%20larval%20zebra%20fish%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpr2cn2J2pu3hXDtMw%2Flink-real-time-mapping-of-neural-activity-in-a-larval-zebra%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5DReal%20time%20mapping%20of%20neural%20activity%20in%20a%20larval%20zebra%20fish%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpr2cn2J2pu3hXDtMw%2Flink-real-time-mapping-of-neural-activity-in-a-larval-zebra", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpr2cn2J2pu3hXDtMw%2Flink-real-time-mapping-of-neural-activity-in-a-larval-zebra", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>https://plus.google.com/109794669788083578017/posts/gLgSnkCtgrR</p>\n<blockquote>Brain function relies on communication between large populations of neurons across multiple brain areas, a full understanding of which would require knowledge of the time-varying activity of all neurons in the central nervous system. Here we use light-sheet microscopy to record activity, reported through the genetically encoded calcium indicator GCaMP5G, from the entire volume of the brain of the larval zebrafish in vivo at 0.8 Hz, capturing more than 80% of all neurons at single-cell resolution. Demonstrating how this technique can be used to reveal functionally defined circuits across the brain, we identify two populations of neurons with correlated activity patterns. One circuit consists of hindbrain neurons functionally coupled to spinal cord neuropil. The other consists of an anatomically symmetric population in the anterior hindbrain, with activity in the left and right halves oscillating in antiphase, on a timescale of 20 s, and coupled to equally slow oscillations in the inferior olive.</blockquote>\n<p>Page down at the link to see the animation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pr2cn2J2pu3hXDtMw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 1.1447000944288824e-06, "legacy": true, "legacyId": "22076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-21T12:17:49.160Z", "modifiedAt": null, "url": null, "title": "Call for Volunteers at Global Future 2045", "slug": "call-for-volunteers-at-global-future-2045", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xvaZooorpZ3Zuot3N/call-for-volunteers-at-global-future-2045", "pageUrlRelative": "/posts/xvaZooorpZ3Zuot3N/call-for-volunteers-at-global-future-2045", "linkUrl": "https://www.lesswrong.com/posts/xvaZooorpZ3Zuot3N/call-for-volunteers-at-global-future-2045", "postedAtFormatted": "Thursday, March 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20Volunteers%20at%20Global%20Future%202045&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20Volunteers%20at%20Global%20Future%202045%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvaZooorpZ3Zuot3N%2Fcall-for-volunteers-at-global-future-2045%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20Volunteers%20at%20Global%20Future%202045%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvaZooorpZ3Zuot3N%2Fcall-for-volunteers-at-global-future-2045", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvaZooorpZ3Zuot3N%2Fcall-for-volunteers-at-global-future-2045", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>This post is on behalf of Amy Willey, former MIRI employee who is Volunteer Coordinator for <a name=\"http://gf2045.com/\"></a><a href=\"http://gf2045.com/\">GF 2045</a>.</p>\n<p>One of the most exciting futurist events this year will be <a href=\"http://gf2045.com/\">Global Future 2045</a>, in New York City on June 15-16.&nbsp;</p>\n<p>They need volunteers! You can apply to be a volunteer <a href=\"https://docs.google.com/forms/d/1rZhX217kwhqmN6HjJ7n8YSEiYpu1ar8RKPNocYw3gD8/viewform\">here</a>.&nbsp;</p>\n<p>Amy Willey, the volunteer coordinator, previously organized Singularity Summit. She is fun to work with! Volunteering at Global Future 2045 is an opportunity to meet many interesting scientists and futurists.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xvaZooorpZ3Zuot3N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 1.144948113715033e-06, "legacy": true, "legacyId": "22077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-21T15:33:12.274Z", "modifiedAt": null, "url": null, "title": "Another community about existential risk - Arctic news", "slug": "another-community-about-existential-risk-arctic-news", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:15.072Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jsuMLNgcmX4xSpXte/another-community-about-existential-risk-arctic-news", "pageUrlRelative": "/posts/jsuMLNgcmX4xSpXte/another-community-about-existential-risk-arctic-news", "linkUrl": "https://www.lesswrong.com/posts/jsuMLNgcmX4xSpXte/another-community-about-existential-risk-arctic-news", "postedAtFormatted": "Thursday, March 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20community%20about%20existential%20risk%20-%20Arctic%20news&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20community%20about%20existential%20risk%20-%20Arctic%20news%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsuMLNgcmX4xSpXte%2Fanother-community-about-existential-risk-arctic-news%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20community%20about%20existential%20risk%20-%20Arctic%20news%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsuMLNgcmX4xSpXte%2Fanother-community-about-existential-risk-arctic-news", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsuMLNgcmX4xSpXte%2Fanother-community-about-existential-risk-arctic-news", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>LW is a community of people who mostly share the idea that AI is the main existential risk. But it is not the only one community in the web which has large ammount of evidence about particular existential risk. And the main problem for my point of view is that both communities (but in fact there are many) do not aware about each other existence. This is a real bias.&nbsp;</p>\n<p>Here I would like to present another group which I recently found in the net. I cant judge their arguments but find them interesting.</p>\n<p>This is <a title=\"Arctic News\" href=\"http://arctic-news.blogspot.ru/\">http://arctic-news.blogspot.ru</a>/</p>\n<p>Their main idea is that Arctic is melting very quickly which could lead to runaway global warming which could start as early as 2015. They explain a lot about positive feedbacks with methane hydrates, water wapors and provide many real time information, maps, satelite images to confirm their point of view.&nbsp;</p>\n<p>My idea is not to start discuss arctic ice on LW or AI in Arctic news, but to point on existening of such separated communties which (because of confirantion bias) concentrated deeply of their own agenda. The other such communties are flutrackers, zerohedge, theoildrum and more.&nbsp;</p>\n<p>And one probable property of confirmation bias is that a person not only overvalue his own pile of evidences but dismisses value of any other piles of evidences. And that is why such X-risks orientated communities exist in isolation from each other.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jsuMLNgcmX4xSpXte", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -1, "extendedScore": null, "score": 1.1450785350388386e-06, "legacy": true, "legacyId": "22078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-21T16:51:00.007Z", "modifiedAt": null, "url": null, "title": "[LINK] Transcendence (2014) -- A movie about \"technological singularity\"", "slug": "link-transcendence-2014-a-movie-about-technological", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:00.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vwHBQKdYewXAMceWf/link-transcendence-2014-a-movie-about-technological", "pageUrlRelative": "/posts/vwHBQKdYewXAMceWf/link-transcendence-2014-a-movie-about-technological", "linkUrl": "https://www.lesswrong.com/posts/vwHBQKdYewXAMceWf/link-transcendence-2014-a-movie-about-technological", "postedAtFormatted": "Thursday, March 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Transcendence%20(2014)%20--%20A%20movie%20about%20%22technological%20singularity%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Transcendence%20(2014)%20--%20A%20movie%20about%20%22technological%20singularity%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwHBQKdYewXAMceWf%2Flink-transcendence-2014-a-movie-about-technological%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Transcendence%20(2014)%20--%20A%20movie%20about%20%22technological%20singularity%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwHBQKdYewXAMceWf%2Flink-transcendence-2014-a-movie-about-technological", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwHBQKdYewXAMceWf%2Flink-transcendence-2014-a-movie-about-technological", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p><a href=\"http://www.imdb.com/title/tt2209764/\">Transcendence</a>&nbsp;is an upcoming movie, starring Johnny Depp:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: Verdana, Arial, sans-serif; font-size: 13px; line-height: 18px;\">Two leading computer scientists work toward their goal of Technological Singularity, as a radical anti-technology organization fights to prevent them from creating a world where computers can transcend the abilities of the human brain.</span></p>\n<p>It could be fun to speculate what the spin will be and how many standard pitfalls they will hit. I also wonder which side I would be rooting for. From the scarce descriptions available, it does not seem to be based on the idea of recursive self-improvement, but rather on mind uploading.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vwHBQKdYewXAMceWf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 1.1451304715362243e-06, "legacy": true, "legacyId": "22079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-22T02:59:56.723Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels Biased Boardgaming", "slug": "meetup-brussels-biased-boardgaming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.206Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a5oRuYt7qkdS99KhD/meetup-brussels-biased-boardgaming", "pageUrlRelative": "/posts/a5oRuYt7qkdS99KhD/meetup-brussels-biased-boardgaming", "linkUrl": "https://www.lesswrong.com/posts/a5oRuYt7qkdS99KhD/meetup-brussels-biased-boardgaming", "postedAtFormatted": "Friday, March 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20Biased%20Boardgaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20Biased%20Boardgaming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5oRuYt7qkdS99KhD%2Fmeetup-brussels-biased-boardgaming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20Biased%20Boardgaming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5oRuYt7qkdS99KhD%2Fmeetup-brussels-biased-boardgaming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5oRuYt7qkdS99KhD%2Fmeetup-brussels-biased-boardgaming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kp'>Brussels Biased Boardgaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 March 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Les Chroniques Ludiques, Rue Vanderkindere 236, 1180 Uccle, Brussels, Belgium</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For this special meetup, we will meet at the game shop Les Chroniques Ludiques to do some Biased Boardgaming, most likely Pandemics. You'll recognize us by our LessWrong sign. The meeting will be in English to accomodate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kp'>Brussels Biased Boardgaming</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a5oRuYt7qkdS99KhD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1455371490410464e-06, "legacy": true, "legacyId": "22080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_Biased_Boardgaming\">Discussion article for the meetup : <a href=\"/meetups/kp\">Brussels Biased Boardgaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 March 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Les Chroniques Ludiques, Rue Vanderkindere 236, 1180 Uccle, Brussels, Belgium</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For this special meetup, we will meet at the game shop Les Chroniques Ludiques to do some Biased Boardgaming, most likely Pandemics. You'll recognize us by our LessWrong sign. The meeting will be in English to accomodate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_Biased_Boardgaming1\">Discussion article for the meetup : <a href=\"/meetups/kp\">Brussels Biased Boardgaming</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels Biased Boardgaming", "anchor": "Discussion_article_for_the_meetup___Brussels_Biased_Boardgaming", "level": 1}, {"title": "Discussion article for the meetup : Brussels Biased Boardgaming", "anchor": "Discussion_article_for_the_meetup___Brussels_Biased_Boardgaming1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-22T04:35:37.259Z", "modifiedAt": null, "url": null, "title": "Stanford talk: easy step to universal colonisation", "slug": "stanford-talk-easy-step-to-universal-colonisation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:23.395Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zHJJmfGFeTtmTA7yo/stanford-talk-easy-step-to-universal-colonisation", "pageUrlRelative": "/posts/zHJJmfGFeTtmTA7yo/stanford-talk-easy-step-to-universal-colonisation", "linkUrl": "https://www.lesswrong.com/posts/zHJJmfGFeTtmTA7yo/stanford-talk-easy-step-to-universal-colonisation", "postedAtFormatted": "Friday, March 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stanford%20talk%3A%20easy%20step%20to%20universal%20colonisation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStanford%20talk%3A%20easy%20step%20to%20universal%20colonisation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzHJJmfGFeTtmTA7yo%2Fstanford-talk-easy-step-to-universal-colonisation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stanford%20talk%3A%20easy%20step%20to%20universal%20colonisation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzHJJmfGFeTtmTA7yo%2Fstanford-talk-easy-step-to-universal-colonisation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzHJJmfGFeTtmTA7yo%2Fstanford-talk-easy-step-to-universal-colonisation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0.0001pt;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">I'll be speaking in Stanford this weekend, at the Advancing Humanity Symposium, looking into space exploration and colonisation - how easy it is to cross the voids between the stars and the galaxies and expand across the whole reachable universe.<br /><br /><a href=\"https://www.facebook.com/events/338219042964586/?fref=ts\" target=\"_blank\">https://www.facebook.com/events/338219042964586/?fref=ts</a></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0.0001pt;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">The talk will cover research described in &ldquo;Eternity in six hours: intergalactic spreading of intelligent life and sharpening the Fermi paradox\", with Dr. Anders Sandberg, forthcoming in the journal Acta Astronautica. For an earlier exploration of related ideas, see this popular talk from last year on the Dyson spheres, von Neumann probes and the Fermi paradox:</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0.0001pt;\"><span style=\"color: #000000;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"http://www.youtube.com/watch?v=zQTfuI-9jIo\">http://www.youtube.com/watch?v=zQTfuI-9jIo</a></span></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0.0001pt;\">&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2JdCpTrNgBMNpJiyB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zHJJmfGFeTtmTA7yo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.1456010681768098e-06, "legacy": true, "legacyId": "22081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-22T05:01:52.957Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] A Sense That More Is Possible", "slug": "seq-rerun-a-sense-that-more-is-possible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AnRxkDqCpWRteNt7u/seq-rerun-a-sense-that-more-is-possible", "pageUrlRelative": "/posts/AnRxkDqCpWRteNt7u/seq-rerun-a-sense-that-more-is-possible", "linkUrl": "https://www.lesswrong.com/posts/AnRxkDqCpWRteNt7u/seq-rerun-a-sense-that-more-is-possible", "postedAtFormatted": "Friday, March 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20A%20Sense%20That%20More%20Is%20Possible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20A%20Sense%20That%20More%20Is%20Possible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnRxkDqCpWRteNt7u%2Fseq-rerun-a-sense-that-more-is-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20A%20Sense%20That%20More%20Is%20Possible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnRxkDqCpWRteNt7u%2Fseq-rerun-a-sense-that-more-is-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnRxkDqCpWRteNt7u%2Fseq-rerun-a-sense-that-more-is-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>Today's post, <a href=\"/lw/2c/a_sense_that_more_is_possible/\">A Sense That More Is Possible</a> was originally published on 13 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#A_Sense_That_More_Is_Possible\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The art of human rationality may have not been much developed because its practitioners lack a sense that vastly more is possible. The level of expertise that most rationalists strive to develop is not on a par with the skills of a professional mathematician - more like that of a strong casual amateur. Self-proclaimed \"rationalists\" don't seem to get huge amounts of personal mileage out of their craft, and no one sees a problem with this. Yet rationalists get less systematic training in a less systematic context than a first-dan black belt gets in hitting people.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h17/seq_rerun_raising_the_sanity_waterline/\">Raising the Sanity Waterline</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AnRxkDqCpWRteNt7u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1456186141718556e-06, "legacy": true, "legacyId": "22082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Nu3wa6npK4Ry66vFp", "N63R3SETvK5vFzuYW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-22T17:24:27.265Z", "modifiedAt": null, "url": null, "title": "Personal Evidence - Superstitions as Rational Beliefs", "slug": "personal-evidence-superstitions-as-rational-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:39.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EgkM9cjunMbYQv3h6/personal-evidence-superstitions-as-rational-beliefs", "pageUrlRelative": "/posts/EgkM9cjunMbYQv3h6/personal-evidence-superstitions-as-rational-beliefs", "linkUrl": "https://www.lesswrong.com/posts/EgkM9cjunMbYQv3h6/personal-evidence-superstitions-as-rational-beliefs", "postedAtFormatted": "Friday, March 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personal%20Evidence%20-%20Superstitions%20as%20Rational%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonal%20Evidence%20-%20Superstitions%20as%20Rational%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgkM9cjunMbYQv3h6%2Fpersonal-evidence-superstitions-as-rational-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personal%20Evidence%20-%20Superstitions%20as%20Rational%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgkM9cjunMbYQv3h6%2Fpersonal-evidence-superstitions-as-rational-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgkM9cjunMbYQv3h6%2Fpersonal-evidence-superstitions-as-rational-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1303, "htmlBody": "<p>I'll start with a confession:</p>\n<p>The evidence I have personally seen suggests haunted houses are, in fact, real, without given any particular credence to any particular explanation of what the haunting is. &nbsp;In particular, I own a house in which bizarre crap has happened since I first moved into it. &nbsp;Persistently. &nbsp;I've moved into another house, and have been making repairs in preparation to sell it; most recently, in a room with almost no furniture, in a space with absolutely no furniture, a key was dropped by myself. &nbsp;Four people searched the area for significant periods of time on three different occasions with no luck. &nbsp;I found it on the floor a week or two ago on top of something that wasn't there when it fell. &nbsp;Which is the straw that broke the camel's back in terms of my skepticism.</p>\n<p>Other bizarre things that have happened include such things as my waking up to discover my recently-purchased bottle of key lime juice had been placed in the oven, and the oven turned on; the plastic bottle had just started to melt when I made the discovery. &nbsp;Another situation involved my sister, who one morning (while home alone) walked into the living room and discovered on a previously empty floor three sonograms of the previous occupant's baby. &nbsp;(There were -many- other things; I'm choosing for the purposes of this post the most unusual and least prone-to-outside-explanation occurrences. &nbsp;Night terrors, for example, are easily explained.)</p>\n<p>Up until the last incident, the key, I was inclined to attribute the events to, say, sleepwalking and confirmation bias. &nbsp;At this point, I do not think the evidence really supports that conclusion anymore. &nbsp;My skepticism has been broken by personal experience; I'm not going to attribute anything to any -particular- explanation, but there is definitely something -not normal- about that house, whatever it may be; it has been the (nearly) sole repository of such experiences in my life. &nbsp;(The only other such experience was the day my grandfather (with whom I was extremely close) died, and given the mental turmoil I was experiencing, I'm disinclined to give that particular experience too much credit. &nbsp;For the curious, I was taking a shower, and the hot water repeatedly (3 times) turned off. &nbsp;As in, the knob was completely rotated to shut off the flow of hot water to the faucet.)</p>\n<p>A key point of rationality is that evidence can in fact change your mind. &nbsp;Well, the evidence has changed my mind.</p>\n<p>From a reader's perspective, this is all anecdotal evidence. &nbsp;So I don't expect to change anybody -else's- mind - indeed, you're probably making a mistake if you -do- change your mind, because out of millions of people, you -should- expect to see a few weird things being related by other people. &nbsp;The odds of somebody else relating an entirely factual series of anecdotes that suggest something unlikely are probably significantly higher than the odds of that unlikely thing being true. &nbsp;However, the odds of such things happening to you personally are considerably -lower- than the odds of hearing about the events from somebody else. &nbsp;Which all leads into a central conclusion: It's possible for the evidence to support one person believing something, while at the same time -not- supporting that anybody else believe that thing. &nbsp;If you win the lottery, that may be evidence for you believing you're living in a simulation or that some other mechanism \"forced\" the outcome - while at the same time the evidence doesn't suggest anything for somebody -else- winning the lottery.</p>\n<p>I have a different purpose in mind: Making the claim that objectively irrational beliefs can, in fact, be subjectively rational. &nbsp;Prior to these experiences, I regarded the idea of a haunted house - I use the idea without prejudice for what \"haunted\" is or refers to - was that it was just superstitious people scaring themselves. &nbsp;At this point I'm forced by the evidence I've seen to conclude that there's something to the idea, even if it's not what people think it is. &nbsp;Maybe EMFs subtly messing with my brain (there is some weak evidence for the idea that electromagnetic fluctuations can induce metabolic changes in neurons - see http://jama.jamanetwork.com/article.aspx?articleid=645813 ), maybe something else.</p>\n<p>If a pattern-recognition algorithm doesn't produce false positives, it's probably getting false negatives, and given that we can test false positives but do not know to test false negatives, pattern-recognition should favor false positives over false negatives. &nbsp;What does this have to do with anything? &nbsp;Well, it means superstitions aren't a product of a poor mind, only -untested- superstitions are. &nbsp;A good intelligence should develop superstitions. &nbsp;It should, when capable, discard them.</p>\n<p>But it should only discard such superstitions as it has evidence to do so.</p>\n<p>Now, the skeptical reader might ask what odds I place on each of these events occurring. &nbsp;My answer is as follows: Each event was highly unlikely in itself, explainable as an independent event only by positing pretty unlikely circumstances (what odds would I place on me or my housemate sleepwalking multiple times when neither of us have any history of such behavior, and such behavior has entirely ceased since leaving that house? &nbsp;Keep in mind that neither I nor my sister were initially inclined to regard such events as even needing explanation; it's only been until the most recent episode that I've decided the evidence suggests anything at all, so the possible explanation that the sleepwalking was a product of disturbance at the first few unusual events seems unlikely). &nbsp;Further evidence has rendered each event less likely as an independent phenomenon - since moving to a different house, the occurrences have ceased. &nbsp;When returning to the house, occurrences resume within its context. &nbsp;My control, while hardly blind, is controlling. &nbsp;But meaningfully, the same evidence doesn't mean the same thing if it is coming from somebody else; out of millions of people, I would expect such things to occur. &nbsp;I simply cannot expect them to occur -to me-. &nbsp;(And I wasn't the only one who found the house to be... off. &nbsp;There's a sense of not-quite-rightness to one basement room which I cannot explain without resorting to Lovecraftian cliches about alien geometries. &nbsp;The house was burgled several times; the only room that was left completely untouched, even when the copper piping was stolen (and subsequently the water meter - I got a waterfall in my basement!), was that room, which is conveniently where I left a thousand or so dollars worth of building materials for a project I hadn't finished yet.)</p>\n<p>Evidence is personal. &nbsp;The odds of something happening are not equal to the odds of that something happening to you. &nbsp;Therefore, while we should not be surprised if miracles (that is, really unlikely and contextually significant events) occur, it is still legitimate to be surprised when they occur to us individually. &nbsp;The qualitative rationality of an individual belief is not equal to the qualitative rationality of the same belief on a social scale; individuals get different evidence than society, even when the same evidence is apparently present both for the individual and the community.</p>\n<p>And just as it is a mistake for people to judge the beliefs of others based on the community standard of evidence, rather than the individual standard of evidence, it is likewise a mistake for an individual to judge society based on the individual standard of evidence, rather than the community. &nbsp;Just as it is possible for the individual to rationally believe something that society should not rationally believe as a whole, it is possible for society to rationally reject something the individual has overwhelming personal evidence for.</p>\n<p>Aumann was, in short, wrong, because Aumann Updating is based on the belief that two individuals -can- share evidence. &nbsp;Evidence is incompletely transferable.</p>\n<p>(Note: Anthropic reasoning can potentially remedy this at least to some extent for -past- experiences; reproducible and continuing experiences somewhat less.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EgkM9cjunMbYQv3h6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 1, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 138, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-23T03:23:39.099Z", "modifiedAt": null, "url": null, "title": "Suggestion: Read Paul Graham", "slug": "suggestion-read-paul-graham", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:42.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/siHLPwCKXpByjp69x/suggestion-read-paul-graham", "pageUrlRelative": "/posts/siHLPwCKXpByjp69x/suggestion-read-paul-graham", "linkUrl": "https://www.lesswrong.com/posts/siHLPwCKXpByjp69x/suggestion-read-paul-graham", "postedAtFormatted": "Saturday, March 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestion%3A%20Read%20Paul%20Graham&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestion%3A%20Read%20Paul%20Graham%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiHLPwCKXpByjp69x%2Fsuggestion-read-paul-graham%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestion%3A%20Read%20Paul%20Graham%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiHLPwCKXpByjp69x%2Fsuggestion-read-paul-graham", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiHLPwCKXpByjp69x%2Fsuggestion-read-paul-graham", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>This isn't really a full post, but merely a note of potential interest. Paul Graham (who runs <a href=\"https://news.ycombinator.com/\">Hacker News</a>) has several very interesting and thought-provoking essays located on <a href=\"http://www.paulgraham.com/articles.html\">his personal website</a>. To me they fit very well with the style of thinking employed and advocated by many people on LW and I'd advise that nearly anyone interested in LW check out his work.</p>\n<p>I especially recommend <a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a>, <a href=\"http://www.paulgraham.com/say.html\">What You Can't Say</a>, and <a href=\"http://www.paulgraham.com/hs.html\">What You'll Wish You'd Known</a>, but nearly every essay up there is interesting to me in some way. Many of them are directly relevant to issues of rationality, while others are only indirectly related, but either way I found them worth my time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "8sh6iLwYWDJ7z3fPo": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "siHLPwCKXpByjp69x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 52, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "22090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-23T18:00:24.830Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Beliefs", "slug": "meetup-moscow-beliefs", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BsP6MBwseXCdNJ5WF/meetup-moscow-beliefs", "pageUrlRelative": "/posts/BsP6MBwseXCdNJ5WF/meetup-moscow-beliefs", "linkUrl": "https://www.lesswrong.com/posts/BsP6MBwseXCdNJ5WF/meetup-moscow-beliefs", "postedAtFormatted": "Saturday, March 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsP6MBwseXCdNJ5WF%2Fmeetup-moscow-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsP6MBwseXCdNJ5WF%2Fmeetup-moscow-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsP6MBwseXCdNJ5WF%2Fmeetup-moscow-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kq'>Moscow, Beliefs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Short presentations. Two or three people will tell us about something interesting.</p></li>\n<li><p>Practical rationality. We will train useful skills.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kq'>Moscow, Beliefs</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BsP6MBwseXCdNJ5WF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1471025865790049e-06, "legacy": true, "legacyId": "22092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs\">Discussion article for the meetup : <a href=\"/meetups/kq\">Moscow, Beliefs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Short presentations. Two or three people will tell us about something interesting.</p></li>\n<li><p>Practical rationality. We will train useful skills.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs1\">Discussion article for the meetup : <a href=\"/meetups/kq\">Moscow, Beliefs</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Beliefs", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Beliefs", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-23T22:24:02.822Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:25.884Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xHRoJYvXw76AHDMJN/meetup-washington-dc-fun-and-games-meetup-1", "pageUrlRelative": "/posts/xHRoJYvXw76AHDMJN/meetup-washington-dc-fun-and-games-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/xHRoJYvXw76AHDMJN/meetup-washington-dc-fun-and-games-meetup-1", "postedAtFormatted": "Saturday, March 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHRoJYvXw76AHDMJN%2Fmeetup-washington-dc-fun-and-games-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHRoJYvXw76AHDMJN%2Fmeetup-washington-dc-fun-and-games-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHRoJYvXw76AHDMJN%2Fmeetup-washington-dc-fun-and-games-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kr'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 March 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games. Sorry for the late posting</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kr'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xHRoJYvXw76AHDMJN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1472791565487899e-06, "legacy": true, "legacyId": "22093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/kr\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 March 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games. Sorry for the late posting</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/kr\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-23T23:12:26.253Z", "modifiedAt": null, "url": null, "title": "How to Not Get Offended", "slug": "how-to-not-get-offended", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zQfjaxhxuXToyaetp/how-to-not-get-offended", "pageUrlRelative": "/posts/zQfjaxhxuXToyaetp/how-to-not-get-offended", "linkUrl": "https://www.lesswrong.com/posts/zQfjaxhxuXToyaetp/how-to-not-get-offended", "postedAtFormatted": "Saturday, March 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Not%20Get%20Offended&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Not%20Get%20Offended%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQfjaxhxuXToyaetp%2Fhow-to-not-get-offended%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Not%20Get%20Offended%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQfjaxhxuXToyaetp%2Fhow-to-not-get-offended", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQfjaxhxuXToyaetp%2Fhow-to-not-get-offended", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2111, "htmlBody": "<p><strong>Followup to:&nbsp;</strong><span class=\"s2\"><a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">Don't Get Offended</a></span></p>\n<p><strong>Draws heavily on: </strong><a href=\"http://plato.stanford.edu/entries/stoicism/\">Stoicism</a>, <a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a>,<strong>&nbsp;</strong><a href=\"http://www.lesswrong.com/lw/1xh/living_luminously/\">Living Luminously</a></p>\n<p><a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">Previously</a>, we discussed why not getting offended might be an effective strategy to adopt in order to increase one's practical&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Epistemic_rationality#Epistemic_rationality\">epistemic rationality</a>. That's all well and good, but just as <a href=\"http://wiki.lesswrong.com/wiki/Debiasing\">knowing about biases isn't the same as protecting ourselves from them</a>, the simple desire to avoid being offended is (usually) insufficient to actually avoid it-- practice, too, is required.</p>\n<p class=\"p4\">So what should you actually practice if you find yourself becoming offended and want to stop?&nbsp;This post aims to address that. In doing so, it also features an expanded discussion of one question that seemed to be a sticking point for several posters in the previous discussion-- if you aren't getting offended, how will you discourage offensive and inappropriate behaviors?</p>\n<p class=\"p4\"><strong>Preparation</strong></p>\n<p class=\"p4\">First, you need to&nbsp;<a href=\"http://en.wikipedia.org/wiki/Alief_(belief)\"><span class=\"s3\">really truly recognize</span></a>&nbsp;that experiencing the feeling of being offended is an undesirable process. <a href=\"/lw/le/lost_purposes/\">You must see <em>why</em></a>&nbsp;experiencing offense <a href=\"http://www.paulgraham.com/identity.html\">runs counter</a> to <a href=\"http://yudkowsky.net/rational/the-simple-truth\">knowing the truth</a>.&nbsp;</p>\n<p class=\"p4\">A good litmus test is to check whether experiencing the feeling of being offended seems obviously bad to you--&nbsp;<em>not the existence of the feeling itself or any behaviors tied to it,&nbsp;</em>but the fact that you are experiencing it. It is important to understand that this refers only to the mental experience of being offended-- this post focuses entirely on the A (<a href=\"http://en.wikipedia.org/wiki/Affect_(philosophy)\">Affect</a>) component of Alicorn's&nbsp;<a href=\"/lw/1y0/the_abcs_of_luminosity/\">ABC model</a>.&nbsp;</p>\n<p class=\"p4\">While it might sound silly to have the preliminary step be simply thinking that being offended is bad, if you don't think that there's not much point in practicing the remaining steps. In fact, if you don't think that, practicing the remaining steps may be harmful.</p>\n<p class=\"p4\"><strong>Part One: Detection</strong></p>\n<p class=\"p4\">In order to stop being offended-- or really alter nearly anything about your mental state-- the first step is to increase your awareness of when you are becoming offended and what that process looks like in as early a stage as possible. As in the case of&nbsp;<a href=\"/lw/21b/ugh_fields/\"><span class=\"s3\">ugh fields</span></a>, being mindful of your reactions and \"watching for the flinch\" is an important early step.</p>\n<p class=\"p4\">As soon as you feel yourself becoming offended, you should notice this. It is then critical to truly&nbsp;<span class=\"s3\"><a href=\"http://www.lesswrong.com/lw/1xh/living_luminously/\">inspect your reactions</a>&nbsp;and d</span>etermine&nbsp;<em>why</em>&nbsp;you are becoming offended. This doesn't mean thinking things like \"I was offended because she insulted my friend,\" which has insufficient detail. Try for something more like \"I was offended because she made a severe criticism of another person in the group and I feel that she did not have the relevant social capital to justify making her statement.\" If you don't have a detailed conception of exactly what it is that is offending you, moving forward will be difficult.</p>\n<p class=\"p4\">At times you will not be able to do this thanks to the heat of the moment. That's okay and in point of fact it is expected-- truly understanding one's own motivations and responses can be difficult even in unemotional situations. If necessary, wait for calmer times to evaluate such issues or ask others for clarifications or predictions. While the inputs of others might not always be useful, close friends (or unusually perceptive unclose friends) can in many cases pinpoint causes to your behavior that you might be blind to.</p>\n<p class=\"p4\">If emotionally possible, testing these models is certainly helpful, though I recognize that this can be challenging at times and do not recommend it to the unprepared. In particular, having your friends try to offend you to test your reactions is often a poor idea, as the emotional responses involved can be unpleasant for multiple parties.</p>\n<p class=\"p3\"><strong>Part Two: Dissolution</strong></p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\">Once you have the ability to detect when and why you are becoming offended, there are multiple steps that one can take. The two techniques that have been most successful for me in the moment are what I like to call <strong>Dissolution</strong> and <strong>Defense</strong>.<sup>[1]</sup></p>\n<p class=\"p3\">The first of those two methods, <strong>Dissolution</strong>, is what I tend to use under normal circumstances. This method attempts to dissolve feelings of offense by simply understanding them really well and then applying the <a href=\"/lw/id/you_can_face_reality/\">Principle of Gendlin</a>. For instance, if someone has said an insulting remark to me, I might think to myself \"If this criticism is false, then it can easily be defeated by the truth. If this criticism is true, well, <a href=\"http://www.goodreads.com/quotes/541138-that-which-can-be-destroyed-by-the-truth-should-be\">you know what P.C. Hodgell says about that...</a>&nbsp;perhaps this criticism was not made in the most optimal manner, but I have no need to be offended, for the criticism will succeed or fail on the basis of the truth, not on the basis of whether it is appropriate.\"</p>\n<p class=\"p3\">For me, Gendlin is a true friend and can resolve most of these issues fairly trivially. However, this does not work the same for all individuals. Other techniques, such as perspective shifting, may be more reliable for others. The important strand that I have found throughout many people who can avoid being offended is the concept that being offended is a matter of one's own reaction, not the external world. I irreverently refer to this as the <a href=\"http://www.enotes.com/shakespeare-quotes/nothing-either-good-bad-but-thinking-makes\">Principle of Hamlet</a>-- there is nothing either good or bad, but thinking makes it so. It is a key tenet of Stoic thought.</p>\n<p class=\"p3\">Note that there are a few other things to consider. For instance, one should <a href=\"/lw/1mu/sorting_out_sticky_brains/\">beware sticky brains</a>&nbsp;when executing this technique. Personally, my brain isn't very sticky, but if yours is you may have to plan around that. There are many considerations similar to this one regarding personal mental styles, and the topic of \"Things You Should Know About Your Brain\" probably merits a post of its own, but I don't really have space to go into it here.<sup>[2] </sup>Suffice it to say that nothing presented here is set in stone and you should make whatever modifications are appropriate in order to fit this into your own mental style.</p>\n<p class=\"p3\">With practice, I have found that the sentiment expressed in&nbsp;<a href=\"/lw/1za/the_spotlight/1t8q\">this comment</a>&nbsp;can apply to reactions as well as to personal traits-- reactions that I don't like having tend to go away soon after I understand them, since I can then apply these methods to their dissolution.</p>\n<p class=\"p3\"><strong>Part Three: Defense</strong></p>\n<p class=\"p3\">However, I've found that there are some times in which I am unable to successfully dissolve my feelings of offense. It may be that I am extremely hungry or tired or otherwise impaired and thus have less than normal ability to control my reactions, or that I am simply too shocked to react normally. In this situation, I resort to the secondary method, <strong>Defense. </strong>This is not glamorous and not cool but it does work. The key to defense is isolating yourself from stimuli that produce undesirable results.</p>\n<p class=\"p3\">There are multiple forms of this-- the most basic one is simply leaving the area. Other simple methods include drowning something out (simple technique: the classic \"I'M NOT LISTENING LA LA LA LA LA,\" except inside your head), suddenly becoming very (authentically) interested in something else, pretending you have to take a call, etc. One&nbsp;<em>extremely important</em>&nbsp;note is that these methods should be a last resort. Otherwise, it has the potential of becoming&nbsp;<a href=\"/lw/hu/the_third_alternative/\">an excuse</a>. I seriously considered not putting them in the post at all because of the risk of it making people not take the first method seriously enough. Ultimately, I decided that it would be better for most people to know than to not know-- but&nbsp;<em>seriously, be careful with this.</em></p>\n<p class=\"p3\">If you do find yourself having to resort to these methods more often than you would like, there is another option-- active defense. I generally&nbsp;<a href=\"http://www.jwz.org/blog/2012/12/superheroes-and-the-protagonist-problem/\">prefer action to reaction</a>, so I tend to prefer active defenses to reactive ones. Active defenses&nbsp;involve self-modification so that certain stimuli&nbsp;<a href=\"/lw/2a5/on_enjoying_disagreeable_company/\">no longer produce undesirable results</a>&nbsp;or produce less undesirable results.</p>\n<p class=\"p3\">For instance, if I know that I am going to encounter someone who may make offensive remarks regarding another of my friends, I may steel myself for this prior to the encounter, saying to myself \"While it may be that some of my friends dislike each other and want to express this to me, I should not fall into the trap of becoming offended and getting into an argument over whether or not one's criticism of the other is valid. <a href=\"http://www.plausiblydeniable.com/opinion/gsf.html\">All my friends don't have to be friends with one another</a>, and trying to enforce this will only add to the trouble. Instead I will make a mild remark and move on.\"</p>\n<p class=\"p3\">This is an especially effective method when it comes to preventing surprised or shocked offended reactions, though of course one must always beware unknown unknowns. Marcus Aurelius engaged in <a href=\"http://www.goodreads.com/quotes/290880-begin-each-day-by-telling-yourself-today-i-shall-be\">an extremely general form of this</a>, advocating that one begin each day by preparing oneself to meet with all sorts of offenses while avoiding anger or irritation. In some respects, the overall practice of <a href=\"http://plato.stanford.edu/entries/stoicism/\">Stoicism</a> could be considered an advanced form of active defense-- though not just against becoming offended, but against all wild or uncontrolled reactions.</p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\"><strong>Part Four: Discouragement</strong></p>\n<p class=\"p6\">This step is where you evaluate whether taking action to prevent further offensive behavior is merited or useful. As stated <a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">earlier</a>, I believe that even in cases when it is instrumentally useful to show offense, one can still perform actions indicating offense without actually experiencing the internal state of being offended. The question then becomes when it is appropriate to do so.</p>\n<p class=\"p6\">In previous discussion, Oligopsony <a href=\"/lw/gux/dont_get_offended/8kl3\">pointed out</a>&nbsp;that taking offense at inappropriate behaviors can be considered a public good. I disagree to an extent, because I think that in many situations claiming to be offended or acting offended can in fact escalate a situation that would otherwise pass with a small amount of awkwardness and concern. However, simply allowing (truly) inappropriate behavior to continue without objection tacitly indicates that that behavior is acceptable and thus carries negative consequences of its own.</p>\n<p class=\"p6\">Overall, I find that generally speaking it is often wise to complain about offensive behaviors if you think it is likely that those behaviors will offend others. You should be wary about <a href=\"/lw/dr/generalizing_from_one_example/\">generalizing from one example</a>, though. I find the sound of silverware contacting teeth to be both off-putting and offensive, but this is not something that I bother to point out with people that I don't expect to interact with often, since I am moderately confident that it is a pet peeve that most people don't care about and aren't offended by. On the other hand, I do bother to point out that fact to people that I expect to interact with frequently if I notice them doing it, since in this case it is worth my time to potentially avert a future instance.</p>\n<p class=\"p6\">A friend of mine who is currently commissioned as a military officer says that one key principle of effective leadership is \"praise in public, punish in private--\" in other words, save criticisms for private encounters so that you don't have to worry about potential status implications of making the criticism around others. In my experience, this is also an effective way to deal with offensive behavior while minimizing social awkwardness and the potential for escalation.</p>\n<p class=\"p6\">In some situations, though, it is simply necessary to stand up when no one else is willing and confront offensive behavior directly. I have done so several times and will say that while it is usually uncomfortable for all those concerned, the result can be worth it. That being said, I urge you to use extreme caution when evaluating whether or not it is necessary to do so. My impression is that many situations that people deem worthy of confrontation could be resolved more effectively through less direct means.</p>\n<p class=\"p3\"><strong>Part Five: CONSTANT VIGILANCE</strong></p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\">As a final thought, I've seen a lot of people, thinking they've eradicated some bad habit, fall back into it, as they now consider themselves \"safe.\" When installing epistemic habits, this risk is especially worrisome, since you may not notice that you have lapsed, in which case you can become the highly annoying sort of person who is weak in domains that they specifically consider themselves strong in and thus find themselves resistant to correction<span style=\"font-size: 11px;\">.</span></p>\n<p class=\"p3\">I must emphasize that I would much rather deal with someone who is offended and knows it than someone who is offended and thinks that he cannot possibly be so. So if you do <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">wish to become</a> a person who does not get offended<em>, do it right. </em>After all, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">it is dangerous to be half a rationalist</a>.&nbsp;</p>\n<p class=\"p3\"><span style=\"white-space: pre;\"> </span></p>\n<p class=\"p3\">[1]&nbsp;This isn't to say that those techniques will necessarily be the most useful for you-- merely that I have found them successful and consider myself sufficiently qualified to explain them. It might be that&nbsp;alternative strategies&nbsp;could be more useful for you-- if so, feel free to post them in the comments, as they could potentially make this post that much more useful for future readers.</p>\n<p class=\"p3\">[2] If anyone wants to take the helm and write this post, they have my blessing-- my queue is overflowing right now. Please do send me a link if you do end up doing this, though.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zQfjaxhxuXToyaetp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "21921", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to:&nbsp;</strong><span class=\"s2\"><a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">Don't Get Offended</a></span></p>\n<p><strong>Draws heavily on: </strong><a href=\"http://plato.stanford.edu/entries/stoicism/\">Stoicism</a>, <a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a>,<strong>&nbsp;</strong><a href=\"http://www.lesswrong.com/lw/1xh/living_luminously/\">Living Luminously</a></p>\n<p><a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">Previously</a>, we discussed why not getting offended might be an effective strategy to adopt in order to increase one's practical&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Epistemic_rationality#Epistemic_rationality\">epistemic rationality</a>. That's all well and good, but just as <a href=\"http://wiki.lesswrong.com/wiki/Debiasing\">knowing about biases isn't the same as protecting ourselves from them</a>, the simple desire to avoid being offended is (usually) insufficient to actually avoid it-- practice, too, is required.</p>\n<p class=\"p4\">So what should you actually practice if you find yourself becoming offended and want to stop?&nbsp;This post aims to address that. In doing so, it also features an expanded discussion of one question that seemed to be a sticking point for several posters in the previous discussion-- if you aren't getting offended, how will you discourage offensive and inappropriate behaviors?</p>\n<p class=\"p4\"><strong id=\"Preparation\">Preparation</strong></p>\n<p class=\"p4\">First, you need to&nbsp;<a href=\"http://en.wikipedia.org/wiki/Alief_(belief)\"><span class=\"s3\">really truly recognize</span></a>&nbsp;that experiencing the feeling of being offended is an undesirable process. <a href=\"/lw/le/lost_purposes/\">You must see <em>why</em></a>&nbsp;experiencing offense <a href=\"http://www.paulgraham.com/identity.html\">runs counter</a> to <a href=\"http://yudkowsky.net/rational/the-simple-truth\">knowing the truth</a>.&nbsp;</p>\n<p class=\"p4\">A good litmus test is to check whether experiencing the feeling of being offended seems obviously bad to you--&nbsp;<em>not the existence of the feeling itself or any behaviors tied to it,&nbsp;</em>but the fact that you are experiencing it. It is important to understand that this refers only to the mental experience of being offended-- this post focuses entirely on the A (<a href=\"http://en.wikipedia.org/wiki/Affect_(philosophy)\">Affect</a>) component of Alicorn's&nbsp;<a href=\"/lw/1y0/the_abcs_of_luminosity/\">ABC model</a>.&nbsp;</p>\n<p class=\"p4\">While it might sound silly to have the preliminary step be simply thinking that being offended is bad, if you don't think that there's not much point in practicing the remaining steps. In fact, if you don't think that, practicing the remaining steps may be harmful.</p>\n<p class=\"p4\"><strong id=\"Part_One__Detection\">Part One: Detection</strong></p>\n<p class=\"p4\">In order to stop being offended-- or really alter nearly anything about your mental state-- the first step is to increase your awareness of when you are becoming offended and what that process looks like in as early a stage as possible. As in the case of&nbsp;<a href=\"/lw/21b/ugh_fields/\"><span class=\"s3\">ugh fields</span></a>, being mindful of your reactions and \"watching for the flinch\" is an important early step.</p>\n<p class=\"p4\">As soon as you feel yourself becoming offended, you should notice this. It is then critical to truly&nbsp;<span class=\"s3\"><a href=\"http://www.lesswrong.com/lw/1xh/living_luminously/\">inspect your reactions</a>&nbsp;and d</span>etermine&nbsp;<em>why</em>&nbsp;you are becoming offended. This doesn't mean thinking things like \"I was offended because she insulted my friend,\" which has insufficient detail. Try for something more like \"I was offended because she made a severe criticism of another person in the group and I feel that she did not have the relevant social capital to justify making her statement.\" If you don't have a detailed conception of exactly what it is that is offending you, moving forward will be difficult.</p>\n<p class=\"p4\">At times you will not be able to do this thanks to the heat of the moment. That's okay and in point of fact it is expected-- truly understanding one's own motivations and responses can be difficult even in unemotional situations. If necessary, wait for calmer times to evaluate such issues or ask others for clarifications or predictions. While the inputs of others might not always be useful, close friends (or unusually perceptive unclose friends) can in many cases pinpoint causes to your behavior that you might be blind to.</p>\n<p class=\"p4\">If emotionally possible, testing these models is certainly helpful, though I recognize that this can be challenging at times and do not recommend it to the unprepared. In particular, having your friends try to offend you to test your reactions is often a poor idea, as the emotional responses involved can be unpleasant for multiple parties.</p>\n<p class=\"p3\"><strong id=\"Part_Two__Dissolution\">Part Two: Dissolution</strong></p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\">Once you have the ability to detect when and why you are becoming offended, there are multiple steps that one can take. The two techniques that have been most successful for me in the moment are what I like to call <strong>Dissolution</strong> and <strong>Defense</strong>.<sup>[1]</sup></p>\n<p class=\"p3\">The first of those two methods, <strong>Dissolution</strong>, is what I tend to use under normal circumstances. This method attempts to dissolve feelings of offense by simply understanding them really well and then applying the <a href=\"/lw/id/you_can_face_reality/\">Principle of Gendlin</a>. For instance, if someone has said an insulting remark to me, I might think to myself \"If this criticism is false, then it can easily be defeated by the truth. If this criticism is true, well, <a href=\"http://www.goodreads.com/quotes/541138-that-which-can-be-destroyed-by-the-truth-should-be\">you know what P.C. Hodgell says about that...</a>&nbsp;perhaps this criticism was not made in the most optimal manner, but I have no need to be offended, for the criticism will succeed or fail on the basis of the truth, not on the basis of whether it is appropriate.\"</p>\n<p class=\"p3\">For me, Gendlin is a true friend and can resolve most of these issues fairly trivially. However, this does not work the same for all individuals. Other techniques, such as perspective shifting, may be more reliable for others. The important strand that I have found throughout many people who can avoid being offended is the concept that being offended is a matter of one's own reaction, not the external world. I irreverently refer to this as the <a href=\"http://www.enotes.com/shakespeare-quotes/nothing-either-good-bad-but-thinking-makes\">Principle of Hamlet</a>-- there is nothing either good or bad, but thinking makes it so. It is a key tenet of Stoic thought.</p>\n<p class=\"p3\">Note that there are a few other things to consider. For instance, one should <a href=\"/lw/1mu/sorting_out_sticky_brains/\">beware sticky brains</a>&nbsp;when executing this technique. Personally, my brain isn't very sticky, but if yours is you may have to plan around that. There are many considerations similar to this one regarding personal mental styles, and the topic of \"Things You Should Know About Your Brain\" probably merits a post of its own, but I don't really have space to go into it here.<sup>[2] </sup>Suffice it to say that nothing presented here is set in stone and you should make whatever modifications are appropriate in order to fit this into your own mental style.</p>\n<p class=\"p3\">With practice, I have found that the sentiment expressed in&nbsp;<a href=\"/lw/1za/the_spotlight/1t8q\">this comment</a>&nbsp;can apply to reactions as well as to personal traits-- reactions that I don't like having tend to go away soon after I understand them, since I can then apply these methods to their dissolution.</p>\n<p class=\"p3\"><strong id=\"Part_Three__Defense\">Part Three: Defense</strong></p>\n<p class=\"p3\">However, I've found that there are some times in which I am unable to successfully dissolve my feelings of offense. It may be that I am extremely hungry or tired or otherwise impaired and thus have less than normal ability to control my reactions, or that I am simply too shocked to react normally. In this situation, I resort to the secondary method, <strong>Defense. </strong>This is not glamorous and not cool but it does work. The key to defense is isolating yourself from stimuli that produce undesirable results.</p>\n<p class=\"p3\">There are multiple forms of this-- the most basic one is simply leaving the area. Other simple methods include drowning something out (simple technique: the classic \"I'M NOT LISTENING LA LA LA LA LA,\" except inside your head), suddenly becoming very (authentically) interested in something else, pretending you have to take a call, etc. One&nbsp;<em>extremely important</em>&nbsp;note is that these methods should be a last resort. Otherwise, it has the potential of becoming&nbsp;<a href=\"/lw/hu/the_third_alternative/\">an excuse</a>. I seriously considered not putting them in the post at all because of the risk of it making people not take the first method seriously enough. Ultimately, I decided that it would be better for most people to know than to not know-- but&nbsp;<em>seriously, be careful with this.</em></p>\n<p class=\"p3\">If you do find yourself having to resort to these methods more often than you would like, there is another option-- active defense. I generally&nbsp;<a href=\"http://www.jwz.org/blog/2012/12/superheroes-and-the-protagonist-problem/\">prefer action to reaction</a>, so I tend to prefer active defenses to reactive ones. Active defenses&nbsp;involve self-modification so that certain stimuli&nbsp;<a href=\"/lw/2a5/on_enjoying_disagreeable_company/\">no longer produce undesirable results</a>&nbsp;or produce less undesirable results.</p>\n<p class=\"p3\">For instance, if I know that I am going to encounter someone who may make offensive remarks regarding another of my friends, I may steel myself for this prior to the encounter, saying to myself \"While it may be that some of my friends dislike each other and want to express this to me, I should not fall into the trap of becoming offended and getting into an argument over whether or not one's criticism of the other is valid. <a href=\"http://www.plausiblydeniable.com/opinion/gsf.html\">All my friends don't have to be friends with one another</a>, and trying to enforce this will only add to the trouble. Instead I will make a mild remark and move on.\"</p>\n<p class=\"p3\">This is an especially effective method when it comes to preventing surprised or shocked offended reactions, though of course one must always beware unknown unknowns. Marcus Aurelius engaged in <a href=\"http://www.goodreads.com/quotes/290880-begin-each-day-by-telling-yourself-today-i-shall-be\">an extremely general form of this</a>, advocating that one begin each day by preparing oneself to meet with all sorts of offenses while avoiding anger or irritation. In some respects, the overall practice of <a href=\"http://plato.stanford.edu/entries/stoicism/\">Stoicism</a> could be considered an advanced form of active defense-- though not just against becoming offended, but against all wild or uncontrolled reactions.</p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\"><strong id=\"Part_Four__Discouragement\">Part Four: Discouragement</strong></p>\n<p class=\"p6\">This step is where you evaluate whether taking action to prevent further offensive behavior is merited or useful. As stated <a href=\"/r/lesswrong/lw/gux/dont_get_offended/\">earlier</a>, I believe that even in cases when it is instrumentally useful to show offense, one can still perform actions indicating offense without actually experiencing the internal state of being offended. The question then becomes when it is appropriate to do so.</p>\n<p class=\"p6\">In previous discussion, Oligopsony <a href=\"/lw/gux/dont_get_offended/8kl3\">pointed out</a>&nbsp;that taking offense at inappropriate behaviors can be considered a public good. I disagree to an extent, because I think that in many situations claiming to be offended or acting offended can in fact escalate a situation that would otherwise pass with a small amount of awkwardness and concern. However, simply allowing (truly) inappropriate behavior to continue without objection tacitly indicates that that behavior is acceptable and thus carries negative consequences of its own.</p>\n<p class=\"p6\">Overall, I find that generally speaking it is often wise to complain about offensive behaviors if you think it is likely that those behaviors will offend others. You should be wary about <a href=\"/lw/dr/generalizing_from_one_example/\">generalizing from one example</a>, though. I find the sound of silverware contacting teeth to be both off-putting and offensive, but this is not something that I bother to point out with people that I don't expect to interact with often, since I am moderately confident that it is a pet peeve that most people don't care about and aren't offended by. On the other hand, I do bother to point out that fact to people that I expect to interact with frequently if I notice them doing it, since in this case it is worth my time to potentially avert a future instance.</p>\n<p class=\"p6\">A friend of mine who is currently commissioned as a military officer says that one key principle of effective leadership is \"praise in public, punish in private--\" in other words, save criticisms for private encounters so that you don't have to worry about potential status implications of making the criticism around others. In my experience, this is also an effective way to deal with offensive behavior while minimizing social awkwardness and the potential for escalation.</p>\n<p class=\"p6\">In some situations, though, it is simply necessary to stand up when no one else is willing and confront offensive behavior directly. I have done so several times and will say that while it is usually uncomfortable for all those concerned, the result can be worth it. That being said, I urge you to use extreme caution when evaluating whether or not it is necessary to do so. My impression is that many situations that people deem worthy of confrontation could be resolved more effectively through less direct means.</p>\n<p class=\"p3\"><strong id=\"Part_Five__CONSTANT_VIGILANCE\">Part Five: CONSTANT VIGILANCE</strong></p>\n<p class=\"p6\"><strong></strong></p>\n<p class=\"p3\">As a final thought, I've seen a lot of people, thinking they've eradicated some bad habit, fall back into it, as they now consider themselves \"safe.\" When installing epistemic habits, this risk is especially worrisome, since you may not notice that you have lapsed, in which case you can become the highly annoying sort of person who is weak in domains that they specifically consider themselves strong in and thus find themselves resistant to correction<span style=\"font-size: 11px;\">.</span></p>\n<p class=\"p3\">I must emphasize that I would much rather deal with someone who is offended and knows it than someone who is offended and thinks that he cannot possibly be so. So if you do <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">wish to become</a> a person who does not get offended<em>, do it right. </em>After all, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">it is dangerous to be half a rationalist</a>.&nbsp;</p>\n<p class=\"p3\"><span style=\"white-space: pre;\"> </span></p>\n<p class=\"p3\">[1]&nbsp;This isn't to say that those techniques will necessarily be the most useful for you-- merely that I have found them successful and consider myself sufficiently qualified to explain them. It might be that&nbsp;alternative strategies&nbsp;could be more useful for you-- if so, feel free to post them in the comments, as they could potentially make this post that much more useful for future readers.</p>\n<p class=\"p3\">[2] If anyone wants to take the helm and write this post, they have my blessing-- my queue is overflowing right now. Please do send me a link if you do end up doing this, though.</p>", "sections": [{"title": "Preparation", "anchor": "Preparation", "level": 1}, {"title": "Part One: Detection", "anchor": "Part_One__Detection", "level": 1}, {"title": "Part Two: Dissolution", "anchor": "Part_Two__Dissolution", "level": 1}, {"title": "Part Three: Defense", "anchor": "Part_Three__Defense", "level": 1}, {"title": "Part Four: Discouragement", "anchor": "Part_Four__Discouragement", "level": 1}, {"title": "Part Five: CONSTANT VIGILANCE", "anchor": "Part_Five__CONSTANT_VIGILANCE", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3Ci2Zxncj3oiB2NP4", "9o3Cjjem7AbmmZfBs", "sP2Hg6uPwpfp3jZJN", "rLuZ6XrGpgjk9BNpX", "EFQ3F6kmt4WHXRqik", "HYWhKXRsMAyvRKRYz", "L4GGomr86sEwxzPvS", "erGipespbbzdG5zYb", "nK5jraMp7E4xPvuNv", "baTWMegR42PAsH9qJ", "DoLQN5ryZ9XkZjq5h", "7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T04:44:35.134Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Epistemic Viciousness", "slug": "seq-rerun-epistemic-viciousness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXCNCCgD5DC7evZ5i/seq-rerun-epistemic-viciousness", "pageUrlRelative": "/posts/sXCNCCgD5DC7evZ5i/seq-rerun-epistemic-viciousness", "linkUrl": "https://www.lesswrong.com/posts/sXCNCCgD5DC7evZ5i/seq-rerun-epistemic-viciousness", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Epistemic%20Viciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Epistemic%20Viciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXCNCCgD5DC7evZ5i%2Fseq-rerun-epistemic-viciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Epistemic%20Viciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXCNCCgD5DC7evZ5i%2Fseq-rerun-epistemic-viciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXCNCCgD5DC7evZ5i%2Fseq-rerun-epistemic-viciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>Today's post, <a href=\"/lw/2i/epistemic_viciousness/\">Epistemic Viciousness</a> was originally published on 13 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Epistemic_Viciousness\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An essay by Gillian Russell on \"Epistemic Viciousness in the Martial Arts\" generalizes amazingly to possible and actual problems with building a community around rationality. Most notably the extreme dangers associated with \"data poverty\" - the difficulty of testing the skills in the real world. But also such factors as the sacredness of the dojo, the investment in teachings long-practiced, the difficulty of book learning that leads into the need to trust a teacher, deference to historical masters, and above all, living in data poverty while continuing to act as if the luxury of trust is possible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h1e/seq_rerun_a_sense_that_more_is_possible/\">A Sense That More Is Possible</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXCNCCgD5DC7evZ5i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.14753410937466e-06, "legacy": true, "legacyId": "22094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T8ddXNtmNSHexhQh8", "AnRxkDqCpWRteNt7u", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T08:11:55.006Z", "modifiedAt": null, "url": null, "title": "Why AI may not foom", "slug": "why-ai-may-not-foom", "viewCount": null, "lastCommentedAt": "2020-07-09T20:49:39.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/77xLbXs6vYQuhT8hq/why-ai-may-not-foom", "pageUrlRelative": "/posts/77xLbXs6vYQuhT8hq/why-ai-may-not-foom", "linkUrl": "https://www.lesswrong.com/posts/77xLbXs6vYQuhT8hq/why-ai-may-not-foom", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20AI%20may%20not%20foom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20AI%20may%20not%20foom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77xLbXs6vYQuhT8hq%2Fwhy-ai-may-not-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20AI%20may%20not%20foom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77xLbXs6vYQuhT8hq%2Fwhy-ai-may-not-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77xLbXs6vYQuhT8hq%2Fwhy-ai-may-not-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3736, "htmlBody": "<h3>Summary</h3>\n<ul>\n<li>There's a decent chance that the intelligence of a self-improving AGI will grow in a relatively smooth exponential or sub-exponential way, not super-exponentially or with large jump discontinuities.</li>\n<li>If this is the case, then an AGI whose effective intelligence matched that of the world's combined AI researchers would make AI progress at the rate they do, taking decades to double its own intelligence.</li>\n<li>The risk that the first successful AGI will quickly monopolize many industries, or quickly hack many of the computers connected to the internet, seems worth worrying about.&nbsp; In either case, the AGI would likely end up using the additional computing power it gained to self-modify so it was superintelligent.</li>\n<li>AI boxing could mitigate both of these risks greatly.</li>\n<li>If hard takeoff could be impossible, it might be best to assume this case and concentrate our resources on ensuring a safe soft takeoff, given that the prospects for a safe hard takeoff look grim.</li>\n</ul>\n<hr />\n<p>&nbsp;</p>\n<h3>Takeoff models discussed in the Hanson-Yudkowsky debate</h3>\n<h4>The supercritical nuclear chain reaction model</h4>\n<p>Yudkowsky alludes to this model repeatedly, starting in <a href=\"/lw/w5/cascades_cycles_insight/\">this</a> post:</p>\n<blockquote>\n<p>When a uranium atom splits, it releases neutrons - some right away, some after delay while byproducts decay further.&nbsp; Some neutrons escape the pile, some neutrons strike another uranium atom and cause an additional fission.&nbsp; The effective neutron multiplication factor, denoted <em>k</em>, is the average number of neutrons from a single fissioning uranium atom that cause another fission...</p>\n<p>It might seem that a cycle, with the same thing happening over and over again, ought to exhibit continuous behavior.&nbsp; In one sense it does.&nbsp; But if you pile on one more uranium brick, or pull out the control rod another twelve inches, there's one hell of a big difference between <em>k</em> of 0.9994 and <em>k</em> of 1.0006.</p>\n</blockquote>\n<p>I don't like this model much for the following reasons:</p>\n<ul>\n<li>The model doesn't offer much insight in to the time scale over which an AI might self-improve.&nbsp; The \"mean generation time\" (time necessary for the next \"generation\" of neutrons to be released) of a nuclear chain reaction is short, and the doubling time for neutron activity in Fermi's experiment was just two minutes, but it hardly seems reasonable to generalize this to self-improving AIs.</li>\n<li>A flurry of insights that either dies out or expands exponentially doesn't seem like a very good description of how human minds work, and I don't think it would describe an AGI well either.&nbsp; <a href=\"/lw/fqg/looking_for_a_likely_cause_of_a_mental_phenomenon/?sort=top\">Many people</a> report that taking time to think about problems is key to their problem-solving process.&nbsp; It seems likely that an AGI unable to immediately generate insight in to some problem would have a slower and more exhaustive \"fallback\" search process that would allow it to continue making progress.&nbsp; (Insight could also work via a search process in the first place--over the space of permutations in one's mental model, say.)</li>\n</ul>\n<h4>The \"differential equations folded on themselves\" model</h4>\n<p>This is another model Eliezer <a href=\"/lw/wf/hard_takeoff/\">alludes</a> to, albeit in a somewhat handwavey fashion:</p>\n<blockquote>\n<p>When you fold a whole chain of differential equations in on itself like this, it should either peter out rapidly as improvements fail to yield further improvements, or else go FOOM.</p>\n</blockquote>\n<p>It's not exactly clear to me what the \"whole chain of differential equations\" is supposed to refer to... there's only one differential equation in the preceding paragraph, and it's a standard exponential (which could be scary or not, depending on the multiplier in the exponent.&nbsp; Rabbit populations and bank account balances both grow exponentially in a way that's slow enough for humans to understand and control.)</p>\n<p>Maybe he's referring to the levels he describes <a href=\"/lw/w6/recursion_magic/\">here</a>: metacognitive, cognitive, metaknowledge, knowledge, and object.&nbsp; How might we paramaterize this system?</p>\n<p>Let's say <em>c</em> is our AGI's cognition ability, <em>dc/dt</em> is the rate of change in our AGI's cognitive ability, <em>m</em> is our AGI's \"metaknowledge\" (about cognition and metaknowledge), and <em>dm/dt</em> is the rate of change in metaknowledge.&nbsp; What I've got in mind is:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{dc}{dt} = p \\cdot c \\cdot m\" alt=\"\" /></p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{dm}{dt} = q \\cdot c \\cdot m\" alt=\"\" /></p>\n<p>where p and q are constants.</p>\n<p>In other words, both change in cognitive ability and change in metaknowledge are each individually directly proportionate to both cognitive ability and metaknowledge.</p>\n<p>I don't know much about understanding systems of differential equations, so if you do, please comment!&nbsp; I put the above system in to <a href=\"http://www.wolframalpha.com/input/?i=dc%2Fdt+%3D+p+*+c+*+m+%3B+dm%2Fdt+%3D+q+*+c+*+m\">Wolfram Alpha</a>, but I'm not exactly sure how to interpret the solution provided.&nbsp; In any case, fooling around with <a href=\"https://gist.github.com/johnmaxwelliv/4942090\">this</a> script suggests sudden, extremely sharp takeoff for a variety of different test parameters.</p>\n<h4>The straight exponential model</h4>\n<p>To me, the \"proportionality thesis\" described by David Chalmers in his <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">singularity paper</a>, \"increases in intelligence (or increases of a certain sort) always lead to proportionate increases in the capacity to design intelligent systems\", suggests a single differential equation that looks like</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{du}{dt} = s \\cdot u\" alt=\"\" /></p>\n<p>where <em>u</em> represents the number of upgrades that have been made to an AGI's source code, and <em>s</em> is some constant.&nbsp; The solution to this differential equation is going to look like</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u(t)%20=%20c_%7B1%7De%5E%7Bst%7D\" alt=\"\" /></p>\n<p>where the constant <em>c<sub>1</sub></em> is determined by our initial conditions.</p>\n<p>(In <a href=\"/lw/we/recursive_selfimprovement/\">Recursive Self-Improvement</a>, Eliezer calls this a \"too-obvious mathematical idiom\".&nbsp; I'm inclined to favor it for its obviousness, or at least use it as a jumping-off point for further analysis.)</p>\n<p>Under this model, the constant <em>s</em> is pretty important... if <em>u(t)</em> was the amount of money in a bank account, <em>s</em> would be the rate of return it was receiving.&nbsp; The parameter <em>s</em> will effectively determine the \"doubling time\" of an AGI's intelligence.&nbsp; It matters a lot whether this \"doubling time\" is on the scale of minutes or years.</p>\n<p>So what's going to determine <em>s</em>?&nbsp; Well, if the AGI's hardware is twice as fast, we'd expect it to come up with upgrades twice as fast.&nbsp; If the AGI had twice as <em>much</em> hardware, and it could parallelize the search for upgrades perfectly (which seems like a reasonable approximation to me), we'd expect the same thing.&nbsp; So let's decompose <em>s</em> and make it the product of two parameters: <em>h</em> representing the hardware available to the AGI, and <em>r</em> representing the ease of finding additional improvements.&nbsp; The AGI's intelligence will be on the order of <em>u</em> * <em>h</em>, i.e. the product of the AGI's software quality and hardware capability.</p>\n<p>&nbsp;</p>\n<h3>Considerations affecting our choice of model</h3>\n<h4>Diminishing returns<br /></h4>\n<p>The consideration here is that the initial improvements implemented by an AGI will tend to be those that are especially easy to implement and/or especially fruitful to implement, with subsequent improvements tending to deliver less intelligence bang for the implementation buck.&nbsp; Chalmers calls this \"perhaps the most serious structural obstacle\" to the proportionality thesis.</p>\n<p>To think about this consideration, one could imagine representing a given improvement as a pair of two values (<em>u</em>, <em>d</em>).&nbsp; <em>u</em> represents a factor by which existing performance will be multiplied, e.g. if <em>u</em> is 1.1, then implementing this improvement will improve performance by a factor of 1.1.&nbsp; <em>d</em> represents the cognitive difficulty or amount of intellectual labor to required to implement a given improvement.&nbsp; If <em>d</em> is doubled, then at any given level of intelligence, implementing this improvement will take twice as long (because it will be harder to discover and/or harder to translate in to code).</p>\n<p>Now let's imagine ordering our improvements in order from highest to lowest <em>u</em> to <em>d</em> ratio, so we implement those improvements that deliver the greatest bang for the buck first.</p>\n<p>Thus ordered, let's imagine separating groups of consecutive improvements in to \"tiers\".&nbsp; Each tier's worth of improvements, when taken together, will represent the doubling of an AGI's software quality, i.e. the product of the <em>u</em>'s in that cluster will be roughly 2.&nbsp; For a steady doubling time, each tier's total difficulty will need sum to approximately twice the difficulty of the tier before it.&nbsp; If tier difficulty tends to more than double, we're likely to see sub-exponential growth.&nbsp; If tier difficulty tends to less than double, we're likely to see super-exponential growth.&nbsp; If a single improvement delivers a more-than-2x improvement, it will span multiple \"tiers\".</p>\n<p>It seems to me that the quality of fruit available at each tier represents a kind of logical uncertainty, similar to asking whether an efficient algorithm exists for some task, and if so, how efficient.</p>\n<p>On the this diminishing returns consideration, Chalmers writes:</p>\n<blockquote>\n<p>If anything, 10% increases in intelligence-related capacities are likely to lead all sorts of intellectual breakthroughs, leading to next-generation increases in intelligence that are significantly greater than 10%. Even among humans, relatively small differences in design capacities (say, the difference between Turing and an average human) seem to lead to large differences in the systems that are designed (say, the difference between a computer and nothing of importance).</p>\n</blockquote>\n<p>Eliezer Yudkowsky's <a href=\"/lw/we/recursive_selfimprovement/\">objection</a> is similar:</p>\n<blockquote>\n<p>...human intelligence does <em>not</em> require a hundred times as much computing power as chimpanzee intelligence.&nbsp; Human brains are merely three times too large, and our prefrontal cortices six times too large, for a primate with our body size.</p>\n<p>Or again:&nbsp; It does not seem to require 1000 times as many genes to build a human brain as to build a chimpanzee brain, even though human brains can build toys that are a thousand times as neat.</p>\n<p>Why is this important?&nbsp; Because it shows that with <em>constant optimization pressure</em> from natural selection and <em>no intelligent insight,</em> there were <em>no diminishing returns</em> to a search for better brain designs up to at least the human level.&nbsp; There were probably <em>accelerating</em> returns (with a low acceleration factor).&nbsp; There are no <em>visible speedbumps,</em> <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">so far as I know</a>.</p>\n</blockquote>\n<p>First, hunter-gatherers can't design toys that are a thousand times as neat as the ones chimps design--they aren't programmed with the software modern humans get through the education (<a href=\"http://en.wikipedia.org/wiki/Pirah%C3%A3_people#Language\">some may be unable to count</a>), and educating apes has produced <a href=\"http://en.wikipedia.org/wiki/Kanzi\">interesting results</a>.</p>\n<p>Speaking as someone who's basically clueless about neuroscience, I can think of many different factors that might contribute to intelligence differences within the human race or between humans and other apes:</p>\n<ul>\n<li>Processing speed.</li>\n<li>Cubic centimeters brain hardware devoted to abstract thinking.&nbsp; (Gifted technical thinkers often seem to suffer from poor social intuition--perhaps a result of reallocation of brain hardware from social to technical processing.)</li>\n<li>Average number of connections per neuron within that brain hardware.</li>\n<li>Average neuron density within that brain hardware.&nbsp; <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=human%20brain%20scaled%20up%20primate%20brain&amp;source=web&amp;cd=1&amp;ved=0CDQQFjAA&amp;url=http%3A%2F%2Fwww.pnas.org%2Fcontent%2Fearly%2F2012%2F06%2F21%2F1201895109.full.pdf&amp;ei=EwogUcatE8bSigKht4HQCQ&amp;usg=AFQjCNGwnrHKwd3f_WaRXt09LMItlNVKuQ&amp;bvm=bv.42661473,d.cGE&amp;cad=rja\">This author</a> seems to think that a large part of the human brain's remarkableness comes largely from the fact that it's the largest primate brain, and primate brains maintain the same neuron density when enlarged while other types of brains don't.&nbsp; \"If absolute brain size is the best predictor of cognitive abilities in a primate (13), and absolute brain size is proportional to number of neurons across primates (24, 26), our superior cognitive abilities might be accounted for simply by the total number of neurons in our brain, which, based on the similar scaling of neuronal densities in rodents, elephants, and cetaceans, we predict to be the largest of any animal on Earth (28).\"</li>\n<li>Propensity to actually use your capacity for deliberate System 2 reasoning.&nbsp; Richard Feynman's second wife on why she divorced him: \"He begins working calculus problems in his head as soon as he awakens. He did calculus while driving in his car, while sitting in the living room, and while lying in bed at night.\"&nbsp; (By the way, does anyone know of research that's been done on getting people to use System 2 more?&nbsp; Seems like it could be really low-hanging fruit for improving intellectual output.&nbsp; Sometimes I wonder if the reason intelligent people tend to like math is because they were reinforced for the behaviour of thinking abstractly as kids (via praise, good grades, etc.) while those <em>not</em> at the top of the class were <em>not</em> so reinforced.)</li>\n<li><a href=\"http://www.slate.com/articles/health_and_science/science/2013/01/evolution_of_childhood_prolonged_development_helped_homo_sapiens_succeed.html\">Extended neuroplasticity in to \"childhood\"</a>.</li>\n<li>Increased calories to think with due to the invention of cooking.</li>\n<li>And finally, mental algorithms (\"software\").&nbsp; Which are probably at least <a href=\"http://newsroom.ucla.edu/portal/ucla/more-sophisticated-wiring-not-237689.aspx\">somewhat</a> important.</li>\n</ul>\n<p>It seems to me like these factors (or ones like them) may multiply together to produce intelligence, i.e. the \"intelligence equation\", as it were, could be something like intelligence = processing_speed * cc_abstract_hardware * neuron_density * connections_per_neuron * propensity_for_abstraction * mental_algorithms.&nbsp; If the ancestral environment rewarded intelligence, we should expect all of these characteristics to be selected for, and this could explain the \"low acceleration factor\" in human intelligence increase.&nbsp; (Increasing your processing speed by a factor of 1.2 does more when you're already pretty smart, so all these sources of intelligence increase would feed in to one another.)</p>\n<p>In other words, it's not that clear what relevance the evolution of human intelligence has to the ease and quality of the upgrades at different \"tiers\" of software improvements, since evolution operates on many non-software factors, but a self-improving AI (properly boxed) can only improve its software.</p>\n<h4>Bottlenecks</h4>\n<p>In the Hanson/Yudkowsky debate, Yudkowsky <a href=\"/lw/w8/engelbart_insufficiently_recursive/\">declares</a> Douglas Englebart's plan to radically bootstrap his team's productivity though improving their computer and software tools \"insufficiently recursive\".&nbsp; I agree with this assessment.&nbsp; Here's my modelling of this phenomenon.</p>\n<p>When a programmer makes an improvement to their code, their work of making the improvement requires the completion of many subtasks:</p>\n<ul>\n<li>choosing a feature to add</li>\n<li>reminding themselves of how the relevant part of the code works and <a href=\"http://www.paulgraham.com/head.html\">loading that information in to their memory</a></li>\n<li>identifying ways to implement the feature</li>\n<li>evaluating different methods of implementing the feature according to simplicity, efficiency, and correctness</li>\n<li>coding their chosen implementation</li>\n<li>testing their chosen implementation, identifying bugs</li>\n<li>identifying the cause of a given bug</li>\n<li>figuring out how to fix the given bug</li>\n</ul>\n<p>Each of those subtasks will consist of further subtasks like poking through their code, staring off in to space, typing, and talking to their rubber duck.</p>\n<p>Now the programmer improves their development environment so they can poke through their code slightly faster.&nbsp; But if poking through their code takes up only 5% of their development time, even an extremely large improvement in code-poking abilities is not going to result in an especially large increase in his development speed... in the best case, where code-poking time is reduced to zero, the programmer will only work about 5% faster.</p>\n<p>This is a reflection of <a href=\"http://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's Law</a>-type thinking.&nbsp; The amount you can gain through speeding something up depends on how much it's slowing you down.</p>\n<p>Relatedly, if intelligence is a complicated, heterogeneous process where computation is spread relatively evenly among many modules, then improving the performance of an AGI gets tougher, because upgrading an individual module does little to improve the performance of the system as a whole.</p>\n<p>And to see orders-of-magnitude performance improvement in such a process, almost <em>all</em> of your AGI's components will need to be improved radically.&nbsp; If even a few prove troublesome, improving your AGI's thinking speed becomes difficult.</p>\n<p>&nbsp;</p>\n<h3>Case studies in technological development speed<br /></h3>\n<h4>Moore's Law</h4>\n<blockquote>\n<p>It has famously been noted that if the automotive industry had achieved similar improvements in performance [to the semiconductor industry] in the last 30 years, a Rolls-Royce would cost only $40 and could circle the globe eight times on one gallon of gas&mdash;with a top speed of 2.4 million miles per hour.</p>\n</blockquote>\n<p>From <a href=\"http://www.mckinsey.com/client_service/semiconductors/latest_thinking/creating_value_in_the_semiconductor_industry\">this McKinsey report</a>.&nbsp; So Moore's Law is an outlier where technological development is concerned.&nbsp; I suspect that making transistors smaller and faster doesn't require finding ways to improve dozens of heterogeneous components.&nbsp; And when you zoom out to view a computer system as a whole, other bottlenecks typically <a href=\"http://en.wikipedia.org/wiki/Moore%27s_law#Importance_of_non-CPU_bottlenecks\">appear</a>.</p>\n<p>(It's also worth noting that research budgets in the semiconductor field have also risen greatly in the semiconductor industry since its inception, but obviously not following the same curve that chip speeds have.)</p>\n<h4>Compiler technology</h4>\n<p><a href=\"http://www.cs.virginia.edu/~techrep/CS-2001-12.pdf\">This paper</a> on \"Proebstig's Law\" suggests that the end result of all the compiler research done between 1970 or so and 2001 was that a typical integer-intensive program was compiled to run 3.3 times faster, and a typical floating-point-intensive program was compiled to run 8.1 times faster.&nbsp; When it comes to making programs run quickly, it seems that software-level compiler improvements are swamped by hardware-level chip improvements--perhaps because, like an AGI, a compiler has to deal with a huge variety of different scenarios, so improving it in the average case is tough.&nbsp; (This represents supertask heterogeneity, rather than subtask heterogeneity, so it's a different objection than the one mentioned above.)</p>\n<h4>Database technology</h4>\n<p>According to <a href=\"http://arxiv.org/pdf/cs/0701162.pdf\">two</a> <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-642-18206-8_9\">analyses</a> (<a href=\"http://www.sendspace.com/file/2ly1x8\">full paper</a> for that second one), it seems that improvement in database performance benchmarks has largely been due to Moore's Law.</p>\n<h4>AI (so far)<br /></h4>\n<p>Robin Hanson's blog post \"<a href=\"http://www.overcomingbias.com/2012/08/ai-progress-estimate.html\">AI Progress Estimate</a>\" was the best resource I could find on this.</p>\n<p>&nbsp;</p>\n<h3>Why smooth exponential growth implies soft takeoff<br /></h3>\n<p>Let's suppose we consider all of the above, deciding that the exponential model is the best, and we agree with <a href=\"http://www.overcomingbias.com/2011/07/debating-yudkowsky.html\">Robin Hanson</a> that there are few deep, chunky, undiscovered AI insights.</p>\n<p>Under the straight exponential model, if you recall, we had</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{du}{dh} = u \\cdot h \\cdot r\" alt=\"\" width=\"101\" height=\"38\" /></p>\n<p>where <em>u</em> is the degree of software quality, <em>h</em> is the hardware availability, and <em>r</em> is a parameter representing the difficulty of doing additional upgrades.&nbsp; Our AGI's overall intelligence is given by<em> u</em> * <em>h</em>--the quality of the software times the amount of hardware.</p>\n<p>Now we can solve for <em>r</em> by substituting in human intelligence for <em>u</em> * <em>h</em>, and substituting in the rate of human AI progress for <em>du</em>/<em>dt</em>.&nbsp; Another way of saying this is: When the AI is as smart as all the world's AI researchers working together, it will produce new AI insights at the rate that all the world's AI researchers working together produce new insights.&nbsp; <strong>At some point our AGI will be just as smart as the world's AI researchers, but we can hardly expect to start seeing super-fast AI progress at that point, because the world's AI researchers haven't produced super-fast AI progress.</strong></p>\n<p>Let's assume AGI that's on par with the world AI research community is reached in 2080 (<a href=\"/lw/8p4/2011_survey_results/\">LW's median \"singularity\" estimate in 2011</a>).&nbsp; We'll pretend AI research has only been going on since 2000, meaning 80 \"standard research years\" of progress have gone in to the AGI's software.&nbsp; So at the moment our shiny new AGI is fired up, <em>u</em> = 80, and it's doing research at the rate of one \"human AGI community research year\" per year, so <em>du</em>/<em>dt</em> = 1.&nbsp; That's an effective rate of return on AI software progress of 1 / 80 = 1.3%, giving a software quality doubling time of around 58 years.</p>\n<p>You could also apply this kind of thinking to individual AI projects.&nbsp; For example, it's possible that at some point <a href=\"http://wiki.lesswrong.com/wiki/EURISKO\">EURISKO</a> was improving itself about as fast as Doug Lenat was improving it.&nbsp; You might be able to do a similar calculation to take a stab at EURISKO's insight level doubling time.</p>\n<p>&nbsp;</p>\n<h2>The importance of hardware<br /></h2>\n<p>According to my model, you double your AGI's intelligence, and thereby the speed with which your AGI improves itself, by doubling the hardware available for your AGI.&nbsp; So if you had an AGI that was interesting, you could make it 4x as smart by giving it 4x the hardware.&nbsp; If an AGI that was 4x as smart could get you 4x as much money (through impressing investors, or playing the stock market, or monopolizing additional industries), that'd be a nice feedback loop.&nbsp; For maximum explosivity, put half your AGI's mind to the task of improving its software, and the other half to the task of making more money with which to buy more hardware.</p>\n<p>But it seems pretty straightforward to prevent a non-superintelligent AI from gaining access to additional hardware with careful planning.&nbsp; (Note: One problem with AI boxing experiments thus far is that all of the AIs have been played by human beings.&nbsp; Human beings have innate understanding of human psychology and possess specialized capabilities for running emulations of one another.&nbsp; It seems pretty easy to prevent an AGI from acquiring such understanding.&nbsp; But there may exist box-breaking techniques that don't rely on understanding human psychology.&nbsp; Another note about boxing: FAI requires getting everything perfect, which is a conjunctive calculation.&nbsp; Given multiple safeguards, only one has to work for the box as a whole to work, which is a disjunctive calculation.)</p>\n<p>&nbsp;</p>\n<h3>AGI's impact on the economy</h3>\n<p>Is it possible that the first group to create a successful AGI might begin monopolizing different sections of the economy?&nbsp; Robin Hanson argues that technology insights typically leak between different companies, due to conferences and employee poaching.&nbsp; But we can't be confident these factors would affect the research an AGI does on itself.&nbsp; And if an AGI is still dumb enough that a significant portion of its software upgrades are coming from human researchers, it can hardly be considered superintelligent.</p>\n<p>Given what looks like a winner-take-all dynamic, an important factor may be the number of serious AGI competitors.&nbsp; If there are only two, the #1 company may not wish to trade insights with the #2 company for fear of losing its lead.&nbsp; If there are more than two, all but the leading company might ally against the leading company in trading insights.&nbsp; If their alliance is significantly stronger than the leading company, perhaps the leading company would wish to join their alliance.</p>\n<p>But if AI is about getting lots of details right, as Hanson <a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">suggests</a>, improvements may not even transfer between different AI architectures.</p>\n<p>&nbsp;</p>\n<h3>What should we do?<br /></h3>\n<p>I've argued that soft takeoff is a strong possibility.&nbsp; Should that change our strategy as people concerned with x-risk?</p>\n<p>If we are <a href=\"/lw/g93/evaluating_the_feasibility_of_sis_plan/\">basically screwed</a> in the event that hard takeoff is possible, it may be that preparing for a soft takeoff is a better use of resources on the margin.&nbsp; Shane Legg has <a href=\"http://www.vetta.org/2009/08/funding-safe-agi/\">proposed</a> that people concerned with friendliness become investors in AGI projects so they can affect the outcome of any that seem to be succeeding.</p>\n<p>&nbsp;</p>\n<h3>Concluding thoughts</h3>\n<p>Expert forecasts are famously unreliable even in the relatively well-understood field of political forecasting.&nbsp; So <strong>given the number of unknowns involved in the emergence of smarter-than-human intelligence, it's hard to say much with certainty</strong>.&nbsp; Picture a few Greek scholars speculating on the industrial revolution.</p>\n<p>I don't have a strong background in these topics, so I fully expect that the above essay will reveal my ignorance, which I'd appreciate your pointing out in the comments.&nbsp; This essay should be taken as at attempt to <a href=\"/lw/8ns/hack_away_at_the_edges/\">hack away at the edges</a>, not come to definitive conclusions.&nbsp; As always, I reserve the right to change my mind about anything ;)</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "oiRp4T6u5poc8r9Tj": 2, "5f5c37ee1b5cdee568cfb285": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "77xLbXs6vYQuhT8hq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 29, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "21567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Summary\">Summary</h3>\n<ul>\n<li>There's a decent chance that the intelligence of a self-improving AGI will grow in a relatively smooth exponential or sub-exponential way, not super-exponentially or with large jump discontinuities.</li>\n<li>If this is the case, then an AGI whose effective intelligence matched that of the world's combined AI researchers would make AI progress at the rate they do, taking decades to double its own intelligence.</li>\n<li>The risk that the first successful AGI will quickly monopolize many industries, or quickly hack many of the computers connected to the internet, seems worth worrying about.&nbsp; In either case, the AGI would likely end up using the additional computing power it gained to self-modify so it was superintelligent.</li>\n<li>AI boxing could mitigate both of these risks greatly.</li>\n<li>If hard takeoff could be impossible, it might be best to assume this case and concentrate our resources on ensuring a safe soft takeoff, given that the prospects for a safe hard takeoff look grim.</li>\n</ul>\n<hr>\n<p>&nbsp;</p>\n<h3 id=\"Takeoff_models_discussed_in_the_Hanson_Yudkowsky_debate\">Takeoff models discussed in the Hanson-Yudkowsky debate</h3>\n<h4 id=\"The_supercritical_nuclear_chain_reaction_model\">The supercritical nuclear chain reaction model</h4>\n<p>Yudkowsky alludes to this model repeatedly, starting in <a href=\"/lw/w5/cascades_cycles_insight/\">this</a> post:</p>\n<blockquote>\n<p>When a uranium atom splits, it releases neutrons - some right away, some after delay while byproducts decay further.&nbsp; Some neutrons escape the pile, some neutrons strike another uranium atom and cause an additional fission.&nbsp; The effective neutron multiplication factor, denoted <em>k</em>, is the average number of neutrons from a single fissioning uranium atom that cause another fission...</p>\n<p>It might seem that a cycle, with the same thing happening over and over again, ought to exhibit continuous behavior.&nbsp; In one sense it does.&nbsp; But if you pile on one more uranium brick, or pull out the control rod another twelve inches, there's one hell of a big difference between <em>k</em> of 0.9994 and <em>k</em> of 1.0006.</p>\n</blockquote>\n<p>I don't like this model much for the following reasons:</p>\n<ul>\n<li>The model doesn't offer much insight in to the time scale over which an AI might self-improve.&nbsp; The \"mean generation time\" (time necessary for the next \"generation\" of neutrons to be released) of a nuclear chain reaction is short, and the doubling time for neutron activity in Fermi's experiment was just two minutes, but it hardly seems reasonable to generalize this to self-improving AIs.</li>\n<li>A flurry of insights that either dies out or expands exponentially doesn't seem like a very good description of how human minds work, and I don't think it would describe an AGI well either.&nbsp; <a href=\"/lw/fqg/looking_for_a_likely_cause_of_a_mental_phenomenon/?sort=top\">Many people</a> report that taking time to think about problems is key to their problem-solving process.&nbsp; It seems likely that an AGI unable to immediately generate insight in to some problem would have a slower and more exhaustive \"fallback\" search process that would allow it to continue making progress.&nbsp; (Insight could also work via a search process in the first place--over the space of permutations in one's mental model, say.)</li>\n</ul>\n<h4 id=\"The__differential_equations_folded_on_themselves__model\">The \"differential equations folded on themselves\" model</h4>\n<p>This is another model Eliezer <a href=\"/lw/wf/hard_takeoff/\">alludes</a> to, albeit in a somewhat handwavey fashion:</p>\n<blockquote>\n<p>When you fold a whole chain of differential equations in on itself like this, it should either peter out rapidly as improvements fail to yield further improvements, or else go FOOM.</p>\n</blockquote>\n<p>It's not exactly clear to me what the \"whole chain of differential equations\" is supposed to refer to... there's only one differential equation in the preceding paragraph, and it's a standard exponential (which could be scary or not, depending on the multiplier in the exponent.&nbsp; Rabbit populations and bank account balances both grow exponentially in a way that's slow enough for humans to understand and control.)</p>\n<p>Maybe he's referring to the levels he describes <a href=\"/lw/w6/recursion_magic/\">here</a>: metacognitive, cognitive, metaknowledge, knowledge, and object.&nbsp; How might we paramaterize this system?</p>\n<p>Let's say <em>c</em> is our AGI's cognition ability, <em>dc/dt</em> is the rate of change in our AGI's cognitive ability, <em>m</em> is our AGI's \"metaknowledge\" (about cognition and metaknowledge), and <em>dm/dt</em> is the rate of change in metaknowledge.&nbsp; What I've got in mind is:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{dc}{dt} = p \\cdot c \\cdot m\" alt=\"\"></p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{dm}{dt} = q \\cdot c \\cdot m\" alt=\"\"></p>\n<p>where p and q are constants.</p>\n<p>In other words, both change in cognitive ability and change in metaknowledge are each individually directly proportionate to both cognitive ability and metaknowledge.</p>\n<p>I don't know much about understanding systems of differential equations, so if you do, please comment!&nbsp; I put the above system in to <a href=\"http://www.wolframalpha.com/input/?i=dc%2Fdt+%3D+p+*+c+*+m+%3B+dm%2Fdt+%3D+q+*+c+*+m\">Wolfram Alpha</a>, but I'm not exactly sure how to interpret the solution provided.&nbsp; In any case, fooling around with <a href=\"https://gist.github.com/johnmaxwelliv/4942090\">this</a> script suggests sudden, extremely sharp takeoff for a variety of different test parameters.</p>\n<h4 id=\"The_straight_exponential_model\">The straight exponential model</h4>\n<p>To me, the \"proportionality thesis\" described by David Chalmers in his <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">singularity paper</a>, \"increases in intelligence (or increases of a certain sort) always lead to proportionate increases in the capacity to design intelligent systems\", suggests a single differential equation that looks like</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{du}{dt} = s \\cdot u\" alt=\"\"></p>\n<p>where <em>u</em> represents the number of upgrades that have been made to an AGI's source code, and <em>s</em> is some constant.&nbsp; The solution to this differential equation is going to look like</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u(t)%20=%20c_%7B1%7De%5E%7Bst%7D\" alt=\"\"></p>\n<p>where the constant <em>c<sub>1</sub></em> is determined by our initial conditions.</p>\n<p>(In <a href=\"/lw/we/recursive_selfimprovement/\">Recursive Self-Improvement</a>, Eliezer calls this a \"too-obvious mathematical idiom\".&nbsp; I'm inclined to favor it for its obviousness, or at least use it as a jumping-off point for further analysis.)</p>\n<p>Under this model, the constant <em>s</em> is pretty important... if <em>u(t)</em> was the amount of money in a bank account, <em>s</em> would be the rate of return it was receiving.&nbsp; The parameter <em>s</em> will effectively determine the \"doubling time\" of an AGI's intelligence.&nbsp; It matters a lot whether this \"doubling time\" is on the scale of minutes or years.</p>\n<p>So what's going to determine <em>s</em>?&nbsp; Well, if the AGI's hardware is twice as fast, we'd expect it to come up with upgrades twice as fast.&nbsp; If the AGI had twice as <em>much</em> hardware, and it could parallelize the search for upgrades perfectly (which seems like a reasonable approximation to me), we'd expect the same thing.&nbsp; So let's decompose <em>s</em> and make it the product of two parameters: <em>h</em> representing the hardware available to the AGI, and <em>r</em> representing the ease of finding additional improvements.&nbsp; The AGI's intelligence will be on the order of <em>u</em> * <em>h</em>, i.e. the product of the AGI's software quality and hardware capability.</p>\n<p>&nbsp;</p>\n<h3 id=\"Considerations_affecting_our_choice_of_model\">Considerations affecting our choice of model</h3>\n<h4 id=\"Diminishing_returns\">Diminishing returns<br></h4>\n<p>The consideration here is that the initial improvements implemented by an AGI will tend to be those that are especially easy to implement and/or especially fruitful to implement, with subsequent improvements tending to deliver less intelligence bang for the implementation buck.&nbsp; Chalmers calls this \"perhaps the most serious structural obstacle\" to the proportionality thesis.</p>\n<p>To think about this consideration, one could imagine representing a given improvement as a pair of two values (<em>u</em>, <em>d</em>).&nbsp; <em>u</em> represents a factor by which existing performance will be multiplied, e.g. if <em>u</em> is 1.1, then implementing this improvement will improve performance by a factor of 1.1.&nbsp; <em>d</em> represents the cognitive difficulty or amount of intellectual labor to required to implement a given improvement.&nbsp; If <em>d</em> is doubled, then at any given level of intelligence, implementing this improvement will take twice as long (because it will be harder to discover and/or harder to translate in to code).</p>\n<p>Now let's imagine ordering our improvements in order from highest to lowest <em>u</em> to <em>d</em> ratio, so we implement those improvements that deliver the greatest bang for the buck first.</p>\n<p>Thus ordered, let's imagine separating groups of consecutive improvements in to \"tiers\".&nbsp; Each tier's worth of improvements, when taken together, will represent the doubling of an AGI's software quality, i.e. the product of the <em>u</em>'s in that cluster will be roughly 2.&nbsp; For a steady doubling time, each tier's total difficulty will need sum to approximately twice the difficulty of the tier before it.&nbsp; If tier difficulty tends to more than double, we're likely to see sub-exponential growth.&nbsp; If tier difficulty tends to less than double, we're likely to see super-exponential growth.&nbsp; If a single improvement delivers a more-than-2x improvement, it will span multiple \"tiers\".</p>\n<p>It seems to me that the quality of fruit available at each tier represents a kind of logical uncertainty, similar to asking whether an efficient algorithm exists for some task, and if so, how efficient.</p>\n<p>On the this diminishing returns consideration, Chalmers writes:</p>\n<blockquote>\n<p>If anything, 10% increases in intelligence-related capacities are likely to lead all sorts of intellectual breakthroughs, leading to next-generation increases in intelligence that are significantly greater than 10%. Even among humans, relatively small differences in design capacities (say, the difference between Turing and an average human) seem to lead to large differences in the systems that are designed (say, the difference between a computer and nothing of importance).</p>\n</blockquote>\n<p>Eliezer Yudkowsky's <a href=\"/lw/we/recursive_selfimprovement/\">objection</a> is similar:</p>\n<blockquote>\n<p>...human intelligence does <em>not</em> require a hundred times as much computing power as chimpanzee intelligence.&nbsp; Human brains are merely three times too large, and our prefrontal cortices six times too large, for a primate with our body size.</p>\n<p>Or again:&nbsp; It does not seem to require 1000 times as many genes to build a human brain as to build a chimpanzee brain, even though human brains can build toys that are a thousand times as neat.</p>\n<p>Why is this important?&nbsp; Because it shows that with <em>constant optimization pressure</em> from natural selection and <em>no intelligent insight,</em> there were <em>no diminishing returns</em> to a search for better brain designs up to at least the human level.&nbsp; There were probably <em>accelerating</em> returns (with a low acceleration factor).&nbsp; There are no <em>visible speedbumps,</em> <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">so far as I know</a>.</p>\n</blockquote>\n<p>First, hunter-gatherers can't design toys that are a thousand times as neat as the ones chimps design--they aren't programmed with the software modern humans get through the education (<a href=\"http://en.wikipedia.org/wiki/Pirah%C3%A3_people#Language\">some may be unable to count</a>), and educating apes has produced <a href=\"http://en.wikipedia.org/wiki/Kanzi\">interesting results</a>.</p>\n<p>Speaking as someone who's basically clueless about neuroscience, I can think of many different factors that might contribute to intelligence differences within the human race or between humans and other apes:</p>\n<ul>\n<li>Processing speed.</li>\n<li>Cubic centimeters brain hardware devoted to abstract thinking.&nbsp; (Gifted technical thinkers often seem to suffer from poor social intuition--perhaps a result of reallocation of brain hardware from social to technical processing.)</li>\n<li>Average number of connections per neuron within that brain hardware.</li>\n<li>Average neuron density within that brain hardware.&nbsp; <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=human%20brain%20scaled%20up%20primate%20brain&amp;source=web&amp;cd=1&amp;ved=0CDQQFjAA&amp;url=http%3A%2F%2Fwww.pnas.org%2Fcontent%2Fearly%2F2012%2F06%2F21%2F1201895109.full.pdf&amp;ei=EwogUcatE8bSigKht4HQCQ&amp;usg=AFQjCNGwnrHKwd3f_WaRXt09LMItlNVKuQ&amp;bvm=bv.42661473,d.cGE&amp;cad=rja\">This author</a> seems to think that a large part of the human brain's remarkableness comes largely from the fact that it's the largest primate brain, and primate brains maintain the same neuron density when enlarged while other types of brains don't.&nbsp; \"If absolute brain size is the best predictor of cognitive abilities in a primate (13), and absolute brain size is proportional to number of neurons across primates (24, 26), our superior cognitive abilities might be accounted for simply by the total number of neurons in our brain, which, based on the similar scaling of neuronal densities in rodents, elephants, and cetaceans, we predict to be the largest of any animal on Earth (28).\"</li>\n<li>Propensity to actually use your capacity for deliberate System 2 reasoning.&nbsp; Richard Feynman's second wife on why she divorced him: \"He begins working calculus problems in his head as soon as he awakens. He did calculus while driving in his car, while sitting in the living room, and while lying in bed at night.\"&nbsp; (By the way, does anyone know of research that's been done on getting people to use System 2 more?&nbsp; Seems like it could be really low-hanging fruit for improving intellectual output.&nbsp; Sometimes I wonder if the reason intelligent people tend to like math is because they were reinforced for the behaviour of thinking abstractly as kids (via praise, good grades, etc.) while those <em>not</em> at the top of the class were <em>not</em> so reinforced.)</li>\n<li><a href=\"http://www.slate.com/articles/health_and_science/science/2013/01/evolution_of_childhood_prolonged_development_helped_homo_sapiens_succeed.html\">Extended neuroplasticity in to \"childhood\"</a>.</li>\n<li>Increased calories to think with due to the invention of cooking.</li>\n<li>And finally, mental algorithms (\"software\").&nbsp; Which are probably at least <a href=\"http://newsroom.ucla.edu/portal/ucla/more-sophisticated-wiring-not-237689.aspx\">somewhat</a> important.</li>\n</ul>\n<p>It seems to me like these factors (or ones like them) may multiply together to produce intelligence, i.e. the \"intelligence equation\", as it were, could be something like intelligence = processing_speed * cc_abstract_hardware * neuron_density * connections_per_neuron * propensity_for_abstraction * mental_algorithms.&nbsp; If the ancestral environment rewarded intelligence, we should expect all of these characteristics to be selected for, and this could explain the \"low acceleration factor\" in human intelligence increase.&nbsp; (Increasing your processing speed by a factor of 1.2 does more when you're already pretty smart, so all these sources of intelligence increase would feed in to one another.)</p>\n<p>In other words, it's not that clear what relevance the evolution of human intelligence has to the ease and quality of the upgrades at different \"tiers\" of software improvements, since evolution operates on many non-software factors, but a self-improving AI (properly boxed) can only improve its software.</p>\n<h4 id=\"Bottlenecks\">Bottlenecks</h4>\n<p>In the Hanson/Yudkowsky debate, Yudkowsky <a href=\"/lw/w8/engelbart_insufficiently_recursive/\">declares</a> Douglas Englebart's plan to radically bootstrap his team's productivity though improving their computer and software tools \"insufficiently recursive\".&nbsp; I agree with this assessment.&nbsp; Here's my modelling of this phenomenon.</p>\n<p>When a programmer makes an improvement to their code, their work of making the improvement requires the completion of many subtasks:</p>\n<ul>\n<li>choosing a feature to add</li>\n<li>reminding themselves of how the relevant part of the code works and <a href=\"http://www.paulgraham.com/head.html\">loading that information in to their memory</a></li>\n<li>identifying ways to implement the feature</li>\n<li>evaluating different methods of implementing the feature according to simplicity, efficiency, and correctness</li>\n<li>coding their chosen implementation</li>\n<li>testing their chosen implementation, identifying bugs</li>\n<li>identifying the cause of a given bug</li>\n<li>figuring out how to fix the given bug</li>\n</ul>\n<p>Each of those subtasks will consist of further subtasks like poking through their code, staring off in to space, typing, and talking to their rubber duck.</p>\n<p>Now the programmer improves their development environment so they can poke through their code slightly faster.&nbsp; But if poking through their code takes up only 5% of their development time, even an extremely large improvement in code-poking abilities is not going to result in an especially large increase in his development speed... in the best case, where code-poking time is reduced to zero, the programmer will only work about 5% faster.</p>\n<p>This is a reflection of <a href=\"http://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's Law</a>-type thinking.&nbsp; The amount you can gain through speeding something up depends on how much it's slowing you down.</p>\n<p>Relatedly, if intelligence is a complicated, heterogeneous process where computation is spread relatively evenly among many modules, then improving the performance of an AGI gets tougher, because upgrading an individual module does little to improve the performance of the system as a whole.</p>\n<p>And to see orders-of-magnitude performance improvement in such a process, almost <em>all</em> of your AGI's components will need to be improved radically.&nbsp; If even a few prove troublesome, improving your AGI's thinking speed becomes difficult.</p>\n<p>&nbsp;</p>\n<h3 id=\"Case_studies_in_technological_development_speed\">Case studies in technological development speed<br></h3>\n<h4 id=\"Moore_s_Law\">Moore's Law</h4>\n<blockquote>\n<p>It has famously been noted that if the automotive industry had achieved similar improvements in performance [to the semiconductor industry] in the last 30 years, a Rolls-Royce would cost only $40 and could circle the globe eight times on one gallon of gas\u2014with a top speed of 2.4 million miles per hour.</p>\n</blockquote>\n<p>From <a href=\"http://www.mckinsey.com/client_service/semiconductors/latest_thinking/creating_value_in_the_semiconductor_industry\">this McKinsey report</a>.&nbsp; So Moore's Law is an outlier where technological development is concerned.&nbsp; I suspect that making transistors smaller and faster doesn't require finding ways to improve dozens of heterogeneous components.&nbsp; And when you zoom out to view a computer system as a whole, other bottlenecks typically <a href=\"http://en.wikipedia.org/wiki/Moore%27s_law#Importance_of_non-CPU_bottlenecks\">appear</a>.</p>\n<p>(It's also worth noting that research budgets in the semiconductor field have also risen greatly in the semiconductor industry since its inception, but obviously not following the same curve that chip speeds have.)</p>\n<h4 id=\"Compiler_technology\">Compiler technology</h4>\n<p><a href=\"http://www.cs.virginia.edu/~techrep/CS-2001-12.pdf\">This paper</a> on \"Proebstig's Law\" suggests that the end result of all the compiler research done between 1970 or so and 2001 was that a typical integer-intensive program was compiled to run 3.3 times faster, and a typical floating-point-intensive program was compiled to run 8.1 times faster.&nbsp; When it comes to making programs run quickly, it seems that software-level compiler improvements are swamped by hardware-level chip improvements--perhaps because, like an AGI, a compiler has to deal with a huge variety of different scenarios, so improving it in the average case is tough.&nbsp; (This represents supertask heterogeneity, rather than subtask heterogeneity, so it's a different objection than the one mentioned above.)</p>\n<h4 id=\"Database_technology\">Database technology</h4>\n<p>According to <a href=\"http://arxiv.org/pdf/cs/0701162.pdf\">two</a> <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-642-18206-8_9\">analyses</a> (<a href=\"http://www.sendspace.com/file/2ly1x8\">full paper</a> for that second one), it seems that improvement in database performance benchmarks has largely been due to Moore's Law.</p>\n<h4 id=\"AI__so_far_\">AI (so far)<br></h4>\n<p>Robin Hanson's blog post \"<a href=\"http://www.overcomingbias.com/2012/08/ai-progress-estimate.html\">AI Progress Estimate</a>\" was the best resource I could find on this.</p>\n<p>&nbsp;</p>\n<h3 id=\"Why_smooth_exponential_growth_implies_soft_takeoff\">Why smooth exponential growth implies soft takeoff<br></h3>\n<p>Let's suppose we consider all of the above, deciding that the exponential model is the best, and we agree with <a href=\"http://www.overcomingbias.com/2011/07/debating-yudkowsky.html\">Robin Hanson</a> that there are few deep, chunky, undiscovered AI insights.</p>\n<p>Under the straight exponential model, if you recall, we had</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{du}{dh} = u \\cdot h \\cdot r\" alt=\"\" width=\"101\" height=\"38\"></p>\n<p>where <em>u</em> is the degree of software quality, <em>h</em> is the hardware availability, and <em>r</em> is a parameter representing the difficulty of doing additional upgrades.&nbsp; Our AGI's overall intelligence is given by<em> u</em> * <em>h</em>--the quality of the software times the amount of hardware.</p>\n<p>Now we can solve for <em>r</em> by substituting in human intelligence for <em>u</em> * <em>h</em>, and substituting in the rate of human AI progress for <em>du</em>/<em>dt</em>.&nbsp; Another way of saying this is: When the AI is as smart as all the world's AI researchers working together, it will produce new AI insights at the rate that all the world's AI researchers working together produce new insights.&nbsp; <strong>At some point our AGI will be just as smart as the world's AI researchers, but we can hardly expect to start seeing super-fast AI progress at that point, because the world's AI researchers haven't produced super-fast AI progress.</strong></p>\n<p>Let's assume AGI that's on par with the world AI research community is reached in 2080 (<a href=\"/lw/8p4/2011_survey_results/\">LW's median \"singularity\" estimate in 2011</a>).&nbsp; We'll pretend AI research has only been going on since 2000, meaning 80 \"standard research years\" of progress have gone in to the AGI's software.&nbsp; So at the moment our shiny new AGI is fired up, <em>u</em> = 80, and it's doing research at the rate of one \"human AGI community research year\" per year, so <em>du</em>/<em>dt</em> = 1.&nbsp; That's an effective rate of return on AI software progress of 1 / 80 = 1.3%, giving a software quality doubling time of around 58 years.</p>\n<p>You could also apply this kind of thinking to individual AI projects.&nbsp; For example, it's possible that at some point <a href=\"http://wiki.lesswrong.com/wiki/EURISKO\">EURISKO</a> was improving itself about as fast as Doug Lenat was improving it.&nbsp; You might be able to do a similar calculation to take a stab at EURISKO's insight level doubling time.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_importance_of_hardware\">The importance of hardware<br></h2>\n<p>According to my model, you double your AGI's intelligence, and thereby the speed with which your AGI improves itself, by doubling the hardware available for your AGI.&nbsp; So if you had an AGI that was interesting, you could make it 4x as smart by giving it 4x the hardware.&nbsp; If an AGI that was 4x as smart could get you 4x as much money (through impressing investors, or playing the stock market, or monopolizing additional industries), that'd be a nice feedback loop.&nbsp; For maximum explosivity, put half your AGI's mind to the task of improving its software, and the other half to the task of making more money with which to buy more hardware.</p>\n<p>But it seems pretty straightforward to prevent a non-superintelligent AI from gaining access to additional hardware with careful planning.&nbsp; (Note: One problem with AI boxing experiments thus far is that all of the AIs have been played by human beings.&nbsp; Human beings have innate understanding of human psychology and possess specialized capabilities for running emulations of one another.&nbsp; It seems pretty easy to prevent an AGI from acquiring such understanding.&nbsp; But there may exist box-breaking techniques that don't rely on understanding human psychology.&nbsp; Another note about boxing: FAI requires getting everything perfect, which is a conjunctive calculation.&nbsp; Given multiple safeguards, only one has to work for the box as a whole to work, which is a disjunctive calculation.)</p>\n<p>&nbsp;</p>\n<h3 id=\"AGI_s_impact_on_the_economy\">AGI's impact on the economy</h3>\n<p>Is it possible that the first group to create a successful AGI might begin monopolizing different sections of the economy?&nbsp; Robin Hanson argues that technology insights typically leak between different companies, due to conferences and employee poaching.&nbsp; But we can't be confident these factors would affect the research an AGI does on itself.&nbsp; And if an AGI is still dumb enough that a significant portion of its software upgrades are coming from human researchers, it can hardly be considered superintelligent.</p>\n<p>Given what looks like a winner-take-all dynamic, an important factor may be the number of serious AGI competitors.&nbsp; If there are only two, the #1 company may not wish to trade insights with the #2 company for fear of losing its lead.&nbsp; If there are more than two, all but the leading company might ally against the leading company in trading insights.&nbsp; If their alliance is significantly stronger than the leading company, perhaps the leading company would wish to join their alliance.</p>\n<p>But if AI is about getting lots of details right, as Hanson <a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">suggests</a>, improvements may not even transfer between different AI architectures.</p>\n<p>&nbsp;</p>\n<h3 id=\"What_should_we_do_\">What should we do?<br></h3>\n<p>I've argued that soft takeoff is a strong possibility.&nbsp; Should that change our strategy as people concerned with x-risk?</p>\n<p>If we are <a href=\"/lw/g93/evaluating_the_feasibility_of_sis_plan/\">basically screwed</a> in the event that hard takeoff is possible, it may be that preparing for a soft takeoff is a better use of resources on the margin.&nbsp; Shane Legg has <a href=\"http://www.vetta.org/2009/08/funding-safe-agi/\">proposed</a> that people concerned with friendliness become investors in AGI projects so they can affect the outcome of any that seem to be succeeding.</p>\n<p>&nbsp;</p>\n<h3 id=\"Concluding_thoughts\">Concluding thoughts</h3>\n<p>Expert forecasts are famously unreliable even in the relatively well-understood field of political forecasting.&nbsp; So <strong>given the number of unknowns involved in the emergence of smarter-than-human intelligence, it's hard to say much with certainty</strong>.&nbsp; Picture a few Greek scholars speculating on the industrial revolution.</p>\n<p>I don't have a strong background in these topics, so I fully expect that the above essay will reveal my ignorance, which I'd appreciate your pointing out in the comments.&nbsp; This essay should be taken as at attempt to <a href=\"/lw/8ns/hack_away_at_the_edges/\">hack away at the edges</a>, not come to definitive conclusions.&nbsp; As always, I reserve the right to change my mind about anything ;)</p>\n<ul>\n</ul>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 2}, {"title": "Takeoff models discussed in the Hanson-Yudkowsky debate", "anchor": "Takeoff_models_discussed_in_the_Hanson_Yudkowsky_debate", "level": 2}, {"title": "The supercritical nuclear chain reaction model", "anchor": "The_supercritical_nuclear_chain_reaction_model", "level": 3}, {"title": "The \"differential equations folded on themselves\" model", "anchor": "The__differential_equations_folded_on_themselves__model", "level": 3}, {"title": "The straight exponential model", "anchor": "The_straight_exponential_model", "level": 3}, {"title": "Considerations affecting our choice of model", "anchor": "Considerations_affecting_our_choice_of_model", "level": 2}, {"title": "Diminishing returns", "anchor": "Diminishing_returns", "level": 3}, {"title": "Bottlenecks", "anchor": "Bottlenecks", "level": 3}, {"title": "Case studies in technological development speed", "anchor": "Case_studies_in_technological_development_speed", "level": 2}, {"title": "Moore's Law", "anchor": "Moore_s_Law", "level": 3}, {"title": "Compiler technology", "anchor": "Compiler_technology", "level": 3}, {"title": "Database technology", "anchor": "Database_technology", "level": 3}, {"title": "AI (so far)", "anchor": "AI__so_far_", "level": 3}, {"title": "Why smooth exponential growth implies soft takeoff", "anchor": "Why_smooth_exponential_growth_implies_soft_takeoff", "level": 2}, {"title": "The importance of hardware", "anchor": "The_importance_of_hardware", "level": 1}, {"title": "AGI's impact on the economy", "anchor": "AGI_s_impact_on_the_economy", "level": 2}, {"title": "What should we do?", "anchor": "What_should_we_do_", "level": 2}, {"title": "Concluding thoughts", "anchor": "Concluding_thoughts", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "81 comments"}], "headingsCount": 20}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dq3KsCsqNotWc8nAK", "h2QdnJBbSwEE36jc9", "tjH8XPxAnr6JRbh7k", "rJLviHqJMTy8WQkow", "JBadX7rwdcRFzGuju", "vNBxmcHpnozjrJnJP", "NCb28Xdv7xDajtqtS", "HAEPbGaMygJq8L59k", "5evRqMmGxTKf98pvT", "6bSHiD9TxsJwe2WqT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T14:20:33.697Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C8Dnw756beR38nqn8/meetup-melbourne-practical-rationality-3", "pageUrlRelative": "/posts/C8Dnw756beR38nqn8/meetup-melbourne-practical-rationality-3", "linkUrl": "https://www.lesswrong.com/posts/C8Dnw756beR38nqn8/meetup-melbourne-practical-rationality-3", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8Dnw756beR38nqn8%2Fmeetup-melbourne-practical-rationality-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8Dnw756beR38nqn8%2Fmeetup-melbourne-practical-rationality-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8Dnw756beR38nqn8%2Fmeetup-melbourne-practical-rationality-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ks'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 April 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ks'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C8Dnw756beR38nqn8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1479201919522881e-06, "legacy": true, "legacyId": "22096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/ks\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 April 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/ks\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T16:37:36.197Z", "modifiedAt": null, "url": null, "title": "Reflection in Probabilistic Logic", "slug": "reflection-in-probabilistic-logic", "viewCount": null, "lastCommentedAt": "2021-09-10T20:42:05.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic", "pageUrlRelative": "/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic", "linkUrl": "https://www.lesswrong.com/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reflection%20in%20Probabilistic%20Logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReflection%20in%20Probabilistic%20Logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduAkuSqJhGDcfMaTA%2Freflection-in-probabilistic-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reflection%20in%20Probabilistic%20Logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduAkuSqJhGDcfMaTA%2Freflection-in-probabilistic-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduAkuSqJhGDcfMaTA%2Freflection-in-probabilistic-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 751, "htmlBody": "<p>Paul Christiano has devised&nbsp;<a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\"><strong>a new fundamental approach</strong></a>&nbsp;to the \"<a href=\"https://www.youtube.com/watch?v=MwriJqBZyoM\">L&ouml;b Problem</a>\" wherein <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">L&ouml;b's Theorem</a> seems to pose an obstacle to AIs building successor AIs, or adopting successor versions of their own code, that trust the same amount of mathematics as the original. &nbsp;(I am currently writing up a more thorough description of the <em>question </em>this preliminary technical report is working on answering. &nbsp;For now the main online description is in a&nbsp;<a href=\"https://www.youtube.com/watch?v=MwriJqBZyoM\">quick Summit talk</a>&nbsp;I gave. &nbsp;See also Benja Fallenstein's description of the problem in the course of presenting a&nbsp;<a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/\">different angle of attack</a>. &nbsp;Roughly the problem is that mathematical systems can only prove the soundness of, aka 'trust', weaker mathematical systems. &nbsp;If you try to write out an exact description of how AIs would build their successors or successor versions of their code in the most obvious way, it looks like the mathematical strength of the proof system would tend to be stepped down each time, which is undesirable.)</p>\n<p>Paul Christiano's approach is inspired by the idea that whereof one cannot prove or disprove, thereof one must assign probabilities: and that although no mathematical system can contain its own&nbsp;<em>truth</em>&nbsp;predicate, a mathematical system might be able to contain a reflectively consistent&nbsp;<em>probability</em>&nbsp;predicate. &nbsp;In particular, it looks like we can have:</p>\n<p>&forall;a, b:<span style=\"white-space: pre;\"> </span>(a &lt; P(&phi;) &lt; b) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&rArr; &nbsp;P(a &lt; P('&phi;') &lt; b) = 1<br />&forall;a, b:<span style=\"white-space: pre;\"> </span>P(a &le; P('&phi;')&nbsp;&le;&nbsp;b) &gt; 0 &nbsp;&rArr; &nbsp;a&nbsp;&le;&nbsp;P(&phi;)&nbsp;&le;&nbsp;b</p>\n<p>Suppose I present you with the human and probabilistic version of a G&ouml;del sentence, the&nbsp;<a href=\"http://books.google.com/books?id=cmX8yyBfP74C&amp;pg=PA317&amp;lpg=PA317&amp;dq=whitely+lucas+cannot+consistently&amp;source=bl&amp;ots=68tuximFfI&amp;sig=GdZro1wy6g_KzO-PXInGTKFrU7Q&amp;hl=en&amp;sa=X&amp;ei=7-FMUb61LojRiAK9hIGQDw&amp;ved=0CGoQ6AEwBg#v=onepage&amp;q=whitely%20lucas%20cannot%20consistently&amp;f=false\">Whitely sentence</a>&nbsp;\"You assign this statement a probability less than 30%.\" &nbsp;If you disbelieve this statement, it is true. &nbsp;If you believe it, it is false. &nbsp;If you assign 30% probability to it, it is false. &nbsp;If you assign 29% probability to it, it is true.</p>\n<p>Paul's approach resolves this problem by restricting your belief about your own probability assignment to within epsilon of 30% for any epsilon. &nbsp;So Paul's approach replies, \"Well, I assign&nbsp;<em>almost</em>&nbsp;exactly 30% probability to that statement - maybe a little more, maybe a little less - in fact I think there's about a 30% chance that I'm a tiny bit under 0.3 probability and a 70% chance that I'm a tiny bit over 0.3 probability.\" &nbsp;A standard fixed-point theorem then implies that a consistent assignment like this should exist. &nbsp;If asked if the probability is over 0.2999 or under 0.30001 you will reply with a definite yes.<a id=\"more\"></a></p>\n<p>We haven't yet worked out a walkthrough showing if/how this solves the L&ouml;b obstacle to self-modification, and the probabilistic theory itself is nonconstructive (we've shown that something like this should exist, but not how to compute it). &nbsp;Even so, a possible fundamental triumph over Tarski's theorem on the undefinability of truth and a number of standard G&ouml;delian limitations is important news as math&nbsp;<em>qua</em>&nbsp;math, though work here is still in very preliminary stages. &nbsp;There are even whispers of unrestricted comprehension in a probabilistic version of set theory with&nbsp;&forall;&phi;: &exist;S: P(x &isin; S) = P(&phi;(x)), though this part is not in the preliminary report and is at even earlier stages and could easily not work out at all.</p>\n<p>It seems important to remark on how this result was developed: &nbsp;Paul Christiano showed up with the idea (of consistent probabilistic reflection via a fixed-point theorem) to a week-long \"math squad\" (aka MIRI Workshop) with Marcello Herreshoff, Mihaly Barasz, and myself; then we all spent the next week proving that version after version of Paul's idea couldn't work or wouldn't yield self-modifying AI; until finally, a day after the workshop was supposed to end, it produced something that looked like it might work. &nbsp;If we hadn't been trying to <em>solve </em>this problem (with hope stemming from how it seemed like the sort of thing a reflective rational agent ought&nbsp;to be able to do somehow), this would be just another batch of impossibility results in the math literature. &nbsp;I remark on this because it may help demonstrate that Friendly AI is a productive approach to math&nbsp;<em>qua&nbsp;</em>math, which may aid some mathematician in becoming interested.</p>\n<p>I further note that this does not mean the L&ouml;bian obstacle is resolved and no further work is required. &nbsp;Before we can conclude that we need a computably specified version of the theory plus a walkthrough for a self-modifying agent using it.</p>\n<p>See also the&nbsp;<a href=\"http://intelligence.org/2013/03/22/early-draft-of-naturalistic-reflection-paper/\">blog post</a>&nbsp;on the MIRI site (and subscribe to MIRI's newsletter&nbsp;<a href=\"http://intelligence.org/\">here</a>&nbsp;to keep abreast of research updates).</p>\n<p>This LW post is the preferred place for feedback on the <a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\">paper</a>.</p>\n<p>EDIT: &nbsp;But see discussion on a Google+ post by John Baez <a href=\"https://plus.google.com/117663015413546257905/posts/jJModdTJ2R3?hl=en\">here</a>. &nbsp;Also see&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting#Using_LaTeX_to_render_mathematics\">here</a>&nbsp;for how to display math LaTeX in comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "6nS8oYmSMuFMaiowF": 1, "wBoHTJs9iQzczNtW3": 1, "NrvXXL3iGjjxu5B7d": 1, "6wzZZfW87aKGQ7Fwr": 2, "JHYaBGQuuKHdwnrAK": 2, "AJDHQ4mFnsNbBzPhT": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "duAkuSqJhGDcfMaTA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 69, "baseScore": 108, "extendedScore": null, "score": 0.00025, "legacy": true, "legacyId": "22088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 108, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 172, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ALCnqX6Xx8bpFMZq3", "DDJr5fuR5jeD47k9g"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 16, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-03-24T16:37:36.197Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T21:19:23.724Z", "modifiedAt": null, "url": null, "title": "[LINK] The power of fiction for moral instruction", "slug": "link-the-power-of-fiction-for-moral-instruction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RTXEWrSWLDqReRiKk/link-the-power-of-fiction-for-moral-instruction", "pageUrlRelative": "/posts/RTXEWrSWLDqReRiKk/link-the-power-of-fiction-for-moral-instruction", "linkUrl": "https://www.lesswrong.com/posts/RTXEWrSWLDqReRiKk/link-the-power-of-fiction-for-moral-instruction", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20power%20of%20fiction%20for%20moral%20instruction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20power%20of%20fiction%20for%20moral%20instruction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTXEWrSWLDqReRiKk%2Flink-the-power-of-fiction-for-moral-instruction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20power%20of%20fiction%20for%20moral%20instruction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTXEWrSWLDqReRiKk%2Flink-the-power-of-fiction-for-moral-instruction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTXEWrSWLDqReRiKk%2Flink-the-power-of-fiction-for-moral-instruction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 234, "htmlBody": "<p>From Medical Daily: <a href=\"http://www.medicaldaily.com/articles/9878/20120514/book-reading-experience-taking-psychology-character-fiction.htm\">Psychologists Discover How People Subconsciously Become Their Favorite Fictional Characters</a></p>\n<p style=\"padding-left: 30px;\">Psychologists have discovered that while reading a book or story, people are prone to subconsciously adopt their behavior, thoughts, beliefs and internal responses to that of fictional characters as if they were their own.</p>\n<p style=\"padding-left: 30px;\">Experts have dubbed this subconscious phenomenon &lsquo;experience-taking,&rsquo; where people actually change their own behaviors and thoughts to match those of a fictional character that they can identify with.</p>\n<p style=\"padding-left: 30px;\">Researcher from the Ohio State University conducted a series of six different experiments on about 500 participants, reporting in the Journal of Personality and Social Psychology, found that in the right situations, &lsquo;experience-taking,&rsquo; may lead to temporary real world changes in the lives of readers.&nbsp;</p>\n<p style=\"padding-left: 30px;\">They found that stories written in the first-person can temporarily transform the way readers view the world, themselves and other social groups.&nbsp;</p>\n<p>I always wondered at how Christopher Hitchens (who, when he wasn't being a columnist, was a professor of English literature) went on and on about the power of fiction for revealing moral truths. This gives me a better idea of how people could imprint on well-written fiction. More so than, say, logically-reasoned philosophical tracts.</p>\n<p>This article is, of course, a popularisation. Anyone have links to the original paper?</p>\n<p><strong>Edit:</strong> Gwern <a href=\"http://www.tiltfactor.org/wp-content/uploads2/Kaufman_Libby2012_JPSPadvanceonlinepublication.pdf\">delivers</a> (PDF): Kaufman, G. F., &amp; Libby, L. K. (2012, March 26). \"Changing Beliefs and Behavior Through Experience-Taking.\" <em>Journal of Personality and Social Psychology</em>. Advance online publication. doi: 10.1037/a0027525</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RTXEWrSWLDqReRiKk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 21, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "22097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-24T23:55:58.929Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 31st March", "slug": "meetup-london-meetup-31st-march", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YhATpRLuwiZhZiYy9/meetup-london-meetup-31st-march", "pageUrlRelative": "/posts/YhATpRLuwiZhZiYy9/meetup-london-meetup-31st-march", "linkUrl": "https://www.lesswrong.com/posts/YhATpRLuwiZhZiYy9/meetup-london-meetup-31st-march", "postedAtFormatted": "Sunday, March 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2031st%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2031st%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhATpRLuwiZhZiYy9%2Fmeetup-london-meetup-31st-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2031st%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhATpRLuwiZhZiYy9%2Fmeetup-london-meetup-31st-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhATpRLuwiZhZiYy9%2Fmeetup-london-meetup-31st-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kt'>London Meetup, 31st March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 March 2013 01:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A usually-fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station.</p>\n\n<p>We're switching from every other Sunday to every other-other Sunday, so this is two weeks in a row, and the next meetup will be two weeks afterwards (14th April).</p>\n\n<p>Everyone is welcome to attend: we're a friendly group and we don't bite. If you're on the fence about coming, err on the side of showing up. It's probably safe to assume that we'd like to meet you.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kt'>London Meetup, 31st March</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YhATpRLuwiZhZiYy9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "22099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__31st_March\">Discussion article for the meetup : <a href=\"/meetups/kt\">London Meetup, 31st March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 March 2013 01:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A usually-fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station.</p>\n\n<p>We're switching from every other Sunday to every other-other Sunday, so this is two weeks in a row, and the next meetup will be two weeks afterwards (14th April).</p>\n\n<p>Everyone is welcome to attend: we're a friendly group and we don't bite. If you're on the fence about coming, err on the side of showing up. It's probably safe to assume that we'd like to meet you.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__31st_March1\">Discussion article for the meetup : <a href=\"/meetups/kt\">London Meetup, 31st March</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 31st March", "anchor": "Discussion_article_for_the_meetup___London_Meetup__31st_March", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 31st March", "anchor": "Discussion_article_for_the_meetup___London_Meetup__31st_March1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-25T11:54:22.153Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Schools Proliferating Without Evidence", "slug": "seq-rerun-schools-proliferating-without-evidence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BZPmjpf7WLzthja6Q/seq-rerun-schools-proliferating-without-evidence", "pageUrlRelative": "/posts/BZPmjpf7WLzthja6Q/seq-rerun-schools-proliferating-without-evidence", "linkUrl": "https://www.lesswrong.com/posts/BZPmjpf7WLzthja6Q/seq-rerun-schools-proliferating-without-evidence", "postedAtFormatted": "Monday, March 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Schools%20Proliferating%20Without%20Evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Schools%20Proliferating%20Without%20Evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZPmjpf7WLzthja6Q%2Fseq-rerun-schools-proliferating-without-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Schools%20Proliferating%20Without%20Evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZPmjpf7WLzthja6Q%2Fseq-rerun-schools-proliferating-without-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZPmjpf7WLzthja6Q%2Fseq-rerun-schools-proliferating-without-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>Today's post, <a href=\"/lw/2j/schools_proliferating_without_evidence/\">Schools Proliferating Without Evidence</a> was originally published on 15 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The branching schools of \"psychotherapy\", another domain in which experimental verification was weak (nonexistent, actually), show that an aspiring craft lives or dies by the degree to which it can be tested in the real world. In the absence of that testing, one becomes prestigious by inventing yet another school and having students, rather than excelling at any visible performance criterion. The field of hedonic psychology (happiness studies) began, to some extent, with the realization that you could <em>measure </em>happiness - that there was a family of measures that by golly did validate well against each other. The act of creating a new measurement creates new science; if it's a <em>good </em>measurement, you get good science.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h1q/seq_rerun_epistemic_viciousness/\">Epistemic Viciousness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BZPmjpf7WLzthja6Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1487882826587455e-06, "legacy": true, "legacyId": "22107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JnKCaGcgZL4Rsep8m", "sXCNCCgD5DC7evZ5i", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T00:23:52.414Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Social meetup+ Birthday party!", "slug": "meetup-washington-dc-social-meetup-birthday-party", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yJihBzoHMqaKS5E2D/meetup-washington-dc-social-meetup-birthday-party", "pageUrlRelative": "/posts/yJihBzoHMqaKS5E2D/meetup-washington-dc-social-meetup-birthday-party", "linkUrl": "https://www.lesswrong.com/posts/yJihBzoHMqaKS5E2D/meetup-washington-dc-social-meetup-birthday-party", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Social%20meetup%2B%20Birthday%20party!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Social%20meetup%2B%20Birthday%20party!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJihBzoHMqaKS5E2D%2Fmeetup-washington-dc-social-meetup-birthday-party%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Social%20meetup%2B%20Birthday%20party!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJihBzoHMqaKS5E2D%2Fmeetup-washington-dc-social-meetup-birthday-party", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJihBzoHMqaKS5E2D%2Fmeetup-washington-dc-social-meetup-birthday-party", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ku'>Washington DC Social meetup+ Birthday party!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 March 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, next meetup coincides with Eileen's birthday party. We'll be meeting to hang out and eat cake!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ku'>Washington DC Social meetup+ Birthday party!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yJihBzoHMqaKS5E2D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.14929170024419e-06, "legacy": true, "legacyId": "22110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_meetup__Birthday_party_\">Discussion article for the meetup : <a href=\"/meetups/ku\">Washington DC Social meetup+ Birthday party!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 March 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, next meetup coincides with Eileen's birthday party. We'll be meeting to hang out and eat cake!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_meetup__Birthday_party_1\">Discussion article for the meetup : <a href=\"/meetups/ku\">Washington DC Social meetup+ Birthday party!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Social meetup+ Birthday party!", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_meetup__Birthday_party_", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Social meetup+ Birthday party!", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_meetup__Birthday_party_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T01:05:08.863Z", "modifiedAt": null, "url": null, "title": "Meetup : Tucson LW Meetup 0", "slug": "meetup-tucson-lw-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "NzwpJ8hz3sA6nLF6G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XXStNTLDXpT5Y8t2S/meetup-tucson-lw-meetup-0", "pageUrlRelative": "/posts/XXStNTLDXpT5Y8t2S/meetup-tucson-lw-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/XXStNTLDXpT5Y8t2S/meetup-tucson-lw-meetup-0", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tucson%20LW%20Meetup%200&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tucson%20LW%20Meetup%200%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXStNTLDXpT5Y8t2S%2Fmeetup-tucson-lw-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tucson%20LW%20Meetup%200%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXStNTLDXpT5Y8t2S%2Fmeetup-tucson-lw-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXStNTLDXpT5Y8t2S%2Fmeetup-tucson-lw-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kv'>Tucson LW Meetup 0</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">943 E University Blvd  Tucson</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at Caffe Luce(943 E University Blvd #191), a coffee shop off of University Ave., near the U of A. I'll have a sign. \nThis is a first meetup, so we'll talk about basic rationality and goals: where we are and where we want to go(and brainstorm how to get there).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kv'>Tucson LW Meetup 0</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XXStNTLDXpT5Y8t2S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.149319434060737e-06, "legacy": true, "legacyId": "22111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tucson_LW_Meetup_0\">Discussion article for the meetup : <a href=\"/meetups/kv\">Tucson LW Meetup 0</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 March 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">943 E University Blvd  Tucson</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at Caffe Luce(943 E University Blvd #191), a coffee shop off of University Ave., near the U of A. I'll have a sign. \nThis is a first meetup, so we'll talk about basic rationality and goals: where we are and where we want to go(and brainstorm how to get there).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tucson_LW_Meetup_01\">Discussion article for the meetup : <a href=\"/meetups/kv\">Tucson LW Meetup 0</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tucson LW Meetup 0", "anchor": "Discussion_article_for_the_meetup___Tucson_LW_Meetup_0", "level": 1}, {"title": "Discussion article for the meetup : Tucson LW Meetup 0", "anchor": "Discussion_article_for_the_meetup___Tucson_LW_Meetup_01", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T02:31:46.908Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] 3 Levels of Rationality Verification", "slug": "seq-rerun-3-levels-of-rationality-verification", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/STBXsfmYJ2ErbGtDK/seq-rerun-3-levels-of-rationality-verification", "pageUrlRelative": "/posts/STBXsfmYJ2ErbGtDK/seq-rerun-3-levels-of-rationality-verification", "linkUrl": "https://www.lesswrong.com/posts/STBXsfmYJ2ErbGtDK/seq-rerun-3-levels-of-rationality-verification", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%203%20Levels%20of%20Rationality%20Verification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%203%20Levels%20of%20Rationality%20Verification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTBXsfmYJ2ErbGtDK%2Fseq-rerun-3-levels-of-rationality-verification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%203%20Levels%20of%20Rationality%20Verification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTBXsfmYJ2ErbGtDK%2Fseq-rerun-3-levels-of-rationality-verification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTBXsfmYJ2ErbGtDK%2Fseq-rerun-3-levels-of-rationality-verification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<p>Today's post, <a href=\"/lw/2s/3_levels_of_rationality_verification/\">3 Levels of Rationality Verification</a> was originally published on 15 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#3_Levels_of_Rationality_Verification\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How far the craft of rationality can be taken, depends largely on what methods can be invented for verifying it. Tests seem usefully stratifiable into reputational, experimental, and organizational. A \"reputational\" test is some real-world problem that tests the ability of a teacher or a school (like running a hedge fund, say) - \"keeping it real\", but without being able to break down exactly what was responsible for success. An \"experimental\" test is one that can be run on each of a hundred students (such as a well-validated survey). An \"organizational\" test is one that can be used to preserve the integrity of organizations by validating individuals or small groups, even in the face of strong incentives to game the test. The strength of solution invented at each level will determine how far the craft of rationality can go in the real world.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h23/seq_rerun_schools_proliferating_without_evidence/\">Schools Proliferating Without Evidence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "STBXsfmYJ2ErbGtDK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.1493776509660228e-06, "legacy": true, "legacyId": "22112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5K7CMa6dEL7TN7sae", "BZPmjpf7WLzthja6Q", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T04:44:17.916Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Probabilistic Graphical Models", "slug": "meetup-west-la-meetup-probabilistic-graphical-models", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vdp3HuPJeJYPDY8uF/meetup-west-la-meetup-probabilistic-graphical-models", "pageUrlRelative": "/posts/vdp3HuPJeJYPDY8uF/meetup-west-la-meetup-probabilistic-graphical-models", "linkUrl": "https://www.lesswrong.com/posts/vdp3HuPJeJYPDY8uF/meetup-west-la-meetup-probabilistic-graphical-models", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Probabilistic%20Graphical%20Models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Probabilistic%20Graphical%20Models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvdp3HuPJeJYPDY8uF%2Fmeetup-west-la-meetup-probabilistic-graphical-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Probabilistic%20Graphical%20Models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvdp3HuPJeJYPDY8uF%2Fmeetup-west-la-meetup-probabilistic-graphical-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvdp3HuPJeJYPDY8uF%2Fmeetup-west-la-meetup-probabilistic-graphical-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kw'>West LA Meetup - Probabilistic Graphical Models</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 March 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, March 27th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> Graphs can make understanding causality very intuitive and easy. They are also a powerful tool for doing more complicated modeling. I will introduce PGMs as a concept, and show a few examples where they can be useful.</p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kw'>West LA Meetup - Probabilistic Graphical Models</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vdp3HuPJeJYPDY8uF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1494667105341298e-06, "legacy": true, "legacyId": "22113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Probabilistic_Graphical_Models\">Discussion article for the meetup : <a href=\"/meetups/kw\">West LA Meetup - Probabilistic Graphical Models</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 March 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, March 27th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Lecture/Discussion:</strong> Graphs can make understanding causality very intuitive and easy. They are also a powerful tool for doing more complicated modeling. I will introduce PGMs as a concept, and show a few examples where they can be useful.</p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Probabilistic_Graphical_Models1\">Discussion article for the meetup : <a href=\"/meetups/kw\">West LA Meetup - Probabilistic Graphical Models</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Probabilistic Graphical Models", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Probabilistic_Graphical_Models", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Probabilistic Graphical Models", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Probabilistic_Graphical_Models1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T04:47:38.411Z", "modifiedAt": null, "url": null, "title": "The cup-holder paradox", "slug": "the-cup-holder-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:05.459Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yrx44uF8aBdKuwi5G/the-cup-holder-paradox", "pageUrlRelative": "/posts/Yrx44uF8aBdKuwi5G/the-cup-holder-paradox", "linkUrl": "https://www.lesswrong.com/posts/Yrx44uF8aBdKuwi5G/the-cup-holder-paradox", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20cup-holder%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20cup-holder%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrx44uF8aBdKuwi5G%2Fthe-cup-holder-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20cup-holder%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrx44uF8aBdKuwi5G%2Fthe-cup-holder-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrx44uF8aBdKuwi5G%2Fthe-cup-holder-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 636, "htmlBody": "<p>I'm shopping for a car, and I've spent many hours this past month reading user reviews of cars. There are seven things American car buyers have cared and complained about consistently for at least the past ten years.&nbsp; In roughly decreasing importance:</p>\n<ul>\n<li>Performance</li>\n<li>Gas mileage</li>\n<li>Frequency and expense of repairs</li>\n<li>Smoothness of ride</li>\n<li>Exterior and interior styling</li>\n<li>Cup-holders</li>\n<li>Cargo space</li>\n</ul>\n<p>Six of these things are complicated design trade-offs. For a good design, increasing any one of them makes most of the other five take a hit.</p>\n<p>Cup-holders are not a complicated design trade-off. This should be a solved problem: Put two large, sturdy cup-holders somewhere accessible from the driver's seat. There is nothing to be gained from saving a few centimeters on cup-holder space that could be worth the millions of buyers who will walk away from a $50,000 car because they don't like its cup-holders.</p>\n<p>Seriously, build the cup-holders first and design the rest of the interior around them. They're that important.</p>\n<p>In the 1970s, no one had cup-holders or knew that they needed them. Things began changing in the 1980s, perhaps due to the expansion of Starbucks, perhaps due to the sudden increase in commute lengths. Today I like to have at least two and preferably three drinks with me for my 1-hour morning commute: A hot coffee to wake up, cold water for when I burn myself with the coffee, and a soda or tea for variety.</p>\n<p>But car manufacturers were glacially slow to respond. I've been looking at used Jaguar XJs. These cars originally cost about $100,000 in today's money. Their owners complained continually about the cheap tiny plastic folding cup-holders that couldn't hold cups. They posted do-it-yourself fixes in online forums. Jaguar didn't even begin to address this until 2004, at least fifteen years into the cup-holder crisis, when they made the cup-holders slightly (but not much) less-crappy, and large enough to hold a small coffee (but not a medium).</p>\n<p>Most new cars today finally have two cup-holders up front, and the collapsible cup-holders that enraged drivers for years by (predictably) collapsing are finally gone, but many cup-holders still aren't large enough to hold a Starbucks venti.</p>\n<p>What the cup-holder paradox implies is that there are many multi-billion dollar care companies that spend hundreds of millions of dollars on product development every year without ever assigning a single summer intern to take one day to read some of the many thousands of user reviews available for free on cars.com, autotrader.com, and other websites. If they had, they'd have realized the depth of America's anger at shoddy cup-holders.</p>\n<p>Or perhaps they read the reviews and dismiss them, because their customers are obviously morons who don't appreciate good auto design. Even today, auto manufacturers post photos of the interiors of all their new cars on their websites, but never in a dozen photos give you a clear view of the cup-holders, which makes me lean toward this view.</p>\n<p>Or perhaps the cup-holders aren't even considered during design, but are added on at the last minute, because cars didn't used to have cup-holders at all and so that's not part of the design process. Perhaps automakers have internalized their process of producing and selling cars, and they can't conceive of adding a new element to that process, at least not until all the old automakers die out.</p>\n<p>My priors say that it's more likely that I'm imagining the whole thing, that I selectively remember reviews complaining about cup-holders because of my own preferences, than that there has been a massive, systematic cognitive failure on the part of all the world's auto-makers, spanning 20 years, during which many of them somehow failed to observe, comprehend, or address this trivially-simple complaint of their customers, despite the billions of dollars at stake.</p>\n<p>Am I?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yrx44uF8aBdKuwi5G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 27, "extendedScore": null, "score": 1.1494689564231415e-06, "legacy": true, "legacyId": "22114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T08:54:44.095Z", "modifiedAt": null, "url": null, "title": "Meetup : Mountain View: Invoking Curiosity", "slug": "meetup-mountain-view-invoking-curiosity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FBERKi9S4adNvT8ds/meetup-mountain-view-invoking-curiosity", "pageUrlRelative": "/posts/FBERKi9S4adNvT8ds/meetup-mountain-view-invoking-curiosity", "linkUrl": "https://www.lesswrong.com/posts/FBERKi9S4adNvT8ds/meetup-mountain-view-invoking-curiosity", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mountain%20View%3A%20Invoking%20Curiosity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mountain%20View%3A%20Invoking%20Curiosity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFBERKi9S4adNvT8ds%2Fmeetup-mountain-view-invoking-curiosity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mountain%20View%3A%20Invoking%20Curiosity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFBERKi9S4adNvT8ds%2Fmeetup-mountain-view-invoking-curiosity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFBERKi9S4adNvT8ds%2Fmeetup-mountain-view-invoking-curiosity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kx'>Mountain View: Invoking Curiosity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 March 2013 07:30:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA 94041</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://lesswrong.com/lw/aa7/get_curious/\">Get Curious</a> is an explicit injunction to practice becoming curious whenever it's useful. Or pleasant, I suppose. I've just started, in the very most tiny way, to think and play with this, and I have a couple more ideas about how we could try to practice this skill. I want to bounce these ideas off of people and see if they work for anyone else. I <em>really</em> want to know if anyone has alternate suggestions for inducing curiosity that work for them.</p>\n\n<p>In any case, <a href=\"http://lesswrong.com/lw/4ln/go_try_things/\">Let&#39;s Try It</a>, and see what comes of it.</p>\n\n<p>Moreover, invoking curiosity seems like a skill that could be fruitfully practiced for something like 5 minutes a day. I'm very interesting in putting together some social mechanism to practice this and similar skills; I'd quite like to set one up at the meetup.</p>\n\n<hr />\n\n<p>(Standard mailing list plug)</p>\n\n<p>If you're in the San Francisco Bay Area, consider joining the Bay Area Less Wrong <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">mailing list</a>\nRegular meetups in Mountain View and Berkeley are announced and discussed there, as are other events in the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kx'>Mountain View: Invoking Curiosity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FBERKi9S4adNvT8ds", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1496350526561385e-06, "legacy": true, "legacyId": "22119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Invoking_Curiosity\">Discussion article for the meetup : <a href=\"/meetups/kx\">Mountain View: Invoking Curiosity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 March 2013 07:30:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA 94041</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://lesswrong.com/lw/aa7/get_curious/\">Get Curious</a> is an explicit injunction to practice becoming curious whenever it's useful. Or pleasant, I suppose. I've just started, in the very most tiny way, to think and play with this, and I have a couple more ideas about how we could try to practice this skill. I want to bounce these ideas off of people and see if they work for anyone else. I <em>really</em> want to know if anyone has alternate suggestions for inducing curiosity that work for them.</p>\n\n<p>In any case, <a href=\"http://lesswrong.com/lw/4ln/go_try_things/\">Let's Try It</a>, and see what comes of it.</p>\n\n<p>Moreover, invoking curiosity seems like a skill that could be fruitfully practiced for something like 5 minutes a day. I'm very interesting in putting together some social mechanism to practice this and similar skills; I'd quite like to set one up at the meetup.</p>\n\n<hr>\n\n<p>(Standard mailing list plug)</p>\n\n<p>If you're in the San Francisco Bay Area, consider joining the Bay Area Less Wrong <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">mailing list</a>\nRegular meetups in Mountain View and Berkeley are announced and discussed there, as are other events in the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Invoking_Curiosity1\">Discussion article for the meetup : <a href=\"/meetups/kx\">Mountain View: Invoking Curiosity</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mountain View: Invoking Curiosity", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Invoking_Curiosity", "level": 1}, {"title": "Discussion article for the meetup : Mountain View: Invoking Curiosity", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Invoking_Curiosity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bGtdeqbgTzuLvZ5zn", "ADaZaEsmJMnKKhRqS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T21:20:40.981Z", "modifiedAt": null, "url": null, "title": "Soylent Orange - Whole food open source soylent", "slug": "soylent-orange-whole-food-open-source-soylent", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:09.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X6APQeHhXH9mbredM/soylent-orange-whole-food-open-source-soylent", "pageUrlRelative": "/posts/X6APQeHhXH9mbredM/soylent-orange-whole-food-open-source-soylent", "linkUrl": "https://www.lesswrong.com/posts/X6APQeHhXH9mbredM/soylent-orange-whole-food-open-source-soylent", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Soylent%20Orange%20-%20Whole%20food%20open%20source%20soylent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASoylent%20Orange%20-%20Whole%20food%20open%20source%20soylent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6APQeHhXH9mbredM%2Fsoylent-orange-whole-food-open-source-soylent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Soylent%20Orange%20-%20Whole%20food%20open%20source%20soylent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6APQeHhXH9mbredM%2Fsoylent-orange-whole-food-open-source-soylent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6APQeHhXH9mbredM%2Fsoylent-orange-whole-food-open-source-soylent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p>Update: John Maxwell and I have a startup making nutritionally complete food, <a href=\"http://www.mealsquares.com\">MealSquares</a>&nbsp;(which is likely better for weightloss see below)</p>\n<p>&nbsp;</p>\n<p>This came up at a meetup a while back. &nbsp;Several people, myself included, expressed frustration with the time, cost, stress, of preparing reasonably healthy and tasty meals. &nbsp;I suspect this frustration is widespread among people who do work that requires a lot of focus. &nbsp;Leaving flow because your body needs maintenance is annoying. &nbsp;So I'm sharing a strategy that has helped me.</p>\n<p>I was encouraged by the success of&nbsp;<a title=\"Soylent\" href=\"http://soylent.me/\">Soylent</a>. &nbsp;I had been playing around with ingredients for post workout shakes for months. But reading the Soylent blog posts inspired me to do a full micronutrient breakdown of what I had been drinking and optimize in a more rigorous fashion. Why not copy the soylent recipe?</p>\n<p>1. I'm not realistically going to source all of those ingredients</p>\n<p>2. He risks (and has already had problems with) misdosing himself to deleterious effect, this problem doesn't exist with whole foods</p>\n<p>3. The absorption of powders vs whole foods is contentious</p>\n<p>4. I don't agree with his criteria for inclusion</p>\n<p>In comparison, my recipe is extremely easy and cheap to source, due to the small number of ingredients.</p>\n<p>There is an immediate problem with meal replacement shakes in that liquid calories tend to have a significantly smaller satiety effect than solid foods. &nbsp;So this will probably not be a good solution for you if have difficulties keeping your overall caloric intake down.</p>\n<p>[EDIT: Removed the link to the recipe, John and I are planning to commercialize this in addition to MealSquares at some point. &nbsp;Get in touch with me if you really need the recipe and won't spread it around.]</p>\n<p>This is a work in progress and I am looking for further ideas for improvement. &nbsp;Subjectively I can say I find this recipe delicious, and hugely prefer it post-workout to even the best junk food (pizza, etc.). &nbsp;The combination of milk, vanilla, banana, and orange juice tastes kind of like an orange julius. It has also been a major stress relief and time saver. &nbsp;I don't worry so much about nutrient deficiencies anymore as this shake in addition to a meat or egg based meal has me pretty well covered.</p>\n<p>I am due for another blood panel and will report any anomalies as I've been drinking a similar concoction for around 6-8 months.</p>\n<p>I am open to debating the merits of my ingredient choices (as well as the overall wisdom of this scheme) in the comments. Also please share any other strategies you have for making food less of a chore.</p>\n<p>&nbsp;</p>\n<p>Edit: I finally got my blood panel back and everything is looking good. Triglycerides unchanged, HDL up, LDL slightly down. &nbsp;All other numbers within the healthy range. &nbsp;I'm a little concerned about my iron level (what is considered normal may not be optimal for longevity), and plan on giving blood to lower it, but this is orthogonal to the use of a dietary shake I believe.</p>\n<p>Edit: Kefir is expensive but highly recommended for lactose intolerant individuals. It is also delicious.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6bdeSR7aKmdSQLw6P": 1, "92SxJsDZ78ApAGq72": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X6APQeHhXH9mbredM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 33, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "22121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-26T23:00:56.142Z", "modifiedAt": null, "url": null, "title": "Want to be on TV?", "slug": "want-to-be-on-tv", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calcsam", "createdAt": "2011-04-30T17:07:18.622Z", "isAdmin": false, "displayName": "calcsam"}, "userId": "YpbtzJj8Qwi4PHGm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WgWHD4zQeGxfPeoGk/want-to-be-on-tv", "pageUrlRelative": "/posts/WgWHD4zQeGxfPeoGk/want-to-be-on-tv", "linkUrl": "https://www.lesswrong.com/posts/WgWHD4zQeGxfPeoGk/want-to-be-on-tv", "postedAtFormatted": "Tuesday, March 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Want%20to%20be%20on%20TV%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWant%20to%20be%20on%20TV%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWgWHD4zQeGxfPeoGk%2Fwant-to-be-on-tv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Want%20to%20be%20on%20TV%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWgWHD4zQeGxfPeoGk%2Fwant-to-be-on-tv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWgWHD4zQeGxfPeoGk%2Fwant-to-be-on-tv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 376, "htmlBody": "<p>We received the following email, so figured I'd pass it along here. You can say you heard about it from Sam Bhagwat at Blueseed.</p>\n<p>Could be free publicity (alert startupers!), but I make no claims as to quality or anything else.</p>\n<p>-----</p>\n<p><em>Subject: Improving the Portrayal of Nerds on TV</em></p>\n<p><span style=\"font-family: Verdana; font-size: 13px;\">I came across your website while searching for math/science/tech-related groups and wanted to reach out to you. I'm currently casting a TV series about \"the real life of nerds\" for a major network. The network's initial casting idea was to find awkward+intelligent people with no social lives and to do the typical \"reality TV thing\" by engineering drama and conflict between them. My company ended up with the casting contract, so I'm trying to find a solid cast of real people to change the network's idea of making a project that feels like Jersey Shore (&lt;-my words, not the network's).</span></p>\n<p><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">I thought you might be willing to point me in the direction of one or two people in your network who would be interested in taking part in the pilot and, potentially, the full series (if the project gets a full greenlight). I think that there is potential here to create positive portrayals of \"nerds\" that are far different than their typical depictions in media.</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">If you have someone who meets most or all of the criteria below, please feel free to contact me, or to pass along my contact details to them.</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Basics:</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-18-26 years old, male or female</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Involved in the hard sciences (research or applications) or IT field</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Passionate about science, math, technology, research, or a related pursuit</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">The next few bullets are not requirements, but would be awesome to find:</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Anybody involved in hackerspaces/hardware hackers</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Aerospace/aeronautics background</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-PhD or Masters research at a university</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Programmer involved at a small startup</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">-Security/IT fields (penetration testers, etc.)</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Thanks very much for taking the time to read this email. Let me know if you have any questions or would like to discuss this further. Any assistance would be greatly appreciated.</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Sincerely,</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Stuart Inman</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Executive Producer</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">No Title Entertainment</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><a style=\"color: #1155cc; font-family: Verdana; font-size: 13px;\" href=\"/tel:209.747.0688\" target=\"_blank\">209.747.0688</a><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">&nbsp;c</span><br style=\"color: #222222; font-family: Verdana; font-size: 13px;\" /><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">Stuart@NoTitleEntertainment.</span><span style=\"color: #222222; font-family: Verdana; font-size: 13px;\">com</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WgWHD4zQeGxfPeoGk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 11, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "22122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T01:20:35.885Z", "modifiedAt": null, "url": null, "title": "A Difficulty in the Concept of CEV", "slug": "a-difficulty-in-the-concept-of-cev", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:37.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "uEZDwW29H9Kfr3yce", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6pBPiEGqS8ncNq8x4/a-difficulty-in-the-concept-of-cev", "pageUrlRelative": "/posts/6pBPiEGqS8ncNq8x4/a-difficulty-in-the-concept-of-cev", "linkUrl": "https://www.lesswrong.com/posts/6pBPiEGqS8ncNq8x4/a-difficulty-in-the-concept-of-cev", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Difficulty%20in%20the%20Concept%20of%20CEV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Difficulty%20in%20the%20Concept%20of%20CEV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6pBPiEGqS8ncNq8x4%2Fa-difficulty-in-the-concept-of-cev%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Difficulty%20in%20the%20Concept%20of%20CEV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6pBPiEGqS8ncNq8x4%2Fa-difficulty-in-the-concept-of-cev", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6pBPiEGqS8ncNq8x4%2Fa-difficulty-in-the-concept-of-cev", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p><strong>Prerequisite reading:</strong> <a href=\"/lw/7sb/cognitive_neuroscience_arrows_impossibility/\">Cognitive Neuroscience, Arrow's Impossibility Theorem, and Coherent Extrapolated Volition</a>.</p>\n<p><strong>Abstract:</strong> Arrow's impossibility theorem poses a challenge to viability of coherent extrapolated volition (CEV) as a model for safe-AI architecture: per the theorem, no algorithm for aggregating ordinal preferences can necessarily obey Arrow's four fairness criteria while simultaneously producing a transitive preference ordering.&nbsp; One approach to exempt CEV from these consequences is to claim that human preferences are cardinal rather than ordinal, and therefore Arrow's theorem does not apply.&nbsp; This approach is shown to ultimately fail and other options are briefly discussed.</p>\n<hr />\n<p>A problem arises when examining CEV from the perspective of welfare economics: according to <a href=\"http://www.jstor.org/discover/10.2307/1828886\">Arrow's impossibility theorem</a>, no algorithm for the aggregation of preferences can necessarily meet four common-sense fairness criteria while simultaneously producing a transitive result.&nbsp; Luke has previously discussed this challenge.&nbsp; (See the post linked above.)</p>\n<p>Arrow's impossibility theorem assumes that human preferences are ordinal but (as Luke pointed out) recent <a href=\"http://www.cns.nyu.edu/~glimcher/PUBLICATIONS/Root.pdf\">neuroscientific findings</a> suggest that human preferences are cardinally encoded.&nbsp; This fact implies that human preferences - and subsequently CEV - are not bound by the consequences of the theorem.</p>\n<p>However, Arrow's impossibility theorem extends to cardinal utilities with the addition of a continuity axiom.&nbsp; This result - termed Samuelson's conjecture - was proven by Ehud Kalai and David Schmeidler in their 1977 paper \"<a href=\"http://www.jstor.org/discover/10.2307/1912309\">Aggregation Procedure for Cardinal Preferences</a>.\"&nbsp; If an AI attempts to model human preferences using a utility theory that relies on the continuity axiom, then the consequences of Arrow's theorem will still apply.&nbsp; For example, this includes an AI using the von Neumann-Morgenstern utility theorem.</p>\n<p>The proof of Samuelson's conjecture limits the solution space for what kind of CEV aggregation procedures are viable.&nbsp; In order to escape the consequences of Arrow's impossibility theorem, a CEV algorithm must accurately model human preferences without using a continuity axiom.&nbsp; It may be the case that we are living in a second-best world where such models are impossible.&nbsp; This scenario would mean we must make a trade-off between employing a fair aggregation procedure and producing a transitive result.</p>\n<p>Supposing this is the case, what kind of trade-offs would be optimal?&nbsp; I am hesitant about weakening the transitivity criterion because an agent with a non-transitive utility function is vulnerable to Dutch-book theorems.&nbsp; This scenario poses a clear existential risk.&nbsp; On the other hand, weakening the independence of irrelevant alternatives criterion may be feasible.&nbsp; My cursory reading of the literature suggests that this is a popular alternative among welfare economists, but there are other choices.</p>\n<p>Going forward, citing Arrow's impossibility theorem may serve as one of the strongest <a href=\"/lw/g35/ideal_advisor_theories_and_personal_cev/\">objections against CEV</a>.&nbsp; Further consideration on how to reconcile CEV with Arrow's impossibility theorem is warranted.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6pBPiEGqS8ncNq8x4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 6, "extendedScore": null, "score": 1.1502981697855621e-06, "legacy": true, "legacyId": "22123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XwzKAeENiv9ZvQeGA", "q9ZSXiiA7wEuRgnkS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T04:28:20.605Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC meetup", "slug": "meetup-durham-nc-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/afBPSAtAFZEtbQF4G/meetup-durham-nc-meetup", "pageUrlRelative": "/posts/afBPSAtAFZEtbQF4G/meetup-durham-nc-meetup", "linkUrl": "https://www.lesswrong.com/posts/afBPSAtAFZEtbQF4G/meetup-durham-nc-meetup", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FafBPSAtAFZEtbQF4G%2Fmeetup-durham-nc-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FafBPSAtAFZEtbQF4G%2Fmeetup-durham-nc-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FafBPSAtAFZEtbQF4G%2Fmeetup-durham-nc-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ky'>Durham NC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">706 9th St., Durham NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Francesca's at 7 for:</p>\n\n<ul>\n<li>rationality checklist revisitation and customization </li>\n<li>discussion of ongoing projects </li>\n<li>chai if you want! </li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ky'>Durham NC meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "afBPSAtAFZEtbQF4G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.150424528861693e-06, "legacy": true, "legacyId": "22124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup\">Discussion article for the meetup : <a href=\"/meetups/ky\">Durham NC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 March 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">706 9th St., Durham NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Francesca's at 7 for:</p>\n\n<ul>\n<li>rationality checklist revisitation and customization </li>\n<li>discussion of ongoing projects </li>\n<li>chai if you want! </li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ky\">Durham NC meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC meetup", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup", "level": 1}, {"title": "Discussion article for the meetup : Durham NC meetup", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T04:33:23.491Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] What Do We Mean By \"Rationality\"?", "slug": "seq-rerun-what-do-we-mean-by-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:27.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GGYBD9wNzMRzuueKz/seq-rerun-what-do-we-mean-by-rationality", "pageUrlRelative": "/posts/GGYBD9wNzMRzuueKz/seq-rerun-what-do-we-mean-by-rationality", "linkUrl": "https://www.lesswrong.com/posts/GGYBD9wNzMRzuueKz/seq-rerun-what-do-we-mean-by-rationality", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20What%20Do%20We%20Mean%20By%20%22Rationality%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20What%20Do%20We%20Mean%20By%20%22Rationality%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGYBD9wNzMRzuueKz%2Fseq-rerun-what-do-we-mean-by-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20What%20Do%20We%20Mean%20By%20%22Rationality%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGYBD9wNzMRzuueKz%2Fseq-rerun-what-do-we-mean-by-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGYBD9wNzMRzuueKz%2Fseq-rerun-what-do-we-mean-by-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>Today's post, <a href=\"/lw/31/what_do_we_mean_by_rationality/\">What Do We Mean By \"Rationality\"?</a> was originally published on 16 March 2009. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#What_Do_We_Mean_By_.22Rationality.22.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When we talk about rationality, we're generally talking about either epistemic rationality (systematic methods of finding out the truth) or instrumental rationality (systematic methods of making the world more like we would like it to be). We can discuss these in the forms of probability theory and decision theory, but this doesn't fully cover the difficulty of being rational as a human. There is a lot more to rationality than just the formal theories.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/h28/seq_rerun_3_levels_of_rationality_verification/\">3 Levels of Rationality Verification</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GGYBD9wNzMRzuueKz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1504279267240672e-06, "legacy": true, "legacyId": "22125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv", "STBXsfmYJ2ErbGtDK", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T04:51:54.419Z", "modifiedAt": null, "url": null, "title": "Solved Problems Repository", "slug": "solved-problems-repository", "viewCount": null, "lastCommentedAt": "2022-01-01T23:45:02.217Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iTzvJ7kKK2TYJhYHB/solved-problems-repository", "pageUrlRelative": "/posts/iTzvJ7kKK2TYJhYHB/solved-problems-repository", "linkUrl": "https://www.lesswrong.com/posts/iTzvJ7kKK2TYJhYHB/solved-problems-repository", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solved%20Problems%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolved%20Problems%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTzvJ7kKK2TYJhYHB%2Fsolved-problems-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solved%20Problems%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTzvJ7kKK2TYJhYHB%2Fsolved-problems-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTzvJ7kKK2TYJhYHB%2Fsolved-problems-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p><strong>Follow-up to:</strong> <a href=\"/lw/gx5/boring_advice_repository/\">Boring Advice Repository</a></p>\n<p>Many practical problems in instrumental rationality appear to be wide open. Two I've been annoyed by recently are \"what should I eat?\" and \"how should I exercise?\" However, some appear to be more or less <strong>solved</strong>. For example, various mnemonic techniques like memory palaces, along with <a href=\"http://www.gwern.net/Spaced%20repetition\">spaced repetition</a>, seem to more or less solve the problem of memorization.</p>\n<p>I would like people to use this thread to post other examples of solved problems in instrumental rationality. I'm pretty sure you all collectively know good examples; there's a comment I can't find from a user who said something like \"taking a flattering photograph of yourself is a solved problem,\" and it's likely that there are other useful examples like this that aren't common knowledge. Err on the side of posting solutions which may not be universal but are still likely to be helpful to many people.&nbsp;</p>\n<p>(This thread is allowed to not be boring! Go wild!)&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "fkABsGCJZ6y9qConW": 1, "Ng8Gice9KNkncxqcj": 1, "Eha62RrqBtEbpcEza": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iTzvJ7kKK2TYJhYHB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 40, "extendedScore": null, "score": 0.00011, "legacy": true, "legacyId": "22126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 273, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HEn2qiMxk5BggN83J"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T05:29:48.210Z", "modifiedAt": null, "url": null, "title": "[LINK] On the unlikelihood of intelligent life", "slug": "link-on-the-unlikelihood-of-intelligent-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:28.441Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RXQ4YYxrvKJQbzHJZ/link-on-the-unlikelihood-of-intelligent-life", "pageUrlRelative": "/posts/RXQ4YYxrvKJQbzHJZ/link-on-the-unlikelihood-of-intelligent-life", "linkUrl": "https://www.lesswrong.com/posts/RXQ4YYxrvKJQbzHJZ/link-on-the-unlikelihood-of-intelligent-life", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20On%20the%20unlikelihood%20of%20intelligent%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20On%20the%20unlikelihood%20of%20intelligent%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ4YYxrvKJQbzHJZ%2Flink-on-the-unlikelihood-of-intelligent-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20On%20the%20unlikelihood%20of%20intelligent%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ4YYxrvKJQbzHJZ%2Flink-on-the-unlikelihood-of-intelligent-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ4YYxrvKJQbzHJZ%2Flink-on-the-unlikelihood-of-intelligent-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p><a href=\"http://www.dailygalaxy.com/my_weblog/2013/03/the-planet-of-the-apes-hypothesis-is-an-intelligence-niche-a-constant-in-the-universe.html#more\">\"The Planet-of-the-Apes Hypothesis\" Revisited --Will Intelligence be a Constant in the Universe?</a></p>\n<blockquote>If intelligence is good for every environment, we would see a trend in the encephalization quotient among all organisms as a function of time. The data does not show that. The evidence on Earth points to exactly the opposite conclusion. Earth had independent experiments in evolution thanks to continental drift. New Zealand, Madagascar, India, South America... half a dozen experiments over 10, 20, 50, even 100 million years of independent evolution did not produce anything that was more human-like than when it started. So it's a silly idea to think that species will evolve toward us.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RXQ4YYxrvKJQbzHJZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.1504658988566586e-06, "legacy": true, "legacyId": "22127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T13:18:16.710Z", "modifiedAt": null, "url": null, "title": "Schelling Day: A Rationalist Holiday", "slug": "schelling-day-a-rationalist-holiday", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:56.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ModusPonies", "createdAt": "2012-04-30T00:59:52.568Z", "isAdmin": false, "displayName": "ModusPonies"}, "userId": "9LEFaHEvri7Rrj8aM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z9Tc3CrZmjTfMc4jj/schelling-day-a-rationalist-holiday", "pageUrlRelative": "/posts/z9Tc3CrZmjTfMc4jj/schelling-day-a-rationalist-holiday", "linkUrl": "https://www.lesswrong.com/posts/z9Tc3CrZmjTfMc4jj/schelling-day-a-rationalist-holiday", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Schelling%20Day%3A%20A%20Rationalist%20Holiday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchelling%20Day%3A%20A%20Rationalist%20Holiday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9Tc3CrZmjTfMc4jj%2Fschelling-day-a-rationalist-holiday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Schelling%20Day%3A%20A%20Rationalist%20Holiday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9Tc3CrZmjTfMc4jj%2Fschelling-day-a-rationalist-holiday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9Tc3CrZmjTfMc4jj%2Fschelling-day-a-rationalist-holiday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 956, "htmlBody": "<h2>The Big Idea</h2>\n<div>&mdash;Holidays are awesome.</div>\n<div>&mdash;Getting to know people is awesome.</div>\n<div>&mdash;Schelling Day is a holiday about getting to know people.</div>\n<div>&mdash;The <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/\" target=\"_blank\">Boston group</a> will celebrate Schelling day.</div>\n<div>&mdash;Hopefully other cities will too.</div>\n<div><br /></div>\n<h2>Why Are We Doing This?</h2>\n<div>Getting to know people&mdash;really, truly getting to know people&mdash;is hard. You have to spend a huge amount of time with them, of course, but that&rsquo;s the easy part. Spending time with people is fun! The challenging part is opening yourself up. Sharing your fondest hopes and deepest fears is a powerful way to make connections, but exposing your soul like that terrifying. Worse, it&rsquo;s awkward. There&rsquo;s no socially appropriate time to bring up stuff like that. I&rsquo;ve talked to a bunch of people who wish there were more opportunities for that sort of sharing, but initiating it is risky. Even when everything works out beautifully, getting it started feels stressful and not-fun.</div>\n<div><br /></div>\n<div>What if we could set aside a time where sharing like that is not merely accepted, but expected? Historically, this doesn&rsquo;t seem too hard to do. As soon as people are in a context where everyone agrees that sharing is normal (e.g. an Alcoholics Anonymous meeting, or a conversation with a therapist), the stigma and self-consciousness don&rsquo;t hold people back nearly as much.</div>\n<div><br /></div>\n<div>All we need is an arbitrary time when we agree to change the social rules, and we&rsquo;re set! In other words, we need a <a href=\"http://en.wikipedia.org/wiki/Schelling_point\" target=\"_blank\">Schelling point</a>. April 14th, the birthday of <a href=\"http://en.wikipedia.org/wiki/Thomas_Schelling\" target=\"_blank\">Thomas Schelling</a>, is as good a time as any.</div>\n<div><br /></div>\n<div>I&rsquo;m creating a ritual around this, for two reasons. First, an established structure makes it easier to do something that feels difficult or strange. Second, in <a href=\"/lw/g0v/ritual_report_2012_life_death_light_darkness_and/\" target=\"_blank\">my experience</a>, adding ritual to powerful, true statements makes them even more powerful.</div>\n<div><br /></div>\n<div><br /></div>\n<div><br /></div>\n<h2>When Are We Doing This?</h2>\n<div>Schelling Day is April 14th, which is a Sunday this year. The <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/\" target=\"_blank\">Boston group</a> will be holding our celebration at 2:30. The ritual will begin sharply at 2:45, so please be on time.</div>\n<div><br /></div>\n<div>Please RSVP to the meetup if you&rsquo;re coming. We&rsquo;re allowed to have up to 20 people in the space, and we&rsquo;ll be using the Meetup site to track this. It&rsquo;s fine if you RSVP at 2:00 on the day of the event, so long as you don&rsquo;t put us above the limit. If for some reason you don&rsquo;t want your RSVP to be public, PM me and I&rsquo;ll reserve you a spot anonymously.</div>\n<div><br /></div>\n<div>There will be a potluck dinner. Everyone who brings a dish gets two Rationality Points.</div>\n<div><br /></div>\n<div><br /></div>\n<h2>What Are We Doing?</h2>\n<div>Everyone sits in a semicircle. At the focal point are two tables. On the first table are five small bowls of delicious snacks. Eating the delicious snacks at this stage is VERBOTEN. On the second table is a single large, empty bowl.</div>\n<div><br /></div>\n<div>Everyone will have a six-sided die.</div>\n<div><br /></div>\n<div>Everyone will have a chance to speak, or to not speak. When it&rsquo;s your turn, roll your die. Showing the result to others is VERBOTEN.</div>\n<div><br /></div>\n<div>If your die shows a six, you MUST speak. If your die shows a one, you MAY NOT speak. Otherwise, you choose whether or not to speak. The die is to provide plausible deniability. Attempting to guess whether someone&rsquo;s decision was forced by the die roll is VERBOTEN.&nbsp;</div>\n<div><br /></div>\n<div>If you speak, take 1-5 minutes* to tell the group about one of your secret Joys, Struggles, Hopes, Confessions, or Something Else Important, as described below. Then scoop some food from the appropriate bowl and put it into the larger bowl.</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>Struggles </strong>(Chocolate):</div>\n<div>Flaws, interpersonal drama, professional challenges, stuff you&rsquo;d say to a therapist</div>\n<div><br /></div>\n<div><strong>Joys </strong>(Raspberries):</div>\n<div>Passions, guilty pleasures, &ldquo;I love you guys&rdquo; speeches</div>\n<div><br /></div>\n<div><strong>Confessions </strong>(Pretzels):</div>\n<div>Burdens, personal secrets, things you&rsquo;re tired of hiding, stuff you&rsquo;d say to a priest</div>\n<div><br /></div>\n<div><strong>Hopes </strong>(Raisins):</div>\n<div>Goals, wishes, deepest desires, crazy schemes</div>\n<div><br /></div>\n<div><strong>Something Else</strong> (Trail mix):</div>\n<div>Because trying to make an exhaustive list would be silly.</div>\n<div><br /></div>\n<div><br /></div>\n<div>After your speak, or after you choose not to speak, the person to your left rolls their die and the process repeats.</div>\n<div><br /></div>\n<div>Once everyone has had a chance to speak or not, take five minutes* to stretch, then do the same thing again.</div>\n<div><br /></div>\n<div>After that, take five minutes to stretch, then begin the BONUS ROUND.</div>\n<div><br /></div>\n<div>The BONUS ROUND is like the first two rounds, with one exception. If you haven&rsquo;t spoken yet, do not roll your die. You MUST speak.</div>\n<div><br /></div>\n<div><br /></div>\n<h2>Then What?</h2>\n<div>We&rsquo;ll pass around the bowl of snacks we&rsquo;ve assembled from our accumulated revelations until everything is eaten.</div>\n<div><br /></div>\n<div>Depending on the timing, the emotional state, and our patience, we might or might not have another round or two.</div>\n<div><br /></div>\n<div>After that, dinner! The rest of the time will be for eating and socializing. We&rsquo;ll break into smaller groups and follow up on the things we said during the ritual. Asking questions about what someone said is actively encouraged! (There is no obligation to answer. &ldquo;I would prefer not to talk about that&rdquo; is a completely acceptable response.) Err on the side of asking an awkward question; if you&rsquo;re over the line, the other person will simply decline to answer, and no harm done. Judging people, or explaining why their revelations were wrong, is of course VERBOTEN&mdash;unless someone specifically asks for feedback, in which case be honest but <a href=\"http://www.admonymous.com/crash_course#giving\" target=\"_self\">don&rsquo;t be a jerk</a>. We&rsquo;ll get the potluck dishes people brought, and we&rsquo;ll eat, drink, and be merry.</div>\n<div><br /></div>\n<div><br /></div>\n<div><br /></div>\n<div><br /></div>\n<div><br /></div>\n<div>*I&rsquo;ll be using a timer! I don&rsquo;t want to be a jerk, but I want to keep things moving.</div>\n<div><br /></div>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div><br /></div>\n<div>UPDATE: My review of the event is <a href=\"/lw/h8d/ritual_report_schelling_day/\">here</a>.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z9Tc3CrZmjTfMc4jj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 51, "extendedScore": null, "score": 0.000169, "legacy": true, "legacyId": "22133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"The_Big_Idea\">The Big Idea</h2>\n<div>\u2014Holidays are awesome.</div>\n<div>\u2014Getting to know people is awesome.</div>\n<div>\u2014Schelling Day is a holiday about getting to know people.</div>\n<div>\u2014The <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/\" target=\"_blank\">Boston group</a> will celebrate Schelling day.</div>\n<div>\u2014Hopefully other cities will too.</div>\n<div><br></div>\n<h2 id=\"Why_Are_We_Doing_This_\">Why Are We Doing This?</h2>\n<div>Getting to know people\u2014really, truly getting to know people\u2014is hard. You have to spend a huge amount of time with them, of course, but that\u2019s the easy part. Spending time with people is fun! The challenging part is opening yourself up. Sharing your fondest hopes and deepest fears is a powerful way to make connections, but exposing your soul like that terrifying. Worse, it\u2019s awkward. There\u2019s no socially appropriate time to bring up stuff like that. I\u2019ve talked to a bunch of people who wish there were more opportunities for that sort of sharing, but initiating it is risky. Even when everything works out beautifully, getting it started feels stressful and not-fun.</div>\n<div><br></div>\n<div>What if we could set aside a time where sharing like that is not merely accepted, but expected? Historically, this doesn\u2019t seem too hard to do. As soon as people are in a context where everyone agrees that sharing is normal (e.g. an Alcoholics Anonymous meeting, or a conversation with a therapist), the stigma and self-consciousness don\u2019t hold people back nearly as much.</div>\n<div><br></div>\n<div>All we need is an arbitrary time when we agree to change the social rules, and we\u2019re set! In other words, we need a <a href=\"http://en.wikipedia.org/wiki/Schelling_point\" target=\"_blank\">Schelling point</a>. April 14th, the birthday of <a href=\"http://en.wikipedia.org/wiki/Thomas_Schelling\" target=\"_blank\">Thomas Schelling</a>, is as good a time as any.</div>\n<div><br></div>\n<div>I\u2019m creating a ritual around this, for two reasons. First, an established structure makes it easier to do something that feels difficult or strange. Second, in <a href=\"/lw/g0v/ritual_report_2012_life_death_light_darkness_and/\" target=\"_blank\">my experience</a>, adding ritual to powerful, true statements makes them even more powerful.</div>\n<div><br></div>\n<div><br></div>\n<div><br></div>\n<h2 id=\"When_Are_We_Doing_This_\">When Are We Doing This?</h2>\n<div>Schelling Day is April 14th, which is a Sunday this year. The <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/\" target=\"_blank\">Boston group</a> will be holding our celebration at 2:30. The ritual will begin sharply at 2:45, so please be on time.</div>\n<div><br></div>\n<div>Please RSVP to the meetup if you\u2019re coming. We\u2019re allowed to have up to 20 people in the space, and we\u2019ll be using the Meetup site to track this. It\u2019s fine if you RSVP at 2:00 on the day of the event, so long as you don\u2019t put us above the limit. If for some reason you don\u2019t want your RSVP to be public, PM me and I\u2019ll reserve you a spot anonymously.</div>\n<div><br></div>\n<div>There will be a potluck dinner. Everyone who brings a dish gets two Rationality Points.</div>\n<div><br></div>\n<div><br></div>\n<h2 id=\"What_Are_We_Doing_\">What Are We Doing?</h2>\n<div>Everyone sits in a semicircle. At the focal point are two tables. On the first table are five small bowls of delicious snacks. Eating the delicious snacks at this stage is VERBOTEN. On the second table is a single large, empty bowl.</div>\n<div><br></div>\n<div>Everyone will have a six-sided die.</div>\n<div><br></div>\n<div>Everyone will have a chance to speak, or to not speak. When it\u2019s your turn, roll your die. Showing the result to others is VERBOTEN.</div>\n<div><br></div>\n<div>If your die shows a six, you MUST speak. If your die shows a one, you MAY NOT speak. Otherwise, you choose whether or not to speak. The die is to provide plausible deniability. Attempting to guess whether someone\u2019s decision was forced by the die roll is VERBOTEN.&nbsp;</div>\n<div><br></div>\n<div>If you speak, take 1-5 minutes* to tell the group about one of your secret Joys, Struggles, Hopes, Confessions, or Something Else Important, as described below. Then scoop some food from the appropriate bowl and put it into the larger bowl.</div>\n<div><br></div>\n<div><br></div>\n<div><strong>Struggles </strong>(Chocolate):</div>\n<div>Flaws, interpersonal drama, professional challenges, stuff you\u2019d say to a therapist</div>\n<div><br></div>\n<div><strong>Joys </strong>(Raspberries):</div>\n<div>Passions, guilty pleasures, \u201cI love you guys\u201d speeches</div>\n<div><br></div>\n<div><strong>Confessions </strong>(Pretzels):</div>\n<div>Burdens, personal secrets, things you\u2019re tired of hiding, stuff you\u2019d say to a priest</div>\n<div><br></div>\n<div><strong>Hopes </strong>(Raisins):</div>\n<div>Goals, wishes, deepest desires, crazy schemes</div>\n<div><br></div>\n<div><strong>Something Else</strong> (Trail mix):</div>\n<div>Because trying to make an exhaustive list would be silly.</div>\n<div><br></div>\n<div><br></div>\n<div>After your speak, or after you choose not to speak, the person to your left rolls their die and the process repeats.</div>\n<div><br></div>\n<div>Once everyone has had a chance to speak or not, take five minutes* to stretch, then do the same thing again.</div>\n<div><br></div>\n<div>After that, take five minutes to stretch, then begin the BONUS ROUND.</div>\n<div><br></div>\n<div>The BONUS ROUND is like the first two rounds, with one exception. If you haven\u2019t spoken yet, do not roll your die. You MUST speak.</div>\n<div><br></div>\n<div><br></div>\n<h2 id=\"Then_What_\">Then What?</h2>\n<div>We\u2019ll pass around the bowl of snacks we\u2019ve assembled from our accumulated revelations until everything is eaten.</div>\n<div><br></div>\n<div>Depending on the timing, the emotional state, and our patience, we might or might not have another round or two.</div>\n<div><br></div>\n<div>After that, dinner! The rest of the time will be for eating and socializing. We\u2019ll break into smaller groups and follow up on the things we said during the ritual. Asking questions about what someone said is actively encouraged! (There is no obligation to answer. \u201cI would prefer not to talk about that\u201d is a completely acceptable response.) Err on the side of asking an awkward question; if you\u2019re over the line, the other person will simply decline to answer, and no harm done. Judging people, or explaining why their revelations were wrong, is of course VERBOTEN\u2014unless someone specifically asks for feedback, in which case be honest but <a href=\"http://www.admonymous.com/crash_course#giving\" target=\"_self\">don\u2019t be a jerk</a>. We\u2019ll get the potluck dishes people brought, and we\u2019ll eat, drink, and be merry.</div>\n<div><br></div>\n<div><br></div>\n<div><br></div>\n<div><br></div>\n<div><br></div>\n<div>*I\u2019ll be using a timer! I don\u2019t want to be a jerk, but I want to keep things moving.</div>\n<div><br></div>\n<div><br></div>\n<div>\n<hr>\n</div>\n<div><br></div>\n<div>UPDATE: My review of the event is <a href=\"/lw/h8d/ritual_report_schelling_day/\">here</a>.</div>", "sections": [{"title": "The Big Idea", "anchor": "The_Big_Idea", "level": 1}, {"title": "Why Are We Doing This?", "anchor": "Why_Are_We_Doing_This_", "level": 1}, {"title": "When Are We Doing This?", "anchor": "When_Are_We_Doing_This_", "level": 1}, {"title": "What Are We Doing?", "anchor": "What_Are_We_Doing_", "level": 1}, {"title": "Then What?", "anchor": "Then_What_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FLq2J9KwEuecYmaSx", "qrA2e4n6JqqAoBYzE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T18:06:36.244Z", "modifiedAt": null, "url": null, "title": "Removing Bias From the Definition of Reductionism", "slug": "removing-bias-from-the-definition-of-reductionism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:04.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RogerS", "createdAt": "2013-02-27T17:28:11.625Z", "isAdmin": false, "displayName": "RogerS"}, "userId": "xCQ7jDkbR33hGqyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vkF45fWgauackWQLF/removing-bias-from-the-definition-of-reductionism", "pageUrlRelative": "/posts/vkF45fWgauackWQLF/removing-bias-from-the-definition-of-reductionism", "linkUrl": "https://www.lesswrong.com/posts/vkF45fWgauackWQLF/removing-bias-from-the-definition-of-reductionism", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Removing%20Bias%20From%20the%20Definition%20of%20Reductionism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARemoving%20Bias%20From%20the%20Definition%20of%20Reductionism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkF45fWgauackWQLF%2Fremoving-bias-from-the-definition-of-reductionism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Removing%20Bias%20From%20the%20Definition%20of%20Reductionism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkF45fWgauackWQLF%2Fremoving-bias-from-the-definition-of-reductionism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkF45fWgauackWQLF%2Fremoving-bias-from-the-definition-of-reductionism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1264, "htmlBody": "<p><span style=\"font-family: Arial, sans-serif; font-size: 10pt; line-height: 115%;\">The test for an unbiased definition of a philosophical position is (surely) that it is equally acceptable to critics and defenders of the position. I think the definition of reductionism in the wiki blatantly fails this test. The same bias is apparent in the old Sequence posting dealing with reductionism. (Some comments called it a &ldquo;straw man&rdquo; without spelling out why.)</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">Consider the definition:-</span></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">Reductionism</span></strong><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">&nbsp;</span><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">is a disbelief that the higher levels of simplified multilevel models are out there in the territory</span><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">, that concepts constructed by mind in themselves play a role in the behaviour of reality. This doesn't contradict the notion that the concepts used in simplified multilevel models refer to the actual clusters of configurations of reality.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">The unavoidable implication is that critics of reductionism believe that the higher levels of simplified multilevel models <em>are</em> out there in the territory. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">Certainly, nobody but the flimsiest of straw men could possibly believe this, since all parts <em>of the models</em> are <em>by definition </em>not part of the territory: they are part of the map. It might be possible to believe, by contrast, that the territory actually has built into it things that correspond in some sense to higher levels of a hierarchical map, whether simplified or no; or that whether there are or are not such things is not decidable or meaningful; or one could believe that there are definitely no such things, that hierarchical higher levels of organisation (clusters) are meaningful only as mental artefacts. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">The second disbelief, that concepts constructed by mind in themselves play a role in the behaviour of reality, also appears to be unimpeachable. However, by attaching this claim to the first claim, it is implied (without examination of the implication) that this statement only applies to the higher levels of simplified models. Yet logically, it cannot be confined to the higher levels but <em>equally applies to the lowest level of the map: </em>it is not<em> </em>the pixels of even the best possible map that themselves play a role in the behaviour of reality, but only something in some way corresponding to them. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">In the Sequence discussion of reductionism we read:</span></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">So is the 747 made of something other than quarks?&nbsp; No...</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">And bigjeff5 later comments</span></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">The territory is&nbsp;only&nbsp;quarks (or whatever quarks may be made of).</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">But baryons, which are part of the map, are made of quarks, which are therefore also part of the map. In fact they are the pixels of the best available map today. The map is not the territory. Therefore quarks are <em>not</em> part of the territory. So nothing is made of quarks except in our current map. To say that reality is made of quarks is an acceptable shorthand in many contexts, but in a discussion whose whole point is to emphasize the need not to confuse the map with the territory, disregarding the distinction at quark level is at least prima facie evidence of a biased approach! </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">TWO KINDS OF SIMPLIFIED MAPS</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">The wiki definition I began with includes the word <em>simplified</em> for reasons that are not clear. The Sequence discussion seems to me to confuse two different senses of the term: simplification by approximation and simplification by selection. Newton&rsquo;s theory is now regarded as a simplified version of Special Relativity that serves as an excellent approximation in certain contexts. In this case the key is that the simplification is an approximation. Treating the forces on an aircraft wing at the aggregate level is leaving out internal details that per se do not affect the result. There will certainly be approximations involved, of course, but they don&rsquo;t stem from the actual process of aggregation, which is essentially a matter of combining all the relevant force equations algebraically, eliminating internal forces, before solving them; rather than combining the calculated forces numerically. So is the definition addressing approximate maps or selective maps?</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">WHICH TERRITORY?</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">A question raised in the discussion but never answered as far as I can see is whether the belief referred to applies to our particular universe or to any universe one could conceive (so is more like a belief about the nature of explanation). While much of the discussion is focussed on our universe, various analogies advanced by commenters suggest that applicability to all universes is intended. I will assume the latter. A further confusion is that whereas the usual definition of reductionism refers to reduction of any system to its elements, and thus, for example, covers the reduction of lifeforms to their genetic recipes, the focus on &ldquo;the territory&rdquo; seems to confine the definition to physics: genes, being &ldquo;made of quarks&rdquo;, are just a level of the map. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">DEFINITIONS </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">A definition that consists of disbelieving a contradiction in terms and then disguising a selective application of a truism, is clearly biased, and leads to apparently biased thinking, so I will attempt an unbiased definition. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">Of course, we could simply import a definition from Wikipedia or elsewhere, but I am trying to capture the particular approach and terminology of this site (from initial impressions) in an unbiased way.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">If I understand it correctly, <strong>reductionists on this site believe that, for the purposes of causal explanation, any &ldquo;territory&rdquo; in the sense of physical reality is best characterised as corresponding only to the lowest hierarchical level of our best map of it, higher levels of organisation existing only in the map</strong>. Is that right?</span></p>\n<p class=\"MsoNormal\">The summary in the SEQ_RERUN is also worth repeating:</p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">We build&nbsp;<em>models&nbsp;</em>of the universe that have many different levels of description. But so far as anyone has been able to determine, the&nbsp;<em>universe itself</em>&nbsp;has only the single level of fundamental physics - reality doesn't explicitly compute protons, only quarks.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">Which seems to mean much the same as my definition (if it means anything to say that &ldquo;the universe computes&rdquo;).</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">Psy-Kosh implicitly criticises the reference to quarks in claiming:</span></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">Reductionism does _NOT_ mean \"reduction to particles\", just \"reduction to simple principles that are the basic thing that give rise to everything else\".</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">But that doesn&rsquo;t exclude &ldquo;simple principles&rdquo; that emerge from higher levels of organization, so doesn&rsquo;t really fit the bill either. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">The most relevant corresponding wording in Wikipedia is interesting because it makes no reference to the model/reality (map/territory) distinction which Eliezer seems to think makes ontological reductionism intelligible:</span></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">In a reductionist framework, [a phenomenon] that can be explained completely in terms of relations between other more fundamental phenomena&nbsp; exerts no causal agency on the fundamental phenomena that explain it.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">This assumes that the term &ldquo;more fundamental&rdquo; is defined, and (like my definition) doesn&rsquo;t distinguish sequential causation from structural causation. Hmm, maybe this needs a separate post sometime.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">FOOTNOTE: WHAT ABOUT THE HEURISTIC SENSE? </span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">As Perplexed pointed out, the discussion of reductionism in the Sequences clearly refers to <em>ontological reductionism</em> by contrast with <em>methodological reductionism.</em> The same applies to the wiki definition and my proposed correction.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">I entirely support this distinction. I am focussing here on ontological reductionism because so many contributors define themselves as &ldquo;reductionist materialists&rdquo; as a matter of belief. One would no more define oneself as a heuristic reductionist than one would define oneself as a hammer user (rather than a screwdriver user, say) as a matter of conviction.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">TEASER</span></p>\n<p><span style=\"font-size: 10pt; line-height: 115%; background-position: initial initial; background-repeat: initial initial;\">This quarrel about definition is not mere pedantry, since it hints at an unconscious bias. Moreover, if there are signs of agreement with this definition (or improved versions of it) and my newbie&rsquo;s karma doesn&rsquo;t suffer, I hope to build on this beginning to suggest that the real difference between proponents and supporters of ontological reductionism is that they are using two subtly different conceptions of what we mean by &ldquo;the territory&rdquo; and &ldquo;the map&rdquo;, both consistent with the definitions.&nbsp; (Both conceptions have been implied by different contributors but without considering that the difference may be one of convention).</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vkF45fWgauackWQLF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 1.150975532812778e-06, "legacy": true, "legacyId": "22136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-27T18:28:46.983Z", "modifiedAt": null, "url": null, "title": "Upgrading moral theories to include complex values", "slug": "upgrading-moral-theories-to-include-complex-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:27.901Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d3TQrGGTnEoDaW4Dp/upgrading-moral-theories-to-include-complex-values", "pageUrlRelative": "/posts/d3TQrGGTnEoDaW4Dp/upgrading-moral-theories-to-include-complex-values", "linkUrl": "https://www.lesswrong.com/posts/d3TQrGGTnEoDaW4Dp/upgrading-moral-theories-to-include-complex-values", "postedAtFormatted": "Wednesday, March 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upgrading%20moral%20theories%20to%20include%20complex%20values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpgrading%20moral%20theories%20to%20include%20complex%20values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3TQrGGTnEoDaW4Dp%2Fupgrading-moral-theories-to-include-complex-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upgrading%20moral%20theories%20to%20include%20complex%20values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3TQrGGTnEoDaW4Dp%2Fupgrading-moral-theories-to-include-complex-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3TQrGGTnEoDaW4Dp%2Fupgrading-moral-theories-to-include-complex-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1587, "htmlBody": "<p>Like many members of this community, reading the sequences has opened my eyes to a heavily neglected aspect of morality.&nbsp; Before reading the sequences I focused mostly on how to best improve people's wellbeing in the present and the future.&nbsp; However, after reading the sequences, I realized that I had neglected a very important question:&nbsp; In the future we will be able to create creatures with virtually <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">any utility function imaginable</a>. What sort of values should we give the creatures of the future?&nbsp; What sort of desires should they have, from what should they gain wellbeing?</p>\n<p>Anyone familiar with the sequences should be familiar with the answer.&nbsp; We should create creatures with the <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complex values</a> that human beings possess (call them \"humane values\").&nbsp; We should avoid creating creatures with simple values that only desire to maximize one thing, like paperclips or pleasure.&nbsp;</p>\n<p>It is important that future theories of ethics formalize this insight.&nbsp; I think we all know what would happen if we programmed an <a href=\"http://wiki.lesswrong.com/wiki/Unfriendly_AI\">AI with conventional utilitarianism</a>:&nbsp; It would exterminate the human race and replace them with creatures whose preferences are easier to satisfy (if you program it with preference utilitarianism) or creatures whom it is easier to make happy (if you program it with hedonic utilitarianism).&nbsp; It is important to develop a theory of ethics that avoids this.</p>\n<p>Lately I have been trying to develop a modified utilitarian theory that formalizes this insight.&nbsp; My focus has been on population ethics.&nbsp; I am essentially arguing that population ethics should not just focus on maximizing welfare, it should also focus on what sort of creatures it is best to create.&nbsp; According to this theory of ethics, it is possible for a population with a lower total level of welfare to be better than a population with a higher total level of welfare, if the lower population consists of creatures that have complex humane values, while the higher welfare population consists of paperclip or pleasure maximizers. (I wrote a <a href=\"/r/discussion/lw/guk/population_ethics_shouldnt_be_about_maximizing/\">previous post on this</a>, but it was long and rambling, I am trying to make this one more accessible).</p>\n<p>One of the key aspects of this theory is that it does not necessarily rate the welfare of creatures with simple values as unimportant.&nbsp; On the contrary, it considers it good for their welfare to be increased and bad for their welfare to be decreased.&nbsp; Because of this, it implies that we ought to <a href=\"/lw/x7/cant_unbirth_a_child/\">avoid creating such creatures in the first place</a>, so it is not necessary to divert resources from creatures with humane values in order to increase their welfare.&nbsp;</p>\n<p>My theory does allow the creation of simple-value creatures for two reasons. One is if the benefits they generate for creatures with humane values outweigh the harms generated when humane-value creatures must divert resources to improving their welfare (companion animals are an obvious example of this).&nbsp; The second is if creatures with humane values are about to go extinct, and the only choices are replacing them with simple value creatures, or replacing them with nothing.</p>\n<p>So far I am satisfied with the development of this theory.&nbsp; However, I have hit one major snag, and would love it if someone else could help me with it.&nbsp; The snag is formulated like this:</p>\n<p>1. It is better to create a small population of creatures with complex humane values (that has positive welfare) than a large population of animals that can only experience pleasure or pain, even if the large population of animals has a greater total amount of positive welfare.&nbsp; For instance, it is better to create a population of humans with 50 total welfare than a population of animals with 100 total welfare.</p>\n<p>2. It is bad to create a small population of creatures with humane values (that has positive welfare) and a large population of animals that are in pain.&nbsp; For instance, it is bad to create a population of animals with -75 total welfare, even if doing so allows you to create a population of humans with 50 total welfare.</p>\n<p>3.&nbsp; However, it seems like, if creating human beings wasn't an option, that it might be okay to create a very large population of animals, the majority of which have positive welfare, but the some of which are in pain.&nbsp; For instance, it seems like it would be good to create a population of animals where one section of the population has 100 total welfare, and another section has -75, since the total welfare is 25.&nbsp;</p>\n<p>The problem is that this leads to what seems like a circular preference.&nbsp; If the population of animals with 100 welfare existed by itself it would be okay to not create it in order to create a population of humans with 50 welfare instead.&nbsp; But if the population we are talking about is the one in (3) then doing that would result in the population discussed in (2), which is bad.</p>\n<p>My current solution to this dilemma is to include a stipulation that a population with negative utility can never be better than one with positive utility.&nbsp; This prevents me from having circular preferences about these scenarios.&nbsp; But it might create some weird problems.&nbsp; If population (2) is created anyway, and the humans in it are unable to help the suffering animals in any way, does that mean they have a duty to create lots of happy animals to get their population's utility up to a positive level?&nbsp; That seems strange, especially since creating the new happy animals won't help the suffering ones in any way.&nbsp; On the other hand, if the humans are able to help the suffering animals, and they do so by means of some sort of utility transfer, then it would be in the best interests to create lots of happy animals, to reduce the amount of utility each person has to transfer.</p>\n<p>So far some of the solutions I am considering include:</p>\n<p>1. Instead of focusing on population ethics, just consider complex humane values to have greater weight in utility calculations than pleasure or paperclips.&nbsp; I find this idea distasteful because it implies it would be acceptable to inflict large harms on animals for relatively small gains for humans.&nbsp; In addition, if the weight is not sufficiently great it could still lead to an AI exterminating the human race and replacing them with happy animals, since animals are easier to take care of and make happy than humans.</p>\n<p>2. It is bad to create the human population in (2) if the only way to do so is to create a huge amount of suffering animals.&nbsp; But once both populations have been created, if the human population is unable to help the animal population, they have no duty to create as many happy animals as they can.&nbsp; This is because the two populations are not causally connected, and that is somehow morally significant. This makes some sense to me, as I don't think the existence of causally disconnected populations in the <a href=\"http://wiki.lesswrong.com/wiki/Many-worlds_interpretation\">vast universe</a> should bear any significance on my decision-making.</p>\n<p>3. There is some sort of overriding consideration besides utility that makes (3) seem desirable.&nbsp; For instance, it might be bad for creatures with any sort of values to go extinct, so it is good to create a population to prevent this, as long as its utility is positive on the net.&nbsp; However, this would change in a situation where utility is negative, such as in (2).</p>\n<p>4. Reasons to create a creature have some kind complex rock-paper-scissors-type \"trumping\" hierarchy.&nbsp; In other words, the fact that the humans have humane values can override the reasons to create a happy animals, but they cannot override the reason to not create suffering animals.&nbsp; The reasons to create happy animals, however, can override the reasons to not create suffering animals.&nbsp; I think that this argument might lead to inconsistent preferences again, but I'm not sure.</p>\n<p>I find none of these solutions that satisfying.&nbsp; I would really appreciate it if someone could help me with solving this dilemma.&nbsp; I'm very hopeful about this ethical theory, and would like to see it improved.</p>\n<p>&nbsp;</p>\n<p>*Update.&nbsp; After considering the issue some more, I realized that my dissatisfaction came from equivocating two different scenarios.&nbsp; I was considering the scenario, \"Animals with 100 utility and animals with -75 utility are created, no humans are created at all\" to be the same as the scenario \"Humans with 50 utility and animals with -75 utility are created, then the humans (before the get to experience their 50 utility) are killed/harmed in order to create more animals without helping the suffering animals in any way\" to be the same scenario.&nbsp; They are clearly not.</p>\n<p>To make the analogy more obvious, imagine I was given a choice between creating a person who would experience 95 utility over the course of their life, or a person who would experience 100 utility over the course of their life.&nbsp; I would choose the person with 100 utility.&nbsp; But if the person destined to experience 95 utility already existed, but had not experienced the majority of that utility yet, I would oppose killing them and replacing them with the 100 utility person.</p>\n<p>Or to put it more succinctly, I am willing to not create some happy humans to prevent some suffering animals from being created.&nbsp; And if the suffering animals and happy humans already exist I am willing to harm the happy humans to help the suffering animals.&nbsp; But if the suffering animals and happy humans already exist I am not willing to harm the happy humans to create some extra happy animals that will not help the existing suffering animals in any way.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d3TQrGGTnEoDaW4Dp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sW7xgc9P2SaNnqKHk", "gb6zWstjmkYHLrbrg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-28T04:55:48.361Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Pascal's Wager Fallacy Fallacy", "slug": "seq-rerun-the-pascal-s-wager-fallacy-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.240Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iQoYQNs9d3aCowuYS/seq-rerun-the-pascal-s-wager-fallacy-fallacy", "pageUrlRelative": "/posts/iQoYQNs9d3aCowuYS/seq-rerun-the-pascal-s-wager-fallacy-fallacy", "linkUrl": "https://www.lesswrong.com/posts/iQoYQNs9d3aCowuYS/seq-rerun-the-pascal-s-wager-fallacy-fallacy", "postedAtFormatted": "Thursday, March 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Pascal's%20Wager%20Fallacy%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Pascal's%20Wager%20Fallacy%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQoYQNs9d3aCowuYS%2Fseq-rerun-the-pascal-s-wager-fallacy-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Pascal's%20Wager%20Fallacy%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQoYQNs9d3aCowuYS%2Fseq-rerun-the-pascal-s-wager-fallacy-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQoYQNs9d3aCowuYS%2Fseq-rerun-the-pascal-s-wager-fallacy-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/z0/the_pascals_wager_fallacy_fallacy/\">The Pascal's Wager Fallacy Fallacy</a> was originally published on 18 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Pascal.27s_Wager_Fallacy_Fallacy\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People hear about a gamble involving a big payoff, and dismiss it as a form of Pascal's Wager. But the size of the payoff is not the flaw in Pascal's Wager. Just because an option has a very large potential payoff does not mean that the probability of getting that payoff is small, or that there are other possibilities that will cancel with it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/h2l/seq_rerun_what_do_we_mean_by_rationality/\">What Do We Mean By \"Rationality\"?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iQoYQNs9d3aCowuYS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.15141302775458e-06, "legacy": true, "legacyId": "22140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TQSb4wd6v5C3p6HX2", "GGYBD9wNzMRzuueKz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-28T05:58:50.447Z", "modifiedAt": null, "url": null, "title": "Noticing the 5-second mindkill", "slug": "noticing-the-5-second-mindkill", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:36.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d7eZPa3PMggf599va/noticing-the-5-second-mindkill", "pageUrlRelative": "/posts/d7eZPa3PMggf599va/noticing-the-5-second-mindkill", "linkUrl": "https://www.lesswrong.com/posts/d7eZPa3PMggf599va/noticing-the-5-second-mindkill", "postedAtFormatted": "Thursday, March 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Noticing%20the%205-second%20mindkill&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANoticing%20the%205-second%20mindkill%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd7eZPa3PMggf599va%2Fnoticing-the-5-second-mindkill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Noticing%20the%205-second%20mindkill%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd7eZPa3PMggf599va%2Fnoticing-the-5-second-mindkill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd7eZPa3PMggf599va%2Fnoticing-the-5-second-mindkill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<p>I've been reading news and a headline popped out at me:</p>\n<p style=\"padding-left: 30px;\">Some Conservative backbenchers stirred up controversy in the House of Commons Tuesday when they accused their own party of preventing them from speaking out in Parliament.</p>\n<p>(For the US audience, the Canadian (and British) House of Commons is like the House of Representatives, only with less democracy.)</p>\n<p><strong>My first thought</strong>: how dare the Prime Minister muzzles democratically elected MPs!</p>\n<p>(For the US audience, the Prime Minister in a majority government has the power of the President and the majority leaders in both chambers combined, and much much more. \"MP\" (Member of Parliament) is the equivalent of a \"Rep.\" Backbenchers are the reps who don't get a portfolio in the administration. Indeed, basically no separation of the legislative power from the executive. As I said, less democracy. Blame the Brits.)</p>\n<p>Then I keep reading:</p>\n<p style=\"padding-left: 30px;\">Warawa [the MP in question] did not specify the topic, but it&rsquo;s widely believed that he wanted to bring up his motion calling on parliamentarians to condemn sex-selective abortion.</p>\n<p><strong>My next thought</strong>: oh, good on the Prime Minister to prevent that crazy lunatic from pushing his pro-life agenda!</p>\n<p>And finally, <strong>my third thought</strong>: WTF did just happen? I changed my mind 180 degrees instantly because I disagree with the person's opinion, even though the original issue didn't go away. Mindkill in action. Had he been trying to promote, say, legalization of marijuana instead, I would have been twice as indignant about the evil PM.&nbsp;</p>\n<p>Now, I do notice this sometimes (often when reading something on LW), but probably not every time it happens to me. I want to notice it more often.</p>\n<p>So, I'm asking people to give your own (hopefully non-political) examples of noticing your instant about-face and hopefully some experience in recognizing it more reliably.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d7eZPa3PMggf599va", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 1.1514555223291186e-06, "legacy": true, "legacyId": "22141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-28T17:14:46.884Z", "modifiedAt": null, "url": null, "title": "Is The Blood Thicker Near The Tropics? Trade-Offs Of Living In The Cold", "slug": "is-the-blood-thicker-near-the-tropics-trade-offs-of-living", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:07.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bN9ZKXP89aGhoL4fY/is-the-blood-thicker-near-the-tropics-trade-offs-of-living", "pageUrlRelative": "/posts/bN9ZKXP89aGhoL4fY/is-the-blood-thicker-near-the-tropics-trade-offs-of-living", "linkUrl": "https://www.lesswrong.com/posts/bN9ZKXP89aGhoL4fY/is-the-blood-thicker-near-the-tropics-trade-offs-of-living", "postedAtFormatted": "Thursday, March 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20The%20Blood%20Thicker%20Near%20The%20Tropics%3F%20Trade-Offs%20Of%20Living%20In%20The%20Cold&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20The%20Blood%20Thicker%20Near%20The%20Tropics%3F%20Trade-Offs%20Of%20Living%20In%20The%20Cold%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbN9ZKXP89aGhoL4fY%2Fis-the-blood-thicker-near-the-tropics-trade-offs-of-living%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20The%20Blood%20Thicker%20Near%20The%20Tropics%3F%20Trade-Offs%20Of%20Living%20In%20The%20Cold%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbN9ZKXP89aGhoL4fY%2Fis-the-blood-thicker-near-the-tropics-trade-offs-of-living", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbN9ZKXP89aGhoL4fY%2Fis-the-blood-thicker-near-the-tropics-trade-offs-of-living", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1366, "htmlBody": "<p>A few centuries ago it was believed that the reason why people near the tropics didn't achieve the level of affluence of their northern conspecifics was that the heat made the blood grow thicker, and that slowed down their movements, and thoughts (thoughts at that time used to take place not only in the head, but also in the heart).&nbsp;</p>\n<p>It's a funny theory, very catchy, as mechanistic as the time demanded and all that. No wonder it was appreciated for a while.&nbsp;</p>\n<p>Many centuries have passed now, and we have a lot of better hypotheses for why there is less development in tropical areas than elsewhere. Here are a few.&nbsp;</p>\n<p>More diseases that consume family resources</p>\n<p>Lower average IQ</p>\n<p>Centuries of exploitation by Europe and US</p>\n<p>Fewer Institutions (There is a terrible paper by Daron Acemoglu, whom I hear otherwise is a great economist, on that)</p>\n<p>Shorter east-west axis within a land area (Guns, Germs and Steel)</p>\n<p>More frequent natural disasters, in particular floods, leading to property damage.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Probably all of those play a small role. I just want to say that primitive as it is as an explanation, I still think that the heat, and sunshine that comes with it, is a very strong factor, still today. Development is not my target though, my target is<em> individual productivity</em>&nbsp;and <em>individual freedom</em>, here thought of as \"amount of things per unit time someone could be doing\", not political freedom.&nbsp;</p>\n<p>So far I've spent three weeks in England, at the Future of Humanity Institute in Oxford, this month. During those 21 days, I have experienced strictly 08 (eight) <em>minutes</em> of sunlight. Outside it is freezing. So no wonder that all interactions I had were had inside walls. Meanwhile talking to friends back home, at the Tropic of Capricorn, they had outside parties, picnics at the park, bike riding days, shopping outside in the streets, free dancing at the streets festivals, learning to do slacklining, swimming pool etc...</p>\n<p>In this grey lowlight world of English weather, with the added factor that by and large interaction between anglophones is mostly linguistic, it is not admirable that many come to office even on Saturdays and Sundays, and also stay late during weekdays. Basically, where else would they go?&nbsp;</p>\n<p>The same distinction I saw while in California when comparing it to Boston. Both I saw in the winter-ish. In Boston you can basically choose in which venue you will eat, and in which venue you will read. In California you could go to the park, or to the mountains, or hike in the woods, or walk through the beach, or even go to a theme park, or that weird place where people surf false waves...</p>\n<p>Brains are devices you can train. If you train them to skateboard, or play with dogs, or play soccer in a park, that is what they will learn. If a brain is compelled to think all day long, and read, speak, listen and write, that is what it will get good at.&nbsp;</p>\n<p>The cold constrains, and a lot, what people do on a daily basis, and thus they become more specialized, and better, in the things they do. I think that this plays an enormous role on why tropical people don't tend to intellectual/high productivity lives as much as people in colder regions.&nbsp;</p>\n<p>A few more subtle considerations: There are human drives relating to outside activity that not even the cold can stop. But it can still significantly hinder. Groups of young people still summon the&nbsp;strength to face the cold in particular for two activities: Training for sport competitions, and staying in line for a dancing club. Curiously, those are <em>ritualized forms of hunting and courtship</em>, something that our most northern relative, the Japanese Macaque, finds worthy of leaving hot baths to do. You'll find Japanese Macaques walking around in the snow for the same reasons you'll find someone walking around in the snow in many of the coldest cities, and that is saying something. Kids in both species also<em> play</em> outside heated areas. Playing, finding food (or defeating rowing arc-rivals), and doing some sort of ritualized courtship are sufficiently worthy, for us and them,to face the spiking thorns of the cold.&nbsp;</p>\n<p>The cold transforms sport into just sport. Get there quick, enter, play, leave. Whichever surrounding rituals could have arisen around sport, either they are left for the summer time, or they will perish culturally.</p>\n<p>Same with the nightclub lines. No one will stay more than one second longer than necessary outside, they become only lines, strictly lines, and mini-skirted women pay in pain the price of wanting to be attractive/sensual. Men do also wearing fewer coats. No extra time before or after the party. And the only kind of making up that is allowed outside is the <em>really drunk</em>&nbsp;kind, since no one whose peripheral nervous system is sending the right signals to their brain would tolerate that cold, the same peripheral nervous system that should be delivering ecstatic feelings of seduction and desire.&nbsp;</p>\n<p>Young people pay quite a price for the cold. But it's nowhere near the price that older people pay. In an Arabic country, there is a disproportion of males in the streets, and a western eye will frequently think that this is prejudice, or something bad, happening against women. Boston and Oxford are university towns, but even accounting for that, the absence of people at the 40-80 age group in the streets is shocking. In Buenos Aires, 23:00 on a Tuesday, you'll see hundreds of people, of many ages, strolling around the streets, chatting, having dinner, drinking beer, laughing etc... same for Rio, or S&atilde;o Paulo. Some people face the cold at older ages in Oxford and Boston, but not so many, they could get a cold after all, and they are mostly done with sport and nightclubs. There are more women walking around in Syria, than 50 year olds walking around in Oxford.&nbsp;</p>\n<p>Lightlevels are also higher in inside areas than outside areas, as far as I recall, both in Boston and in Oxford, though not in California, Florida or the Latin cities cited. One more reason to stay inside.</p>\n<p>My claim is then that life is more productive in the cold because the cold significantly constrains what people do, and it constrains it in the way that makes them produce for longer periods output of linguistic sort -including maths and programming and everything that is mostly parsing, coding, transforming symbols etc... - I'd further claim that this effect cannot be accounted for by the six factors mentioned above, and that it will at least be comparable in intensity with whichever one ends up being the strongest one among those.&nbsp;</p>\n<p>A further claim is that because life in cold areas is significantly constrained, moving to colder areas is a costly signal of willingness to do lots of work. This could partially explain why most of the top 20 universities in the world are in very cold areas. You must really love studying if you are willing to constrain your life that much, and conversely, once your life is constrained, you'd better love studying. &nbsp;</p>\n<p>Speaking of love, stats famously show that people in California are not happier than people in New England. Julia Galef famously <a href=\"http://measureofdoubt.com/author/measureofdoubt/\">disagrees</a>. I don't know if the effect is neutral if you compare people born in one place who moved to the other. Like her, I'd bet highly it isn't. Sure after a long period there is a regression towards base level happiness, but I'll bet the regression is slow and incomplete, and the process takes very long.&nbsp;</p>\n<p>I've spent about six months of my life in cold areas, partly travelling, partly working/researching. Despite all the costs that it entails, at this moment my inclination is to decide to live in one of those cold lowlight areas for a while. Get some work done, or some more work done, of a research kind, now that movement building already took some 2 years of me. I wrote this partly to better understand the trade-offs, to more clearly think about this decision. I hope it helps someone else who is thinking about similar, or opposite, decisions, I've met at least one person here, and one back in the US who were thinking of doing the reverse.&nbsp;</p>\n<p>No wonder I'm writing from Oxford...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bN9ZKXP89aGhoL4fY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 9, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "22143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-28T18:59:24.860Z", "modifiedAt": null, "url": null, "title": "Suggestion: LW Meditation Hall ", "slug": "suggestion-lw-meditation-hall", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "d6PEZ3D4EBLEeTsvi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cXy7vfrRspnNhG9Fv/suggestion-lw-meditation-hall", "pageUrlRelative": "/posts/cXy7vfrRspnNhG9Fv/suggestion-lw-meditation-hall", "linkUrl": "https://www.lesswrong.com/posts/cXy7vfrRspnNhG9Fv/suggestion-lw-meditation-hall", "postedAtFormatted": "Thursday, March 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestion%3A%20LW%20Meditation%20Hall%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestion%3A%20LW%20Meditation%20Hall%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXy7vfrRspnNhG9Fv%2Fsuggestion-lw-meditation-hall%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestion%3A%20LW%20Meditation%20Hall%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXy7vfrRspnNhG9Fv%2Fsuggestion-lw-meditation-hall", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXy7vfrRspnNhG9Fv%2Fsuggestion-lw-meditation-hall", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>Lets make up a LW Meditation Hall, analog to the LW Study Hall.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Judging from my own (limited) experience, meditation might profit even more from doing it together than working.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">It should work more or less like the study hall, with one exception:</p>\n<p style=\"margin-bottom: 0cm;\">Sessions need to be scheduled in advance (There is probably not enough interest in meditation to keep the hall filled 24/7, and people will want to take part from the beginning instead of stumbling in in the middle).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cXy7vfrRspnNhG9Fv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 1.1519819724471243e-06, "legacy": true, "legacyId": "22144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-28T19:33:15.032Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Rationality Habits and Friendship", "slug": "meetup-vancouver-rationality-habits-and-friendship", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eapK6DnHt7SkSFzre/meetup-vancouver-rationality-habits-and-friendship", "pageUrlRelative": "/posts/eapK6DnHt7SkSFzre/meetup-vancouver-rationality-habits-and-friendship", "linkUrl": "https://www.lesswrong.com/posts/eapK6DnHt7SkSFzre/meetup-vancouver-rationality-habits-and-friendship", "postedAtFormatted": "Thursday, March 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Rationality%20Habits%20and%20Friendship&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Rationality%20Habits%20and%20Friendship%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeapK6DnHt7SkSFzre%2Fmeetup-vancouver-rationality-habits-and-friendship%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Rationality%20Habits%20and%20Friendship%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeapK6DnHt7SkSFzre%2Fmeetup-vancouver-rationality-habits-and-friendship", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeapK6DnHt7SkSFzre%2Fmeetup-vancouver-rationality-habits-and-friendship", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/kz'>Vancouver Rationality Habits and Friendship</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 April 2013 03:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will discuss Anna's <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">list of rationality habits</a>, and then we will discuss how to make more and better friends, and the importance of this.</p>\n\n<p>Join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>This post sounds really dry and boring but I promise we are fun and exciting!</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/kz'>Vancouver Rationality Habits and Friendship</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eapK6DnHt7SkSFzre", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.152004802548944e-06, "legacy": true, "legacyId": "22145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Rationality_Habits_and_Friendship\">Discussion article for the meetup : <a href=\"/meetups/kz\">Vancouver Rationality Habits and Friendship</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 April 2013 03:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W broadway, vancouver bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will discuss Anna's <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">list of rationality habits</a>, and then we will discuss how to make more and better friends, and the importance of this.</p>\n\n<p>Join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>This post sounds really dry and boring but I promise we are fun and exciting!</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Rationality_Habits_and_Friendship1\">Discussion article for the meetup : <a href=\"/meetups/kz\">Vancouver Rationality Habits and Friendship</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Rationality Habits and Friendship", "anchor": "Discussion_article_for_the_meetup___Vancouver_Rationality_Habits_and_Friendship", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Rationality Habits and Friendship", "anchor": "Discussion_article_for_the_meetup___Vancouver_Rationality_Habits_and_Friendship1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-29T04:45:28.140Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Rationalist Fiction", "slug": "seq-rerun-rationalist-fiction", "viewCount": null, "lastCommentedAt": "2013-03-29T04:45:28.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Gk5p2xyKjiDBachf/seq-rerun-rationalist-fiction", "pageUrlRelative": "/posts/2Gk5p2xyKjiDBachf/seq-rerun-rationalist-fiction", "linkUrl": "https://www.lesswrong.com/posts/2Gk5p2xyKjiDBachf/seq-rerun-rationalist-fiction", "postedAtFormatted": "Friday, March 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Rationalist%20Fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Rationalist%20Fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Gk5p2xyKjiDBachf%2Fseq-rerun-rationalist-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Rationalist%20Fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Gk5p2xyKjiDBachf%2Fseq-rerun-rationalist-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Gk5p2xyKjiDBachf%2Fseq-rerun-rationalist-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/3m/rationalist_fiction/\">Rationalist Fiction</a> was originally published on 19 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Rationalist_Fiction\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What works of fiction are out there that show characters who have acquired their skills at rationality through practice, and who we can watch in the act of employing those skills?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h30/seq_rerun_the_pascals_wager_fallacy_fallacy/\">The Pascal's Wager Fallacy Fallacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Gk5p2xyKjiDBachf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1523775111397624e-06, "legacy": true, "legacyId": "22148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q79vYjHAE9KHcAjSs", "iQoYQNs9d3aCowuYS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-29T13:02:32.262Z", "modifiedAt": null, "url": null, "title": "Meetup : Padeborn Meetup April 3th", "slug": "meetup-padeborn-meetup-april-3th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/egL5JPzC7nk4cszYe/meetup-padeborn-meetup-april-3th", "pageUrlRelative": "/posts/egL5JPzC7nk4cszYe/meetup-padeborn-meetup-april-3th", "linkUrl": "https://www.lesswrong.com/posts/egL5JPzC7nk4cszYe/meetup-padeborn-meetup-april-3th", "postedAtFormatted": "Friday, March 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Padeborn%20Meetup%20April%203th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Padeborn%20Meetup%20April%203th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FegL5JPzC7nk4cszYe%2Fmeetup-padeborn-meetup-april-3th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Padeborn%20Meetup%20April%203th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FegL5JPzC7nk4cszYe%2Fmeetup-padeborn-meetup-april-3th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FegL5JPzC7nk4cszYe%2Fmeetup-padeborn-meetup-april-3th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l0'>Padeborn Meetup April 3th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 April 2013 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening will probably include some techniques on how to invoke curiosity in yourself and in other people, and some thinking on how to introduce rationality to someone who doesn't know what you are talking about.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l0'>Padeborn Meetup April 3th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "egL5JPzC7nk4cszYe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1527131800514498e-06, "legacy": true, "legacyId": "22150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Padeborn_Meetup_April_3th\">Discussion article for the meetup : <a href=\"/meetups/l0\">Padeborn Meetup April 3th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 April 2013 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening will probably include some techniques on how to invoke curiosity in yourself and in other people, and some thinking on how to introduce rationality to someone who doesn't know what you are talking about.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Padeborn_Meetup_April_3th1\">Discussion article for the meetup : <a href=\"/meetups/l0\">Padeborn Meetup April 3th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Padeborn Meetup April 3th", "anchor": "Discussion_article_for_the_meetup___Padeborn_Meetup_April_3th", "level": 1}, {"title": "Discussion article for the meetup : Padeborn Meetup April 3th", "anchor": "Discussion_article_for_the_meetup___Padeborn_Meetup_April_3th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-29T15:46:15.374Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Austin, Berlin, Durham, London, Montreal, Vancouver", "slug": "weekly-lw-meetups-atlanta-austin-berlin-durham-london", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4u5jnMENLgYKFPbYi/weekly-lw-meetups-atlanta-austin-berlin-durham-london", "pageUrlRelative": "/posts/4u5jnMENLgYKFPbYi/weekly-lw-meetups-atlanta-austin-berlin-durham-london", "linkUrl": "https://www.lesswrong.com/posts/4u5jnMENLgYKFPbYi/weekly-lw-meetups-atlanta-austin-berlin-durham-london", "postedAtFormatted": "Friday, March 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Montreal%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Montreal%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4u5jnMENLgYKFPbYi%2Fweekly-lw-meetups-atlanta-austin-berlin-durham-london%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Montreal%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4u5jnMENLgYKFPbYi%2Fweekly-lw-meetups-atlanta-austin-berlin-durham-london", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4u5jnMENLgYKFPbYi%2Fweekly-lw-meetups-atlanta-austin-berlin-durham-london", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 493, "htmlBody": "<p><strong>This summary was posted to LW main on March 22nd. The following week's summary is <a href=\"/lw/h3b/weekly_lw_meetups_atlanta_brussels_london/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/km\">Durham HPMoR meetup, ch. 47-50:&nbsp;<span class=\"date\">23 March 2013 11:30AM</span></a></li>\n<li><a href=\"/meetups/ko\">Vancouver:&nbsp;<span class=\"date\">23 March 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/kh\">Berlin social:&nbsp;<span class=\"date\">23 March 2013 05:00PM</span></a></li>\n<li><a href=\"/meetups/kd\">London Meetup, 24th March: Steelmanning:&nbsp;<span class=\"date\">24 March 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/kn\">Montreal Meetup - Bayes' Theorem:&nbsp;<span class=\"date\">25 March 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/kj\">Atlanta: ATLessWrong Meetup:&nbsp;<span class=\"date\">29 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/kp\">Brussels Biased Boardgaming:&nbsp;<span class=\"date\">30 March 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/jp\">Munich Meetup (updated):&nbsp;<span class=\"date\">01 April 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/k9\">Vienna Meetup #2 - :&nbsp;<span class=\"date\">13 April 2013 07:06PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">23 March 2019 01:30PM</span></a><a href=\"/meetups/kg\"></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4u5jnMENLgYKFPbYi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1528237765068724e-06, "legacy": true, "legacyId": "22083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ivs7m53n8AntGutf", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-29T17:20:23.897Z", "modifiedAt": null, "url": null, "title": "Robin Hanson's Cryonics Hour", "slug": "robin-hanson-s-cryonics-hour", "viewCount": null, "lastCommentedAt": "2020-01-23T01:40:30.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bQN4KcRZXRjhoCnZk/robin-hanson-s-cryonics-hour", "pageUrlRelative": "/posts/bQN4KcRZXRjhoCnZk/robin-hanson-s-cryonics-hour", "linkUrl": "https://www.lesswrong.com/posts/bQN4KcRZXRjhoCnZk/robin-hanson-s-cryonics-hour", "postedAtFormatted": "Friday, March 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robin%20Hanson's%20Cryonics%20Hour&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobin%20Hanson's%20Cryonics%20Hour%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQN4KcRZXRjhoCnZk%2Frobin-hanson-s-cryonics-hour%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robin%20Hanson's%20Cryonics%20Hour%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQN4KcRZXRjhoCnZk%2Frobin-hanson-s-cryonics-hour", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQN4KcRZXRjhoCnZk%2Frobin-hanson-s-cryonics-hour", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>I'm writing to recommend something awesome to anyone who's recently signed up for cryonics (and to the future self of anyone who's about to do so). Robin Hanson has a longstanding offer that <a href=\"http://www.overcomingbias.com/2009/03/my-cryonics-hour.html\">anyone who's newly signed up for cryonics can have an hour's discussion with him on any topic</a>, and I took him up on that last week.</p>\n<p>I expected to have a fascinating and wide-ranging discussion on various facets of futurism. <strong>My expectations were exceeded.</strong> Even if you've been reading <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> for a long time, talking with Robin is an order of magnitude more stimulating/persuasive/informative than reading OB or even watching him debate someone else, and I'm now reconsidering my thinking on a number of topics as a result.</p>\n<p>So if you've recently signed up, <a href=\"mailto:rhanson@gmu.edu\">email Robin</a>; and if you're intending to sign up, let this be one more incentive to quit procrastinating!</p>\n<p><strong>Relevant links:</strong></p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">The LessWrong Wiki article on cryonics</a>&nbsp;is a good place to start if you have a bunch of questions about the topic.</p>\n<p>If you want to argue about whether signing up for cryonics is a good idea, two good and relatively recent threads on that subject are under the posts on <a href=\"/lw/1r0/a_survey_of_anticryonics_writing/\">A survey of anti-cryonics writing</a> and <a href=\"/lw/fz9/more_cryonics_probability_estimates/\">More Cryonics Probability Estimates</a>.</p>\n<p>And if you are cryocrastinating (you've decided that you should sign up for cryonics, but you haven't yet), <a href=\"/lw/e5e/how_to_get_cryocrastinators_to_actually_sign_up/\">here's a LW thread</a>&nbsp;about taking the first step.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bQN4KcRZXRjhoCnZk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 47, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "22152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZXaRHHLsxaTTQQsZb", "TK5McpcF584e9mFCy", "E4RXMhsXyagTW2qKf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-29T19:08:09.781Z", "modifiedAt": null, "url": null, "title": "[Link] Researchers devise technique to allow X-ray crystallography of un-crystallized molecule groups", "slug": "link-researchers-devise-technique-to-allow-x-ray", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X3ntArSfvEXQFpbPD/link-researchers-devise-technique-to-allow-x-ray", "pageUrlRelative": "/posts/X3ntArSfvEXQFpbPD/link-researchers-devise-technique-to-allow-x-ray", "linkUrl": "https://www.lesswrong.com/posts/X3ntArSfvEXQFpbPD/link-researchers-devise-technique-to-allow-x-ray", "postedAtFormatted": "Friday, March 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Researchers%20devise%20technique%20to%20allow%20X-ray%20crystallography%20of%20un-crystallized%20molecule%20groups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Researchers%20devise%20technique%20to%20allow%20X-ray%20crystallography%20of%20un-crystallized%20molecule%20groups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3ntArSfvEXQFpbPD%2Flink-researchers-devise-technique-to-allow-x-ray%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Researchers%20devise%20technique%20to%20allow%20X-ray%20crystallography%20of%20un-crystallized%20molecule%20groups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3ntArSfvEXQFpbPD%2Flink-researchers-devise-technique-to-allow-x-ray", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3ntArSfvEXQFpbPD%2Flink-researchers-devise-technique-to-allow-x-ray", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<blockquote>\n<p><a style=\"font-family: Arial, Helvetica, Sans; font-size: 12.000001907348633px;\" href=\"http://phys.org/news/2013-03-technique-x-ray-crystallography-un-crystallized-molecule.html#jCp\">(Phys.org) &mdash;A team of researchers working in Japan has developed a method for allowing X-ray crystallography to work on molecular groups that have not first been crystallized. In their paper published in the journal Nature, the group describes how they built small scaffolds that resemble pockets for the molecules to rest in, securing them in place and allowing for X-ray crystallography analysis.&nbsp;</a></p>\n</blockquote>\n<p>If this process works reliably it's probably the biggest scientific breakthrough of the year. Nanotechonlogy will get a boost from the ability to inexpensively determine the structure of a lot of molecule that we couldn't visualize beforehand.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X3ntArSfvEXQFpbPD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 1.1529601960579676e-06, "legacy": true, "legacyId": "22153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T04:32:42.980Z", "modifiedAt": null, "url": null, "title": "Drowning In An Information Ocean", "slug": "drowning-in-an-information-ocean", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fRr8625imjoP7FLFs/drowning-in-an-information-ocean", "pageUrlRelative": "/posts/fRr8625imjoP7FLFs/drowning-in-an-information-ocean", "linkUrl": "https://www.lesswrong.com/posts/fRr8625imjoP7FLFs/drowning-in-an-information-ocean", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drowning%20In%20An%20Information%20Ocean&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrowning%20In%20An%20Information%20Ocean%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRr8625imjoP7FLFs%2Fdrowning-in-an-information-ocean%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drowning%20In%20An%20Information%20Ocean%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRr8625imjoP7FLFs%2Fdrowning-in-an-information-ocean", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRr8625imjoP7FLFs%2Fdrowning-in-an-information-ocean", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1777, "htmlBody": "<p><strong>Drowning In An Information Ocean</strong></p>\n<p>I decided to take a look at the books hanging around the Future of Humanity Institute. It is a sobering and sad experience. I'd say there are little less than 2 thousand books.</p>\n<p>80% of books I wouldn't mind reading,</p>\n<p>1/2 I would read,</p>\n<p>1/3 I should read</p>\n<p>and 1/5 I must read! &nbsp;</p>\n<p>I predict that I'll read actually 1/400, counting the ones there, and their enhanced successors. How emotionally terrible is it to live in such a technically competent society and want to understand the world! Since 2000 I've abandoned TV, videogames, celebrity gossip, musical ability, knowledge about bands, politics, theater classes, dancing classes, handball, tennis, reading fiction, reading parts of Facebook, maintaining contact with groups X and Y of friends, newspapers, magazines and comics. All in the name of keeping up with human knowledge on some areas that fascinate me. Mostly areas having to do with the nature of minds and mental states. Come to think of it, the only two things that <em>really, really interest me</em> are minds and evolution. My curiosity is very narrow, it should be no trouble to learn a satisfactory amount about two things, right? So if you want to know what a mind is and what it does, and to get a grasp on the outlook of evolved stuff, you need to go through areas like:</p>\n<p>Positive Psychology, Evolutionary Psychology, Animal Cognition (Ethology), Cultural Evolution, Cognitive Neuroscience, Cognitive Science, Artificial Intelligence, Philosophy of Mind, Philosophy of Cognitive Science, Primatology, Physical and Biological Anthropology. &nbsp;</p>\n<p>Which I did.&nbsp;</p>\n<p>Dig up a bit and you'll find that those require knowledge from Evolutionary Biology, Neuroeconomics, Basic Neuroscience, Genetics, Proof Theory, Formal Logic, Anthropic Reasoning - And from Anthropic Reasoning, you get a lot of physics requirements, mostly in cosmology and a bit in particle physics. Dig a little further and you can't get a lot of what is up there without grasping Maynard Smith and Trivers thoughts on biology, which come from economics, and by the time you notice you are surrounded by isoquants, comparing stable equilibriums across disciplines and thinking of economic metaphors for how the PreMedial Ventral Cortex settles some decision issues. Which of course requires that you understand metaphors, and you'll have to check some Hofstadter and Pinker on those issues, which will require at least some very basic linguistics, or at least an outlook of&nbsp;philosophy&nbsp;of language. Did I mention that most of this only works if you are rational, and that means you'd better have read the sequences prior to all this stuff?</p>\n<p>Then there are the nagging exact sciences people. They come to you at night, haunt you in your dreams, telling you how much you should study math, how math is important for this, for that, and for that. Most disagree which branches of math are important, stats being the most universal like. If I were to learn to all the math I was told to learn, that would be at least 3 years more. Scott Young can do an entire university course (CS) in one year, Nick Bostrom kept that pace for 6 or 7 years. Most people don't get the mix of time, luck, capacity, resources and most importantly, motivation, to pursue such Homeric tasks.</p>\n<p>I've never doubted Math is awesome. What I did doubt, and to this day I have seen few who doubt with me, but good examples being Peter Thiel, more strongly, and Jared Diamond and Dan Dennett, less strongly, is that so many young talents should be drawn into physics and math (and chess). Why should we make people who are really smart do the things in which it is easier to detect being smart? &nbsp;Companies don't ask their best employees to devise ever better and more complicated IQ tests just because IQ tests are good predictors of how good a worker will be. The goal is not to costly signal being near the upper bound in intelligence. The goal is using your intelligence to pursue your goals. Sure, lots of it will be signalling instrumentally, but once the dust settles, don't get fixed in proving the constructibility of enormously large polygons, or beating Kasparov. &nbsp;</p>\n<p>So far I've tried to make two cases: Even with <em>prima facie</em>&nbsp;narrow interests, anyone is bound to be drowning in an ocean of information, and the interconnectedness and requirements to understand narrow interests may be much larger than one's initial expectancy. Secondly the main modulator of what to do with intelligence (your own, or someone else's) should be to tune it with goals and interests, not with easy detectability.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Swimming Upwards</strong></p>\n<p>To avoid drowning in the ocean, I've already mentioned a lot of weight I found I could live without: <em>TV, videogames, celebrity gossip, musical ability, knowledge about bands, or politics, theater classes, dancing classes, handball, tennis, reading fiction, reading parts of Facebook, maintaining contact with groups X and Y of friends, newspapers, magazines and comics. </em>Those were not easy choices, each comes with a cost, a sadness, and a feeling that something valuable has been lost. The richness of flavors of life got somewhat poorer, because at least about minds and evolution, I wanted to keep track of human knowledge.&nbsp;</p>\n<p>It is hard enough not to go after understading Muons better, or knowing if really Brontosaurs had extra little brains throughout their neck, or why is it that vegetables are healthier than a double bacon cheeseburguer. But this tradeoff is knowing X versus knowing Y. It gets messy when it becomes earning X versus knowing Y, loving G versus knowing Y, containing curiosity about facebook update F versus knowing Y, and going to U's party versus knowing Y.&nbsp;</p>\n<p>Keeping a <a href=\"/lw/grk/positive_information_diet_take_the_challenge/\">positive information diet</a>&nbsp;helps, but I'm unsure even that stringent criterion is enough to know as much as one would like about one's narrow interests. Thus here I am, surrounded by 400 books I must read, and imagining how often new books that I'd put in the \"must read\" category are created every month. Probably same goes for amount of pages of scientific and philosophical journal papers. Stephen Hawking points out that you'd have to run faster than a car to read all written knowledge being created. I think the drowning metaphor is better because if books were liquid, you would quite likely not be able to swim even an aquarium of your own interests. I'm even considering moving to<a href=\"/lw/h33/is_the_blood_thicker_near_the_tropics_tradeoffs/\"> cold lowlight areas of the world</a>, just for the purpose of having less (distractions) weight even, so I can swim a little longer.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Writing, Advocating and Teaching&nbsp;</strong></p>\n<p>Finally, there is the ultimate tradeoff. Being a child versus being a parent. Getting memes versus spreading memes. Learning versus teaching. Exploration versus exploitation. Being directed versus directing. Paying attention versus becoming focus. Riding versus driving.&nbsp;</p>\n<p>Writing takes a ridiculously long time. To write this text so far took me about 2 hours. It is simple, autobiographical, uses mostly folk psychological concepts, and not very theory-laden. My rule of thumb for writing technical stuff is one hour per page. In that time I could read up to 40 times as much. Assuming a publishability of a page per 3, the choice is writing three books or reading the 400 that surround me. Surely a lot of learning requires reprocessing, and one of the best ways to learn is to reconfigure our mental constructs, and use inter-areas knowledge to compose new ideas out of read ones (Pasupathi2012). Writing is learning, but it is still costly learning.</p>\n<p>When thinking whether<a href=\"http://80000hours.org/blog/65-should-you-go-into-research-part-1\"> you should go into research</a>, not only all the different sorts of considerations suggested by the 80000hours community should be looked at, but also how much is that individual driven to sharing knowledge, once acquired. Some people really want to output as much as possible, but many care, by and large, mostly about the input, and given writing one book may cost reading up to 120, they can rest assured there will be very interesting material eager to be read, always jumping ahead their priority list. In the last two years, the Teens and Twenties, a conference for young cryonicists (many of whom lesswrongers) had, out of four personality types, a vast majority of curiosity driven individuals. Much more incentives are needed to get people writing their thesis than to get them reading about their thesis topics.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>The Examined Swim&nbsp;</strong></p>\n<p>From many perspectives, in particular that of technical achievement and development, it is great and fascinating that we live in such accelerated scientific age. In other states of mind, or ways of thinking, it is not that great. Those states of mind are not frequently ones that show up in books, specially not in academic courses. They deal not with the speed or depth of things, but breadth, gravity, resonance, luminance, sacredness. Some books, like <a href=\"http://www.amazon.com/The-Examined-Life-Philosophical-Meditations/dp/0671725017/ref=cm_cr_pr_product_top\">The Examined Life</a>, <a href=\"http://www.amazon.com/My-Life-Experiment-Becoming-Washington/dp/B004KAB4GA/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1364615177&amp;sr=1-1&amp;keywords=guinea+pig+diaries\">The Guinea Pig Diaries</a>, <a href=\"http://www.amazon.com/Mortals-Others-American-1931-1935-Paperbacks/dp/0415125855/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1364615387&amp;sr=1-1&amp;keywords=mortals+and+other\">Mortals and Others</a>, and lots of <a href=\"http://www.youtube.com/watch?v=El2l__hRta0\">songs</a> and movies deal with those aspects.&nbsp;</p>\n<p>Wearing the transhumanist technoprogressive hat, what I don't like about drowning is similar to what I don't like about the cosmological constant, it <em>would be really cool</em>&nbsp;if the speed of creation and my speed of&nbsp;absorption were exactly the same, and it would be really cool if the universe was stable instead of getting cold. It's something I can shrug about and move on.&nbsp;</p>\n<p>Wearing the other hat, the surrounding ocean of great books has a more sinister message to tell. It reminds me of the finitude of the human condition, it is a visual reminder of all I'll never know, never see, taste, borrow or steal. More than that, because all aspects of life are in constant dispute of attentional resources, it takes a lot of effort and anguish to choose to go for those books, the plunge is deep, and wearing this hat, I can't help but to think it may not be worth it.</p>\n<p>In a recent conversation with one of the enhancement researchers here he pointed out that it may be the case that for the individual Modafinil is not an enhancement, but for society as a whole it is. An individual won't change much due to taking Modafinil, and may pay costs if it has some particularly adverse effects for that person. Society on the other hand will be greatly benefited by the additional capacity of hundreds of thousands of scientists, each a little smarter.</p>\n<p>It may well be that society needs you not to drown, and incentivizes you to swim as fast as you can, cost whom it may, it sure is the case in the corporate world. Thinking of yourself as an utility function and wearing the technoprogressive hat sure signal your allegiance to (this) society's cause. Yet wearing the other hat, as I often do, sometimes tempts me to let go and delve into the Siren's songs... &nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fRr8625imjoP7FLFs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 36, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "22155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Drowning_In_An_Information_Ocean\">Drowning In An Information Ocean</strong></p>\n<p>I decided to take a look at the books hanging around the Future of Humanity Institute. It is a sobering and sad experience. I'd say there are little less than 2 thousand books.</p>\n<p>80% of books I wouldn't mind reading,</p>\n<p>1/2 I would read,</p>\n<p>1/3 I should read</p>\n<p>and 1/5 I must read! &nbsp;</p>\n<p>I predict that I'll read actually 1/400, counting the ones there, and their enhanced successors. How emotionally terrible is it to live in such a technically competent society and want to understand the world! Since 2000 I've abandoned TV, videogames, celebrity gossip, musical ability, knowledge about bands, politics, theater classes, dancing classes, handball, tennis, reading fiction, reading parts of Facebook, maintaining contact with groups X and Y of friends, newspapers, magazines and comics. All in the name of keeping up with human knowledge on some areas that fascinate me. Mostly areas having to do with the nature of minds and mental states. Come to think of it, the only two things that <em>really, really interest me</em> are minds and evolution. My curiosity is very narrow, it should be no trouble to learn a satisfactory amount about two things, right? So if you want to know what a mind is and what it does, and to get a grasp on the outlook of evolved stuff, you need to go through areas like:</p>\n<p>Positive Psychology, Evolutionary Psychology, Animal Cognition (Ethology), Cultural Evolution, Cognitive Neuroscience, Cognitive Science, Artificial Intelligence, Philosophy of Mind, Philosophy of Cognitive Science, Primatology, Physical and Biological Anthropology. &nbsp;</p>\n<p>Which I did.&nbsp;</p>\n<p>Dig up a bit and you'll find that those require knowledge from Evolutionary Biology, Neuroeconomics, Basic Neuroscience, Genetics, Proof Theory, Formal Logic, Anthropic Reasoning - And from Anthropic Reasoning, you get a lot of physics requirements, mostly in cosmology and a bit in particle physics. Dig a little further and you can't get a lot of what is up there without grasping Maynard Smith and Trivers thoughts on biology, which come from economics, and by the time you notice you are surrounded by isoquants, comparing stable equilibriums across disciplines and thinking of economic metaphors for how the PreMedial Ventral Cortex settles some decision issues. Which of course requires that you understand metaphors, and you'll have to check some Hofstadter and Pinker on those issues, which will require at least some very basic linguistics, or at least an outlook of&nbsp;philosophy&nbsp;of language. Did I mention that most of this only works if you are rational, and that means you'd better have read the sequences prior to all this stuff?</p>\n<p>Then there are the nagging exact sciences people. They come to you at night, haunt you in your dreams, telling you how much you should study math, how math is important for this, for that, and for that. Most disagree which branches of math are important, stats being the most universal like. If I were to learn to all the math I was told to learn, that would be at least 3 years more. Scott Young can do an entire university course (CS) in one year, Nick Bostrom kept that pace for 6 or 7 years. Most people don't get the mix of time, luck, capacity, resources and most importantly, motivation, to pursue such Homeric tasks.</p>\n<p>I've never doubted Math is awesome. What I did doubt, and to this day I have seen few who doubt with me, but good examples being Peter Thiel, more strongly, and Jared Diamond and Dan Dennett, less strongly, is that so many young talents should be drawn into physics and math (and chess). Why should we make people who are really smart do the things in which it is easier to detect being smart? &nbsp;Companies don't ask their best employees to devise ever better and more complicated IQ tests just because IQ tests are good predictors of how good a worker will be. The goal is not to costly signal being near the upper bound in intelligence. The goal is using your intelligence to pursue your goals. Sure, lots of it will be signalling instrumentally, but once the dust settles, don't get fixed in proving the constructibility of enormously large polygons, or beating Kasparov. &nbsp;</p>\n<p>So far I've tried to make two cases: Even with <em>prima facie</em>&nbsp;narrow interests, anyone is bound to be drowning in an ocean of information, and the interconnectedness and requirements to understand narrow interests may be much larger than one's initial expectancy. Secondly the main modulator of what to do with intelligence (your own, or someone else's) should be to tune it with goals and interests, not with easy detectability.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Swimming_Upwards\">Swimming Upwards</strong></p>\n<p>To avoid drowning in the ocean, I've already mentioned a lot of weight I found I could live without: <em>TV, videogames, celebrity gossip, musical ability, knowledge about bands, or politics, theater classes, dancing classes, handball, tennis, reading fiction, reading parts of Facebook, maintaining contact with groups X and Y of friends, newspapers, magazines and comics. </em>Those were not easy choices, each comes with a cost, a sadness, and a feeling that something valuable has been lost. The richness of flavors of life got somewhat poorer, because at least about minds and evolution, I wanted to keep track of human knowledge.&nbsp;</p>\n<p>It is hard enough not to go after understading Muons better, or knowing if really Brontosaurs had extra little brains throughout their neck, or why is it that vegetables are healthier than a double bacon cheeseburguer. But this tradeoff is knowing X versus knowing Y. It gets messy when it becomes earning X versus knowing Y, loving G versus knowing Y, containing curiosity about facebook update F versus knowing Y, and going to U's party versus knowing Y.&nbsp;</p>\n<p>Keeping a <a href=\"/lw/grk/positive_information_diet_take_the_challenge/\">positive information diet</a>&nbsp;helps, but I'm unsure even that stringent criterion is enough to know as much as one would like about one's narrow interests. Thus here I am, surrounded by 400 books I must read, and imagining how often new books that I'd put in the \"must read\" category are created every month. Probably same goes for amount of pages of scientific and philosophical journal papers. Stephen Hawking points out that you'd have to run faster than a car to read all written knowledge being created. I think the drowning metaphor is better because if books were liquid, you would quite likely not be able to swim even an aquarium of your own interests. I'm even considering moving to<a href=\"/lw/h33/is_the_blood_thicker_near_the_tropics_tradeoffs/\"> cold lowlight areas of the world</a>, just for the purpose of having less (distractions) weight even, so I can swim a little longer.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Writing__Advocating_and_Teaching_\">Writing, Advocating and Teaching&nbsp;</strong></p>\n<p>Finally, there is the ultimate tradeoff. Being a child versus being a parent. Getting memes versus spreading memes. Learning versus teaching. Exploration versus exploitation. Being directed versus directing. Paying attention versus becoming focus. Riding versus driving.&nbsp;</p>\n<p>Writing takes a ridiculously long time. To write this text so far took me about 2 hours. It is simple, autobiographical, uses mostly folk psychological concepts, and not very theory-laden. My rule of thumb for writing technical stuff is one hour per page. In that time I could read up to 40 times as much. Assuming a publishability of a page per 3, the choice is writing three books or reading the 400 that surround me. Surely a lot of learning requires reprocessing, and one of the best ways to learn is to reconfigure our mental constructs, and use inter-areas knowledge to compose new ideas out of read ones (Pasupathi2012). Writing is learning, but it is still costly learning.</p>\n<p>When thinking whether<a href=\"http://80000hours.org/blog/65-should-you-go-into-research-part-1\"> you should go into research</a>, not only all the different sorts of considerations suggested by the 80000hours community should be looked at, but also how much is that individual driven to sharing knowledge, once acquired. Some people really want to output as much as possible, but many care, by and large, mostly about the input, and given writing one book may cost reading up to 120, they can rest assured there will be very interesting material eager to be read, always jumping ahead their priority list. In the last two years, the Teens and Twenties, a conference for young cryonicists (many of whom lesswrongers) had, out of four personality types, a vast majority of curiosity driven individuals. Much more incentives are needed to get people writing their thesis than to get them reading about their thesis topics.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Examined_Swim_\">The Examined Swim&nbsp;</strong></p>\n<p>From many perspectives, in particular that of technical achievement and development, it is great and fascinating that we live in such accelerated scientific age. In other states of mind, or ways of thinking, it is not that great. Those states of mind are not frequently ones that show up in books, specially not in academic courses. They deal not with the speed or depth of things, but breadth, gravity, resonance, luminance, sacredness. Some books, like <a href=\"http://www.amazon.com/The-Examined-Life-Philosophical-Meditations/dp/0671725017/ref=cm_cr_pr_product_top\">The Examined Life</a>, <a href=\"http://www.amazon.com/My-Life-Experiment-Becoming-Washington/dp/B004KAB4GA/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1364615177&amp;sr=1-1&amp;keywords=guinea+pig+diaries\">The Guinea Pig Diaries</a>, <a href=\"http://www.amazon.com/Mortals-Others-American-1931-1935-Paperbacks/dp/0415125855/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1364615387&amp;sr=1-1&amp;keywords=mortals+and+other\">Mortals and Others</a>, and lots of <a href=\"http://www.youtube.com/watch?v=El2l__hRta0\">songs</a> and movies deal with those aspects.&nbsp;</p>\n<p>Wearing the transhumanist technoprogressive hat, what I don't like about drowning is similar to what I don't like about the cosmological constant, it <em>would be really cool</em>&nbsp;if the speed of creation and my speed of&nbsp;absorption were exactly the same, and it would be really cool if the universe was stable instead of getting cold. It's something I can shrug about and move on.&nbsp;</p>\n<p>Wearing the other hat, the surrounding ocean of great books has a more sinister message to tell. It reminds me of the finitude of the human condition, it is a visual reminder of all I'll never know, never see, taste, borrow or steal. More than that, because all aspects of life are in constant dispute of attentional resources, it takes a lot of effort and anguish to choose to go for those books, the plunge is deep, and wearing this hat, I can't help but to think it may not be worth it.</p>\n<p>In a recent conversation with one of the enhancement researchers here he pointed out that it may be the case that for the individual Modafinil is not an enhancement, but for society as a whole it is. An individual won't change much due to taking Modafinil, and may pay costs if it has some particularly adverse effects for that person. Society on the other hand will be greatly benefited by the additional capacity of hundreds of thousands of scientists, each a little smarter.</p>\n<p>It may well be that society needs you not to drown, and incentivizes you to swim as fast as you can, cost whom it may, it sure is the case in the corporate world. Thinking of yourself as an utility function and wearing the technoprogressive hat sure signal your allegiance to (this) society's cause. Yet wearing the other hat, as I often do, sometimes tempts me to let go and delve into the Siren's songs... &nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Drowning In An Information Ocean", "anchor": "Drowning_In_An_Information_Ocean", "level": 1}, {"title": "Swimming Upwards", "anchor": "Swimming_Upwards", "level": 1}, {"title": "Writing, Advocating and Teaching\u00a0", "anchor": "Writing__Advocating_and_Teaching_", "level": 1}, {"title": "The Examined Swim\u00a0", "anchor": "The_Examined_Swim_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2wgu6JSL42ghz6QQe", "bN9ZKXP89aGhoL4fY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T06:03:22.218Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Why Our Kind Can't Cooperate", "slug": "seq-rerun-why-our-kind-can-t-cooperate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.880Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4wnTsuQfq7zgn9NAn/seq-rerun-why-our-kind-can-t-cooperate", "pageUrlRelative": "/posts/4wnTsuQfq7zgn9NAn/seq-rerun-why-our-kind-can-t-cooperate", "linkUrl": "https://www.lesswrong.com/posts/4wnTsuQfq7zgn9NAn/seq-rerun-why-our-kind-can-t-cooperate", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Why%20Our%20Kind%20Can't%20Cooperate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Why%20Our%20Kind%20Can't%20Cooperate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wnTsuQfq7zgn9NAn%2Fseq-rerun-why-our-kind-can-t-cooperate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Why%20Our%20Kind%20Can't%20Cooperate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wnTsuQfq7zgn9NAn%2Fseq-rerun-why-our-kind-can-t-cooperate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wnTsuQfq7zgn9NAn%2Fseq-rerun-why-our-kind-can-t-cooperate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>Today's post, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate</a> was originally published on 20 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd, aka \"the nonconformist cluster\", seems to be stunningly bad at coordinating group projects. There are a number of reasons for this, but one of them is that people are as reluctant to speak agreement out loud, as they are eager to voice disagreements - the exact opposite of the situation that obtains in more cohesive and powerful communities. This is not rational either! It is dangerous to be half a rationalist (in general), and this also applies to teaching only disagreement but not agreement, or only lonely defiance but not coordination. The pseudo-rationalist taboo against expressing strong feelings probably doesn't help either.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h38/seq_rerun_rationalist_fiction/\">Rationalist Fiction</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4wnTsuQfq7zgn9NAn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1534030877233497e-06, "legacy": true, "legacyId": "22156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC", "2Gk5p2xyKjiDBachf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T09:43:38.902Z", "modifiedAt": null, "url": null, "title": "Buridan's ass and the psychological origins of objective probability", "slug": "buridan-s-ass-and-the-psychological-origins-of-objective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:31.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "common_law", "createdAt": "2012-07-03T06:17:42.167Z", "isAdmin": false, "displayName": "common_law"}, "userId": "PJzXMdPcBFR6cpXPY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kv5B8SEhFZuTdbXGo/buridan-s-ass-and-the-psychological-origins-of-objective", "pageUrlRelative": "/posts/Kv5B8SEhFZuTdbXGo/buridan-s-ass-and-the-psychological-origins-of-objective", "linkUrl": "https://www.lesswrong.com/posts/Kv5B8SEhFZuTdbXGo/buridan-s-ass-and-the-psychological-origins-of-objective", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Buridan's%20ass%20and%20the%20psychological%20origins%20of%20objective%20probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuridan's%20ass%20and%20the%20psychological%20origins%20of%20objective%20probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv5B8SEhFZuTdbXGo%2Fburidan-s-ass-and-the-psychological-origins-of-objective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Buridan's%20ass%20and%20the%20psychological%20origins%20of%20objective%20probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv5B8SEhFZuTdbXGo%2Fburidan-s-ass-and-the-psychological-origins-of-objective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv5B8SEhFZuTdbXGo%2Fburidan-s-ass-and-the-psychological-origins-of-objective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1807, "htmlBody": "<p><a href=\"/The medieval philosopher Buridan reportedly constructed a thought experiment to support his view that human behavior was determined rather than &ldquo;free&rdquo;&mdash;hence rational agents couldn&rsquo;t choose between two equally good alternatives. In the Buridan&rsquo;s Ass Paradox, an ass finds itself between two equal equidistant bales of hay, noticed simultaneously; the bales&rsquo; distance and size are the only variables influencing the ass&rsquo;s behavior. Under these idealized conditions, the ass must starve, its predicament indistinguishable from a physical object suspended between opposite forces, such as a planet that neither falls into the sun nor escapes into outer space. (Since the ass served Buridan as metaphor for the human agent, in what follows, I speak of &ldquo;ass&rdquo; and &ldquo;agent&rdquo; interchangeably.) Computer scientist Leslie Lamport formalized the paradox as &ldquo;Buridan&rsquo;s Principle,&rdquo; which states that the ass will starve if it is situated in a range of possibilities that include midpoints where two opposing forces are equal and it must choose in a sufficiently short time span. We assume, based on a principle of physical continuity, that the larger the bale of hay compared to the other, the faster will the ass be able to decide. Since this is true on the left and on the right, at the midpoint, where the bales are equal, symmetry requires an infinite decision time Conclusion: within some range of bale comparisons, the ass will require decision time greater than a given bounded time interval. (For rigorous treatment, see Buridan&rsquo;s Principle (1984).) Buridan&rsquo;s Principle is counterintuitive, as Lamport discovered when he first tried to publish. Among the objections to Buridan&rsquo;s Principle summarized by Lamport, the main objection provides an insight about the source of the mind-projection fallacy, which treats probability as a feature of the world. The most common objection is that when the agent can&rsquo;t decide it may use a default metarule. Lamport points out this substitutes another decision subject to the same limits: the agent must decide that it can&rsquo;t decide. My point differs from that of Lamport, who proves that binary decisions in the face of continuous inputs are unavoidable and that with minimal assumptions they preclude deciding in bounded time; whereas I draw a stronger conclusion: no decision is substitutable when you adhere strictly to the problem&rsquo;s conditions specifying that the agent be equally balanced between the options. Any inclination to substitute a different decision is a bias toward making the decision that the substitute decision entails. In the simplest variant, the ass may use the rule: turn left when you can&rsquo;t decide, potentially entrapping it in the limbo between deciding whether it can&rsquo;t decide. If the ass has a metarule resolving conflicting to favor the left, it has an extraneous bias. Lamport&rsquo;s analysis discerns a kind of physical law; mine elucidates the origins of the mind-projection fallacy. What&rsquo;s psychologically telling is that the most common metarule is to decide at random. But if by random we mean only apparently random, the strategy still doesn&rsquo;t free the ass from its straightjacket. If it flips a coin, an agent is, in fact, biased toward whatever the coin will dictate, bias, here, means an inclination to use means causally connected with a certain outcome, but the coin flip&rsquo;s apparent randomness is due to our ignorance of microconditions; truly random responding would allow the agent to circumvent the paradox&rsquo;s conditions. The theory that the agent might use a random strategy expresses the intuition that the agent could turn either way. It seems a route to where the opposites of functioning according to physical law and acting &ldquo;freely&rdquo; in perceived self-interest are reconciled. This false reconciliation comes through confusing two kinds of symmetry: the epistemic symmetry of &ldquo;chance&rdquo; events and the dynamic symmetry in the Buridan&rsquo;s ass paradox. If you flip a coin, the symmetry of the coin (along with your lack of control over the flip) is what makes your reasons for preferring heads and tails equivalent, justifying assigning each the same probability. We encounter another symmetry with Buridan&rsquo;s ass, where we also have the same reason to think the ass will turn in either direction. Since the intuition of &ldquo;free will&rdquo; precludes impossible decisions, we construe our epistemic uncertainty as describing a decision that&rsquo;s possible but inherently uncertain. When we conceive of the ass as a purely physical process subject to two opposite forces (which, of course, it is), and then it&rsquo;s obvious that the ass can be &ldquo;stuck.&rdquo; What miscues intuition is that the ass need not be confined to one decision rule. But if by hypothesis it is confined to one rule, the rule may preclude decision. This hypothetical is made relevant by the necessity of there being some ultimate decision rule. The intuitive physics of an agent that can&rsquo;t get stuck entails: a) two equal forces act on an object producing an equilibrium; b) without breaking the equilibrium, an additional natural law is added specifying that the ass will turn. Rather than conclude this is impossible, intuition &ldquo;resolves&rdquo; the contradiction through conceiving that the ass will go in each direction half the time: the probability of either course is deemed .5. Confusion of kinds of symmetry, fueled by the intuition of free will, makes Buridan&rsquo;s Principle counter-intuitive and objective probabilities intuitive. How do we know that reality can&rsquo;t be like this intuitive physics? We know because realizing a and b would mean that the physical forces involved don&rsquo;t vary continuously. It would make an exception, a kind of singularity, of the midpoint. \">[Crossposted]</a></p>\n<p>The medieval philosopher Buridan reportedly constructed a thought experiment to support his view that human behavior was determined rather than <a href=\"http://juridicalcoherence.blogspot.com/2012/05/1011-another-refutation-of.html\">&ldquo;free&rdquo;</a>&mdash;hence rational agents couldn&rsquo;t choose between two equally good alternatives. In the Buridan&rsquo;s Ass Paradox, an ass finds itself between two equal equidistant bales of hay, noticed simultaneously; the bales&rsquo; distance and size are the only variables influencing the ass&rsquo;s behavior. Under these idealized conditions, the ass must starve, its predicament indistinguishable from a physical object suspended between opposite forces, such as a planet that neither falls into the sun nor escapes into outer space. (Since the ass served Buridan as metaphor for the human agent, in what follows, I speak of &ldquo;ass&rdquo; and &ldquo;agent&rdquo; interchangeably.)</p>\n<p>Computer scientist Leslie Lamport formalized the paradox as &ldquo;Buridan&rsquo;s Principle,&rdquo; which states that the ass will starve if it is situated in a range of possibilities that include midpoints where two opposing forces are equal and it must choose in a sufficiently short time span. We assume, based on a principle of physical continuity, that the larger the bale of hay compared to the other, the faster will the ass be able to decide. Since this is true on the left and on the right, at the midpoint, where the bales are equal, symmetry requires an infinite decision time &nbsp;Conclusion: within some range of bale comparisons, the ass will require decision time greater than a given bounded time interval. (For rigorous treatment, see <em><a href=\"http://research.microsoft.com/en-us/um/people/lamport/pubs/buridan.pdf\">Buridan&rsquo;s Principle</a></em> (1984).)</p>\n<p>Buridan&rsquo;s Principle is counterintuitive, as Lamport discovered when he first tried to publish. Among the objections to Buridan&rsquo;s Principle summarized by Lamport, the main objection provides an insight about the source of the mind-projection fallacy, which treats probability as a feature of the world. The most common objection is that when the agent can&rsquo;t decide it may use a default metarule. Lamport points out this substitutes another decision subject to the same limits: the agent must decide that it can&rsquo;t decide. My point differs from that of Lamport, who proves that binary decisions in the face of continuous inputs are unavoidable and that with minimal assumptions they preclude deciding in bounded time; whereas I draw a stronger conclusion: no decision is substitutable when you adhere strictly to the problem&rsquo;s conditions specifying that the agent be equally balanced between the options. Any inclination to substitute a different decision is a bias toward making the decision that the substitute decision entails. In the simplest variant, the ass may use the rule: turn left when you can&rsquo;t decide, potentially entrapping it in the limbo between deciding whether it can&rsquo;t decide. If the ass has a metarule resolving conflicting to favor the left, it has an extraneous bias.</p>\n<p>Lamport&rsquo;s analysis discerns a kind of physical law; mine elucidates the origins of the mind-projection fallacy. What&rsquo;s psychologically telling is that the most common metarule is to decide at random. But if by random we mean only apparently random, the strategy still doesn&rsquo;t free the ass from its straightjacket. If it flips a coin, an agent is, in fact, biased toward whatever the coin will dictate, bias, here, means an inclination to use means causally connected with a certain outcome, but the coin flip&rsquo;s apparent randomness is due to our ignorance of microconditions; truly random responding would allow the agent to circumvent the paradox&rsquo;s conditions. <strong>The theory that the agent might use a random strategy expresses the intuition that the agent could turn either way. It seems a route to where the opposites of functioning according to physical law and acting &ldquo;freely&rdquo; in perceived self-interest are reconciled.</strong></p>\n<p>This false reconciliation comes through confusing two kinds of symmetry: the epistemic symmetry of &ldquo;chance&rdquo; events and the dynamic symmetry in the Buridan&rsquo;s ass paradox. If you flip a coin, the symmetry of the coin (along with your lack of control over the flip) is what makes your reasons for preferring heads and tails equivalent, justifying assigning each the same probability. We encounter another symmetry with Buridan&rsquo;s ass, where we also have the same reason to think the ass will turn in either direction. Since the intuition of &ldquo;free will&rdquo; precludes impossible decisions, we construe our epistemic uncertainty as describing a decision that&rsquo;s possible but inherently uncertain.</p>\n<p>When we conceive of the ass as a purely physical process &nbsp;subject to two opposite forces (which, of course, it is), and then it&rsquo;s obvious that the ass can be &ldquo;stuck.&rdquo; What miscues intuition is that the ass need not be confined to one decision rule. But if by hypothesis it is confined to one rule, the rule may preclude decision. This hypothetical is made relevant by the necessity of there being some ultimate decision rule.</p>\n<p><strong>The intuitive physics of an agent that can&rsquo;t get stuck entails: a) two equal forces act on an object producing an equilibrium; b) without breaking the equilibrium, an additional natural law is added specifying that the ass will turn. Rather than conclude this is impossible, intuition &ldquo;resolves&rdquo; the contradiction through conceiving that the ass will go in each direction half the time: the probability of either course is deemed .5. Confusion of kinds of symmetry, fueled by the intuition of free will, makes Buridan&rsquo;s Principle counter-intuitive and objective probabilities intuitive.</strong></p>\n<p>How do we know that reality can&rsquo;t be like this intuitive physics? We know because realizing a and b would mean that the physical forces involved don&rsquo;t vary continuously. It would make an exception, a kind of singularity, of the midpoint. &nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kv5B8SEhFZuTdbXGo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": -8, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "22154", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T13:56:28.062Z", "modifiedAt": null, "url": null, "title": "Meetup : Shanghai Meetup", "slug": "meetup-shanghai-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Barry_Cotter", "createdAt": "2010-04-19T16:29:03.629Z", "isAdmin": false, "displayName": "Barry_Cotter"}, "userId": "5pZXxaf79kj37Rwq2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9xnKBkSsTg8T6ezC9/meetup-shanghai-meetup-0", "pageUrlRelative": "/posts/9xnKBkSsTg8T6ezC9/meetup-shanghai-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/9xnKBkSsTg8T6ezC9/meetup-shanghai-meetup-0", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Shanghai%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Shanghai%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xnKBkSsTg8T6ezC9%2Fmeetup-shanghai-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Shanghai%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xnKBkSsTg8T6ezC9%2Fmeetup-shanghai-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xnKBkSsTg8T6ezC9%2Fmeetup-shanghai-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l1'>Shanghai Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 April 2013 07:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Boxing Cat Brewery, 82 Fuxing Xi Lu, near Yongfu Lu, Shanghai, China</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will be <a href=\"http://www.smartshanghai.com/venue/4430/Boxing_Cat_Brewery_(Fuxing_Lu\" rel=\"nofollow\">here</a>_shanghai)) from 7. I look like <a href=\"http://www.okcupid.com/profile/Hibernian/photos\" rel=\"nofollow\">this</a>. The food has been decent every time I've been and it's non smoking inside.\nIf you want, read the <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">Checklist of Rationality Habits</a> before you come and we can discuss that. No compulsion to do so, one of the important things is to have fun. The nearest metro station is Shanghai Library.\n82 Fuxing Xi Lu, near Yongfu Lu \u590d\u5174\u8def82\u53f7, \u8fd1\u6c38\u798f\u8def</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l1'>Shanghai Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9xnKBkSsTg8T6ezC9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1537230680684332e-06, "legacy": true, "legacyId": "22157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Meetup\">Discussion article for the meetup : <a href=\"/meetups/l1\">Shanghai Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 April 2013 07:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Boxing Cat Brewery, 82 Fuxing Xi Lu, near Yongfu Lu, Shanghai, China</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will be <a href=\"http://www.smartshanghai.com/venue/4430/Boxing_Cat_Brewery_(Fuxing_Lu\" rel=\"nofollow\">here</a>_shanghai)) from 7. I look like <a href=\"http://www.okcupid.com/profile/Hibernian/photos\" rel=\"nofollow\">this</a>. The food has been decent every time I've been and it's non smoking inside.\nIf you want, read the <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">Checklist of Rationality Habits</a> before you come and we can discuss that. No compulsion to do so, one of the important things is to have fun. The nearest metro station is Shanghai Library.\n82 Fuxing Xi Lu, near Yongfu Lu \u590d\u5174\u8def82\u53f7, \u8fd1\u6c38\u798f\u8def</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/l1\">Shanghai Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Shanghai Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Shanghai Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T16:13:12.929Z", "modifiedAt": null, "url": null, "title": "LW wiki spam filtering", "slug": "lw-wiki-spam-filtering", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.005Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KEekEf6SykdKnk8oQ/lw-wiki-spam-filtering", "pageUrlRelative": "/posts/KEekEf6SykdKnk8oQ/lw-wiki-spam-filtering", "linkUrl": "https://www.lesswrong.com/posts/KEekEf6SykdKnk8oQ/lw-wiki-spam-filtering", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20wiki%20spam%20filtering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20wiki%20spam%20filtering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEekEf6SykdKnk8oQ%2Flw-wiki-spam-filtering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20wiki%20spam%20filtering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEekEf6SykdKnk8oQ%2Flw-wiki-spam-filtering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEekEf6SykdKnk8oQ%2Flw-wiki-spam-filtering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<p>Long-time readers may have noticed that spam on the wiki has been a very persistent problem for the past 2 years or so; I've been dealing with it so far by hand, but I recently reached a breaking point and asked Trike to resolve it or find a new wiki administrator. (Speaking of which, is anyone interested?)</p>\n<p>So Trike has enabled a MediaWiki extension called the <a href=\"https://www.mediawiki.org/wiki/Extension:AbuseFilter\">edit filter</a>: a small functional programming language which lets you define predicates applied to edits which trigger one of a set of actions, like banning a user, deleting an edit/page, or stopping an edit from going through. I have so far defined one rule: page creation is forbidden for users younger than 24 hours. This so far seems to have worked well; spam pages have fallen from 5-10/day to ~5 over the past 2 weeks. This is much more manageable, and I am hopeful that this new anti-spam measure will be effective longer than the previous additions did (but if it doesn't, I'll look into adding more rules dealing with images and external links, and perhaps also ban users whose names end in a numeric digit as almost all the spam accounts do).</p>\n<p>If you've run into this edit filter before by making a page and seeing the submission rejected with an error message, fret not: merely wait 24 hours. (If your account is more than a day old and you're still getting errors, please contact me or Trike.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KEekEf6SykdKnk8oQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 40, "extendedScore": null, "score": 1.1538155869773412e-06, "legacy": true, "legacyId": "22158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T19:16:14.720Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin, practical rationality", "slug": "meetup-berlin-practical-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bPddEgnwwZC6zgZwd/meetup-berlin-practical-rationality", "pageUrlRelative": "/posts/bPddEgnwwZC6zgZwd/meetup-berlin-practical-rationality", "linkUrl": "https://www.lesswrong.com/posts/bPddEgnwwZC6zgZwd/meetup-berlin-practical-rationality", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPddEgnwwZC6zgZwd%2Fmeetup-berlin-practical-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPddEgnwwZC6zgZwd%2Fmeetup-berlin-practical-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPddEgnwwZC6zgZwd%2Fmeetup-berlin-practical-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l2'>Berlin, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 April 2013 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">S Wuhletal, 12621 Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As announced on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lw-berlin\" rel=\"nofollow\">mailing list</a>:\nThe next meetup will happen on Friday, April 5th, 19:30 at my house and we'll try something new.\nI declare it a 'practical rationality' or 'work' meetup, in which I'd like to establish some differences from the regular social gatherings we've done so far.</p>\n\n<ul>\n<li>Have a plan. Schedule food and activities. Actually follow it.</li>\n<li>Moderate discussion more strongly. Each subgroup will have one person responsible for keeping things on track, stop inadvertent topic switches, interrupt excessive anecdotes.</li>\n<li>Will probably alternate with social meetups.</li>\n</ul>\n\n<p>Here's what I suggest for Friday:</p>\n\n<p>19:30 - 20:00 - Settling down and food.</p>\n\n<p>Edit: Fixme.</p>\n\n<p>20:00 - 20:30 - What have you learned since we last met? What have you done?</p>\n\n<p>This could be an interesting regular activity where we get a glimpse of what you're doing and which could help seed discussion later. Everyone is prompted to talk, answers should be short (&lt; 3 min).</p>\n\n<p>20:30 - 21:15 - Discussion groups?</p>\n\n<p>I'd like to try breaking into discussion groups. The idea is that there are a several people who've announced beforehand that they'd lead a discussion on some topic and have briefly studied it. At the meetup, they give a mini-presentation (5 minute-ish) and everyone else decides which group to join. At the end of the session, one person from each group summarises for everyone else.</p>\n\n<p>Check the mailing list for topics.</p>\n\n<p>21:15 - end - Chat.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l2'>Berlin, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bPddEgnwwZC6zgZwd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1539394393131192e-06, "legacy": true, "legacyId": "22159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/l2\">Berlin, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 April 2013 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">S Wuhletal, 12621 Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As announced on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lw-berlin\" rel=\"nofollow\">mailing list</a>:\nThe next meetup will happen on Friday, April 5th, 19:30 at my house and we'll try something new.\nI declare it a 'practical rationality' or 'work' meetup, in which I'd like to establish some differences from the regular social gatherings we've done so far.</p>\n\n<ul>\n<li>Have a plan. Schedule food and activities. Actually follow it.</li>\n<li>Moderate discussion more strongly. Each subgroup will have one person responsible for keeping things on track, stop inadvertent topic switches, interrupt excessive anecdotes.</li>\n<li>Will probably alternate with social meetups.</li>\n</ul>\n\n<p>Here's what I suggest for Friday:</p>\n\n<p>19:30 - 20:00 - Settling down and food.</p>\n\n<p>Edit: Fixme.</p>\n\n<p>20:00 - 20:30 - What have you learned since we last met? What have you done?</p>\n\n<p>This could be an interesting regular activity where we get a glimpse of what you're doing and which could help seed discussion later. Everyone is prompted to talk, answers should be short (&lt; 3 min).</p>\n\n<p>20:30 - 21:15 - Discussion groups?</p>\n\n<p>I'd like to try breaking into discussion groups. The idea is that there are a several people who've announced beforehand that they'd lead a discussion on some topic and have briefly studied it. At the meetup, they give a mini-presentation (5 minute-ish) and everyone else decides which group to join. At the end of the session, one person from each group summarises for everyone else.</p>\n\n<p>Check the mailing list for topics.</p>\n\n<p>21:15 - end - Chat.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/l2\">Berlin, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin, practical rationality", "anchor": "Discussion_article_for_the_meetup___Berlin__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Berlin, practical rationality", "anchor": "Discussion_article_for_the_meetup___Berlin__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-30T20:14:20.498Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Tolerate Tolerance", "slug": "seq-rerun-tolerate-tolerance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:36.149Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HxmLEgPQkiqDLZqdr/seq-rerun-tolerate-tolerance", "pageUrlRelative": "/posts/HxmLEgPQkiqDLZqdr/seq-rerun-tolerate-tolerance", "linkUrl": "https://www.lesswrong.com/posts/HxmLEgPQkiqDLZqdr/seq-rerun-tolerate-tolerance", "postedAtFormatted": "Saturday, March 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Tolerate%20Tolerance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Tolerate%20Tolerance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxmLEgPQkiqDLZqdr%2Fseq-rerun-tolerate-tolerance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Tolerate%20Tolerance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxmLEgPQkiqDLZqdr%2Fseq-rerun-tolerate-tolerance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxmLEgPQkiqDLZqdr%2Fseq-rerun-tolerate-tolerance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>Today's post, <a href=\"/lw/42/tolerate_tolerance/\">Tolerate Tolerance</a> was originally published on 21 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>One of the likely characteristics of someone who sets out to be a \"rationalist\" is a lower-than-usual tolerance for flawed thinking. This makes it very important to tolerate other people's tolerance - to avoid rejecting them because they tolerate people you wouldn't - since otherwise we must all have exactly the same standards of tolerance in order to work together, which is unlikely. Even if someone has a nice word to say about complete lunatics and crackpots - so long as they don't literally believe the same ideas themselves - try to be nice to them? Intolerance of tolerance corresponds to punishment of non-punishers, a very dangerous game-theoretic idiom that can lock completely arbitrary systems in place even when they benefit no one at all.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h3g/seq_rerun_why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HxmLEgPQkiqDLZqdr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.153978756735022e-06, "legacy": true, "legacyId": "22160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JKxxFseBWz8SHkTgt", "4wnTsuQfq7zgn9NAn", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-31T00:52:46.589Z", "modifiedAt": null, "url": null, "title": "Existential risks open thread", "slug": "existential-risks-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:08.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k9gSN9yBZn4iwQinG/existential-risks-open-thread", "pageUrlRelative": "/posts/k9gSN9yBZn4iwQinG/existential-risks-open-thread", "linkUrl": "https://www.lesswrong.com/posts/k9gSN9yBZn4iwQinG/existential-risks-open-thread", "postedAtFormatted": "Sunday, March 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existential%20risks%20open%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExistential%20risks%20open%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk9gSN9yBZn4iwQinG%2Fexistential-risks-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existential%20risks%20open%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk9gSN9yBZn4iwQinG%2Fexistential-risks-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk9gSN9yBZn4iwQinG%2Fexistential-risks-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>We talk about a wide variety of stuff on LW, but we don't spend much time trying to identify the very highest-utility stuff to discuss and promoting additional discussion of it.&nbsp; This thread is a stab at that.&nbsp; Since it's just comments, you can feel more comfortable bringing up ideas that might be wrong or unoriginal (but nevertheless have relatively high expected value, since existential risks are such an important topic).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k9gSN9yBZn4iwQinG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 1.154167224065518e-06, "legacy": true, "legacyId": "22161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-31T04:18:40.926Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] You're Calling *Who* A Cult Leader?", "slug": "seq-rerun-you-re-calling-who-a-cult-leader", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.240Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c9nwbY4hwwRkGJf6J/seq-rerun-you-re-calling-who-a-cult-leader", "pageUrlRelative": "/posts/c9nwbY4hwwRkGJf6J/seq-rerun-you-re-calling-who-a-cult-leader", "linkUrl": "https://www.lesswrong.com/posts/c9nwbY4hwwRkGJf6J/seq-rerun-you-re-calling-who-a-cult-leader", "postedAtFormatted": "Sunday, March 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20You're%20Calling%20*Who*%20A%20Cult%20Leader%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20You're%20Calling%20*Who*%20A%20Cult%20Leader%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9nwbY4hwwRkGJf6J%2Fseq-rerun-you-re-calling-who-a-cult-leader%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20You're%20Calling%20*Who*%20A%20Cult%20Leader%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9nwbY4hwwRkGJf6J%2Fseq-rerun-you-re-calling-who-a-cult-leader", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9nwbY4hwwRkGJf6J%2Fseq-rerun-you-re-calling-who-a-cult-leader", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p>Today's post, <a href=\"/lw/4d/youre_calling_who_a_cult_leader/\">You're Calling *Who* A Cult Leader?</a> was originally published on 22 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Paul Graham gets exactly the same accusations about \"cults\" and \"echo chambers\" and \"coteries\" that I do, in exactly the same tone - e.g. comparing the long hours worked by Y Combinator startup founders to the sleep-deprivation tactic used in cults, or claiming that founders were asked to move to the Bay Area startup hub as a cult tactic of separation from friends and family. This is bizarre, considering our relative surface risk factors. It just seems to be a failure mode of the nonconformist community in general. By far the most cultish-looking behavior on Hacker News is people trying to show off how willing they are to disagree with Paul Graham, which, I can personally testify, feels really bizarre when you're the target. Admiring someone shouldn't be so scary - I don't hold back so much when praising e.g. Douglas Hofstadter; in this world there are people who have pulled off awesome feats and it is okay to admire them highly.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h3k/seq_rerun_tolerate_tolerance/\">Tolerate Tolerance</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c9nwbY4hwwRkGJf6J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.1543066327445962e-06, "legacy": true, "legacyId": "22162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cyzXoCv7nagDWCMNS", "HxmLEgPQkiqDLZqdr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-03-31T07:47:13.910Z", "modifiedAt": null, "url": null, "title": "What are some in depth / meta-analytic, professionally edited wiki's? Examples inside", "slug": "what-are-some-in-depth-meta-analytic-professionally-edited", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.810Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "someonewrongonthenet", "createdAt": "2012-08-14T02:41:42.884Z", "isAdmin": false, "displayName": "someonewrongonthenet"}, "userId": "de4BQQNAFXHofT5wh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hwi4bnLqsNeZwFNmj/what-are-some-in-depth-meta-analytic-professionally-edited", "pageUrlRelative": "/posts/Hwi4bnLqsNeZwFNmj/what-are-some-in-depth-meta-analytic-professionally-edited", "linkUrl": "https://www.lesswrong.com/posts/Hwi4bnLqsNeZwFNmj/what-are-some-in-depth-meta-analytic-professionally-edited", "postedAtFormatted": "Sunday, March 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20some%20in%20depth%20%2F%20meta-analytic%2C%20professionally%20edited%20wiki's%3F%20Examples%20inside&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20some%20in%20depth%20%2F%20meta-analytic%2C%20professionally%20edited%20wiki's%3F%20Examples%20inside%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHwi4bnLqsNeZwFNmj%2Fwhat-are-some-in-depth-meta-analytic-professionally-edited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20some%20in%20depth%20%2F%20meta-analytic%2C%20professionally%20edited%20wiki's%3F%20Examples%20inside%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHwi4bnLqsNeZwFNmj%2Fwhat-are-some-in-depth-meta-analytic-professionally-edited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHwi4bnLqsNeZwFNmj%2Fwhat-are-some-in-depth-meta-analytic-professionally-edited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>I'm sure most people here are familiar with http://www.scholarpedia.org , a well curated encyclopedia of scientific topics.<br /><br />I recently came across http://examine.com/ which is a combination of encyclopedia and meta-analysis for supplements and cognitive enhancers. Given the popularity of the \"Practical\" section on http://www.gwern.net/ I think most of us would be interested in this information, and it's incredibly readable and well presented.<br /><br />Can anyone suggest websites of similar quality? To be specific, I'm talking about</p>\n<p>1) many searchable topics collected in one place</p>\n<p>2) Well cited and in-depth reviews of literature and/or thorough meta-analyses&nbsp;<br /><br />3) Adequate quality control. For example, all examine.com edits must pass two-man review team and Scholarpedia does decentralized peer review.&nbsp;<br /><br />Topic or focus is not important, as long as there is a wealth of dense, high quality, **well cited** information. Suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hwi4bnLqsNeZwFNmj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.1544478619749203e-06, "legacy": true, "legacyId": "22163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T00:42:58.954Z", "modifiedAt": null, "url": null, "title": "A Rational Altruist Punch in The Stomach", "slug": "a-rational-altruist-punch-in-the-stomach", "viewCount": null, "lastCommentedAt": "2020-03-17T03:09:53.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Neotenic", "createdAt": "2013-03-04T02:28:23.403Z", "isAdmin": false, "displayName": "Neotenic"}, "userId": "qMgZoftatigAeMMhL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TYAA4iNCFaDvPD6gB/a-rational-altruist-punch-in-the-stomach", "pageUrlRelative": "/posts/TYAA4iNCFaDvPD6gB/a-rational-altruist-punch-in-the-stomach", "linkUrl": "https://www.lesswrong.com/posts/TYAA4iNCFaDvPD6gB/a-rational-altruist-punch-in-the-stomach", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Rational%20Altruist%20Punch%20in%20The%20Stomach&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Rational%20Altruist%20Punch%20in%20The%20Stomach%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYAA4iNCFaDvPD6gB%2Fa-rational-altruist-punch-in-the-stomach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Rational%20Altruist%20Punch%20in%20The%20Stomach%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYAA4iNCFaDvPD6gB%2Fa-rational-altruist-punch-in-the-stomach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYAA4iNCFaDvPD6gB%2Fa-rational-altruist-punch-in-the-stomach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 482, "htmlBody": "<p>&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\"><a href=\"http://www.overcomingbias.com/2008/01/protecting-acro.html\">Robin Hanson wrote, five years ago</a>:&nbsp;</p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">Very distant future times are ridiculously easy to help via investment.&nbsp; A 2% annual return adds up to a googol (10^100) return over 12,000 years, even if there is only a 1/1000 chance they will exist or receive it.&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">So if you are not&nbsp;<em style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\">incredibly eager</em>&nbsp;to invest this way to help them, how can you claim to care the tiniest bit about them?&nbsp; How can you think&nbsp;<em style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\">anyone</em>&nbsp;on Earth so cares?&nbsp; And if no one cares the tiniest bit, how can you say it is \"moral\" to care about them, not just somewhat, but almost&nbsp;<em style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\">equally</em>&nbsp;to people now?&nbsp; Surely if you are representing a group, instead of spending your own wealth, you shouldn&rsquo;t assume they care much.</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">So why do many people seem to care about policy that effects far future folk?&nbsp; &nbsp;I suspect our paternalistic itch pushes us to control the future, rather than to enrich it.&nbsp; We care that the future celebrates our foresight, not that they are happy.&nbsp;</p>\n</blockquote>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">In the comments &nbsp;some people gave counterarguments. For those in a rush, the best ones are Toby Ord's. But I didn't bite any of the counterarguments <em>to the extent that it would be necessary to counter the 10^100</em>. I have some trouble conceiving of what would beat a consistent argument a googol fold. &nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">Things that changed my behavior significantly over the last few years have not been many, but I think I'm facing one of them. Understanding biological immortality was one, it meant 150 000 non-deaths per day. Understanding the posthuman potential was another. Then came the 10^52 potential lives lost in case of X-risk, or if you are conservative and think only biological stuff can have moral lives on it, 10^31. You can argue about which movie you'll watch, which teacher would be best to have, who should you marry. But (if consequentialist) you can't argue your way out of 10^31 or 10^52. You won't find a counteracting force that exactly matches, or really reduces the value of future stuff by</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">3 000 000 634 803 867 000 000 000 000 000 000 777 000 000 000 999 &nbsp;fold&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">Which is way less than 10^52&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">You may find a fundamental and qualitative counterargument \"actually I'd rather future people didn't exist\", but you won't find a quantitative one. Thus I spend a lot of time on X-risk related things.&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">Back to Robin's argument: so unless someone gives me a good argument against investing some money in the far future (and discovering some vague techniques of how to do it that will make it at least one in a millionth possibility) I'll set aside a block of money X, a block of time Y, and will invest in future people 12 thousand years from now. If you don't think you can beat 10^100, join me.&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">And if you are not in a rush, read <a href=\"http://rationalaltruist.com/2013/02/22/four-flavors-of-time-discounting-i-endorse-and-one-i-do-not/#more-90\">this</a> also, for a bright reflection on similar issues.&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">&nbsp;</p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TYAA4iNCFaDvPD6gB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 12, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "22166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T05:02:55.120Z", "modifiedAt": null, "url": null, "title": "Meetup : Mountain View: Cure Light Aversions", "slug": "meetup-mountain-view-cure-light-aversions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BikjZYxqasR4eePtS/meetup-mountain-view-cure-light-aversions", "pageUrlRelative": "/posts/BikjZYxqasR4eePtS/meetup-mountain-view-cure-light-aversions", "linkUrl": "https://www.lesswrong.com/posts/BikjZYxqasR4eePtS/meetup-mountain-view-cure-light-aversions", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mountain%20View%3A%20Cure%20Light%20Aversions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mountain%20View%3A%20Cure%20Light%20Aversions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBikjZYxqasR4eePtS%2Fmeetup-mountain-view-cure-light-aversions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mountain%20View%3A%20Cure%20Light%20Aversions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBikjZYxqasR4eePtS%2Fmeetup-mountain-view-cure-light-aversions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBikjZYxqasR4eePtS%2Fmeetup-mountain-view-cure-light-aversions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l3'>Mountain View: Cure Light Aversions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 April 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">167 Jasmine Ct, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the time I've spent volunteering with <a href=\"http://www.idodi.org\" rel=\"nofollow\">DI</a>, I've learned a few fun improv games. I also know a few other tricks for evoking helpful, unusual social atmospheres. Let's try these:</p>\n\n<ul>\n<li><a href=\"http://actingcoachscotland.co.uk/2011/11/whats-the-repetition-game-or-exercise/\" rel=\"nofollow\">The Repetition Game</a> - this sounds absurd. It feels absurd, too, but maybe not how you'd expect. You should try it.</li>\n<li>Various improv games. I know several, and will be prepared with more. If you have some favorites, let's do those, too.</li>\n<li>Comfort-Zone Expansion: a brief, mild round of comfort-zone expansion.</li>\n</ul>\n\n<hr />\n\n<p>Aaron and Jess will open the doors at 7:00pm; we'll get started at 7:30.</p>\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>.\nRegular meetups in Mountain View and Berkeley are announced and discussed there, as are other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l3'>Mountain View: Cure Light Aversions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BikjZYxqasR4eePtS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1553124191026954e-06, "legacy": true, "legacyId": "22169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Cure_Light_Aversions\">Discussion article for the meetup : <a href=\"/meetups/l3\">Mountain View: Cure Light Aversions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 April 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">167 Jasmine Ct, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the time I've spent volunteering with <a href=\"http://www.idodi.org\" rel=\"nofollow\">DI</a>, I've learned a few fun improv games. I also know a few other tricks for evoking helpful, unusual social atmospheres. Let's try these:</p>\n\n<ul>\n<li><a href=\"http://actingcoachscotland.co.uk/2011/11/whats-the-repetition-game-or-exercise/\" rel=\"nofollow\">The Repetition Game</a> - this sounds absurd. It feels absurd, too, but maybe not how you'd expect. You should try it.</li>\n<li>Various improv games. I know several, and will be prepared with more. If you have some favorites, let's do those, too.</li>\n<li>Comfort-Zone Expansion: a brief, mild round of comfort-zone expansion.</li>\n</ul>\n\n<hr>\n\n<p>Aaron and Jess will open the doors at 7:00pm; we'll get started at 7:30.</p>\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>.\nRegular meetups in Mountain View and Berkeley are announced and discussed there, as are other events of interest to the local community.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Cure_Light_Aversions1\">Discussion article for the meetup : <a href=\"/meetups/l3\">Mountain View: Cure Light Aversions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mountain View: Cure Light Aversions", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Cure_Light_Aversions", "level": 1}, {"title": "Discussion article for the meetup : Mountain View: Cure Light Aversions", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Cure_Light_Aversions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T05:23:19.747Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] On Things that are Awesome", "slug": "seq-rerun-on-things-that-are-awesome", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZ48PYqwduLBqwKKm/seq-rerun-on-things-that-are-awesome", "pageUrlRelative": "/posts/LZ48PYqwduLBqwKKm/seq-rerun-on-things-that-are-awesome", "linkUrl": "https://www.lesswrong.com/posts/LZ48PYqwduLBqwKKm/seq-rerun-on-things-that-are-awesome", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20On%20Things%20that%20are%20Awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20On%20Things%20that%20are%20Awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ48PYqwduLBqwKKm%2Fseq-rerun-on-things-that-are-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20On%20Things%20that%20are%20Awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ48PYqwduLBqwKKm%2Fseq-rerun-on-things-that-are-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ48PYqwduLBqwKKm%2Fseq-rerun-on-things-that-are-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p>Today's post, <a href=\"/lw/4y/on_things_that_are_awesome/\">On Things that are Awesome</a> was originally published on 24 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Seven thoughts: I can list more than one thing that is awesome; when I think of \"Douglas Hofstadter\" I am really thinking of his all-time greatest work; the greatest work is not the person; when we imagine other people we are imagining their output, so the real Douglas Hofstadter is the source of \"Douglas Hofstadter\"; I most strongly get the sensation of awesomeness when I see someone outdoing me overwhelmingly, at some task I've actually tried; we tend to admire unique detailed awesome things and overlook common nondetailed awesome things; religion and its bastard child \"spirituality\" tends to make us overlook human awesomeness.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h3m/seq_rerun_youre_calling_who_a_cult_leader/\">You're Calling *Who* A Cult Leader?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZ48PYqwduLBqwKKm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.155326260951352e-06, "legacy": true, "legacyId": "22170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YC3ArwKM8xhNjYqQK", "c9nwbY4hwwRkGJf6J", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T05:41:22.221Z", "modifiedAt": null, "url": null, "title": "A hypothesis testing video game", "slug": "a-hypothesis-testing-video-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:36.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmy", "createdAt": "2009-02-28T00:36:34.416Z", "isAdmin": false, "displayName": "Swimmy"}, "userId": "pNdunthLRqh3unTry", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FkvHMXPKpMGnBdEtw/a-hypothesis-testing-video-game", "pageUrlRelative": "/posts/FkvHMXPKpMGnBdEtw/a-hypothesis-testing-video-game", "linkUrl": "https://www.lesswrong.com/posts/FkvHMXPKpMGnBdEtw/a-hypothesis-testing-video-game", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20hypothesis%20testing%20video%20game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20hypothesis%20testing%20video%20game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFkvHMXPKpMGnBdEtw%2Fa-hypothesis-testing-video-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20hypothesis%20testing%20video%20game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFkvHMXPKpMGnBdEtw%2Fa-hypothesis-testing-video-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFkvHMXPKpMGnBdEtw%2Fa-hypothesis-testing-video-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 292, "htmlBody": "<p><a href=\"http://l.j-factor.com/gmhtml5/The_Blob_Family/\">The Blob Family</a> is a simple game made by Leon Arnott. At heart, it's a game about testing hypotheses and getting the right answer with the least amount of evidence you can.</p>\n<p>The mechanics work like so: Balls bounce around the screen randomly and you control a character who needs to avoid them. You can aim the mouse anywhere and activate a sonar. On the right side are rules for how various balls will react to this, and your goal is to figure out which ball is which. As you use the sonar more, the balls speed up, so it becomes more difficult to stay alive, thus giving an incentive to test your hypothesis in as few clicks as possible.</p>\n<p>It very nicely illustrates the principle that, to test a hypothesis, you must design tests to falisfy your intuitions rather than to confirm them. For example, in one level, when you use the sonar:</p>\n<ul>\n<li>1 ball heads toward the center</li>\n<li>1 ball heads away from the center</li>\n<li>1 ball heads away from the mouse</li>\n<li>1 ball heads away from you</li>\n</ul>\n<p>I found myself mistakenly clicking in the center of the screen to test hypothesis 1, but this is insufficient. To design the proper tests, you need to keep the mouse out of the center, keep it away from you, and depending on the position of the balls keep it off a straight line from you.</p>\n<p>It could also demonstrate the ability of a fast brain to test hypotheses quickly. For many levels, if you could slow time down and set up a very good test, you could solve the problem with a single click. But we humans aren't usually so attentive.</p>\n<p>Just thought the LW crowd might enjoy it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FkvHMXPKpMGnBdEtw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 1.155338496300444e-06, "legacy": true, "legacyId": "22171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T15:00:03.560Z", "modifiedAt": "2022-02-03T17:52:45.437Z", "url": null, "title": "Open Thread, April 1-15, 2013", "slug": "open-thread-april-1-15-2013", "viewCount": null, "lastCommentedAt": "2014-09-14T21:25:10.723Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RspqaNmJKKBnXTqwk/open-thread-april-1-15-2013", "pageUrlRelative": "/posts/RspqaNmJKKBnXTqwk/open-thread-april-1-15-2013", "linkUrl": "https://www.lesswrong.com/posts/RspqaNmJKKBnXTqwk/open-thread-april-1-15-2013", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20April%201-15%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20April%201-15%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRspqaNmJKKBnXTqwk%2Fopen-thread-april-1-15-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20April%201-15%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRspqaNmJKKBnXTqwk%2Fopen-thread-april-1-15-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRspqaNmJKKBnXTqwk%2Fopen-thread-april-1-15-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_ahd\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RspqaNmJKKBnXTqwk", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1557175063207864e-06, "legacy": true, "legacyId": "22172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 262, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-04-01T15:00:03.560Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T16:19:17.933Z", "modifiedAt": null, "url": null, "title": "Welcome to Less Wrong! (5th thread, March 2013)", "slug": "welcome-to-less-wrong-5th-thread-march-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u3jRwh7uWHvnizGyR/welcome-to-less-wrong-5th-thread-march-2013", "pageUrlRelative": "/posts/u3jRwh7uWHvnizGyR/welcome-to-less-wrong-5th-thread-march-2013", "linkUrl": "https://www.lesswrong.com/posts/u3jRwh7uWHvnizGyR/welcome-to-less-wrong-5th-thread-march-2013", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20Less%20Wrong!%20(5th%20thread%2C%20March%202013)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20Less%20Wrong!%20(5th%20thread%2C%20March%202013)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3jRwh7uWHvnizGyR%2Fwelcome-to-less-wrong-5th-thread-march-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20Less%20Wrong!%20(5th%20thread%2C%20March%202013)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3jRwh7uWHvnizGyR%2Fwelcome-to-less-wrong-5th-thread-march-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3jRwh7uWHvnizGyR%2Fwelcome-to-less-wrong-5th-thread-march-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1529, "htmlBody": "<div id=\"entry_t3_2ku\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div id=\"entry_t3_b9\" class=\"content clear\">\n<div class=\"md\">\n<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a> or how you found us. You can <a href=\"/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.</div>\n<div><br /></div>\n<div>(This is the fifth incarnation of&nbsp;the welcome thread; once a post gets over 500 comments, it stops showing them all by default, so we make a new one. Besides, a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves.)</div>\n<h4><a id=\"more\"></a>A few notes about the site mechanics<br /></h4>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp;(you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div><br /></div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning&mdash; not just that they disagree with you!<strong> If you have any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong><br /></strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/tag/open_thread\">open comment threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br /></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br /></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br /></div>\n<div>\n<h4>A few notes about the community<br /></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><br /></div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)</div>\n<div><br /></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)</div>\n<div><br /></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>. There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>. If you have your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br /> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br /> * <a href=\"/user/Randaly\">Randaly</a> <br /> * <a href=\"/user/shokwave\">shokwave</a> <br /> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4>A list of some posts that are pretty awesome<br /></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n<li><a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Worst Argument in the World</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site!</p>\n<p><strong>Note from orthonormal:</strong> MBlume and other contributors wrote the original version of this welcome post, and I've edited it a fair bit. If there's anything I should add or update on this post (especially broken links), please send me a private message&mdash;I may not notice a comment on the post. Finally, once this gets past 500 comments, anyone is welcome to copy and edit this intro to start the next welcome thread.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u3jRwh7uWHvnizGyR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 37, "extendedScore": null, "score": 1.1557712795532147e-06, "legacy": true, "legacyId": "22165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"entry_t3_2ku\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div id=\"entry_t3_b9\" class=\"content clear\">\n<div class=\"md\">\n<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a> or how you found us. You can <a href=\"/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.</div>\n<div><br></div>\n<div>(This is the fifth incarnation of&nbsp;the welcome thread; once a post gets over 500 comments, it stops showing them all by default, so we make a new one. Besides, a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves.)</div>\n<h4 id=\"A_few_notes_about_the_site_mechanics\"><a id=\"more\"></a>A few notes about the site mechanics<br></h4>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp;(you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div><br></div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning\u2014 not just that they disagree with you!<strong> If you have any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong><br></strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/tag/open_thread\">open comment threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br></div>\n<div>\n<h4 id=\"A_few_notes_about_the_community\">A few notes about the community<br></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><br></div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)</div>\n<div><br></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)</div>\n<div><br></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>. There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>. If you have your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br> * <a href=\"/user/Randaly\">Randaly</a> <br> * <a href=\"/user/shokwave\">shokwave</a> <br> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4 id=\"A_list_of_some_posts_that_are_pretty_awesome\">A list of some posts that are pretty awesome<br></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n<li><a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Worst Argument in the World</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site!</p>\n<p><strong>Note from orthonormal:</strong> MBlume and other contributors wrote the original version of this welcome post, and I've edited it a fair bit. If there's anything I should add or update on this post (especially broken links), please send me a private message\u2014I may not notice a comment on the post. Finally, once this gets past 500 comments, anyone is welcome to copy and edit this intro to start the next welcome thread.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "sections": [{"title": "A few notes about the site mechanics", "anchor": "A_few_notes_about_the_site_mechanics", "level": 1}, {"title": "A few notes about the community", "anchor": "A_few_notes_about_the_community", "level": 1}, {"title": "A list of some posts that are pretty awesome", "anchor": "A_list_of_some_posts_that_are_pretty_awesome", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1746 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1761, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s", "BHMBBFupzb4s8utts", "Psp8ZpYLCDJjshpRb", "ZiQqsgGX6a42Sfpii", "6FmqiAgS8h4EJm86s", "msJA6B9ZjiiZxT6EZ", "CPm5LTwHrvBJCa9h5", "2ftJ38y9SRBCBsCzy", "zJZvoiwydJ5zvzTHK", "zNcLnqHF5rvrTsQJx", "knpAQ4F3gmguxy39z", "buixYfcXBah9hbSNZ", "neQ7eXuaXpiYw7SBy", "erGipespbbzdG5zYb", "xgicQnkrdA5FehhnQ", "6ddcsdA2c2XpNpE5x", "HFyWNBnDNEDsDNLrZ", "QsMJQSFj7WfoTMNgW", "PeSzc9JTBxhaYRp9b", "5wMcKNAwB6X4mp9og", "yCWPkLi8wJvewPbEp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-01T22:44:49.827Z", "modifiedAt": null, "url": null, "title": "Upcoming meditation workshop in the Bay Area", "slug": "upcoming-meditation-workshop-in-the-bay-area", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xMTLygwxyzQExbH87/upcoming-meditation-workshop-in-the-bay-area", "pageUrlRelative": "/posts/xMTLygwxyzQExbH87/upcoming-meditation-workshop-in-the-bay-area", "linkUrl": "https://www.lesswrong.com/posts/xMTLygwxyzQExbH87/upcoming-meditation-workshop-in-the-bay-area", "postedAtFormatted": "Monday, April 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upcoming%20meditation%20workshop%20in%20the%20Bay%20Area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpcoming%20meditation%20workshop%20in%20the%20Bay%20Area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMTLygwxyzQExbH87%2Fupcoming-meditation-workshop-in-the-bay-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upcoming%20meditation%20workshop%20in%20the%20Bay%20Area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMTLygwxyzQExbH87%2Fupcoming-meditation-workshop-in-the-bay-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMTLygwxyzQExbH87%2Fupcoming-meditation-workshop-in-the-bay-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>\n<p>Hey everybody,</p>\n<p>Michael \"Valentine\" Smith is teaching a Meditation workshop in Alamo, CA on May 3-5. The workshop is aimed at rationalist-type folk who know about the benefits of meditation and would like to be meditating more, but all are welcome to join. Val is a curriculum designer at CFAR and has been developing his family's meditation tradition for his whole life.</p>\n<p><a href=\"http://www.mindfulnessengineering.com/\">http://www.mindfulnessengineering.com/</a></p>\n<p>Note: This is not a CFAR event.</p>\n<div><br /></div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xMTLygwxyzQExbH87", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.1560329708085647e-06, "legacy": true, "legacyId": "22177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T02:10:03.624Z", "modifiedAt": null, "url": null, "title": "The Unintuitive Power Laws of Giving", "slug": "the-unintuitive-power-laws-of-giving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:58.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zoAWSxj9f26NzcDBj/the-unintuitive-power-laws-of-giving", "pageUrlRelative": "/posts/zoAWSxj9f26NzcDBj/the-unintuitive-power-laws-of-giving", "linkUrl": "https://www.lesswrong.com/posts/zoAWSxj9f26NzcDBj/the-unintuitive-power-laws-of-giving", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Unintuitive%20Power%20Laws%20of%20Giving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Unintuitive%20Power%20Laws%20of%20Giving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzoAWSxj9f26NzcDBj%2Fthe-unintuitive-power-laws-of-giving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Unintuitive%20Power%20Laws%20of%20Giving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzoAWSxj9f26NzcDBj%2Fthe-unintuitive-power-laws-of-giving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzoAWSxj9f26NzcDBj%2Fthe-unintuitive-power-laws-of-giving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p>Why give globally? Why give money? Why health charities? Why single-issue organizations? At first glance these all seem like arbitrary choices: what if I would rather volunteer, or donate to local charities? Why does it matter? It comes down to two distributions: cost-effectiveness and income.</p>\n<p>&nbsp;</p>\n<p><em>DALYs per $1000</em><br /> <img src=\"http://www.jefftk.com/daly-per-1000-usd-sm.png\" alt=\"\" /></p>\n<p>This shows the cost-effectiveness of a large number of health interventions, with taller bars in cases where we can avert more death and suffering per dollar. The shape of this chart is important: while we can do a lot of good if we pick an intervention at random or support a 'horizontal' effort that works on everything, we can do 300 times better by picking one in the top 10%. This is why single-issue charities make sense: you can pick one that focuses on a top intervention.</p>\n<p>(Don't let the small bars on the left fool you: nearly every intervention on that chart is worth doing [1], some are just far more valuable than others.)</p>\n<p>This only considers health: what about other ways of helping people? Political advocacy, development, literacy, human rights, why not them? The big thing health has going for it is that we can measure impact, which lets us choose only the best options. In other fields where we can't measure we could end up anywhere on the impact curve.</p>\n<p>Let's look at another distribution:</p>\n<p><img src=\"http://www.jefftk.com/world-income-distribution.png\" alt=\"\" /></p>\n<p>So some people have a lot more money that other people, we knew that, right? But have a look at the scale. Someone earning at the poverty line in the US is richer than 90% of people. This is why giving globally is so powerful: small amounts of your money can mean a huge amount to people who have so much less.</p>\n<p>Neither of these distributions are intuitive: we don't feel that rich, and charities all seem kind of interchangeable. But understanding them can make the difference between trying to do good and really succeeding.</p>\n<p>(I first saw these charts in a talk by Toby Ord of <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> (GWWC). The data for the first chart, DALYs per $1000, comes from the <a href=\"http://www.dcp2.org/\">DCP2</a>. This was a project that, among other things, compiled cost effectiveness estimates for a very wide range of health interventions. I made the chart from the csv version of the data <a href=\"http://www.dcp2.org/page/main/BrowseInterventions.html\">from here</a>, excluding the ~60 interventions (of 171) that didn't have estimates. The second chart is straight from GWWC's website, and you can read the details <a href=\"http://www.givingwhatwecan.org/why-give/how-rich-am-i\">there</a> by clicking on footnote 4.)</p>\n<p><br /> [1] The median intervention there is $207/DALY, which roughly means it can give someone an extra year of healthy life for $207. Which is an incredible deal, that I think most of us would jump at. And it's less than 5 pixels high.</p>\n<p><em><small>I also posted this <a href=\"http://www.jefftk.com/news/2013-04-01\">on my blog</a>.</small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zoAWSxj9f26NzcDBj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 39, "extendedScore": null, "score": 1.1561723196119412e-06, "legacy": true, "legacyId": "22179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T03:49:10.589Z", "modifiedAt": null, "url": null, "title": "We Don't Have a Utility Function", "slug": "we-don-t-have-a-utility-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:25.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kHG7BouAdXh74nZ6j/we-don-t-have-a-utility-function", "pageUrlRelative": "/posts/kHG7BouAdXh74nZ6j/we-don-t-have-a-utility-function", "linkUrl": "https://www.lesswrong.com/posts/kHG7BouAdXh74nZ6j/we-don-t-have-a-utility-function", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20Don't%20Have%20a%20Utility%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20Don't%20Have%20a%20Utility%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHG7BouAdXh74nZ6j%2Fwe-don-t-have-a-utility-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20Don't%20Have%20a%20Utility%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHG7BouAdXh74nZ6j%2Fwe-don-t-have-a-utility-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHG7BouAdXh74nZ6j%2Fwe-don-t-have-a-utility-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1142, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/ggm/pinpointing_utility/\">Pinpointing Utility</a></p>\n<p>If I ever say \"my utility function\", you could reasonably accuse me of  <a href=\"https://en.wikipedia.org/wiki/Cargo_cult_science\">cargo-cult</a> rationality; trying to become more rational by superficially immitating the abstract rationalists we study makes about as much sense as building an air traffic control station out of grass to summon cargo planes.</p>\n<p>There are two ways an agent could be said to have a utility function:</p>\n<ol>\n<li>\n<p>It could behave in accordance with the VNM axioms; always choosing in a sane and consistent manner, such that \"there exists a U\". The agent need not have  an explicit representation of U.</p>\n</li>\n<li>\n<p>It could have an explicit utility function that it tries to  expected-maximize. The agent need not perfectly follow the VNM axioms all  the time. (Real bounded decision systems will take shortcuts for  efficiency and may not achieve perfect rationality, like how real floating  point arithmetic isn't associative).</p>\n</li>\n</ol>\n<p>Neither of these is true of humans. Our behaviour and preferences are not  consistent and sane enough to be VNM, and we are generally quite confused about  what we even want, never mind having reduced it to a utility function.  Nevertheless, you still see the occasional reference to \"my utility function\".</p>\n<p>Sometimes \"my\" refers to \"abstract me who has solved moral philosophy and or  become perfectly rational\", which at least doesn't run afoul of the math, but is  probably still wrong about the particulars of what such an abstract idealized  self would actually want. But other times it's a more glaring error like using  \"utility function\" as shorthand for \"entire self-reflective moral system\", which  may not even be VNMish.</p>\n<p>But this post isn't really about all the ways people misuse terminology, it's  about where we're actually at on the whole problem for which a utility function  might be the solution.</p>\n<p>As above, I don't think any of us have a utility function in either sense; we  are not VNM, and we haven't worked out what we want enough to make a convincing  attempt at trying. Maybe someone out there has a utility function in the second  sense, but I doubt that it actually represents what they would want.</p>\n<p>Perhaps then we should speak of what we want in terms of \"terminal values\"? For  example, I might say that it is a terminal value of mine that I should not  murder, or that freedom from authority is good.</p>\n<p>But what does \"terminal value\" mean? Usually, it means that the value of  something is not contingent on or derived from other facts or situations, like  for example, I may value beautiful things in a way that is not derived from what they get me. The recursive chain of valuableness <em>terminates</em> at some set of values.</p>\n<p>There's another connotation, though, which is that your terminal values are akin  to <em>axioms</em>; not subject to argument or evidence or derivation, and simply  given, that there's no point in trying to reconcile them with people who don't  share them. This is the meaning people are sometimes getting at when they  explain failure to agree with someone as \"terminal value differences\" or  \"different set of moral axioms\". This is completely reasonable, if and only if that is in fact the nature of the beliefs in question.</p>\n<p>About two years ago, it very much felt like freedom from authority was a  terminal value for me. Those hated authoritarians and fascists were simply  <em>wrong</em>, probably due to some fundamental neurological fault that could not  be reasoned with. The very prototype of \"terminal value differences\".</p>\n<p>And yet here I am today, having been reasoned out of that \"terminal value\", such  that I even appreciate a certain aesthetic in bowing to a strong leader.</p>\n<p>If that was a terminal value, I'm afraid the term has lost much of its meaning  to me. If it was not, if even the most fundamental-seeming moral feelings are  subject to argument, I wonder if there is any coherent sense in which I could be  said to have terminal values at all.</p>\n<p>The situation here with \"terminal values\" is a lot like the situation with \"beliefs\" in other circles. Ask someone what they believe in most confidently, and they will take the opportunity to differentiate themselves from the opposing tribe on uncertain controversial issues; god exists, god does not exist, racial traits are genetic, race is a social construct. The pedant answer of course is that the sky is probably blue, and that that box over there is about a meter long.</p>\n<p>Likewise, ask someone for their terminal values, and they will take the opportunity to declare that those hated <a href=\"/lw/gt/a_fable_of_science_and_politics/\">greens</a> are utterly wrong on morality, and blueness is wired into their very core, rather than the obvious things like beauty and friendship being valuable, and paperclips not.</p>\n<p>So besides not having a utility function, those aren't your terminal values. I'd be suprised if even the most pedantic answer weren't subject to argument; I don't seem to have anything like a stable and non-negotiable value system at all, and I don't think that I am even especially confused relative to the rest of you.</p>\n<p>Instead of a nice consistent value system, we have a mess of intuitions and  hueristics and beliefs that often contradict, fail to give an answer, and change  with time and mood and memes. And that's all we have. One of the intuitions is that we want to fix this mess.</p>\n<p>People have tried to do this \"Moral Philosophy\" thing before, <a href=\"/lw/9nm/terminal_bias/\">myself included</a>,  but it hasn't generally turned out well. We've made all kinds of overconfident  leaps to what turn out to be unjustified conclusions (utilitarianism, egoism,  hedonism, etc), or just ended up wallowing in confused despair.</p>\n<p>The zeroth step in solving a problem is to notice that we have a problem.</p>\n<p>The problem here, in my humble opinion, is that we have no idea what we are  doing when we try to do Moral Philosophy. We need to go up a meta-level and get  a handle on Moral MetaPhilosophy. What's the problem? What are the relevent  knowns? What are the unknowns? What's the solution process?</p>\n<p>Ideally, we could do for Moral Philosphy approximately what Bayesian probability  theory has done for Epistemology. My moral intuitions are a horrible mess, but  so are my epistemic intuitions, and yet we more-or-less know what we are doing  in epistemology. A problem like this has been solved before, and this one seems  solvable too, if a bit harder.</p>\n<p>It might be that when we figure this problem out to the point where we can be  said to have a consistent moral system with real terminal values, we will end up  with a utility function, but on the other hand, we might not. Either way, let's  keep in mind that we are still on rather shaky ground, and at least refrain from believing the confident declarations of moral wisdom that we so like to make.</p>\n<p>Moral Philosophy is an important problem, but the way is not clear yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "wfW6iL96u26mbatep": 1, "k6igEkzKYY2EpY7Su": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kHG7BouAdXh74nZ6j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 70, "extendedScore": null, "score": 0.000181, "legacy": true, "legacyId": "22181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 124, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CQkGJ2t5Rw8GcZKJm", "6hfGNLf4Hg5DXqJCF", "QGKFjaZNDtJnBTbxS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T05:37:35.867Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Sacred Mundane", "slug": "seq-rerun-the-sacred-mundane", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qqKSTQYfDrXseri2S/seq-rerun-the-sacred-mundane", "pageUrlRelative": "/posts/qqKSTQYfDrXseri2S/seq-rerun-the-sacred-mundane", "linkUrl": "https://www.lesswrong.com/posts/qqKSTQYfDrXseri2S/seq-rerun-the-sacred-mundane", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Sacred%20Mundane&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Sacred%20Mundane%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqqKSTQYfDrXseri2S%2Fseq-rerun-the-sacred-mundane%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Sacred%20Mundane%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqqKSTQYfDrXseri2S%2Fseq-rerun-the-sacred-mundane", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqqKSTQYfDrXseri2S%2Fseq-rerun-the-sacred-mundane", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/57/the_sacred_mundane/\">The Sacred Mundane</a> was originally published on 25 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Sacred_Mundane\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are a lot of bad habits of thought that have developed to defend religious and spiritual experience. They aren't worth saving, even if we discard the original lie. Let's just admit that we were wrong, and enjoy the universe that's actually here.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h3u/seq_rerun_on_things_that_are_awesome/\">On Things that are Awesome</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qqKSTQYfDrXseri2S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1563132653155365e-06, "legacy": true, "legacyId": "22182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Fwt4sDDacko8Sh5iR", "LZ48PYqwduLBqwKKm", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T07:39:12.403Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Applied Rationality, take two", "slug": "meetup-moscow-applied-rationality-take-two", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:56.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/24bZvbPFe66kYawMB/meetup-moscow-applied-rationality-take-two", "pageUrlRelative": "/posts/24bZvbPFe66kYawMB/meetup-moscow-applied-rationality-take-two", "linkUrl": "https://www.lesswrong.com/posts/24bZvbPFe66kYawMB/meetup-moscow-applied-rationality-take-two", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Applied%20Rationality%2C%20take%20two&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Applied%20Rationality%2C%20take%20two%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24bZvbPFe66kYawMB%2Fmeetup-moscow-applied-rationality-take-two%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Applied%20Rationality%2C%20take%20two%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24bZvbPFe66kYawMB%2Fmeetup-moscow-applied-rationality-take-two", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24bZvbPFe66kYawMB%2Fmeetup-moscow-applied-rationality-take-two", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l4'>Moscow, Applied Rationality, take two</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is regular Sunday gathering, it will have the same practical part as <a href=\"http://lesswrong.com/meetups/l5\">10th April meetup</a>.</p>\n\n<p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied Rationality, useful aspects.</p></li>\n<li><p>Applied Rationality, practice.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l4'>Moscow, Applied Rationality, take two</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "24bZvbPFe66kYawMB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1563958681600942e-06, "legacy": true, "legacyId": "22186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality__take_two\">Discussion article for the meetup : <a href=\"/meetups/l4\">Moscow, Applied Rationality, take two</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is regular Sunday gathering, it will have the same practical part as <a href=\"http://lesswrong.com/meetups/l5\">10th April meetup</a>.</p>\n\n<p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied Rationality, useful aspects.</p></li>\n<li><p>Applied Rationality, practice.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality__take_two1\">Discussion article for the meetup : <a href=\"/meetups/l4\">Moscow, Applied Rationality, take two</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Applied Rationality, take two", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality__take_two", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Applied Rationality, take two", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality__take_two1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T08:06:30.751Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Applied Rationality for beginners", "slug": "meetup-moscow-applied-rationality-for-beginners", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W4CWa2fNZ2Yn3kRKE/meetup-moscow-applied-rationality-for-beginners", "pageUrlRelative": "/posts/W4CWa2fNZ2Yn3kRKE/meetup-moscow-applied-rationality-for-beginners", "linkUrl": "https://www.lesswrong.com/posts/W4CWa2fNZ2Yn3kRKE/meetup-moscow-applied-rationality-for-beginners", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Applied%20Rationality%20for%20beginners&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Applied%20Rationality%20for%20beginners%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4CWa2fNZ2Yn3kRKE%2Fmeetup-moscow-applied-rationality-for-beginners%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Applied%20Rationality%20for%20beginners%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4CWa2fNZ2Yn3kRKE%2Fmeetup-moscow-applied-rationality-for-beginners", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4CWa2fNZ2Yn3kRKE%2Fmeetup-moscow-applied-rationality-for-beginners", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l5'>Moscow, Applied Rationality for beginners</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 April 2013 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, naberezhnaya Luzhnetskaya 2/4 \u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 17</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the first meetup on weekdays. It will have the same practical part as <a href=\"http://lesswrong.com/meetups/l4\">14th April meetup</a>, so you can choose which one to attend. If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to register for the gathering.</p>\n\n<p>We will meet you in the office 444. You need metal door with number 17 (building number) near it. Enter and follow white labels \u201cNe\u00faron\u201d until you will get to office 444. We will start at 19:00, so please come in advance. Meetup will take 2-2.5 hours.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied Rationality, introduction.</p></li>\n<li><p>Applied Rationality, practice.</p></li>\n</ul>\n\n<p>You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l5'>Moscow, Applied Rationality for beginners</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W4CWa2fNZ2Yn3kRKE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1564144170549959e-06, "legacy": true, "legacyId": "22187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality_for_beginners\">Discussion article for the meetup : <a href=\"/meetups/l5\">Moscow, Applied Rationality for beginners</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 April 2013 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, naberezhnaya Luzhnetskaya 2/4 \u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 17</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the first meetup on weekdays. It will have the same practical part as <a href=\"http://lesswrong.com/meetups/l4\">14th April meetup</a>, so you can choose which one to attend. If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to register for the gathering.</p>\n\n<p>We will meet you in the office 444. You need metal door with number 17 (building number) near it. Enter and follow white labels \u201cNe\u00faron\u201d until you will get to office 444. We will start at 19:00, so please come in advance. Meetup will take 2-2.5 hours.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied Rationality, introduction.</p></li>\n<li><p>Applied Rationality, practice.</p></li>\n</ul>\n\n<p>You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Applied_Rationality_for_beginners1\">Discussion article for the meetup : <a href=\"/meetups/l5\">Moscow, Applied Rationality for beginners</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Applied Rationality for beginners", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality_for_beginners", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Applied Rationality for beginners", "anchor": "Discussion_article_for_the_meetup___Moscow__Applied_Rationality_for_beginners1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T11:48:28.650Z", "modifiedAt": null, "url": null, "title": "Differential reproduction for men and women.", "slug": "differential-reproduction-for-men-and-women", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:26.070Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cHEQSEPz4eipGHFy9/differential-reproduction-for-men-and-women", "pageUrlRelative": "/posts/cHEQSEPz4eipGHFy9/differential-reproduction-for-men-and-women", "linkUrl": "https://www.lesswrong.com/posts/cHEQSEPz4eipGHFy9/differential-reproduction-for-men-and-women", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Differential%20reproduction%20for%20men%20and%20women.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADifferential%20reproduction%20for%20men%20and%20women.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHEQSEPz4eipGHFy9%2Fdifferential-reproduction-for-men-and-women%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Differential%20reproduction%20for%20men%20and%20women.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHEQSEPz4eipGHFy9%2Fdifferential-reproduction-for-men-and-women", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHEQSEPz4eipGHFy9%2Fdifferential-reproduction-for-men-and-women", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>There's an idea I've seen a number of times that 80% of women have had descendants, but only 40% of men. A little research tracked it back to <a href=\"http://www.psy.fsu.edu/~baumeistertice/goodaboutmen.htm\">this</a>, but the speech doesn't have a cite and I haven't found a source.</p>\n<p>The reproduction rates for men and women (possibly for the whole history of the species) seems like the sort of thing which could be found out, but I'd like more solid information.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cHEQSEPz4eipGHFy9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 6, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "22190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T14:49:34.108Z", "modifiedAt": null, "url": null, "title": "Journalist's piece about predicting AI", "slug": "journalist-s-piece-about-predicting-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.916Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8AzqTS75XBxWQDGMs/journalist-s-piece-about-predicting-ai", "pageUrlRelative": "/posts/8AzqTS75XBxWQDGMs/journalist-s-piece-about-predicting-ai", "linkUrl": "https://www.lesswrong.com/posts/8AzqTS75XBxWQDGMs/journalist-s-piece-about-predicting-ai", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Journalist's%20piece%20about%20predicting%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJournalist's%20piece%20about%20predicting%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8AzqTS75XBxWQDGMs%2Fjournalist-s-piece-about-predicting-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Journalist's%20piece%20about%20predicting%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8AzqTS75XBxWQDGMs%2Fjournalist-s-piece-about-predicting-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8AzqTS75XBxWQDGMs%2Fjournalist-s-piece-about-predicting-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>Here's a <a href=\"http://www.wired.co.uk/news/archive/2013-03/29/predicting-artificial-intelligence\">piece</a> by Mark Piesing in Wired UK about the difficulty and challenges in predicting AI. It covers a lot of our (Stuart Armstrong, Kaj Sotala and Se&aacute;n &Oacute;h &Eacute;igeartaigh) research into AI prediction, along with Robin Hanson's response. It will hopefully cause people to look more deeply into our work, as <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">published</a> <a href=\"/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">online</a>, in the Pilsen <a href=\"http://beyondai.zcu.cz/\">Beyond AI</a> conference <a href=\"http://beyondai.zcu.cz/files/BAI2011_proceedings.pdf\">proceedings</a>, and forthcoming as \"The errors, insights and lessons of famous AI predictions and what they mean for the future\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8AzqTS75XBxWQDGMs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.1566882754305943e-06, "legacy": true, "legacyId": "22191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["47ci9ixyEbGKWENwR", "fHSf8ACvTCvH9fFyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T17:50:16.480Z", "modifiedAt": "2021-04-05T21:15:33.943Z", "url": null, "title": "Anki decks by LW users", "slug": "anki-decks-by-lw-users", "viewCount": null, "lastCommentedAt": "2021-04-04T07:52:11.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kbp8riSbpKv8jie8o/anki-decks-by-lw-users", "pageUrlRelative": "/posts/Kbp8riSbpKv8jie8o/anki-decks-by-lw-users", "linkUrl": "https://www.lesswrong.com/posts/Kbp8riSbpKv8jie8o/anki-decks-by-lw-users", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anki%20decks%20by%20LW%20users&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnki%20decks%20by%20LW%20users%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbp8riSbpKv8jie8o%2Fanki-decks-by-lw-users%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anki%20decks%20by%20LW%20users%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbp8riSbpKv8jie8o%2Fanki-decks-by-lw-users", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbp8riSbpKv8jie8o%2Fanki-decks-by-lw-users", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><a href=\"http://www.stafforini.com/blog/anki-decks-by-lesswrong-users/\"><i>This post is now available on my blog, where I can keep it updated. </i></a><i>\u2014Pablo (April 2021)</i></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 1, "fF9GEdWXKJ3z73TmB": 1, "TkZ7MFwCi4D63LJ5n": 1, "GQyPQcdEQF4zXhJBq": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kbp8riSbpKv8jie8o", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 44, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "22175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-02T20:10:49.222Z", "modifiedAt": null, "url": null, "title": "Meetup : Buffalo Meetup at UB", "slug": "meetup-buffalo-meetup-at-ub", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MYt43JPPNtueEqAmg/meetup-buffalo-meetup-at-ub", "pageUrlRelative": "/posts/MYt43JPPNtueEqAmg/meetup-buffalo-meetup-at-ub", "linkUrl": "https://www.lesswrong.com/posts/MYt43JPPNtueEqAmg/meetup-buffalo-meetup-at-ub", "postedAtFormatted": "Tuesday, April 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Buffalo%20Meetup%20at%20UB&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Buffalo%20Meetup%20at%20UB%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYt43JPPNtueEqAmg%2Fmeetup-buffalo-meetup-at-ub%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Buffalo%20Meetup%20at%20UB%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYt43JPPNtueEqAmg%2Fmeetup-buffalo-meetup-at-ub", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYt43JPPNtueEqAmg%2Fmeetup-buffalo-meetup-at-ub", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 599, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l6'>Buffalo Meetup at UB</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 April 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Capen Cafe, Buffalo, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Meet at the Capen Cafe (its on the ground floor of Capen Hall below the Capen Library). We may decide to move into a classroom (31 Capen worked pretty well last time). In case you're unfamiliar with UB and can't find us, Contact me at 716-341-9042</p>\n\n<p>We'll kick the meeting off with ASK LESS WRONG. Think of something in your everyday life that's bothering you and we'll help you smooth it out. Purpose: increase the fun in each others' lives through the magic of friendship. Secondary purpose: train ourselves to notice things that are suboptimal and view them as problems that can be solved.</p>\n\n<p>The main part of the meeting will be a RATIONAL DEBATE. (I totally got this idea  from another meetup group.)</p>\n\n<p>I'll be chairing, so don't worry about keeping track of this vast list of meta stuff - that's my job. It'll go something like this:</p>\n\n<p>In a conventional debate, you win by sounding more plausible than the other person. In a rational debate, you win if and only if you end up believing the truth. This makes it a cooperative game - it's possible for everyone to win or for everyone to lose. (Incidentally it also means you don't actually know whether you've won or not).</p>\n\n<p>Initially, each person answers the question separately, choosing how they wish to frame their answer. If people come up with very different ways of framing the question, we will take each one in turn and try to approach the question from that direction. (The point of this is to avoid fighting over the framing of the discussion and instead address the issues directly).</p>\n\n<p>I'll keep track of structural stuff - different ways of framing the question, agreed subtopics of discussion, and binary chopping to find points of disagreement (which involves lists of statements and verbally how plausible we each think they are).</p>\n\n<p>If you didn't attend the meetup on combat reflexes, I'd strongly recommend reading what we covered that day: <a href=\"http://www.meetup.com/Less-Wrong-Buffalo/pages/Combat_Reflexes_%28Meetup_Jan24%29\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Buffalo/pages/Combat_Reflexes_%28Meetup_Jan24%29</a></p>\n\n<p>When arguing against something, construct a steel man first - rephrase the opposing argument in your own words, making it as strong and plausible as you can, before you try and defeat it.</p>\n\n<p>Be bold and specific - make sure you're saying something substantial, even if you're not completely sure it's true.</p>\n\n<p>The social aspect: make sure we're providing status and rewards for the right things. Changing your mind should be rewarded rather than discouraged. Likewise, being good at admitting fault is a strength, not a weakness.</p>\n\n<p>Leave a line of retreat. What would I do if I was wrong about this?</p>\n\n<p>Try to notice when you're replying to somebody's cached thought with a cached thought of your own. I'll try and do the same.</p>\n\n<p>Try to find something to change your mind about, even if it's something small.</p>\n\n<p>Separate out disagreement about facts from disagreement about values (and disagreement about strategy, which combines both). Separate out semantic confusion.</p>\n\n<p>If possible, identify which of these techniques you're trying to put into practice. I'll do the same. (By drawing attention to this we'll help keep things purposeful, and also hopefully learn which techniques seem particularly useful).</p>\n\n<p>Resources on rational debate:</p>\n\n<p><a href=\"http://lesswrong.com/lw/3k/how_to_not_lose_an_argument/\" rel=\"nofollow\">http://lesswrong.com/lw/3k/how_to_not_lose_an_argument/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/85h/better_disagreement/\" rel=\"nofollow\">http://lesswrong.com/lw/85h/better_disagreement/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/o4/leave_a_line_of_retreat/\" rel=\"nofollow\">http://lesswrong.com/lw/o4/leave_a_line_of_retreat/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/gm9/philosophical_landmines/\" rel=\"nofollow\">http://lesswrong.com/lw/gm9/philosophical_landmines/</a></p>\n\n<p>Here are some possible topics we can debate:</p>\n\n<p>\"will rationality make you rich\"</p>\n\n<p>\"Is there intelligent life elsewhere in the universe\"</p>\n\n<p>\"Should you vote\"</p>\n\n<p>If anyone has some good ideas on debate topics, Reply in the comments, I'd love to here them for future debates.</p>\n\n<p>Hope to see you all at UB on Sunday! Let me know if you can come.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l6'>Buffalo Meetup at UB</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MYt43JPPNtueEqAmg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1569066337283167e-06, "legacy": true, "legacyId": "22192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup_at_UB\">Discussion article for the meetup : <a href=\"/meetups/l6\">Buffalo Meetup at UB</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 April 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Capen Cafe, Buffalo, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Meet at the Capen Cafe (its on the ground floor of Capen Hall below the Capen Library). We may decide to move into a classroom (31 Capen worked pretty well last time). In case you're unfamiliar with UB and can't find us, Contact me at 716-341-9042</p>\n\n<p>We'll kick the meeting off with ASK LESS WRONG. Think of something in your everyday life that's bothering you and we'll help you smooth it out. Purpose: increase the fun in each others' lives through the magic of friendship. Secondary purpose: train ourselves to notice things that are suboptimal and view them as problems that can be solved.</p>\n\n<p>The main part of the meeting will be a RATIONAL DEBATE. (I totally got this idea  from another meetup group.)</p>\n\n<p>I'll be chairing, so don't worry about keeping track of this vast list of meta stuff - that's my job. It'll go something like this:</p>\n\n<p>In a conventional debate, you win by sounding more plausible than the other person. In a rational debate, you win if and only if you end up believing the truth. This makes it a cooperative game - it's possible for everyone to win or for everyone to lose. (Incidentally it also means you don't actually know whether you've won or not).</p>\n\n<p>Initially, each person answers the question separately, choosing how they wish to frame their answer. If people come up with very different ways of framing the question, we will take each one in turn and try to approach the question from that direction. (The point of this is to avoid fighting over the framing of the discussion and instead address the issues directly).</p>\n\n<p>I'll keep track of structural stuff - different ways of framing the question, agreed subtopics of discussion, and binary chopping to find points of disagreement (which involves lists of statements and verbally how plausible we each think they are).</p>\n\n<p>If you didn't attend the meetup on combat reflexes, I'd strongly recommend reading what we covered that day: <a href=\"http://www.meetup.com/Less-Wrong-Buffalo/pages/Combat_Reflexes_%28Meetup_Jan24%29\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Buffalo/pages/Combat_Reflexes_%28Meetup_Jan24%29</a></p>\n\n<p>When arguing against something, construct a steel man first - rephrase the opposing argument in your own words, making it as strong and plausible as you can, before you try and defeat it.</p>\n\n<p>Be bold and specific - make sure you're saying something substantial, even if you're not completely sure it's true.</p>\n\n<p>The social aspect: make sure we're providing status and rewards for the right things. Changing your mind should be rewarded rather than discouraged. Likewise, being good at admitting fault is a strength, not a weakness.</p>\n\n<p>Leave a line of retreat. What would I do if I was wrong about this?</p>\n\n<p>Try to notice when you're replying to somebody's cached thought with a cached thought of your own. I'll try and do the same.</p>\n\n<p>Try to find something to change your mind about, even if it's something small.</p>\n\n<p>Separate out disagreement about facts from disagreement about values (and disagreement about strategy, which combines both). Separate out semantic confusion.</p>\n\n<p>If possible, identify which of these techniques you're trying to put into practice. I'll do the same. (By drawing attention to this we'll help keep things purposeful, and also hopefully learn which techniques seem particularly useful).</p>\n\n<p>Resources on rational debate:</p>\n\n<p><a href=\"http://lesswrong.com/lw/3k/how_to_not_lose_an_argument/\" rel=\"nofollow\">http://lesswrong.com/lw/3k/how_to_not_lose_an_argument/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/85h/better_disagreement/\" rel=\"nofollow\">http://lesswrong.com/lw/85h/better_disagreement/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/o4/leave_a_line_of_retreat/\" rel=\"nofollow\">http://lesswrong.com/lw/o4/leave_a_line_of_retreat/</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/gm9/philosophical_landmines/\" rel=\"nofollow\">http://lesswrong.com/lw/gm9/philosophical_landmines/</a></p>\n\n<p>Here are some possible topics we can debate:</p>\n\n<p>\"will rationality make you rich\"</p>\n\n<p>\"Is there intelligent life elsewhere in the universe\"</p>\n\n<p>\"Should you vote\"</p>\n\n<p>If anyone has some good ideas on debate topics, Reply in the comments, I'd love to here them for future debates.</p>\n\n<p>Hope to see you all at UB on Sunday! Let me know if you can come.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup_at_UB1\">Discussion article for the meetup : <a href=\"/meetups/l6\">Buffalo Meetup at UB</a></h2>", "sections": [{"title": "Discussion article for the meetup : Buffalo Meetup at UB", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup_at_UB", "level": 1}, {"title": "Discussion article for the meetup : Buffalo Meetup at UB", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup_at_UB1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6yTShbTdtATxKonY5", "FhH8m5n8qGSSHsAgG", "3XgYbghWruBMrPTAL", "L4HQ3gnSrBETRdcGu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T02:45:39.608Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Your Price for Joining", "slug": "seq-rerun-your-price-for-joining", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qS6FGFmfi9kQotNWq/seq-rerun-your-price-for-joining", "pageUrlRelative": "/posts/qS6FGFmfi9kQotNWq/seq-rerun-your-price-for-joining", "linkUrl": "https://www.lesswrong.com/posts/qS6FGFmfi9kQotNWq/seq-rerun-your-price-for-joining", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Your%20Price%20for%20Joining&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Your%20Price%20for%20Joining%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqS6FGFmfi9kQotNWq%2Fseq-rerun-your-price-for-joining%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Your%20Price%20for%20Joining%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqS6FGFmfi9kQotNWq%2Fseq-rerun-your-price-for-joining", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqS6FGFmfi9kQotNWq%2Fseq-rerun-your-price-for-joining", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>Today's post, <a href=\"/lw/5j/your_price_for_joining/\">Your Price for Joining</a> was originally published on 26 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The game-theoretical puzzle of the Ultimatum game has its reflection in a real-world dilemma: How much do you demand that an existing group adjust toward you, before you will adjust toward it? Our hunter-gatherer instincts will be tuned to groups of 40 with very minimal administrative demands and equal participation, meaning that we underestimate the inertia of larger and more specialized groups and demand too much before joining them. In other groups this resistance can be overcome by affective death spirals and conformity, but rationalists think themselves too good for this - with the result that people in the nonconformist cluster often set their joining prices way way way too high, like an 50-way split with each player demanding 20% of the money. Nonconformists need to move in the direction of joining groups more easily, even in the face of annoyances and apparent unresponsiveness. If an issue isn't worth <em>personally fixing by however much effort it takes</em>, it's not worth a refusal to contribute.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h46/seq_rerun_the_sacred_mundane/\">The Sacred Mundane</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qS6FGFmfi9kQotNWq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.1571751104958203e-06, "legacy": true, "legacyId": "22196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q8evewZW5SeidLdbA", "qqKSTQYfDrXseri2S", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T02:56:22.161Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 51-55", "slug": "meetup-durham-hpmor-discussion-chapters-51-55", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/phApNqMr3kYYqsCTk/meetup-durham-hpmor-discussion-chapters-51-55", "pageUrlRelative": "/posts/phApNqMr3kYYqsCTk/meetup-durham-hpmor-discussion-chapters-51-55", "linkUrl": "https://www.lesswrong.com/posts/phApNqMr3kYYqsCTk/meetup-durham-hpmor-discussion-chapters-51-55", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2051-55&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2051-55%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphApNqMr3kYYqsCTk%2Fmeetup-durham-hpmor-discussion-chapters-51-55%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2051-55%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphApNqMr3kYYqsCTk%2Fmeetup-durham-hpmor-discussion-chapters-51-55", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FphApNqMr3kYYqsCTk%2Fmeetup-durham-hpmor-discussion-chapters-51-55", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l7'>Durham HPMoR Discussion, chapters 51-55</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 April 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Fullsteam, 726 Rigsbee Ave, Durham, NC </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll get together to discuss Harry Potter and the Methods of Rationality, chapters 51-55. Also known as the first portion of the Stanford Prison Experiment section. As always, we'll be happy to summarize the chapters if you haven't read all of them, haven't read them recently enough, etc.</p>\n\n<p>Come join us for food, beverages, rationality, philosophy, and general awesomeness. Food can be found at the food trucks by the farmer's market or outside Fullsteam; coffee can be found at Cocoa Cinnamon and possibly the farmer's market.</p>\n\n<p>We'll start gathering around 12, and on-topic discussion will start at 12:30; that should give everyone enough time to settle in and eat their food and such (which, btw, you're welcome to bring to Fullsteam).</p>\n\n<p>If you need directions, parking advice, transportation help, etc., please email the RTLW list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l7'>Durham HPMoR Discussion, chapters 51-55</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "phApNqMr3kYYqsCTk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1571823939022289e-06, "legacy": true, "legacyId": "22197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_51_55\">Discussion article for the meetup : <a href=\"/meetups/l7\">Durham HPMoR Discussion, chapters 51-55</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 April 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Fullsteam, 726 Rigsbee Ave, Durham, NC </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll get together to discuss Harry Potter and the Methods of Rationality, chapters 51-55. Also known as the first portion of the Stanford Prison Experiment section. As always, we'll be happy to summarize the chapters if you haven't read all of them, haven't read them recently enough, etc.</p>\n\n<p>Come join us for food, beverages, rationality, philosophy, and general awesomeness. Food can be found at the food trucks by the farmer's market or outside Fullsteam; coffee can be found at Cocoa Cinnamon and possibly the farmer's market.</p>\n\n<p>We'll start gathering around 12, and on-topic discussion will start at 12:30; that should give everyone enough time to settle in and eat their food and such (which, btw, you're welcome to bring to Fullsteam).</p>\n\n<p>If you need directions, parking advice, transportation help, etc., please email the RTLW list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_51_551\">Discussion article for the meetup : <a href=\"/meetups/l7\">Durham HPMoR Discussion, chapters 51-55</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 51-55", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_51_55", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 51-55", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_51_551", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T03:03:28.992Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham: Luminosity (New location!)", "slug": "meetup-durham-luminosity-new-location", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ytkYh9dKCQwAo9mLA/meetup-durham-luminosity-new-location", "pageUrlRelative": "/posts/ytkYh9dKCQwAo9mLA/meetup-durham-luminosity-new-location", "linkUrl": "https://www.lesswrong.com/posts/ytkYh9dKCQwAo9mLA/meetup-durham-luminosity-new-location", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%3A%20Luminosity%20(New%20location!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%3A%20Luminosity%20(New%20location!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FytkYh9dKCQwAo9mLA%2Fmeetup-durham-luminosity-new-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%3A%20Luminosity%20(New%20location!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FytkYh9dKCQwAo9mLA%2Fmeetup-durham-luminosity-new-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FytkYh9dKCQwAo9mLA%2Fmeetup-durham-luminosity-new-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l8'>Durham: Luminosity (New location!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 April 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cocoa Cinnamon, 420 W Geer St, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss the start of the Luminosity sequence by Alicorn: how and why to know what's going on inside your own head.\nSuggested reading: <br />\n<a href=\"http://lesswrong.com/lw/1xh/living_luminously/\">Living luminously</a> is the intro to the sequence. <br />\n<a href=\"http://lesswrong.com/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">You Are Likely To Be Eaten By A Grue</a> <br />\n<a href=\"http://lesswrong.com/lw/1xq/let_there_be_light/\">Let There Be Light</a> <br />\nThe background material linked from these articles may be helpful as well; wikiwalk as you deem appropriate.\nWe'll meet at our usual 7:00 to say hello, get coffee, chat, and wait for latecomers. On-topic discussion will start at 7:30.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l8'>Durham: Luminosity (New location!)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ytkYh9dKCQwAo9mLA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1571872320839081e-06, "legacy": true, "legacyId": "22198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham__Luminosity__New_location__\">Discussion article for the meetup : <a href=\"/meetups/l8\">Durham: Luminosity (New location!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 April 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cocoa Cinnamon, 420 W Geer St, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss the start of the Luminosity sequence by Alicorn: how and why to know what's going on inside your own head.\nSuggested reading: <br>\n<a href=\"http://lesswrong.com/lw/1xh/living_luminously/\">Living luminously</a> is the intro to the sequence. <br>\n<a href=\"http://lesswrong.com/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">You Are Likely To Be Eaten By A Grue</a> <br>\n<a href=\"http://lesswrong.com/lw/1xq/let_there_be_light/\">Let There Be Light</a> <br>\nThe background material linked from these articles may be helpful as well; wikiwalk as you deem appropriate.\nWe'll meet at our usual 7:00 to say hello, get coffee, chat, and wait for latecomers. On-topic discussion will start at 7:30.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham__Luminosity__New_location__1\">Discussion article for the meetup : <a href=\"/meetups/l8\">Durham: Luminosity (New location!)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham: Luminosity (New location!)", "anchor": "Discussion_article_for_the_meetup___Durham__Luminosity__New_location__", "level": 1}, {"title": "Discussion article for the meetup : Durham: Luminosity (New location!)", "anchor": "Discussion_article_for_the_meetup___Durham__Luminosity__New_location__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "r6diXRLvkZBLpSoTf", "Y6TpEEKZq6HXfhWxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T09:52:06.181Z", "modifiedAt": null, "url": null, "title": "A Call for Constant Vigilance", "slug": "a-call-for-constant-vigilance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:04.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eFPkiusrPKW6JBw4h/a-call-for-constant-vigilance", "pageUrlRelative": "/posts/eFPkiusrPKW6JBw4h/a-call-for-constant-vigilance", "linkUrl": "https://www.lesswrong.com/posts/eFPkiusrPKW6JBw4h/a-call-for-constant-vigilance", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Call%20for%20Constant%20Vigilance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Call%20for%20Constant%20Vigilance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeFPkiusrPKW6JBw4h%2Fa-call-for-constant-vigilance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Call%20for%20Constant%20Vigilance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeFPkiusrPKW6JBw4h%2Fa-call-for-constant-vigilance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeFPkiusrPKW6JBw4h%2Fa-call-for-constant-vigilance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 584, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/31/what_do_we_mean_by_rationality/\">What Do We Mean By \"Rationality?\"</a></p>\n<p>Rationality has many facets, both relatively simple and quite complex. As a result, it can often be hard to determine what aspects of rationality you should or shouldn't stress.</p>\n<p>An extremely basic and abstract model of how rationality works might look a little something like this:</p>\n<ol>\n<li>Collect evidence about your environment from various sources</li>\n<li>Update your model of reality based on evidence collected (optimizing the updating process is more or less what we know as <a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">epistemic rationality</a>)</li>\n<li>Act in accordance with what your model of reality indicates is best for <a href=\"/lw/7i/rationality_is_systematized_winning/\">achieving your goals</a> (optimizing the actions you take is more or less what we know as <a href=\"http://wiki.lesswrong.com/wiki/Rationality#Instrumental_rationality\">instrumental rationality</a>)</li>\n<li>Repeat continually forever</li>\n</ol>\n<div>A lot of thought, both on LessWrong and within the academic literature on heuristics and biases, has gone into improving epistemic rationality, and while improving instrumental rationality was less of a focus at first, recently the community has been focusing more on it. On the other hand, improving your ability to collect evidence has been relatively neglected-- hence the (in-progress as of this writing)&nbsp;<a href=\"/lw/g4q/the_zeroth_skillset/\">Situational Awareness sequence</a>.</div>\n<div><br /></div>\n<div>But most neglected of all has been the last step, \"repeat continually forever.\" This sounds like a trivial instruction but is in fact highly important to emphasize. All your skills and training and techniques mean nothing if you don't use them, and unfortunately there are many reasons that you might not use your skills.</div>\n<div><br /></div>\n<div>You might be&nbsp;<a href=\"/lw/gux/dont_get_offended/\">offended</a>, angry, hurt, or otherwise <a href=\"http://www.acceleratingfuture.com/steven/?p=147\">emotionally compromised</a>. Similarly, you might be sleepy, inebriated, hungry, or otherwise physically compromised. You might be overconfident in your ability to handle a certain type of problem or situation, and hence not bother to think of other ways that might work better.<sup>[1]</sup> You might simply not bother to apply your skills because you don't think they're necessary, missing out on potential gains that you don't see at a glance-- or maybe even <a href=\"http://en.wikipedia.org/wiki/There_are_known_knowns\">don't know exist</a>. All in all, there are many times in which you may be missing out on the benefits that your skills can provide.<br /></div>\n<div><br /></div>\n<div>It may therefore be worthwhile to occasionally check whether or not you are actually applying your skills.<span style=\"font-size: 11.111111640930176px;\"> </span>Further, try to make this sort of check a habit, especially when encountering circumstances where people would <a href=\"/lw/gux/dont_get_offended/\">typically be less than rational</a>. If you find that you aren't using your skills as often as you'd expect, that may be cause for alarm, and at the very least is cause for introspection. After all, <a href=\"/lw/7i/rationality_is_systematized_winning/\">if rationality skills can be constantly applied to being successful in everyday life</a>, we should be constantly on the watch for opportunities to apply them, as well as for potential lapses in our vigilance.</div>\n<div><br /></div>\n<div>I indeed suspect that most LessWrong users would benefit more from being more vigilant in practicing and applying basic rationality skills than they would from learning cool advanced techniques.<sup> </sup>This principle is generally true in the martial arts, and both the inside and outside view strongly suggest to me that it is true for the art of rationality as well.</div>\n<div><br /></div>\n<div>All in all, improving your rationality is a matter of serious practice and changing your mindset, not just learning cool new life hacks-- so next time you think about improving your rationality, don't look for new tricks, but new ways to <a href=\"/lw/la/truly_part_of_you/\">truly integrate</a> the principles you are already familiar with.</div>\n<div><br /></div>\n<div><br /></div>\n<div>[1] The footnote to&nbsp;<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>&nbsp;describes several examples where this might apply.</div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "CD6DGZJD4ningyzWF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eFPkiusrPKW6JBw4h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 40, "extendedScore": null, "score": 1.1574652000311851e-06, "legacy": true, "legacyId": "22149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv", "4ARtkT3EYox3THYjF", "vmBHCPZxunwdbFvaJ", "3Ci2Zxncj3oiB2NP4", "fg9fXrHpeaDD6pEPL", "PBRWb2Em5SNeWYwwB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T22:53:50.516Z", "modifiedAt": null, "url": null, "title": "Anybody want to meet in Leipzig, Germany?", "slug": "anybody-want-to-meet-in-leipzig-germany", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:55.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sZcYR6MdipjLGqhDW/anybody-want-to-meet-in-leipzig-germany", "pageUrlRelative": "/posts/sZcYR6MdipjLGqhDW/anybody-want-to-meet-in-leipzig-germany", "linkUrl": "https://www.lesswrong.com/posts/sZcYR6MdipjLGqhDW/anybody-want-to-meet-in-leipzig-germany", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anybody%20want%20to%20meet%20in%20Leipzig%2C%20Germany%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnybody%20want%20to%20meet%20in%20Leipzig%2C%20Germany%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZcYR6MdipjLGqhDW%2Fanybody-want-to-meet-in-leipzig-germany%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anybody%20want%20to%20meet%20in%20Leipzig%2C%20Germany%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZcYR6MdipjLGqhDW%2Fanybody-want-to-meet-in-leipzig-germany", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZcYR6MdipjLGqhDW%2Fanybody-want-to-meet-in-leipzig-germany", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>Hey guys, does anybody else here live in Leipzig, Germany? I'd love to meet up and find/found an LW community here!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sZcYR6MdipjLGqhDW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1579973156365328e-06, "legacy": true, "legacyId": "22202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-03T23:33:27.275Z", "modifiedAt": null, "url": null, "title": "An Introduction to Control Markets", "slug": "an-introduction-to-control-markets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:38.325Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hxhFuwBaqetT8RSKp/an-introduction-to-control-markets", "pageUrlRelative": "/posts/hxhFuwBaqetT8RSKp/an-introduction-to-control-markets", "linkUrl": "https://www.lesswrong.com/posts/hxhFuwBaqetT8RSKp/an-introduction-to-control-markets", "postedAtFormatted": "Wednesday, April 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Introduction%20to%20Control%20Markets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Introduction%20to%20Control%20Markets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxhFuwBaqetT8RSKp%2Fan-introduction-to-control-markets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Introduction%20to%20Control%20Markets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxhFuwBaqetT8RSKp%2Fan-introduction-to-control-markets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxhFuwBaqetT8RSKp%2Fan-introduction-to-control-markets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1247, "htmlBody": "<p>Control markets are systems where the control of resources and the system is determined by a market, the currency of that market is given out dependent upon how well the system as a whole is doing.</p>\n<p>They have been discussed and explored in computer systems for a while with <a href=\"http://e-drexler.com/d/09/00/AgoricsPapers/agoricpapers.html\">Agorics</a>, <a href=\"http://en.wikipedia.org/wiki/Learning_classifier_system\">Learning Classifier systems</a> and <a href=\"http://www.whatisthought.com/eric.html\">Eric Baum's</a> work being two notable examples (<a href=\"http://www.eskimo.com/~wilson/ps/zcs.pdf\">ZCS</a> being the closest LCS to it). These are limited and constrained markets, in that the communication and computational expressiveness of them are limited. However unlimited control markets may be of use in the real world to control an organization in a flexible fashion. For example it might be useful for shareholders&nbsp; to control a board or possibly the whole company. Even if it isn't compatible with human motivational systems and real world conditions, discussing and thinking about these sorts of systems may enable us to find better organizational structures than our current ones.</p>\n<p>Control market can be seen as trying to embed reinforcement learning into an organization.</p>\n<p><a id=\"more\"></a></p>\n<p>When we create organizations we don't tend to design their control structures from a theoretical basis. We adopt the prevailing structures, such as a representatives, boards of directors or trustees. So are there organizational systems that can do better at keeping the organizations to their purpose<a href=\"#1\"><sup>1</sup></a>? Any exploration will have to be a mix of theory and practice.</p>\n<p>Let us define a few terms:</p>\n<ul>\n<li>Stakeholders: People with some information about how the organization should act. E.g. Voters.</li>\n<li>Feedback: That which stakeholders can provide to change the organization. E.g. Voting would count.</li>\n<li>Actors: The people that act within the organization. Generally considered at least somewhat selfish.</li>\n<li>Resources: Parts of the organization that can be controlled by an Actor.</li>\n</ul>\n<p>We have little control on what stakeholders and actors are like, so we can only control the nature of feedback. So what desiderata do we want for feedback?</p>\n<ul>\n<li>Feedback should count: People have argued that voting is <a href=\"/lw/30j/voting_is_not_rational_usually/\">generally irrational</a> or pointless. Lets avoid that.</li>\n<li>Feedback should be easy: A choice, a single number or small group of numbers.</li>\n<li>Feedback should work in a predictable fashion: Stakeholders and Actors are limited computational agents so cannot evaluate complex mechanisms.</li>\n<li>Negative feedback shouldn't hurt the organization: Some forms of feedback, such as voting with your wallet when choosing which charity to donate to, hurt the organization's ability to perform their goal. You may not wish to do this, but simply change a small facet of the organization.</li>\n<li>Making feedback about issues not people: A priori feedback, such as voting, means that you only have the promises and personality of the person to make a decision upon. Post-hoc feedback allows you to see how an Actor has performed during a time period.</li>\n<li>Actors should be encouraged to show rationality: Politicians are generally optimistic about things like the economy. If the system encouraged them to show what they thought the future held and be accurate, this would be valuable.</li>\n<li>Checks and balances: No actor or group of actors should be able to get too much power</li>\n</ul>\n<p>Ignoring checks and balances for a moment, a simple Control Market would be this:</p>\n<ul>\n<li>There is one resource.</li>\n<li>There is an internal currency called funge (short for fungible, rhymes with sponge) that is used by Actors to bid upon control of the resource for a time period. The one that bids the highest wins (I favour vickrey auctions<a href=\"#2\"><sup>2</sup></a>). </li>\n<li>Stakeholders evaluate how the resource has been used in the time period and give a number. That number is aggregated in some simple fashion (say summed) and given as funges to the controller of that resource.</li>\n</ul>\n<p>There are any number of ways of designing the auctions and the system. You probably want to try and avoid inflation. Some theory for this would be good to develop.</p>\n<p>So funge is cycled around, sunk into bidding for a resource and sourced from the feedback. So how are our desiderata?</p>\n<ul>\n<li>Feedback should count: Changing your feedback will make a slight change in the amount of funges the Actor gets. Even an individual.</li>\n<li>Feedback should be easy: Yes. Just a single number.</li>\n<li>Feedback should work in a predictable fashion: Just a few rules, so pretty simple.</li>\n<li>Negative feedback shouldn't hurt the organization: If you give on average 5 funges giving less than that will be considered negative (due to expectations)</li>\n<li>Making feedback about issues not people: It is post-hoc so you can see how people have done. There is still an issue of cult of personality, you might give more negative feedback to someone you don't like vs someone you do. You don't need to know who is in control, so that might offer a way out.</li>\n<li>Actors should be encouraged to show rationality: If a bad period is expected in the future, then people will bid less for a resource as the expected funge they will get back will be lower. If someone over bids they won't do so well.</li>\n</ul>\n<p>However it gives <strong>ultimate power</strong> to one person. So fails the checks and balances desideratum. Checks and balances can be achieved somewhat by adding in more Resources that can be controlled by multiple Actors. How many resources you need to create to make sufficient checks and balances, is a question I don't know how to tackle, especially if each Resource gets individual Feedback which adds more complexity for the Stakeholder.&nbsp; You might have non-Feedback giving Resources and say that they cannot be controlled by the same Actor so that power is spread.</p>\n<p>There is at least one other issue: Bootstrapping, how does an person become an Actor and get funges, if you need funge to buy resources to get funge? It seems likely that parties will spring up and loan funge to up and coming potential Actors in expectation that they pay it back with interest. Whether these parties will be poisonous and powerful as a current ones are, is worth investigating. Aspiring Actors may also help Actors perform their jobs, with out any control of a Resource, with funge as a reward. This would allow them to save it up to be able to get control of a resource.</p>\n<p>If funge is transferable to money, there is an issue, Actors might simply get money and then use that as an incentive for people, so that they will never have to give funge to someone (and potentially create a competitor for them).</p>\n<p>If you are getting a feeling of deja vu I linked to my blog about this a while back. The reason why I am posting this now is that I am somewhat in a position to put some time/money towards creating infrastructure for testing a Control Market. I'll do another post with a call for participation.</p>\n<p>Coda: So what is wrong with our current currencies? Fractional Reserve banking and money printing. This gives the power to people who can inflate the money supply, devaluing the effort people have previously put in to the system.</p>\n<p><sup><a name=\"1\"></a>1</sup> A group of highly motivated people with a common vision are probably the best way. But most companies and charities probably can't hope for this. Also if an organization starts off like this it may not stay like it over time, e.g. USA.</p>\n<p><a name=\"2\"></a><sup>2 </sup>Vickery is sealed second bid. So people can't see what other people are bidding but it <a href=\"http://en.wikipedia.org/wiki/Vickrey_auction#Proof_of_dominance_of_truthful_bidding\">encourages truthful bidding</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hxhFuwBaqetT8RSKp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 1.1580242907530654e-06, "legacy": true, "legacyId": "3510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qeq7EmkNGFRq4yaFr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T05:40:08.789Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Can Humanism Match Religion's Output?", "slug": "seq-rerun-can-humanism-match-religion-s-output", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mfGx6hdWzufHAZRL8/seq-rerun-can-humanism-match-religion-s-output", "pageUrlRelative": "/posts/mfGx6hdWzufHAZRL8/seq-rerun-can-humanism-match-religion-s-output", "linkUrl": "https://www.lesswrong.com/posts/mfGx6hdWzufHAZRL8/seq-rerun-can-humanism-match-religion-s-output", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Can%20Humanism%20Match%20Religion's%20Output%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Can%20Humanism%20Match%20Religion's%20Output%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfGx6hdWzufHAZRL8%2Fseq-rerun-can-humanism-match-religion-s-output%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Can%20Humanism%20Match%20Religion's%20Output%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfGx6hdWzufHAZRL8%2Fseq-rerun-can-humanism-match-religion-s-output", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfGx6hdWzufHAZRL8%2Fseq-rerun-can-humanism-match-religion-s-output", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>Today's post, <a href=\"/lw/5t/can_humanism_match_religions_output/\">Can Humanism Match Religion's Output?</a> was originally published on 27 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Anyone with a<em> simple and obvious</em> charitable project - responding with food and shelter to a tidal wave in Thailand, say - would be better off by far pleading with the Pope to mobilize the Catholics, rather than with Richard Dawkins to mobilize the atheists. <em>For so long as this is true</em>, any increase in atheism at the expense of Catholicism will be something of a hollow victory, regardless of all other benefits. Can no rationalist match the motivation that comes from the irrational fear of Hell? Or does the real story have more to do with the motivating power of <em>physically meeting</em> others who share your cause, and group norms of participating?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h4k/seq_rerun_your_price_for_joining/\">Your Price for Joining</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mfGx6hdWzufHAZRL8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1582740507020951e-06, "legacy": true, "legacyId": "22203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3fNL2ssfvRzpApvdN", "qS6FGFmfi9kQotNWq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T06:33:39.313Z", "modifiedAt": null, "url": null, "title": "Meetup : Mountain View: Board Game Night", "slug": "meetup-mountain-view-board-game-night", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ttzg6kTWaet9gebhH/meetup-mountain-view-board-game-night", "pageUrlRelative": "/posts/ttzg6kTWaet9gebhH/meetup-mountain-view-board-game-night", "linkUrl": "https://www.lesswrong.com/posts/ttzg6kTWaet9gebhH/meetup-mountain-view-board-game-night", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mountain%20View%3A%20Board%20Game%20Night&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mountain%20View%3A%20Board%20Game%20Night%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fttzg6kTWaet9gebhH%2Fmeetup-mountain-view-board-game-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mountain%20View%3A%20Board%20Game%20Night%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fttzg6kTWaet9gebhH%2Fmeetup-mountain-view-board-game-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fttzg6kTWaet9gebhH%2Fmeetup-mountain-view-board-game-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/l9'>Mountain View: Board Game Night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 April 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's been four whole weeks since we last played board games! How has this happened!?</p>\n\n<p>The Quixey office has quite a few board games, so we won't be short, but you're more than welcome to bring anything else you might like to play. I will ensure that both <a href=\"http://www.koryheath.com/games/zendo/\" rel=\"nofollow\">Zendo</a> and <a href=\"http://www.indieboardsandcards.com/resistance.php\" rel=\"nofollow\">The Resistance</a> are available. :)</p>\n\n<hr />\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>.\nRegular meetups in Mountain View and Berkeley are announced and discussed there, and other events of interest to the local community.</p>\n\n<p>Entering the Quixeyplex: Our doors will be locked in the evening, and it's not always the case that someone's watching them for passerby. If you need to be let in, you can call me at 608.698.2959.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/l9'>Mountain View: Board Game Night</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ttzg6kTWaet9gebhH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.158310504356789e-06, "legacy": true, "legacyId": "22204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Board_Game_Night\">Discussion article for the meetup : <a href=\"/meetups/l9\">Mountain View: Board Game Night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 April 2013 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">278 Castro St, Mountain View, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's been four whole weeks since we last played board games! How has this happened!?</p>\n\n<p>The Quixey office has quite a few board games, so we won't be short, but you're more than welcome to bring anything else you might like to play. I will ensure that both <a href=\"http://www.koryheath.com/games/zendo/\" rel=\"nofollow\">Zendo</a> and <a href=\"http://www.indieboardsandcards.com/resistance.php\" rel=\"nofollow\">The Resistance</a> are available. :)</p>\n\n<hr>\n\n<p>If you're in the San Francisco Bay area and reading this, consider joining the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/bayarealesswrong\">Bay Area Less Wrong mailing list</a>.\nRegular meetups in Mountain View and Berkeley are announced and discussed there, and other events of interest to the local community.</p>\n\n<p>Entering the Quixeyplex: Our doors will be locked in the evening, and it's not always the case that someone's watching them for passerby. If you need to be let in, you can call me at 608.698.2959.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Mountain_View__Board_Game_Night1\">Discussion article for the meetup : <a href=\"/meetups/l9\">Mountain View: Board Game Night</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mountain View: Board Game Night", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Board_Game_Night", "level": 1}, {"title": "Discussion article for the meetup : Mountain View: Board Game Night", "anchor": "Discussion_article_for_the_meetup___Mountain_View__Board_Game_Night1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T12:31:10.064Z", "modifiedAt": null, "url": null, "title": "[Link] Diversity and Academic Open Mindedness ", "slug": "link-diversity-and-academic-open-mindedness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:59.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yi4bAb3PqxqhniHXn/link-diversity-and-academic-open-mindedness", "pageUrlRelative": "/posts/yi4bAb3PqxqhniHXn/link-diversity-and-academic-open-mindedness", "linkUrl": "https://www.lesswrong.com/posts/yi4bAb3PqxqhniHXn/link-diversity-and-academic-open-mindedness", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Diversity%20and%20Academic%20Open%20Mindedness%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Diversity%20and%20Academic%20Open%20Mindedness%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyi4bAb3PqxqhniHXn%2Flink-diversity-and-academic-open-mindedness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Diversity%20and%20Academic%20Open%20Mindedness%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyi4bAb3PqxqhniHXn%2Flink-diversity-and-academic-open-mindedness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyi4bAb3PqxqhniHXn%2Flink-diversity-and-academic-open-mindedness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 691, "htmlBody": "<div style=\"text-align: justify;\">\n<p><strong>Related: </strong><a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">Heuristics for Evaluating the Soundness of the Academic Mainstream</a>, <a href=\"/lw/e1b/link_admitting_to_bias/\">Admitting to Bias</a>, <a href=\"http://econlog.econlib.org/archives/2011/06/the_ideological.html\">The Ideological Turing Test</a></p>\n</div>\n<div style=\"text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/David_D._Friedman\">David Friedman</a> writes on his <a href=\"http://daviddfriedman.blogspot.com/2013/03/diversity-and-academic-open-mindedness.html\">blog</a>.<br /></div>\n<div style=\"text-align: justify;\"><br /></div>\n<blockquote>\n<div style=\"text-align: justify;\">I had an interesting recent conversation with a fellow academic that I think worth a blog post. It started with my commenting that I thought support for \"diversity\" in the sense in which the term is usually used in the academic context&mdash;having students or faculty from particular groups, in particular blacks but also, in some contexts, gays, perhaps hispanics, perhaps women&mdash;in practice anticorrelated with support for the sort of diversity, diversity of ideas, that ought to matter to a university.</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">I offered my standard example. Imagine that a university department has an opening and is down to two or three well qualified candidates. They learn that one of them is an articulate supporter of South African Apartheid. Does the chance of hiring him go up or down? If the university is actually committed to intellectual diversity, the chance should go up&mdash;it is, after all, a position that neither faculty nor students are likely to have been exposed to. In fact, in any university I am familiar with, it would go sharply down.</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">The response was that that he considered himself very open minded, getting along with people across the political spectrum, but that that position was so obviously beyond the bounds of reasonable discourse that refusing to hire the candidate was the correct response.&nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">The question I should have asked and didn't was whether he had ever been exposed to an intelligent and articulate defense of apartheid. Having spent my life in the same general environment&mdash;American academia&mdash;as he spent his, I think the odds are pretty high that he had not been. If so, he was in the position of a judge who, having heard the case for the prosecution, convicted the defendant without bothering to hear the defense. Worse still, he was not only concluding that the position was wrong&mdash;we all have limited time and energy, and so must often reach such conclusions on an inadequate basis&mdash;he was concluding it with a level of certainty so high that he was willing to rule out the possibility that the argument on the other side might be worth listening to.</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">An alternative question I might have put to him was whether he could make the argument for apartheid about as well as a competent defender of that system could. That, I think, is a pretty good test of whether one has an adequate basis to reject a position&mdash;if you don't know the arguments for it, you probably don't know whether those arguments are wrong, although there might be exceptions. I doubt that he could have. At least, in the case of political controversies where I have been a supporter of the less popular side, my experience is that those on the other side considerably overestimate their knowledge of the arguments they reject.</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">Which reminds me of something that happened to me almost fifty years ago&mdash;in 1964, when Barry Goldwater was running for President. I got into a friendly conversation with a stranger, probably set off by my wearing a Goldwater pin and his curiosity as to how someone could possibly support that position.&nbsp;</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">We ran through a series of issues.<strong> </strong>In each case,<em> </em>it was clear that he had never heard the arguments I was offering in defense of Goldwater's position and had no immediate rebuttal. <strong>At the end he asked me, in a don't-want-to-offend-you tone of voice, whether I was taking all of these positions as a joke.&nbsp;</strong></div>\n<div style=\"text-align: justify;\"><strong> </strong><br /></div>\n<div style=\"text-align: justify;\"><strong>I interpreted it, and still do, as the intellectual equivalent of \"what is a nice girl like you doing in a place like this?\"</strong> How could I be intelligent enough to make what seemed like convincing arguments for positions he knew were wrong, and yet stupid enough to believe them?</div>\n</blockquote>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\">Yup. (Q_Q)</div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><strong>Follow up post:&nbsp;</strong><a href=\"http://daviddfriedman.blogspot.com/2013/04/academic-orthodoxy-official-lies.html\">Academic Orthodoxy: Official Lies</a></div>\n<div style=\"text-align: justify;\"><br /></div>\n<div style=\"text-align: justify;\"><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yi4bAb3PqxqhniHXn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "22205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fyZBtNB3Ki3fM4a6Y", "H7iNv58YKmzjoAd68"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T12:43:03.601Z", "modifiedAt": null, "url": null, "title": "[Link] Son of low-hanging fruit", "slug": "link-son-of-low-hanging-fruit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:31.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cw6CpcBfyTyPWH3Ny/link-son-of-low-hanging-fruit", "pageUrlRelative": "/posts/Cw6CpcBfyTyPWH3Ny/link-son-of-low-hanging-fruit", "linkUrl": "https://www.lesswrong.com/posts/Cw6CpcBfyTyPWH3Ny/link-son-of-low-hanging-fruit", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Son%20of%20low-hanging%20fruit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Son%20of%20low-hanging%20fruit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCw6CpcBfyTyPWH3Ny%2Flink-son-of-low-hanging-fruit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Son%20of%20low-hanging%20fruit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCw6CpcBfyTyPWH3Ny%2Flink-son-of-low-hanging-fruit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCw6CpcBfyTyPWH3Ny%2Flink-son-of-low-hanging-fruit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p><strong>Related:</strong> <a href=\"/r/discussion/lw/cwk/link_thick_and_thin/\">Thick and Thin</a>, <a href=\"/r/discussion/lw/85b/link_loss_of_local_knowledge_affecting/\">Loss of local knowledge affecting intellectual trends </a></p>\n<p>An <a href=\"http://westhunt.wordpress.com/2012/04/28/son-of-low-hanging-fruit/\">entry</a> I found in the archives on <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p><a href=\"http://westhunt.files.wordpress.com/2012/04/400px-upperatmoslight1.jpg\"><img class=\"alignnone size-full wp-image-412\" title=\"400px-Upperatmoslight1\" src=\"http://westhunt.files.wordpress.com/2012/04/400px-upperatmoslight1.jpg?w=640\" alt=\"\" /></a></p>\n<p>In yet another example of&nbsp; long-delayed discovery, forms of high-altitude lightning were observed for at least a century before becoming officially real (as opposed to really real).</p>\n<p>Some thunderstorms manage to generate blue jets shooting out of their thunderheads, or&nbsp; glowing red rings and associated tentacles around 70 kilometers up.&nbsp;&nbsp; C T R Wilson predicted this long ago, back in the 1920s.&nbsp; He had a simple model that gets you started.</p>\n<p>You see, you can think of the thunderstorm, after a ground discharge,&nbsp; as a vertical dipole. Its electrical field drops as the cube of altitude.&nbsp; The threshold voltage for atmospheric breakdown is proportional to pressure, while pressure drops <em>exponentially</em> with altitude: and as everyone knows, a negative exponential drops faster than any power.</p>\n<p>The curves must cross.&nbsp;&nbsp; Electrical breakdown occurs.&nbsp; Weird lightning, way above the clouds.</p>\n<p>As I said, people reported sprites at least a hundred years ago, and they have probably been observed occasionally since the dawn of time. However, they&rsquo;re far easier to see if you&rsquo;re above the clouds &ndash; pilots often do.</p>\n<p><strong>Pilots also learned not to talk about it, because nobody listened. </strong>&nbsp; Military and commercial pilots have to pass periodic medical exams known as &lsquo;flight physicals&rsquo;,&nbsp; and there was a suspicion that reporting glowing red cephalopods in the sky might interfere with that.&nbsp; Generally, you had to see the things that were officially real (whether they were really real or not), and only those things.</p>\n<p><strong>Sprites became real when someone recorded one by accident on a fast camera in 1989.</strong>&nbsp; Since then it&rsquo;s turned into a real subject, full of strangeness: turns out that thunderstorms&nbsp; sometimes generate gamma-rays and even antimatter.</p>\n<p>Presumably we&rsquo;ve gotten over all that ignoring your lying eyes stuff by now.</p>\n</blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/May_you_live_in_interesting_times\">May you tell others what you see.</a> (~_^)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cw6CpcBfyTyPWH3Ny", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 34, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "22206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5qMxWi9ryt9Ym4xmW", "hf9ZP2wnPwZ5pcujv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T18:59:58.126Z", "modifiedAt": null, "url": null, "title": "Call for participation - Centre for Organisational Rationality", "slug": "call-for-participation-centre-for-organisational-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YNzoowJWKcQJ3Ef8P/call-for-participation-centre-for-organisational-rationality", "pageUrlRelative": "/posts/YNzoowJWKcQJ3Ef8P/call-for-participation-centre-for-organisational-rationality", "linkUrl": "https://www.lesswrong.com/posts/YNzoowJWKcQJ3Ef8P/call-for-participation-centre-for-organisational-rationality", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20participation%20-%20Centre%20for%20Organisational%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20participation%20-%20Centre%20for%20Organisational%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNzoowJWKcQJ3Ef8P%2Fcall-for-participation-centre-for-organisational-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20participation%20-%20Centre%20for%20Organisational%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNzoowJWKcQJ3Ef8P%2Fcall-for-participation-centre-for-organisational-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNzoowJWKcQJ3Ef8P%2Fcall-for-participation-centre-for-organisational-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<ol> </ol>\n<p>Or something like that<sup>1</sup>. As per this article on <a href=\"/r/discussion/lw/2pi/an_introduction_to_control_markets/\">Control Markets</a> I am looking to experiment with them. This requires an organization of some sort. This post is my first step to the creation of the organization.<a id=\"more\"></a></p>\n<ol> </ol>\n<p>My skills are in technology and not UI or people particularly. So I have mainly been doing what I know<a href=\"/2\"><sup>2</sup></a>, working around my day job.</p>\n<p>So I decided it probably wasn't the best path, I was likely to get distracted and lose focus by myself. So other people would be useful.</p>\n<p>There are three rough ways this can be attempted:</p>\n<ol>\n<li>Try and create a swanky high profile website to spark interest: Get a co-founder type person/team. &nbsp; Stay stealthy and make a nice web app for managing funge for a charity based on exploring control marketers using personal funds to start with, get on social media. Raise money from donations to maintain infrastructure, refine the web app and possibly fund academic research on control markets when people can try it out for themselves. This webapp can then be open sourced for other people to use. </li>\n<li>Get the theory started: Start a small organization with volunteer Actors and bootstrap using a google spread sheet to record funge transfers and bids, initially, and then use lessons learnt from that process to inform the design of a webapp. Leading to </li>\n<li>Get someone who regularly wants to talk about control markets, just to keep focus.</li>\n</ol>\n<p>So what is possible with people from lesswrong? And which would people advise? Or is there a different way to attack this problem.</p>\n<p>I've thought about kickstarter, but I don't think it is within the ToS. Also I would want a decent demo/visuals before trying this style of thing.</p>\n<p>I suppose there is also.</p>\n<ul>\n<li>Create a webapp. Sell it as the next big thing for management consultants. But this is so far outside my comfort zone, I don't know I could do it.</li>\n</ul>\n<p><a name=\"1\"></a><sup>1</sup> I would want to use a kitty for a logo if we went with this name. This is probably why I shouldn't be allowed to make these sorts of decisions.</p>\n<p><a name=\"2\"></a><sup>2 </sup>I've been noodling around with a django backend using tastypie to make a restful API, I was thinking about using AngularJS on the client (which I would need to learn). Still needs a lot of work. I also have a bad habit of wanting to make it highly available/scalable and other such things that aren't appropriate at this point.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YNzoowJWKcQJ3Ef8P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -4, "extendedScore": null, "score": 1.1588191537742795e-06, "legacy": true, "legacyId": "22208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hxhFuwBaqetT8RSKp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-04T22:55:02.889Z", "modifiedAt": null, "url": null, "title": "April 2013 Media Thread", "slug": "april-2013-media-thread", "viewCount": null, "lastCommentedAt": "2019-03-09T08:59:21.234Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4rSDRPJ32dDneCJDL/april-2013-media-thread", "pageUrlRelative": "/posts/4rSDRPJ32dDneCJDL/april-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/4rSDRPJ32dDneCJDL/april-2013-media-thread", "postedAtFormatted": "Thursday, April 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20April%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApril%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rSDRPJ32dDneCJDL%2Fapril-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=April%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rSDRPJ32dDneCJDL%2Fapril-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rSDRPJ32dDneCJDL%2Fapril-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4rSDRPJ32dDneCJDL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.1589794537661078e-06, "legacy": true, "legacyId": "22210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T04:36:56.570Z", "modifiedAt": null, "url": null, "title": "Anybody want to join a Math Club?", "slug": "anybody-want-to-join-a-math-club", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:03.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "smoofra", "createdAt": "2009-03-06T05:09:13.052Z", "isAdmin": false, "displayName": "smoofra"}, "userId": "v6xFjPq5z6RPDpyed", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uHy3HWKrqXcKkhmct/anybody-want-to-join-a-math-club", "pageUrlRelative": "/posts/uHy3HWKrqXcKkhmct/anybody-want-to-join-a-math-club", "linkUrl": "https://www.lesswrong.com/posts/uHy3HWKrqXcKkhmct/anybody-want-to-join-a-math-club", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anybody%20want%20to%20join%20a%20Math%20Club%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnybody%20want%20to%20join%20a%20Math%20Club%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHy3HWKrqXcKkhmct%2Fanybody-want-to-join-a-math-club%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anybody%20want%20to%20join%20a%20Math%20Club%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHy3HWKrqXcKkhmct%2Fanybody-want-to-join-a-math-club", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHy3HWKrqXcKkhmct%2Fanybody-want-to-join-a-math-club", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<p>I've found it's hard to teach myself math without an objective.&nbsp;&nbsp;&nbsp; If I don't have a specific question I'm trying to answer, my eyes just start to skip over equations, trying to get to the \"good part\".&nbsp;&nbsp; Pretty soon I've left the boring parts I know far behind.&nbsp; I've also skipped the less boring parts that i sorta know, and now I'm skipping forward even faster because I only understand&nbsp; half of what I'm reading.&nbsp;&nbsp; I wind up skimming the whole book, but not really absorbing much of it.&nbsp;&nbsp; I think I'd do better if I was planning on discussing what I'm reading with others.</p>\n<p>So here's my idea: a math club.&nbsp;&nbsp; We pick a book, and we read it together.&nbsp;&nbsp; Every (week | two weeks | month) we read the next chapter in the book, and then we meet up and discuss it.&nbsp;&nbsp;&nbsp; Anything we can't figure out on our own, we discuss with the other members of the math club until we get it.&nbsp;&nbsp; The impending deadline of having to actually explain the material to other humans servs to focus and motive the reading.&nbsp;&nbsp;&nbsp;</p>\n<p>Anybody interested?</p>\n<p>Possible topics:</p>\n<ul>\n<li><em><a href=\"http://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712\">Probability Theory</a>.&nbsp;&nbsp;&nbsp;</em></li>\n<li><a href=\"http://www.amazon.com/Lectures-Curry-Howard-Isomorphism-Foundations-Mathematics/dp/0444520775/ref=sr_1_fkmr1_1?s=books&amp;ie=UTF8&amp;qid=1365136418&amp;sr=1-1-fkmr1&amp;keywords=lecture+notes+on+the+curry+howard+isomorphism\"><em>Lecture Notes on the Curry Howard Isomorphism</em></a></li>\n<li><em><a href=\"http://www.amazon.com/gp/product/3540693181/ref=wms_ohs_product?ie=UTF8&amp;psc=1\">Proof Theory: The First Step into Impredicativity</a></em></li>\n<li><em><a href=\"http://www.amazon.com/Geometry-Ordinary-Variational-Equations-Mathematics/dp/3540638326/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1365136585&amp;sr=8-1-fkmr0&amp;keywords=geomety+of+ordinary+variational+equations\">The Geometry of Ordinary Variational Equations</a></em></li>\n<li>or anything else.&nbsp; math is fun!</li>\n</ul>\n<p><strong>EDIT:</strong> Benito made a facebook group so we can get organized and do this!&nbsp; see: http://lesswrong.com/r/discussion/lw/h5y/lw_study_group_facebook_page/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uHy3HWKrqXcKkhmct", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.1592126609120019e-06, "legacy": true, "legacyId": "22211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T04:43:13.572Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Church vs. Taskforce", "slug": "seq-rerun-church-vs-taskforce", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:59.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBrJRSgjvbiqsQMet/seq-rerun-church-vs-taskforce", "pageUrlRelative": "/posts/SBrJRSgjvbiqsQMet/seq-rerun-church-vs-taskforce", "linkUrl": "https://www.lesswrong.com/posts/SBrJRSgjvbiqsQMet/seq-rerun-church-vs-taskforce", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Church%20vs.%20Taskforce&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Church%20vs.%20Taskforce%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBrJRSgjvbiqsQMet%2Fseq-rerun-church-vs-taskforce%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Church%20vs.%20Taskforce%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBrJRSgjvbiqsQMet%2Fseq-rerun-church-vs-taskforce", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBrJRSgjvbiqsQMet%2Fseq-rerun-church-vs-taskforce", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>Today's post, <a href=\"/lw/5v/church_vs_taskforce/\">Church vs. Taskforce</a> was originally published on 28 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Churches serve a role of providing community - but they aren't explicitly optimized for this, because their nominal role is different. If we desire community without church, can we go one better in the course of deleting religion? There's a great deal of work to be done in the world; rationalist communities might potentially organize themselves around good causes, while explicitly optimizing for community.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h4r/seq_rerun_can_humanism_match_religions_output/\">Can Humanism Match Religion's Output?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBrJRSgjvbiqsQMet", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1592169475539924e-06, "legacy": true, "legacyId": "22212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p5DmraxDmhvMoZx8J", "mfGx6hdWzufHAZRL8", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T13:43:29.563Z", "modifiedAt": null, "url": null, "title": "Call for help: volunteers needed to proofread MIRI's publications", "slug": "call-for-help-volunteers-needed-to-proofread-miri-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:00.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WfB4REtXjJgo6K8Sz/call-for-help-volunteers-needed-to-proofread-miri-s", "pageUrlRelative": "/posts/WfB4REtXjJgo6K8Sz/call-for-help-volunteers-needed-to-proofread-miri-s", "linkUrl": "https://www.lesswrong.com/posts/WfB4REtXjJgo6K8Sz/call-for-help-volunteers-needed-to-proofread-miri-s", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20help%3A%20volunteers%20needed%20to%20proofread%20MIRI's%20publications&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20help%3A%20volunteers%20needed%20to%20proofread%20MIRI's%20publications%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfB4REtXjJgo6K8Sz%2Fcall-for-help-volunteers-needed-to-proofread-miri-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20help%3A%20volunteers%20needed%20to%20proofread%20MIRI's%20publications%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfB4REtXjJgo6K8Sz%2Fcall-for-help-volunteers-needed-to-proofread-miri-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfB4REtXjJgo6K8Sz%2Fcall-for-help-volunteers-needed-to-proofread-miri-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p><a href=\"http://intelligence.org\">MIRI</a> needs volunteers to proofread our soon-to-be-released publications, such as Eliezer's \"Intelligence Explosion Microeconomics.\" Some reasons to get involved:</p>\n<ul>\n<li>Get a sneak peek at our <a href=\"http://intelligence.org/research/\">publications</a>&nbsp;before they become publicly available.</li>\n<li>Earn points at <a href=\"http://mirivolunteers.org/\">MIRIvolunteers.org</a>, our online volunteer system that runs on <a href=\"http://www.youtopia.com/\">Youtopia</a>. (Even if you're not interested in the points, tracking your time through Youtopia helps us manage and quantify the volunteer proofreading effort.)</li>\n<li>Having polished and well-written publications is of high-value to MIRI. </li>\n<li>Help speed up our publication process. Proofreading is currently our biggest bottle-neck.</li>\n</ul>\n<p>Some of the papers that are sitting in the pipeline and ready for proofreading <em>right now</em> (or will be very soon):</p>\n<ul>\n<li>\"Avoiding Unintended AI Behaviors\" by Bill Hibbard</li>\n<li>\"Decision Support for Safe AI Design\" by Bill Hibbard</li>\n<li>\"A Comparison of Decision Algorithms on Newcomblike Problems\" by Alex Altair</li>\n<li>\"Intelligence Explosion Microeconomics\" by Eliezer Yudkowsky</li>\n</ul>\n<p>How proofreading works:</p>\n<ul>\n<li>Youtopia, with the help of some shared Google Docs, is used to manage and track the available documents and who's proofread what. </li>\n<li>Proofreading entails checking for basic grammar, spelling, punctuation, and formatting errors; pointing out areas of confusion or concern; and making general style and flow suggestions.</li>\n<li>Don't worry, you don't have to proofread entire documents, just as many individual pages as you like.</li>\n<li>(This is explained in more detail once you've joined the MIRI Proofreaders group.)</li>\n</ul>\n<p>How to join Youtopia and specifically the MIRI Proofreaders group:</p>\n<ol>\n<li>Go go <a href=\"http://mirivolunteers.org/\">MIRIvolunteers.org</a>.</li>\n<li>In the right sidebar click on Register as a Volunteer and fill out your info.</li>\n<li>Once your Youtopia account is created (this could take a day or two), head <a href=\"https://www.youtopia.com/organizations/354\">here</a> and click on \"Join Organization.&rdquo;</li>\n<li>Once your membership is approved you will have have access to detailed proofreading instructions and draft versions of our publications.</li>\n</ol>\n<p>Questions can be directed to alexv@intelligence.org.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WfB4REtXjJgo6K8Sz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.159585636301872e-06, "legacy": true, "legacyId": "22213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T15:21:16.601Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, April 5-14", "slug": "group-rationality-diary-april-5-14", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/86ZaKtcLNfFmM8qQ3/group-rationality-diary-april-5-14", "pageUrlRelative": "/posts/86ZaKtcLNfFmM8qQ3/group-rationality-diary-april-5-14", "linkUrl": "https://www.lesswrong.com/posts/86ZaKtcLNfFmM8qQ3/group-rationality-diary-april-5-14", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20April%205-14&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20April%205-14%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F86ZaKtcLNfFmM8qQ3%2Fgroup-rationality-diary-april-5-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20April%205-14%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F86ZaKtcLNfFmM8qQ3%2Fgroup-rationality-diary-april-5-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F86ZaKtcLNfFmM8qQ3%2Fgroup-rationality-diary-april-5-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for April 5-14. &nbsp;</span></p>\n<blockquote>\n<p><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to <a href=\"/user/cata/\">cata</a> for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a href=\"/r/discussion/lw/h7s/group_rationality_diary_april_1529/\">Next post: &nbsp;4/15-29</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "86ZaKtcLNfFmM8qQ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.159652388215212e-06, "legacy": true, "legacyId": "22214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rb7KN7bQKJ8WetbLF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T15:42:25.049Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berlin, Buffalo, Durham (2), Moscow, Mountain View, Vancouver", "slug": "weekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bkw8mdi9JLoriSJMD/weekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "pageUrlRelative": "/posts/bkw8mdi9JLoriSJMD/weekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "linkUrl": "https://www.lesswrong.com/posts/bkw8mdi9JLoriSJMD/weekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berlin%2C%20Buffalo%2C%20Durham%20(2)%2C%20Moscow%2C%20Mountain%20View%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berlin%2C%20Buffalo%2C%20Durham%20(2)%2C%20Moscow%2C%20Mountain%20View%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbkw8mdi9JLoriSJMD%2Fweekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berlin%2C%20Buffalo%2C%20Durham%20(2)%2C%20Moscow%2C%20Mountain%20View%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbkw8mdi9JLoriSJMD%2Fweekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbkw8mdi9JLoriSJMD%2Fweekly-lw-meetups-berlin-buffalo-durham-2-moscow-mountain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p><strong>Special Note:</strong> There's now an online study group at <a href=\"http://tinychat.com/lesswrong\">http://tinychat.com/lesswrong</a>, where LessWrongers are meeting 24 hours a day. Check it out!</p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/l2\">Berlin, practical rationality:&nbsp;<span class=\"date\">05 April 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/kz\">Vancouver Rationality Habits and Friendship:&nbsp;<span class=\"date\">06 April 2013 03:00AM</span></a></li>\n<li><a href=\"/meetups/l7\">Durham HPMoR Discussion, chapters 51-55:&nbsp;<span class=\"date\">06 April 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/l6\">Buffalo Meetup at UB:&nbsp;<span class=\"date\">07 April 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/l5\">Moscow, Applied Rationality for beginners:&nbsp;<span class=\"date\">10 April 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/l8\">Durham: Luminosity (New location!):&nbsp;<span class=\"date\">11 April 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/k9\">Vienna Meetup #2 - :&nbsp;<span class=\"date\">13 April 2013 07:06PM</span></a></li>\n<li><a href=\"/meetups/l4\">Moscow, Applied Rationality, take two:&nbsp;<span class=\"date\">14 April 2013 04:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/l9\">Mountain View: Board Game Night:&nbsp;<span class=\"date\">09 April 2013 07:30PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bkw8mdi9JLoriSJMD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.1596668207998661e-06, "legacy": true, "legacyId": "22215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T15:43:17.324Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Brussels, London, Melbourne, Moscow, Munich, Paderborn, Tucson, Washington DC", "slug": "weekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4ivs7m53n8AntGutf/weekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "pageUrlRelative": "/posts/4ivs7m53n8AntGutf/weekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "linkUrl": "https://www.lesswrong.com/posts/4ivs7m53n8AntGutf/weekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20London%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Paderborn%2C%20Tucson%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20London%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Paderborn%2C%20Tucson%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ivs7m53n8AntGutf%2Fweekly-lw-meetups-atlanta-brussels-london-melbourne-moscow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20London%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Paderborn%2C%20Tucson%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ivs7m53n8AntGutf%2Fweekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ivs7m53n8AntGutf%2Fweekly-lw-meetups-atlanta-brussels-london-melbourne-moscow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 508, "htmlBody": "<p><strong>This summary was posted to LW Main on March 29th. The following week's summary is <a href=\"/r/discussion/lw/h53/weekly_lw_meetups_berlin_buffalo_durham_2_moscow/\">here</a>.<br /></strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/kj\">Atlanta: ATLessWrong Meetup:&nbsp;<span class=\"date\">29 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/kq\">Moscow, Beliefs:&nbsp;<span class=\"date\">30 March 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/kp\">Brussels Biased Boardgaming:&nbsp;<span class=\"date\">30 March 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/kv\">Tucson LW Meetup 0:&nbsp;<span class=\"date\">30 March 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/kt\">London Meetup, 31st March:&nbsp;<span class=\"date\">31 March 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/ku\">Washington DC Social meetup+ Birthday party!:&nbsp;<span class=\"date\">31 March 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/jp\">Munich Meetup (updated):&nbsp;<span class=\"date\">01 April 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/l0\">Padeborn Meetup April 3th:&nbsp;<span class=\"date\">03 April 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/kz\">Vancouver Rationality Habits and Friendship:&nbsp;<span class=\"date\">06 April 2013 03:00AM</span></a></li>\n<li><a href=\"/meetups/k9\">Vienna Meetup #2 - :&nbsp;<span class=\"date\">13 April 2013 07:06PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/ks\">Melbourne, practical rationality:&nbsp;<span class=\"date\">05 April 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4ivs7m53n8AntGutf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1596674155655232e-06, "legacy": true, "legacyId": "22151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bkw8mdi9JLoriSJMD", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T19:49:27.066Z", "modifiedAt": null, "url": null, "title": "I need help: Device of imaginary results by I J Good", "slug": "i-need-help-device-of-imaginary-results-by-i-j-good", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:31.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZLjTt3FWGhCvZSGu/i-need-help-device-of-imaginary-results-by-i-j-good", "pageUrlRelative": "/posts/KZLjTt3FWGhCvZSGu/i-need-help-device-of-imaginary-results-by-i-j-good", "linkUrl": "https://www.lesswrong.com/posts/KZLjTt3FWGhCvZSGu/i-need-help-device-of-imaginary-results-by-i-j-good", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20need%20help%3A%20Device%20of%20imaginary%20results%20by%20I%20J%20Good&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20need%20help%3A%20Device%20of%20imaginary%20results%20by%20I%20J%20Good%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZLjTt3FWGhCvZSGu%2Fi-need-help-device-of-imaginary-results-by-i-j-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20need%20help%3A%20Device%20of%20imaginary%20results%20by%20I%20J%20Good%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZLjTt3FWGhCvZSGu%2Fi-need-help-device-of-imaginary-results-by-i-j-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZLjTt3FWGhCvZSGu%2Fi-need-help-device-of-imaginary-results-by-i-j-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 411, "htmlBody": "<p>In the <a href=\"http://omega.albany.edu:8008/ETJ-PS/cc5d.ps\">chapter 5 of the <em>Probability Theory: Logic of Science</em></a> you can read about so-called <em>device of imaginary results</em> which seems to go back to the book of I J Good named <em>Probability and the Weighing of Evidence</em>.</p>\n<p>The idea is simple and fascinating:</p>\n<p>1) You want to estimate your probability of something, and you know that this probability is very, very far from 0.5. For the sake of simplicity, let's assume that it's some hypothesis A and P(A|X) &lt;&lt; 0.5</p>\n<p>2) You imagine the situation where the A and some well-posed alternative ~A are the only possibilities.</p>\n<p>(For example, A = \"Mr Smith has extrasensory perception and can guess the number you've written down\" and ~A = \"Mr Smith can guess your number purely by luck\". Maybe Omega told you that the room where the experiment is located makes it's impossible for Smith to secretly look at your paper, and you are totally safe from every other form of deception.)</p>\n<p>3) You imagine the evidence which would convince you otherwise: P(E|A,X) ~ 1 and P(E|~A,X) is small (you should select E and ~A that way that it's possible to evaluate P(E|~A,X) )</p>\n<p>4) After a while, you feel that you are truly in doubt about A: P(A|E1,E2,..., X) ~ 0.5</p>\n<p>5) And now you can backtrack everything back to your prior P(A|X) since you know every P(E|A) and P(E|~A).</p>\n<p>&nbsp;</p>\n<p>After this explanation with the example about Mr Smith's telepathic powers, Jaynes gives reader the following exercise:</p>\n<p style=\"padding-left: 30px;\">Exercise 5.1. By applying the device of imaginary results, find your own strength of<br />belief in any three of the following propositions: (1) Julius Caesar is a real historical<br />person (i.e. not a myth invented by later writers); (2) Achilles is a real historical person;<br />(3) the Earth is more than a million years old; (4) dinosaurs did not die out; they are<br />still living in remote places; (5) owls can see in total darkness; (6) the configuration of<br />the planets influences our destiny; (7) automobile seat belts do more harm than good;<br />(8) high interest rates combat inflation; (9) high interest rates cause inflation.</p>\n<p>I have trouble tackling the first two propositions and would be glad to hear your thoughts about another seven. Anybody care to help me?</p>\n<p>(I decided not to share details of my attempt to solve this exercise unless asked. I don't think that my perspective is so valuable and anchoring would be bad.)</p>\n<p>&nbsp;</p>\n<p>UPD: <a href=\"/lw/h54/i_need_help_device_of_imaginary_results_by_i_j/8pow\">here</a> is my attempt to solve the Julius Caesar problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZLjTt3FWGhCvZSGu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.1598354914718397e-06, "legacy": true, "legacyId": "22216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-05T22:32:52.108Z", "modifiedAt": null, "url": null, "title": "CFAR is hiring a logistics manager", "slug": "cfar-is-hiring-a-logistics-manager", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:08.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBhHayHFqnPYdAmyX/cfar-is-hiring-a-logistics-manager", "pageUrlRelative": "/posts/SBhHayHFqnPYdAmyX/cfar-is-hiring-a-logistics-manager", "linkUrl": "https://www.lesswrong.com/posts/SBhHayHFqnPYdAmyX/cfar-is-hiring-a-logistics-manager", "postedAtFormatted": "Friday, April 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20is%20hiring%20a%20logistics%20manager&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20is%20hiring%20a%20logistics%20manager%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBhHayHFqnPYdAmyX%2Fcfar-is-hiring-a-logistics-manager%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20is%20hiring%20a%20logistics%20manager%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBhHayHFqnPYdAmyX%2Fcfar-is-hiring-a-logistics-manager", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBhHayHFqnPYdAmyX%2Fcfar-is-hiring-a-logistics-manager", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>CFAR is hiring an additional logistics manager. &nbsp;Please click on our form for more information, or to fill out an application:</p>\n<p><a href=\"https://docs.google.com/forms/d/1ACTvM1oYsw1zzHMumrLzffCVVak3eA5A-5uJzyIYOKM/viewform\">https://docs.google.com/forms/d/1ACTvM1oYsw1zzHMumrLzffCVVak3eA5A-5uJzyIYOKM/viewform</a></p>\n<p>We hope to choose a candidate within the next week or so, so if you're interested, do apply ASAP.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBhHayHFqnPYdAmyX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 21, "extendedScore": null, "score": 1.1599470941594765e-06, "legacy": true, "legacyId": "22217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T00:16:35.798Z", "modifiedAt": null, "url": null, "title": "How do you interpret your \"% positive\"?", "slug": "how-do-you-interpret-your-positive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:51.046Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JHH6wibXe35zeivb6/how-do-you-interpret-your-positive", "pageUrlRelative": "/posts/JHH6wibXe35zeivb6/how-do-you-interpret-your-positive", "linkUrl": "https://www.lesswrong.com/posts/JHH6wibXe35zeivb6/how-do-you-interpret-your-positive", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20interpret%20your%20%22%25%20positive%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20interpret%20your%20%22%25%20positive%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHH6wibXe35zeivb6%2Fhow-do-you-interpret-your-positive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20interpret%20your%20%22%25%20positive%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHH6wibXe35zeivb6%2Fhow-do-you-interpret-your-positive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHH6wibXe35zeivb6%2Fhow-do-you-interpret-your-positive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p>I just noticed that if I hover my mouse over the big green dot with my total karma, it says, \"81% positive\".&nbsp; Presumably 81% of the votes on my posts and/or comments have been positive.</p>\n<p>I checked out the % positive for everyone on the all-time top 15 list:</p>\n<ul>\n<li>Eliezer_Yudkowsky (223304) 94% </li>\n<li>Yvain (68331) 97% </li>\n<li>lukeprog (52586) 92% </li>\n<li>Alicorn (34512) 86% </li>\n<li>Kaj_Sotala (30919) 94% </li>\n<li>wedrifid (26242) 83% </li>\n<li>gwern (25567) 92% </li>\n<li>PhilGoetz (22008) 81% </li>\n<li>Wei_Dai (19049) 94% </li>\n<li>AnnaSalamon (18625) 97% </li>\n<li>Vladimir_Nesov (18232) 86% </li>\n<li>cousin_it (17073) 90% </li>\n<li>NancyLebovitz (14436) 92% </li>\n<li>orthonormal (12781) 94% </li>\n<li>Konkvistador (11887) 87% </li>\n</ul>\n<p>Average = 90.6%, Standard deviation = 4.93%</p>\n<p>So I'm 1.95 standard deviations below average for the top 15.&nbsp; Not only am I at the bottom of the list, we would expect me to be at the bottom of the list of the top 39 users.&nbsp; (Assuming these numbers are representative of the top 39 LessWrong users, which is dubious, and that LessWrong users are \"normal\", which sounds even more dubious, 97.44% of them have a higher upvote/downvote ratio than me.)&nbsp; I've gotten about 6744 down-votes, a bit more than Alicorn's 6711, but still second to Eliezer's 15225.</p>\n<p>How should I interpret this?&nbsp; I could say that I'm the most-controversial poster on the top 15 list, and be proud of that.&nbsp; But if I'd had the highest %positive score, I'm sure I'd be proud of that, too.&nbsp; As long as I'm extreme in some way.&nbsp; Or if I were closest to the average, I suppose I could also be proud of that.</p>\n<p>Before checking, would you guess that the top 15 have higher, or lower, % positive scores than most users?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JHH6wibXe35zeivb6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "22220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T04:24:24.498Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Rationality: Common Interest of Many Causes", "slug": "seq-rerun-rationality-common-interest-of-many-causes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BRBWv3x5FEfE9Zroa/seq-rerun-rationality-common-interest-of-many-causes", "pageUrlRelative": "/posts/BRBWv3x5FEfE9Zroa/seq-rerun-rationality-common-interest-of-many-causes", "linkUrl": "https://www.lesswrong.com/posts/BRBWv3x5FEfE9Zroa/seq-rerun-rationality-common-interest-of-many-causes", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Rationality%3A%20Common%20Interest%20of%20Many%20Causes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Rationality%3A%20Common%20Interest%20of%20Many%20Causes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRBWv3x5FEfE9Zroa%2Fseq-rerun-rationality-common-interest-of-many-causes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Rationality%3A%20Common%20Interest%20of%20Many%20Causes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRBWv3x5FEfE9Zroa%2Fseq-rerun-rationality-common-interest-of-many-causes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRBWv3x5FEfE9Zroa%2Fseq-rerun-rationality-common-interest-of-many-causes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>Today's post, <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">Rationality: Common Interest of Many Causes</a> was originally published on 29 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Many causes benefit particularly from the spread of rationality - because it takes a little more rationality than usual to see their case, as a supporter, or even just a supportive bystander. Not just the obvious causes like atheism, but things like marijuana legalization. In the case of my own work this effect was strong enough that after years of bogging down I threw up my hands and explicitly recursed on creating rationalists. If such causes can come to terms with <em>not individually capturing all the rationalists they create</em>, then they can mutually benefit from mutual effort on creating rationalists. This cooperation may require learning to shut up about disagreements between such causes, and not fight over priorities, <em>except </em>in specialized venues clearly marked.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h50/seq_rerun_church_vs_taskforce/\">Church vs. Taskforce</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BRBWv3x5FEfE9Zroa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.160187235797112e-06, "legacy": true, "legacyId": "22221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4PPE6D635iBcGPGRy", "SBrJRSgjvbiqsQMet", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T04:46:38.709Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality", "slug": "meetup-toronto-our-guest-cat-lavigne-from-the-center-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4semMZbA3evA7ufMz/meetup-toronto-our-guest-cat-lavigne-from-the-center-for", "pageUrlRelative": "/posts/4semMZbA3evA7ufMz/meetup-toronto-our-guest-cat-lavigne-from-the-center-for", "linkUrl": "https://www.lesswrong.com/posts/4semMZbA3evA7ufMz/meetup-toronto-our-guest-cat-lavigne-from-the-center-for", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%3A%20Our%20guest%3A%20Cat%20Lavigne%20from%20the%20Center%20for%20Applied%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%3A%20Our%20guest%3A%20Cat%20Lavigne%20from%20the%20Center%20for%20Applied%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4semMZbA3evA7ufMz%2Fmeetup-toronto-our-guest-cat-lavigne-from-the-center-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%3A%20Our%20guest%3A%20Cat%20Lavigne%20from%20the%20Center%20for%20Applied%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4semMZbA3evA7ufMz%2Fmeetup-toronto-our-guest-cat-lavigne-from-the-center-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4semMZbA3evA7ufMz%2Fmeetup-toronto-our-guest-cat-lavigne-from-the-center-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/la'>Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 April 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">20 Edward Street, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Sorry, new location again! We're at the World's Biggest Bookstore in the Second Floor Meeting Room (at the back of the bookstore, up the stairs. Look for the paperclip sign).</p>\n\n<p>This is our first guest event so let's all be friendly and welcoming to Cat, who's in Toronto just for the day! Cat volunteers for the Center for Applied Rationality, which you've no doubt heard a lot about already.</p>\n\n<p>I don't want to set a fixed agenda for the discussion (since we're trying out a new format here with the invited guest) but let's just say I have a hunch this meeting's going to go well. :D</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/la'>Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4semMZbA3evA7ufMz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1602024290302903e-06, "legacy": true, "legacyId": "22222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto__Our_guest__Cat_Lavigne_from_the_Center_for_Applied_Rationality\">Discussion article for the meetup : <a href=\"/meetups/la\">Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 April 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">20 Edward Street, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Sorry, new location again! We're at the World's Biggest Bookstore in the Second Floor Meeting Room (at the back of the bookstore, up the stairs. Look for the paperclip sign).</p>\n\n<p>This is our first guest event so let's all be friendly and welcoming to Cat, who's in Toronto just for the day! Cat volunteers for the Center for Applied Rationality, which you've no doubt heard a lot about already.</p>\n\n<p>I don't want to set a fixed agenda for the discussion (since we're trying out a new format here with the invited guest) but let's just say I have a hunch this meeting's going to go well. :D</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto__Our_guest__Cat_Lavigne_from_the_Center_for_Applied_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/la\">Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Toronto__Our_guest__Cat_Lavigne_from_the_Center_for_Applied_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Toronto: Our guest: Cat Lavigne from the Center for Applied Rationality", "anchor": "Discussion_article_for_the_meetup___Toronto__Our_guest__Cat_Lavigne_from_the_Center_for_Applied_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T16:29:21.197Z", "modifiedAt": null, "url": null, "title": "Intelligence Metrics and Decision Theories", "slug": "intelligence-metrics-and-decision-theories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vPtMSvnF8B5hM5LdL/intelligence-metrics-and-decision-theories", "pageUrlRelative": "/posts/vPtMSvnF8B5hM5LdL/intelligence-metrics-and-decision-theories", "linkUrl": "https://www.lesswrong.com/posts/vPtMSvnF8B5hM5LdL/intelligence-metrics-and-decision-theories", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Metrics%20and%20Decision%20Theories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Metrics%20and%20Decision%20Theories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPtMSvnF8B5hM5LdL%2Fintelligence-metrics-and-decision-theories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Metrics%20and%20Decision%20Theories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPtMSvnF8B5hM5LdL%2Fintelligence-metrics-and-decision-theories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPtMSvnF8B5hM5LdL%2Fintelligence-metrics-and-decision-theories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2093, "htmlBody": "<p>In this article I review a few previously proposed mathematical metrics of general intelligence and propose my own approach. This approach has a peculiar relation to decision theory, different decision theories corresponding to different notions of \"maximally intelligent agent\". I propose a formalization of UDT in this context.</p>\n<h1>Background<br /></h1>\n<p>The first (to my knowledge) general intelligence metric was proposed by Legg and Hutter in \"<a href=\"http://arxiv.org/abs/0712.3329\">Universal Intelligence</a>\". Their proposal was to consider an agent A interacting with environment V where A exerts a sequence of actions <strong>a<sub>i</sub></strong> on V and in turn receives from V the observations <strong>o<sub>i</sub></strong>. Their original proposal was including in the observations a special utility channel <strong>u<sub>i</sub></strong>, so that the utility of A is defined by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U%20=%20%5Csum_i%20u_i\" alt=\"\" width=\"81\" height=\"40\" /></p>\n<p>This is essentially reinforcement learning. However, general utility functions of the form <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20a_i%20%5Crbrace,%20%5Clbrace%20o_i%20%5Crbrace)\" alt=\"\" width=\"98\" height=\"19\" /> can be just as easily accommodated by the method.</p>\n<p>The Legg-Hutter intelligence metric is given by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I(A)=E%5BU(%5Clbrace%20a_i(A,%20%5Clbrace%20o_j%20%5Crbrace_%7Bj%3Ci%7D)%20%5Crbrace,%20%5Clbrace%20o_i(V,%20%5Clbrace%20a_j%20%5Crbrace_%7Bj%20%5Cle%20i%7D)%20%5Crbrace)%5D\" alt=\"\" width=\"363\" height=\"20\" /></p>\n<p>Where the expectation value is taken with respect to a <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff distribution</a> for V.</p>\n<p>The maximally intelligent agent w.r.t. this metric is the famous <a href=\"/wiki.lesswrong.com/wiki/AIXI\">AIXI</a>.</p>\n<p>This definition suffers from a number of problems:</p>\n<ul>\n<li><strong>Efficiency problem:</strong> The are no constraints on the function <img src=\"http://www.codecogs.com/png.latex?a_i(%5Clbrace%20o_j%20%5Crbrace_%7Bj%3Ci%7D)\" alt=\"\" width=\"81\" height=\"20\" /> implemented by A. The computing resources allowed for evaluating it are unlimited and it can even be uncomputable (as it is in the case of AIXI).</li>\n<li><strong>Duality (<a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">Anvil</a>) problem:</strong> A is not a part of the physical universe (V) it attempts to model. A is \"unbreakble\" i.e. nothing that happens in V can modify it as far as the evaluation of U is concerned.</li>\n<li><strong><a href=\"/lw/cze/reply_to_holden_on_tool_ai/70rt\">Ontology problem</a><sup>1</sup>:</strong> U is defined in terms of actions and observations rather than in terms of intrinsic properties of the universe. A Legg-Hutter intelligent A with a naively constructed U would be inclined to <a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wirehead</a> itself.</li>\n<li><strong>Degeneracy problem </strong>(my own observation): This problem is superficially absent in the original (reinforcement learning) formulation, but for a general U the question arises whether this truly corresponds to the intuitive notion of intelligence. It is clear that for some \"degenerate\" choices of U very simple agents would be optimal which wouldn't do any of the things we commonly ascribe to intelligence (e.g. modeling the external universe). In other words, it might make sense to ask whether a given agent is intelligent without specifying U in advance.</li>\n<li><strong>Verifiability problem</strong> (my own observation): A descriptive model of intelligence should work for real-world intelligent agents (humans). In the real world, intelligence developed by natural selection and therefore must be a verifiable property: that it, it must be possible to measure the intelligence of an agent using a limit amount of computing resource (since Nature only possesses a limited amount of computing resources, as far as we know). Now, the Solomonoff distribution involves randomly generating a program R for computing the transition probabilities of V. Since there are no limitations on the computing resources consumed by R, simulating V can be very costly. On average it will insanely costly, thx to the busy beaver function. This inhibits any feasible way of computing I(A).<br />Side note: due to <img src=\"http://www.codecogs.com/png.latex?P%20%5Cne%20NP\" alt=\"\" width=\"68\" height=\"17\" /> it might be that the \"correct\" definition of I(A) is efficiently computable but it's not feasible to compute A with human-like level of I(A) (that is it's not possible to compute it using a short program and scarce computing resources). In this case the success of natural evolution of intelligence would have to be blamed on the Anthropic Principle. It would also mean we cannot construct an AI without using the human brain as a \"template\" in some sense. Moreover there would be a physical bound on constructing intelligent agents which might rule out the \"AI foom\" scenario.</li>\n</ul>\n<p><a href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">Orseau and Ring</a><sup>2</sup> proposed solutions for the efficiency and duality problem.</p>\n<p>Solving the efficiency problem is straightforward. Consider a computing machine M (e.g. a universal Turing machine) which produces an action <strong>a</strong> every fixed cycle (for example <strong>a</strong> is read off a register in M) and able to receive an observation <strong>o</strong> every fixed cycle (for example the <strong>o</strong> is written to a register in M). We can then evaluate I(A(Q)), where A(Q) is the abstract (Legg-Hutter) agent computed by M primed with the program Q. We can then ask for Q*(n), the maximally intelligent program of length at most n. In general it seems to be uncomputable.</p>\n<p>The duality problem is more tricky. Orseau and Ring suggest considering a natural number Q which primes the stochastic process</p>\n<p><img src=\"http://www.codecogs.com/png.latex?Q_0%20=%20Q\" alt=\"\" width=\"59\" height=\"17\" /></p>\n<p><img src=\"http://www.codecogs.com/png.latex?P(Q_n%7C%5Clbrace%20Q_i%20%5Crbrace_%7Bi%3Cn%7D)=%5Crho(Q_n,%5Clbrace%20Q_i%20%5Crbrace_%7Bi%3Cn%7D)\" alt=\"\" width=\"253\" height=\"19\" /></p>\n<p>Given a utility function <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20Q_i%20%5Crbrace)\" alt=\"\" width=\"63\" height=\"19\" /> we can define the intelligence of Q to be</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I(Q)%20=%20E%5BU(%5Clbrace%20Q_i%20%5Crbrace)%5D\" alt=\"\" width=\"149\" height=\"19\" /></p>\n<p>For example we can imagine Q to be the state of a computer controlling a robot, or the state of a set of cells within a cellular automaton. This removes the unphysical separation of A and V. However, for me this framework seems <em>too</em><strong> </strong>general if we don't impose any constraints on <strong>&rho;</strong>.</p>\n<h1>Intelligence in a Quasi-Solomonoff Universe</h1>\n<p>Consider a universe Y described by a sequence of states {<strong>&upsilon;<sub>i</sub></strong>}. In order for the Solomonoff distribution to be applicable, we need the state <strong>&upsilon;<sub>i</sub></strong> to be represented as a natural number. For example Y can consist of a computing machine M interacting with an environment V through actions <strong>a</strong> and observations <strong>o</strong>. Or, Y can be a cellular automaton with all but a finite number of cells set to a stationary \"vacuum\" state.</p>\n<p>Consider further a tentative model D of the dynamics of Y. There are several types of model which might be worth considering:</p>\n<ul>\n<li>A <em>constraint model</em> only distinguishes between admissible and inadmissible state transitions, without assigning any probabilities when multiple state transitions are possible. This allows considering the minimalistic setting in which Y is a computing machine M with input register <strong>o</strong> and D-admissible transitions respect the dynamics of M while allowing the value of <strong>o</strong> to change in arbitrary way.</li>\n<li>A <em>complete model</em> prescribes the conditional probabilities <img src=\"http://www.codecogs.com/png.latex?P(%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D)\" alt=\"\" width=\"102\" height=\"20\" /> for all <strong>i</strong>. This allows elaborate models with a rich set of degrees of freedom. In particular it will serve to solve the ontology problem since U will be allowed to depend on any degrees of freedom we include the model.</li>\n<li>A <em>dynamics model</em> prescribes conditional probabilities of the form <img src=\"http://www.codecogs.com/png.latex?P(%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D)%20=%20%5Crho_D(%5Cupsilon_i,%20%5Cupsilon_%7Bi-1%7D,%20...%20,%5Cupsilon_%7Bi-k_D%7D)\" alt=\"\" width=\"266\" height=\"20\" />. Here&nbsp;<img src=\"http://www.codecogs.com/png.latex?k_D\" alt=\"\" width=\"19\" height=\"16\" /> is fixed and <strong>&rho;<sub>D</sub></strong> is a function of&nbsp;<img src=\"http://www.codecogs.com/png.latex?k_D+1\" alt=\"\" width=\"50\" height=\"16\" /> variables. In particular D doesn't define <img src=\"http://www.codecogs.com/png.latex?P%28%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D%29\" alt=\"\" width=\"102\" height=\"20\" /> for&nbsp;<img src=\"http://www.codecogs.com/png.latex?i%20%3C%20k\" alt=\"\" width=\"39\" height=\"14\" />. That is, D describes time-translation invariant dynamics with unspecified initial conditions. This also allows for elaborate models.</li>\n</ul>\n<p>The <em>quasi-Solomonoff</em> probability distribution for {<strong>&upsilon;<sub>i</sub></strong>} associated with model D and learning time <strong>t</strong> is defined by</p>\n<ul>\n<li>For a constraint model, the conditional probabilities of a Solomonoff distribution given that all transitions are D-admissible for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\" />.</li>\n<li>For a complete model we consider a stochastic process the transition probabilities of which are governed by D for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\" /> and by the Solomonoff distribution for <img src=\"http://www.codecogs.com/png.latex?i%20%3E%20t\" alt=\"\" width=\"36\" height=\"14\" />.</li>\n<li>For a dynamics model, the distribution is constructed as follows. Consider a joint distribution for two universes&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace\" alt=\"\" width=\"31\" height=\"19\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon%27_i%20%5Crbrace_%7Bi%20%5Cge%20k%7D\" alt=\"\" width=\"53\" height=\"20\" /> where <img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace\" alt=\"\" width=\"31\" height=\"19\" /> is distributed according to Solomonoff and <img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon%27_i%20%5Crbrace_%7Bi%20%5Cge%20k%7D\" alt=\"\" width=\"53\" height=\"20\" /> is distributed according to D with&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace_%7Bi%20%3C%20k%7D\" alt=\"\" width=\"53\" height=\"19\" /> serving as initial conditions. Now take the conditional distribution given that <img src=\"http://www.codecogs.com/png.latex?%5Cupsilon_i=%5Cupsilon%27_i\" alt=\"\" width=\"53\" height=\"19\" /> for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\" />.</li>\n</ul>\n<p>Here <strong>t</strong> is a parameter representing the amount of evidence in favor of D.</p>\n<p>The agent A is now defined by a property q(<strong>&upsilon;<sub>t</sub></strong>) of <strong>&upsilon;<sub>t</sub></strong>, for example some part of the state of M.<strong><sub> </sub></strong>Given a utility function <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)\" alt=\"\" width=\"58\" height=\"19\" /> we can consider</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I_%7BEDT%7D(Q)%20=%20E%5BU(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)%7Cq(%5Cupsilon_t)=Q%5D\" alt=\"\" width=\"255\" height=\"19\" /></p>\n<p>where the expectation value is taken with respect to the quasi-Solomonoff distribution. This can be interpreted as an intelligence metric.</p>\n<p>We now take the meta-level point of view where Q is regarded as a <em>decision</em> allowing to make the link to <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a>. For example we can think of an AI programmer making the decision of what code to enter into her robot, or natural evolution making a \"decision\" regarding the DNA of h. sapiens. According to this point of view, I<sub>EDT</sub>(Q) is the quality of the decision Q according to <a href=\"http://wiki.lesswrong.com/wiki/Evidential_Decision_Theory\">Evidential Decision Theory</a>. It is possible to define the <a href=\"http://wiki.lesswrong.com/wiki/Causal_Decision_Theory\">Causal Decision Theory</a> analogue I<sub>CDT</sub>(Q) by imaging a \"divine intervention\" which changes the q(<strong>&upsilon;<sub>t</sub></strong>) into Q regardless of previous history. We have</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I_%7BCDT%7D(Q)%20=%20E_%7Bq(%5Cupsilon_t)%20%5Crightarrow%20Q%7D%5BU(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)%5D\" alt=\"\" width=\"226\" height=\"20\" /></p>\n<p>where <img src=\"http://www.codecogs.com/png.latex?E_%7Bq(%5Cupsilon_t)%20%5Crightarrow%20Q%7D\" alt=\"\" width=\"65\" height=\"19\" /> is expectation value with respect to an unconditional quasi-Solomonoff distribution modified by \"divine intervention\" as above.</p>\n<p>These intelligence metrics are prone to the standard criticism of the respective decision theories. I<sub>EDT</sub>(Q) factors in the indirect effect Q has on U by giving more weight to universes in which Q is likely. I<sub>CDT</sub>(Q) favors agents that fail on certain <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb-like</a> problems. Specifically, suppose A discovers that the universe contains another agent &Omega; which created two boxes B<sub>1</sub> and B<sub>2</sub> and placed utility in each before <strong>t</strong>. Then a CDT-intelligent A would prefer taking B<sub>1</sub> + B<sub>2</sub> over taking B<sub>1</sub> only even if it knows &Omega; predicted A's decision and adjusted the utility in the boxes in such a way that taking B<sub>1</sub> only would be preferable.</p>\n<p>I suggest solving the problem by applying a certain formalization of UDT.</p>\n<p>The Solomonoff distribution works by randomly producing a program R which computes<sup>3</sup> the transition probabilities of the universe. When we apply the Solomonoff distribution to Y, we can imagine the event space as consisting of pairs (R, {<strong>&eta;<sub>i,&upsilon;</sub></strong>}). Here&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\eta_%7Bi,%5Cupsilon%7D%20%5Cin%20%5B0,1%5D\" alt=\"\" width=\"81\" height=\"20\" /> are uniformly distributed numbers used to determine the occurrence of random events. That is, (R, {<strong>&eta;</strong><strong><sub>i,</sub></strong><sub><strong>&upsilon;</strong></sub>}) determines {<strong>&upsilon;<sub>i</sub></strong>} by the recursive rule</p>\n<p><img src=\"http://www.codecogs.com/png.latex?%5Cupsilon_i(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)=%5Cmin%20%5Clbrace%20n%7CP_R(%5Cupsilon_i=n%7C%5Cupsilon_i%20%5Cge%20n;%5Clbrace%20%5Cupsilon_j(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace_%7Bj%3Ci%7D)%3E%5Ceta_%7Bi,n%7D%20%5Crbrace\" alt=\"\" width=\"519\" height=\"20\" /></p>\n<p>Here P<sub>R</sub> stands for the conditional probability computed by R and</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P_R(%5Cupsilon_i=n%7C%5Cupsilon_i%20%5Cge%20n;...)=P_R(%5Cupsilon_i=n%7C...)-%5Csum%5Climits_%7Bm=0%7D%5E%7Bn-1%7D%20P_R(%5Cupsilon_i=m%7C...)\" alt=\"\" width=\"455\" height=\"53\" /></p>\n<p>We define U<sub>T</sub> as the program receiving (R, {<strong>&eta;</strong><strong><sub>i,&upsilon;</sub></strong>}) as input (it's infinite but it doesn't pose a problem) and producing a binary fraction as output, s.t.</p>\n<ul>\n<li>U<sub>T</sub> halts in time T at most on any input</li>\n<li><img src=\"http://www.codecogs.com/png.latex?E%5B(U_T(R,%5Clbrace%20\\eta_%7Bk,%5Cupsilon%7D%20%5Crbrace)-U(%5Clbrace%20%5Cupsilon_i(R,%5Clbrace%20\\eta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace))%5E2%5D\" alt=\"\" width=\"311\" height=\"21\" /> is minimal among all such programs. Here the expectation value is taken w.r.t. the quasi-Solomonoff distirbution.</li>\n</ul>\n<p>Further, we define U*<sub>T</sub> as the program receiving (R, {<strong>&eta;</strong><strong><sub>i,&upsilon;</sub></strong>}, Q) as input and producing a binary fraction as output, s.t.</p>\n<ul>\n<li>U*<sub>T</sub> halts in time T at most on any input</li>\n<li><img src=\"http://www.codecogs.com/png.latex?E%5B(U%5E*_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace,q(%5Cupsilon_t(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace))-U(%5Clbrace%20%5Cupsilon_i(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace))%5E2%5D\" alt=\"\" width=\"433\" height=\"21\" /> is minimal among all such programs.</li>\n</ul>\n<p>Thus U<sub>T</sub> and U*<sub>T</sub> are least-squares optimal predictors for U under the constraint of running time T where U<sup>*</sup><sub>T</sub> is provided the additional information q(<strong>&upsilon;<sub>t</sub></strong>).</p>\n<p>Consider</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u(Q,T)=E%5BU%5E*_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace,Q)-U_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%7Cq(%5Cupsilon_t(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)=Q%5D\" alt=\"\" width=\"513\" height=\"20\" /></p>\n<p>u(Q,T) is the gain in estimated utility obtained by adding the information q(<strong>&upsilon;<sub>t</sub></strong>)=Q, provided that the underlying physics (R, {<strong>&eta;<sub>i,&upsilon;</sub></strong>}) of Y (which allows predicting q(<strong>&upsilon;<sub>t</sub></strong>)=Q) is already known. For T &gt;&gt; 0, u(Q,T) approaches 0 (since the additional piece of information that q(<strong>&upsilon;<sub>t</sub></strong>)=Q can be easily deduced within given computing resources). For T approaching 0, u(Q,T) approaches 0 as well, since there's not enough time to process the actual input anyway. Hence it is interesting to consider the quantity</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u%5E*(Q)=%5Cmax_T%7Bu(Q,T)%7D\" alt=\"\" width=\"163\" height=\"26\" /></p>\n<p>It is tempting to interpret u* as an intelligence metric. However it doesn't seem sensible to compare u*(Q) for different values of Q. Denote T* the value of T for which the maximum above is achieved (<img src=\"http://www.codecogs.com/png.latex?u%5E*(Q)=u(Q,T%5E*)\" alt=\"\" width=\"134\" height=\"19\" />). Instead of defining an intelligence metric per se, we can define Q to be a maximally UDT-intelligent agent if we have</p>\n<p><img src=\"http://www.codecogs.com/png.latex?%5Cforall%20Q%27:u(Q%27,T%5E*)%20%5Cle%20u(Q,T%5E*)\" alt=\"\" width=\"203\" height=\"19\" /></p>\n<p>This represents the (non-vacuous) self-referential principle that Q is the best agent given the information that Y is the sort of universe that gives rise to Q. The role of T* is to single out the extent of logical uncertainty for which Q cannot be predicted but given Q the consequences for U can be predicted.</p>\n<h1>Directions for Further Research<br /></h1>\n<ul>\n<li>It is important to obtain existence results for maximally UDT-intelligent agents for the concept to make sense.</li>\n<li>We need a way to consider non-maximally UDT-intelligent agents. For example we can consider the parameter <img src=\"http://www.codecogs.com/png.latex?%5Czeta(Q)=%5Cfrac%7Bu(Q,T%5E*)-%5Cmin_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D%7D%7B%5Cmax_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D-%5Cmin_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D%7D\" alt=\"\" width=\"327\" height=\"45\" /> as an intelligence metric but I'm not sure this is the correct choice.</li>\n<li>My proposal doesn't solve the degeneracy and verifiability problems. The latter problem can be tackled by replacing the Solomonoff distribution by something allowing efficient induction. For example we can use the distribution with the minimal Fisher distance to the Solomonoff distribution under some computing resources constraints. However I'm not convinced this would be conceptually correct.</li>\n<li>It is possible to consider multi-agent systems. In this settings, the problem transforms from a sort-of optimization problem to a game-theory problem. It should then be possible to study Nash equilibria etc.&nbsp;</li>\n<li>It is tempting to look for a way to close the loop between the basic point of view \"Q is a program\" and the meta point of view \"Q is a decision\". This might provide an interesting attack angle on self-improving AI. Of course the AI in this framework is already self-improving if M capable of reprogramming itself.</li>\n</ul>\n<ul>\n</ul>\n<h1>Footnotes<br /></h1>\n<p><sup>1</sup>The name \"ontology problem\" is courtesy <a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/8e52\">pengvado</a></p>\n<p><sup>2</sup>I was exposed to this work by reading <a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/\">Anja</a></p>\n<p><sup>3</sup>It computes them in the weak sense of producing a convergent sequence of lower bounds</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vPtMSvnF8B5hM5LdL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 1.160682733448903e-06, "legacy": true, "legacyId": "22209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In this article I review a few previously proposed mathematical metrics of general intelligence and propose my own approach. This approach has a peculiar relation to decision theory, different decision theories corresponding to different notions of \"maximally intelligent agent\". I propose a formalization of UDT in this context.</p>\n<h1 id=\"Background\">Background<br></h1>\n<p>The first (to my knowledge) general intelligence metric was proposed by Legg and Hutter in \"<a href=\"http://arxiv.org/abs/0712.3329\">Universal Intelligence</a>\". Their proposal was to consider an agent A interacting with environment V where A exerts a sequence of actions <strong>a<sub>i</sub></strong> on V and in turn receives from V the observations <strong>o<sub>i</sub></strong>. Their original proposal was including in the observations a special utility channel <strong>u<sub>i</sub></strong>, so that the utility of A is defined by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U%20=%20%5Csum_i%20u_i\" alt=\"\" width=\"81\" height=\"40\"></p>\n<p>This is essentially reinforcement learning. However, general utility functions of the form <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20a_i%20%5Crbrace,%20%5Clbrace%20o_i%20%5Crbrace)\" alt=\"\" width=\"98\" height=\"19\"> can be just as easily accommodated by the method.</p>\n<p>The Legg-Hutter intelligence metric is given by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I(A)=E%5BU(%5Clbrace%20a_i(A,%20%5Clbrace%20o_j%20%5Crbrace_%7Bj%3Ci%7D)%20%5Crbrace,%20%5Clbrace%20o_i(V,%20%5Clbrace%20a_j%20%5Crbrace_%7Bj%20%5Cle%20i%7D)%20%5Crbrace)%5D\" alt=\"\" width=\"363\" height=\"20\"></p>\n<p>Where the expectation value is taken with respect to a <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff distribution</a> for V.</p>\n<p>The maximally intelligent agent w.r.t. this metric is the famous <a href=\"/wiki.lesswrong.com/wiki/AIXI\">AIXI</a>.</p>\n<p>This definition suffers from a number of problems:</p>\n<ul>\n<li><strong>Efficiency problem:</strong> The are no constraints on the function <img src=\"http://www.codecogs.com/png.latex?a_i(%5Clbrace%20o_j%20%5Crbrace_%7Bj%3Ci%7D)\" alt=\"\" width=\"81\" height=\"20\"> implemented by A. The computing resources allowed for evaluating it are unlimited and it can even be uncomputable (as it is in the case of AIXI).</li>\n<li><strong>Duality (<a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">Anvil</a>) problem:</strong> A is not a part of the physical universe (V) it attempts to model. A is \"unbreakble\" i.e. nothing that happens in V can modify it as far as the evaluation of U is concerned.</li>\n<li><strong><a href=\"/lw/cze/reply_to_holden_on_tool_ai/70rt\">Ontology problem</a><sup>1</sup>:</strong> U is defined in terms of actions and observations rather than in terms of intrinsic properties of the universe. A Legg-Hutter intelligent A with a naively constructed U would be inclined to <a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wirehead</a> itself.</li>\n<li><strong>Degeneracy problem </strong>(my own observation): This problem is superficially absent in the original (reinforcement learning) formulation, but for a general U the question arises whether this truly corresponds to the intuitive notion of intelligence. It is clear that for some \"degenerate\" choices of U very simple agents would be optimal which wouldn't do any of the things we commonly ascribe to intelligence (e.g. modeling the external universe). In other words, it might make sense to ask whether a given agent is intelligent without specifying U in advance.</li>\n<li><strong>Verifiability problem</strong> (my own observation): A descriptive model of intelligence should work for real-world intelligent agents (humans). In the real world, intelligence developed by natural selection and therefore must be a verifiable property: that it, it must be possible to measure the intelligence of an agent using a limit amount of computing resource (since Nature only possesses a limited amount of computing resources, as far as we know). Now, the Solomonoff distribution involves randomly generating a program R for computing the transition probabilities of V. Since there are no limitations on the computing resources consumed by R, simulating V can be very costly. On average it will insanely costly, thx to the busy beaver function. This inhibits any feasible way of computing I(A).<br>Side note: due to <img src=\"http://www.codecogs.com/png.latex?P%20%5Cne%20NP\" alt=\"\" width=\"68\" height=\"17\"> it might be that the \"correct\" definition of I(A) is efficiently computable but it's not feasible to compute A with human-like level of I(A) (that is it's not possible to compute it using a short program and scarce computing resources). In this case the success of natural evolution of intelligence would have to be blamed on the Anthropic Principle. It would also mean we cannot construct an AI without using the human brain as a \"template\" in some sense. Moreover there would be a physical bound on constructing intelligent agents which might rule out the \"AI foom\" scenario.</li>\n</ul>\n<p><a href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">Orseau and Ring</a><sup>2</sup> proposed solutions for the efficiency and duality problem.</p>\n<p>Solving the efficiency problem is straightforward. Consider a computing machine M (e.g. a universal Turing machine) which produces an action <strong>a</strong> every fixed cycle (for example <strong>a</strong> is read off a register in M) and able to receive an observation <strong>o</strong> every fixed cycle (for example the <strong>o</strong> is written to a register in M). We can then evaluate I(A(Q)), where A(Q) is the abstract (Legg-Hutter) agent computed by M primed with the program Q. We can then ask for Q*(n), the maximally intelligent program of length at most n. In general it seems to be uncomputable.</p>\n<p>The duality problem is more tricky. Orseau and Ring suggest considering a natural number Q which primes the stochastic process</p>\n<p><img src=\"http://www.codecogs.com/png.latex?Q_0%20=%20Q\" alt=\"\" width=\"59\" height=\"17\"></p>\n<p><img src=\"http://www.codecogs.com/png.latex?P(Q_n%7C%5Clbrace%20Q_i%20%5Crbrace_%7Bi%3Cn%7D)=%5Crho(Q_n,%5Clbrace%20Q_i%20%5Crbrace_%7Bi%3Cn%7D)\" alt=\"\" width=\"253\" height=\"19\"></p>\n<p>Given a utility function <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20Q_i%20%5Crbrace)\" alt=\"\" width=\"63\" height=\"19\"> we can define the intelligence of Q to be</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I(Q)%20=%20E%5BU(%5Clbrace%20Q_i%20%5Crbrace)%5D\" alt=\"\" width=\"149\" height=\"19\"></p>\n<p>For example we can imagine Q to be the state of a computer controlling a robot, or the state of a set of cells within a cellular automaton. This removes the unphysical separation of A and V. However, for me this framework seems <em>too</em><strong> </strong>general if we don't impose any constraints on <strong>\u03c1</strong>.</p>\n<h1 id=\"Intelligence_in_a_Quasi_Solomonoff_Universe\">Intelligence in a Quasi-Solomonoff Universe</h1>\n<p>Consider a universe Y described by a sequence of states {<strong>\u03c5<sub>i</sub></strong>}. In order for the Solomonoff distribution to be applicable, we need the state <strong>\u03c5<sub>i</sub></strong> to be represented as a natural number. For example Y can consist of a computing machine M interacting with an environment V through actions <strong>a</strong> and observations <strong>o</strong>. Or, Y can be a cellular automaton with all but a finite number of cells set to a stationary \"vacuum\" state.</p>\n<p>Consider further a tentative model D of the dynamics of Y. There are several types of model which might be worth considering:</p>\n<ul>\n<li>A <em>constraint model</em> only distinguishes between admissible and inadmissible state transitions, without assigning any probabilities when multiple state transitions are possible. This allows considering the minimalistic setting in which Y is a computing machine M with input register <strong>o</strong> and D-admissible transitions respect the dynamics of M while allowing the value of <strong>o</strong> to change in arbitrary way.</li>\n<li>A <em>complete model</em> prescribes the conditional probabilities <img src=\"http://www.codecogs.com/png.latex?P(%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D)\" alt=\"\" width=\"102\" height=\"20\"> for all <strong>i</strong>. This allows elaborate models with a rich set of degrees of freedom. In particular it will serve to solve the ontology problem since U will be allowed to depend on any degrees of freedom we include the model.</li>\n<li>A <em>dynamics model</em> prescribes conditional probabilities of the form <img src=\"http://www.codecogs.com/png.latex?P(%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D)%20=%20%5Crho_D(%5Cupsilon_i,%20%5Cupsilon_%7Bi-1%7D,%20...%20,%5Cupsilon_%7Bi-k_D%7D)\" alt=\"\" width=\"266\" height=\"20\">. Here&nbsp;<img src=\"http://www.codecogs.com/png.latex?k_D\" alt=\"\" width=\"19\" height=\"16\"> is fixed and <strong>\u03c1<sub>D</sub></strong> is a function of&nbsp;<img src=\"http://www.codecogs.com/png.latex?k_D+1\" alt=\"\" width=\"50\" height=\"16\"> variables. In particular D doesn't define <img src=\"http://www.codecogs.com/png.latex?P%28%5Cupsilon_i%7C%5Clbrace%20%5Cupsilon_j%20%5Crbrace_%7Bj%20%3C%20i%7D%29\" alt=\"\" width=\"102\" height=\"20\"> for&nbsp;<img src=\"http://www.codecogs.com/png.latex?i%20%3C%20k\" alt=\"\" width=\"39\" height=\"14\">. That is, D describes time-translation invariant dynamics with unspecified initial conditions. This also allows for elaborate models.</li>\n</ul>\n<p>The <em>quasi-Solomonoff</em> probability distribution for {<strong>\u03c5<sub>i</sub></strong>} associated with model D and learning time <strong>t</strong> is defined by</p>\n<ul>\n<li>For a constraint model, the conditional probabilities of a Solomonoff distribution given that all transitions are D-admissible for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\">.</li>\n<li>For a complete model we consider a stochastic process the transition probabilities of which are governed by D for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\"> and by the Solomonoff distribution for <img src=\"http://www.codecogs.com/png.latex?i%20%3E%20t\" alt=\"\" width=\"36\" height=\"14\">.</li>\n<li>For a dynamics model, the distribution is constructed as follows. Consider a joint distribution for two universes&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace\" alt=\"\" width=\"31\" height=\"19\"> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon%27_i%20%5Crbrace_%7Bi%20%5Cge%20k%7D\" alt=\"\" width=\"53\" height=\"20\"> where <img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace\" alt=\"\" width=\"31\" height=\"19\"> is distributed according to Solomonoff and <img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon%27_i%20%5Crbrace_%7Bi%20%5Cge%20k%7D\" alt=\"\" width=\"53\" height=\"20\"> is distributed according to D with&nbsp;<img src=\"http://www.codecogs.com/png.latex?%5Clbrace%20%5Cupsilon_i%20%5Crbrace_%7Bi%20%3C%20k%7D\" alt=\"\" width=\"53\" height=\"19\"> serving as initial conditions. Now take the conditional distribution given that <img src=\"http://www.codecogs.com/png.latex?%5Cupsilon_i=%5Cupsilon%27_i\" alt=\"\" width=\"53\" height=\"19\"> for <img src=\"http://www.codecogs.com/png.latex?i%20%5Cle%20t\" alt=\"\" width=\"37\" height=\"17\">.</li>\n</ul>\n<p>Here <strong>t</strong> is a parameter representing the amount of evidence in favor of D.</p>\n<p>The agent A is now defined by a property q(<strong>\u03c5<sub>t</sub></strong>) of <strong>\u03c5<sub>t</sub></strong>, for example some part of the state of M.<strong><sub> </sub></strong>Given a utility function <img src=\"http://www.codecogs.com/png.latex?U(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)\" alt=\"\" width=\"58\" height=\"19\"> we can consider</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I_%7BEDT%7D(Q)%20=%20E%5BU(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)%7Cq(%5Cupsilon_t)=Q%5D\" alt=\"\" width=\"255\" height=\"19\"></p>\n<p>where the expectation value is taken with respect to the quasi-Solomonoff distribution. This can be interpreted as an intelligence metric.</p>\n<p>We now take the meta-level point of view where Q is regarded as a <em>decision</em> allowing to make the link to <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a>. For example we can think of an AI programmer making the decision of what code to enter into her robot, or natural evolution making a \"decision\" regarding the DNA of h. sapiens. According to this point of view, I<sub>EDT</sub>(Q) is the quality of the decision Q according to <a href=\"http://wiki.lesswrong.com/wiki/Evidential_Decision_Theory\">Evidential Decision Theory</a>. It is possible to define the <a href=\"http://wiki.lesswrong.com/wiki/Causal_Decision_Theory\">Causal Decision Theory</a> analogue I<sub>CDT</sub>(Q) by imaging a \"divine intervention\" which changes the q(<strong>\u03c5<sub>t</sub></strong>) into Q regardless of previous history. We have</p>\n<p><img src=\"http://www.codecogs.com/png.latex?I_%7BCDT%7D(Q)%20=%20E_%7Bq(%5Cupsilon_t)%20%5Crightarrow%20Q%7D%5BU(%5Clbrace%20%5Cupsilon_i%20%5Crbrace)%5D\" alt=\"\" width=\"226\" height=\"20\"></p>\n<p>where <img src=\"http://www.codecogs.com/png.latex?E_%7Bq(%5Cupsilon_t)%20%5Crightarrow%20Q%7D\" alt=\"\" width=\"65\" height=\"19\"> is expectation value with respect to an unconditional quasi-Solomonoff distribution modified by \"divine intervention\" as above.</p>\n<p>These intelligence metrics are prone to the standard criticism of the respective decision theories. I<sub>EDT</sub>(Q) factors in the indirect effect Q has on U by giving more weight to universes in which Q is likely. I<sub>CDT</sub>(Q) favors agents that fail on certain <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb-like</a> problems. Specifically, suppose A discovers that the universe contains another agent \u03a9 which created two boxes B<sub>1</sub> and B<sub>2</sub> and placed utility in each before <strong>t</strong>. Then a CDT-intelligent A would prefer taking B<sub>1</sub> + B<sub>2</sub> over taking B<sub>1</sub> only even if it knows \u03a9 predicted A's decision and adjusted the utility in the boxes in such a way that taking B<sub>1</sub> only would be preferable.</p>\n<p>I suggest solving the problem by applying a certain formalization of UDT.</p>\n<p>The Solomonoff distribution works by randomly producing a program R which computes<sup>3</sup> the transition probabilities of the universe. When we apply the Solomonoff distribution to Y, we can imagine the event space as consisting of pairs (R, {<strong>\u03b7<sub>i,\u03c5</sub></strong>}). Here&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\eta_%7Bi,%5Cupsilon%7D%20%5Cin%20%5B0,1%5D\" alt=\"\" width=\"81\" height=\"20\"> are uniformly distributed numbers used to determine the occurrence of random events. That is, (R, {<strong>\u03b7</strong><strong><sub>i,</sub></strong><sub><strong>\u03c5</strong></sub>}) determines {<strong>\u03c5<sub>i</sub></strong>} by the recursive rule</p>\n<p><img src=\"http://www.codecogs.com/png.latex?%5Cupsilon_i(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)=%5Cmin%20%5Clbrace%20n%7CP_R(%5Cupsilon_i=n%7C%5Cupsilon_i%20%5Cge%20n;%5Clbrace%20%5Cupsilon_j(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace_%7Bj%3Ci%7D)%3E%5Ceta_%7Bi,n%7D%20%5Crbrace\" alt=\"\" width=\"519\" height=\"20\"></p>\n<p>Here P<sub>R</sub> stands for the conditional probability computed by R and</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P_R(%5Cupsilon_i=n%7C%5Cupsilon_i%20%5Cge%20n;...)=P_R(%5Cupsilon_i=n%7C...)-%5Csum%5Climits_%7Bm=0%7D%5E%7Bn-1%7D%20P_R(%5Cupsilon_i=m%7C...)\" alt=\"\" width=\"455\" height=\"53\"></p>\n<p>We define U<sub>T</sub> as the program receiving (R, {<strong>\u03b7</strong><strong><sub>i,\u03c5</sub></strong>}) as input (it's infinite but it doesn't pose a problem) and producing a binary fraction as output, s.t.</p>\n<ul>\n<li>U<sub>T</sub> halts in time T at most on any input</li>\n<li><img src=\"http://www.codecogs.com/png.latex?E%5B(U_T(R,%5Clbrace%20\\eta_%7Bk,%5Cupsilon%7D%20%5Crbrace)-U(%5Clbrace%20%5Cupsilon_i(R,%5Clbrace%20\\eta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace))%5E2%5D\" alt=\"\" width=\"311\" height=\"21\"> is minimal among all such programs. Here the expectation value is taken w.r.t. the quasi-Solomonoff distirbution.</li>\n</ul>\n<p>Further, we define U*<sub>T</sub> as the program receiving (R, {<strong>\u03b7</strong><strong><sub>i,\u03c5</sub></strong>}, Q) as input and producing a binary fraction as output, s.t.</p>\n<ul>\n<li>U*<sub>T</sub> halts in time T at most on any input</li>\n<li><img src=\"http://www.codecogs.com/png.latex?E%5B(U%5E*_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace,q(%5Cupsilon_t(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace))-U(%5Clbrace%20%5Cupsilon_i(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%20%5Crbrace))%5E2%5D\" alt=\"\" width=\"433\" height=\"21\"> is minimal among all such programs.</li>\n</ul>\n<p>Thus U<sub>T</sub> and U*<sub>T</sub> are least-squares optimal predictors for U under the constraint of running time T where U<sup>*</sup><sub>T</sub> is provided the additional information q(<strong>\u03c5<sub>t</sub></strong>).</p>\n<p>Consider</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u(Q,T)=E%5BU%5E*_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace,Q)-U_T(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)%7Cq(%5Cupsilon_t(R,%5Clbrace%20%5Ceta_%7Bk,%5Cupsilon%7D%20%5Crbrace)=Q%5D\" alt=\"\" width=\"513\" height=\"20\"></p>\n<p>u(Q,T) is the gain in estimated utility obtained by adding the information q(<strong>\u03c5<sub>t</sub></strong>)=Q, provided that the underlying physics (R, {<strong>\u03b7<sub>i,\u03c5</sub></strong>}) of Y (which allows predicting q(<strong>\u03c5<sub>t</sub></strong>)=Q) is already known. For T &gt;&gt; 0, u(Q,T) approaches 0 (since the additional piece of information that q(<strong>\u03c5<sub>t</sub></strong>)=Q can be easily deduced within given computing resources). For T approaching 0, u(Q,T) approaches 0 as well, since there's not enough time to process the actual input anyway. Hence it is interesting to consider the quantity</p>\n<p><img src=\"http://www.codecogs.com/png.latex?u%5E*(Q)=%5Cmax_T%7Bu(Q,T)%7D\" alt=\"\" width=\"163\" height=\"26\"></p>\n<p>It is tempting to interpret u* as an intelligence metric. However it doesn't seem sensible to compare u*(Q) for different values of Q. Denote T* the value of T for which the maximum above is achieved (<img src=\"http://www.codecogs.com/png.latex?u%5E*(Q)=u(Q,T%5E*)\" alt=\"\" width=\"134\" height=\"19\">). Instead of defining an intelligence metric per se, we can define Q to be a maximally UDT-intelligent agent if we have</p>\n<p><img src=\"http://www.codecogs.com/png.latex?%5Cforall%20Q%27:u(Q%27,T%5E*)%20%5Cle%20u(Q,T%5E*)\" alt=\"\" width=\"203\" height=\"19\"></p>\n<p>This represents the (non-vacuous) self-referential principle that Q is the best agent given the information that Y is the sort of universe that gives rise to Q. The role of T* is to single out the extent of logical uncertainty for which Q cannot be predicted but given Q the consequences for U can be predicted.</p>\n<h1 id=\"Directions_for_Further_Research\">Directions for Further Research<br></h1>\n<ul>\n<li>It is important to obtain existence results for maximally UDT-intelligent agents for the concept to make sense.</li>\n<li>We need a way to consider non-maximally UDT-intelligent agents. For example we can consider the parameter <img src=\"http://www.codecogs.com/png.latex?%5Czeta(Q)=%5Cfrac%7Bu(Q,T%5E*)-%5Cmin_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D%7D%7B%5Cmax_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D-%5Cmin_%7BQ%27%7D%7Bu(Q%27,T%5E*)%7D%7D\" alt=\"\" width=\"327\" height=\"45\"> as an intelligence metric but I'm not sure this is the correct choice.</li>\n<li>My proposal doesn't solve the degeneracy and verifiability problems. The latter problem can be tackled by replacing the Solomonoff distribution by something allowing efficient induction. For example we can use the distribution with the minimal Fisher distance to the Solomonoff distribution under some computing resources constraints. However I'm not convinced this would be conceptually correct.</li>\n<li>It is possible to consider multi-agent systems. In this settings, the problem transforms from a sort-of optimization problem to a game-theory problem. It should then be possible to study Nash equilibria etc.&nbsp;</li>\n<li>It is tempting to look for a way to close the loop between the basic point of view \"Q is a program\" and the meta point of view \"Q is a decision\". This might provide an interesting attack angle on self-improving AI. Of course the AI in this framework is already self-improving if M capable of reprogramming itself.</li>\n</ul>\n<ul>\n</ul>\n<h1 id=\"Footnotes\">Footnotes<br></h1>\n<p><sup>1</sup>The name \"ontology problem\" is courtesy <a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/8e52\">pengvado</a></p>\n<p><sup>2</sup>I was exposed to this work by reading <a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/\">Anja</a></p>\n<p><sup>3</sup>It computes them in the weak sense of producing a convergent sequence of lower bounds</p>\n<ul>\n</ul>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Intelligence in a Quasi-Solomonoff Universe", "anchor": "Intelligence_in_a_Quasi_Solomonoff_Universe", "level": 1}, {"title": "Directions for Further Research", "anchor": "Directions_for_Further_Research", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RxQE4m9QgNwuq764M"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T19:24:44.217Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong/HPMoR Harvard", "slug": "meetup-lesswrong-hpmor-harvard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K5pudEHvnxSXwSDEk/meetup-lesswrong-hpmor-harvard", "pageUrlRelative": "/posts/K5pudEHvnxSXwSDEk/meetup-lesswrong-hpmor-harvard", "linkUrl": "https://www.lesswrong.com/posts/K5pudEHvnxSXwSDEk/meetup-lesswrong-hpmor-harvard", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%2FHPMoR%20Harvard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%2FHPMoR%20Harvard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5pudEHvnxSXwSDEk%2Fmeetup-lesswrong-hpmor-harvard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%2FHPMoR%20Harvard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5pudEHvnxSXwSDEk%2Fmeetup-lesswrong-hpmor-harvard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5pudEHvnxSXwSDEk%2Fmeetup-lesswrong-hpmor-harvard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/lb\">LessWrong/HPMoR Harvard</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 April 2013 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Sever 105</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Are you a Harvard student who's reading this?</p>\n<p>Relocated to Sever 105, since Leverett classroom D is filled.</p>\n<p>Cheers, Aaron</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/lb\">LessWrong/HPMoR Harvard</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K5pudEHvnxSXwSDEk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "22224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_HPMoR_Harvard\">Discussion article for the meetup : <a href=\"/meetups/lb\">LessWrong/HPMoR Harvard</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 April 2013 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Sever 105</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Are you a Harvard student who's reading this?</p>\n<p>Relocated to Sever 105, since Leverett classroom D is filled.</p>\n<p>Cheers, Aaron</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___LessWrong_HPMoR_Harvard1\">Discussion article for the meetup : <a href=\"/meetups/lb\">LessWrong/HPMoR Harvard</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong/HPMoR Harvard", "anchor": "Discussion_article_for_the_meetup___LessWrong_HPMoR_Harvard", "level": 1}, {"title": "Discussion article for the meetup : LessWrong/HPMoR Harvard", "anchor": "Discussion_article_for_the_meetup___LessWrong_HPMoR_Harvard1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T19:35:42.067Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 14th April: Hedonic hacks", "slug": "meetup-london-meetup-14th-april-hedonic-hacks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cStxQD8aYLrx2jpzF/meetup-london-meetup-14th-april-hedonic-hacks", "pageUrlRelative": "/posts/cStxQD8aYLrx2jpzF/meetup-london-meetup-14th-april-hedonic-hacks", "linkUrl": "https://www.lesswrong.com/posts/cStxQD8aYLrx2jpzF/meetup-london-meetup-14th-april-hedonic-hacks", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2014th%20April%3A%20Hedonic%20hacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2014th%20April%3A%20Hedonic%20hacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcStxQD8aYLrx2jpzF%2Fmeetup-london-meetup-14th-april-hedonic-hacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2014th%20April%3A%20Hedonic%20hacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcStxQD8aYLrx2jpzF%2Fmeetup-london-meetup-14th-april-hedonic-hacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcStxQD8aYLrx2jpzF%2Fmeetup-london-meetup-14th-april-hedonic-hacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/lc'>London Meetup, 14th April: Hedonic hacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm.</p>\n\n<p>The theme for this meetup is hedonic hacks: what simple things can you do to become happier?</p>\n\n<p>Everyone is welcome to attend: we're a friendly group and we don't bite. If you're on the fence about coming, err on the side of showing up. It's probably safe to assume that we'd like to meet you.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/lc'>London Meetup, 14th April: Hedonic hacks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cStxQD8aYLrx2jpzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "22225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__14th_April__Hedonic_hacks\">Discussion article for the meetup : <a href=\"/meetups/lc\">London Meetup, 14th April: Hedonic hacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A fortnightly meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm.</p>\n\n<p>The theme for this meetup is hedonic hacks: what simple things can you do to become happier?</p>\n\n<p>Everyone is welcome to attend: we're a friendly group and we don't bite. If you're on the fence about coming, err on the side of showing up. It's probably safe to assume that we'd like to meet you.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__14th_April__Hedonic_hacks1\">Discussion article for the meetup : <a href=\"/meetups/lc\">London Meetup, 14th April: Hedonic hacks</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 14th April: Hedonic hacks", "anchor": "Discussion_article_for_the_meetup___London_Meetup__14th_April__Hedonic_hacks", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 14th April: Hedonic hacks", "anchor": "Discussion_article_for_the_meetup___London_Meetup__14th_April__Hedonic_hacks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-06T22:16:33.429Z", "modifiedAt": null, "url": null, "title": "Game for organizational structure testing", "slug": "game-for-organizational-structure-testing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:02.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tKQccJ9cS5yrS3Cwn/game-for-organizational-structure-testing", "pageUrlRelative": "/posts/tKQccJ9cS5yrS3Cwn/game-for-organizational-structure-testing", "linkUrl": "https://www.lesswrong.com/posts/tKQccJ9cS5yrS3Cwn/game-for-organizational-structure-testing", "postedAtFormatted": "Saturday, April 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Game%20for%20organizational%20structure%20testing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGame%20for%20organizational%20structure%20testing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKQccJ9cS5yrS3Cwn%2Fgame-for-organizational-structure-testing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Game%20for%20organizational%20structure%20testing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKQccJ9cS5yrS3Cwn%2Fgame-for-organizational-structure-testing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKQccJ9cS5yrS3Cwn%2Fgame-for-organizational-structure-testing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 707, "htmlBody": "<p>Say we want to try out new organizational structures. Zaine <a href=\"/r/discussion/lw/h4w/call_for_participation_centre_for_organisational/8pen\">suggests</a> that a game might be a good method. However rather than a game to test a specific method of organizing people, I'm going to make a game where different organizational structures can be pitted against each other and statistics about their operation over time can be collected to inform new organisation designs.&nbsp;</p>\n<p>Some organizational structures that might be tested include Democracy, <a href=\"http://hanson.gmu.edu/futarchy.html\">Futarchy</a>, <a href=\"/r/discussion/lw/2pi/an_introduction_to_control_markets/\">Control Markets</a>, <a href=\"/lw/9g4/histocracy_open_effective_group_decisionmaking/\">Histocracy</a>, some form of Meritocracy and Direct Democracy.</p>\n<p>The conditions under which organizations suffer from corruption of purpose more frequently are when the people inside the organization are generally selfish and only moderately interested in the goals of the organization. So it makes sense to concentrate on these sorts of conditions.<a id=\"more\"></a></p>\n<p>I will be using the terminology defined in&nbsp;<a href=\"/r/discussion/lw/2pi/an_introduction_to_control_markets/\">this article</a>&nbsp;to talk about different facets of an organization.</p>\n<p>One other bit of terminology: &nbsp;Team, a group of players given an organizational structure to test.</p>\n<p>Although to simplify things we shall ignore Stakeholders, unless they are strictly necessary, instead relying on how well the teams perform in the game as Feedback.</p>\n<h2>Social Condition Creation</h2>\n<p>In order to make people selfish we need at least an individual high score table. Also people should be&nbsp;anonymous, assigned their teams randomly and communication restricted between them so that they interact with each other like strangers. This would avoid camaraderie, team spirit and reputation management being organisational factors.</p>\n<h2>Game play</h2>\n<p>The design of the game is a tricky subject in itself.</p>\n<p>It would need to be:</p>\n<ul>\n<li>Interesting - so that people played it.&nbsp;</li>\n<li>Deep - so that people with more skill did better and there wasn't a dominant strategy.&nbsp;</li>\n<li>Require team work - so that one person can't do everything themselves.</li>\n</ul>\n<div>It would be split into matches and rounds. Each match would create new teams randomly and be composed of a few rounds. Each round each team would be scored by the game acting as Feedback. &nbsp;Between rounds there would be elections for Democracies or auctions for Control Markets. Futarchies might be harder to fit in you would need a vote and then a period of actions being suggested by the leader and trading on the outcome of those actions.&nbsp;</div>\n<div><br /></div>\n<div>A team vs team game would seem to be the best way forward, as creating an engaging game world would be too complex. Unless an existing game could be adapted (I'm thinking something like co-operative Dwarf Fortress).&nbsp;</div>\n<div><br /></div>\n<div>Perhaps a space-based crew game would work. With people able to move a ship, fire weapons, &nbsp;scan areas, lay traps for other teams and communicate with them. Or perhaps a game with limited action points per team member. Maybe something <a href=\"http://boardgamegeek.com/boardgame/38453/space-alert\">space alert</a>y but TvT.&nbsp;</div>\n<h2>Scoring</h2>\n<p>I envision the high score table being an average of how you do during each game, with people having to not be below a couple of standard deviations of the average number of games played to be ranked. You couldn't play one game, ace it and retire, you would have to be consistently good.</p>\n<p>Each organisation type would have a different scoring method and different high score tables.</p>\n<p>Control Markets would naturally have the amount of funge acquired by a team member as a score.</p>\n<p>Members of a Futarchy would have their remaining money as a score, perhaps scaled by the score of the team.</p>\n<p>Democracies might allow the leader(s) to pick a percentage of the score acquired for a round to disburse to the general team members as an incentive for them to help out and pick a good leader, the rest being kept by the leader(s). Or more cynically the score for a democracy match would be the number of times you got elected. Perhaps both scores could be tracked.</p>\n<h2>Metrics</h2>\n<p>The simplest metric to collect would be which organization types did the best on average. But in depth information could be collected on why teams fail in different organization types. The reasons might include in such as lack of engagement, infighting or underhand sabotage. Actor behaviour over different organization types could be analysed.</p>\n<h2>Downsides</h2>\n<p>It is a pretty artificial setting, so even if one structure did well in the game, it might not do well in real life. When you add in Stakeholders or an external economy the dynamics may well change a lot.</p>\n<p>Comments and ideas appreciated!</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tKQccJ9cS5yrS3Cwn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -2, "extendedScore": null, "score": 1.160920178781639e-06, "legacy": true, "legacyId": "22219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Say we want to try out new organizational structures. Zaine <a href=\"/r/discussion/lw/h4w/call_for_participation_centre_for_organisational/8pen\">suggests</a> that a game might be a good method. However rather than a game to test a specific method of organizing people, I'm going to make a game where different organizational structures can be pitted against each other and statistics about their operation over time can be collected to inform new organisation designs.&nbsp;</p>\n<p>Some organizational structures that might be tested include Democracy, <a href=\"http://hanson.gmu.edu/futarchy.html\">Futarchy</a>, <a href=\"/r/discussion/lw/2pi/an_introduction_to_control_markets/\">Control Markets</a>, <a href=\"/lw/9g4/histocracy_open_effective_group_decisionmaking/\">Histocracy</a>, some form of Meritocracy and Direct Democracy.</p>\n<p>The conditions under which organizations suffer from corruption of purpose more frequently are when the people inside the organization are generally selfish and only moderately interested in the goals of the organization. So it makes sense to concentrate on these sorts of conditions.<a id=\"more\"></a></p>\n<p>I will be using the terminology defined in&nbsp;<a href=\"/r/discussion/lw/2pi/an_introduction_to_control_markets/\">this article</a>&nbsp;to talk about different facets of an organization.</p>\n<p>One other bit of terminology: &nbsp;Team, a group of players given an organizational structure to test.</p>\n<p>Although to simplify things we shall ignore Stakeholders, unless they are strictly necessary, instead relying on how well the teams perform in the game as Feedback.</p>\n<h2 id=\"Social_Condition_Creation\">Social Condition Creation</h2>\n<p>In order to make people selfish we need at least an individual high score table. Also people should be&nbsp;anonymous, assigned their teams randomly and communication restricted between them so that they interact with each other like strangers. This would avoid camaraderie, team spirit and reputation management being organisational factors.</p>\n<h2 id=\"Game_play\">Game play</h2>\n<p>The design of the game is a tricky subject in itself.</p>\n<p>It would need to be:</p>\n<ul>\n<li>Interesting - so that people played it.&nbsp;</li>\n<li>Deep - so that people with more skill did better and there wasn't a dominant strategy.&nbsp;</li>\n<li>Require team work - so that one person can't do everything themselves.</li>\n</ul>\n<div>It would be split into matches and rounds. Each match would create new teams randomly and be composed of a few rounds. Each round each team would be scored by the game acting as Feedback. &nbsp;Between rounds there would be elections for Democracies or auctions for Control Markets. Futarchies might be harder to fit in you would need a vote and then a period of actions being suggested by the leader and trading on the outcome of those actions.&nbsp;</div>\n<div><br></div>\n<div>A team vs team game would seem to be the best way forward, as creating an engaging game world would be too complex. Unless an existing game could be adapted (I'm thinking something like co-operative Dwarf Fortress).&nbsp;</div>\n<div><br></div>\n<div>Perhaps a space-based crew game would work. With people able to move a ship, fire weapons, &nbsp;scan areas, lay traps for other teams and communicate with them. Or perhaps a game with limited action points per team member. Maybe something <a href=\"http://boardgamegeek.com/boardgame/38453/space-alert\">space alert</a>y but TvT.&nbsp;</div>\n<h2 id=\"Scoring\">Scoring</h2>\n<p>I envision the high score table being an average of how you do during each game, with people having to not be below a couple of standard deviations of the average number of games played to be ranked. You couldn't play one game, ace it and retire, you would have to be consistently good.</p>\n<p>Each organisation type would have a different scoring method and different high score tables.</p>\n<p>Control Markets would naturally have the amount of funge acquired by a team member as a score.</p>\n<p>Members of a Futarchy would have their remaining money as a score, perhaps scaled by the score of the team.</p>\n<p>Democracies might allow the leader(s) to pick a percentage of the score acquired for a round to disburse to the general team members as an incentive for them to help out and pick a good leader, the rest being kept by the leader(s). Or more cynically the score for a democracy match would be the number of times you got elected. Perhaps both scores could be tracked.</p>\n<h2 id=\"Metrics\">Metrics</h2>\n<p>The simplest metric to collect would be which organization types did the best on average. But in depth information could be collected on why teams fail in different organization types. The reasons might include in such as lack of engagement, infighting or underhand sabotage. Actor behaviour over different organization types could be analysed.</p>\n<h2 id=\"Downsides\">Downsides</h2>\n<p>It is a pretty artificial setting, so even if one structure did well in the game, it might not do well in real life. When you add in Stakeholders or an external economy the dynamics may well change a lot.</p>\n<p>Comments and ideas appreciated!</p>\n<p>&nbsp;</p>", "sections": [{"title": "Social Condition Creation", "anchor": "Social_Condition_Creation", "level": 1}, {"title": "Game play", "anchor": "Game_play", "level": 1}, {"title": "Scoring", "anchor": "Scoring", "level": 1}, {"title": "Metrics", "anchor": "Metrics", "level": 1}, {"title": "Downsides", "anchor": "Downsides", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hxhFuwBaqetT8RSKp", "MDQnDGEKQuCAggRLe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T01:01:13.477Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Montreal Biweekly Meetup", "slug": "meetup-lesswrong-montreal-biweekly-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CKjmb4r4kxSWTWi4Z/meetup-lesswrong-montreal-biweekly-meetup", "pageUrlRelative": "/posts/CKjmb4r4kxSWTWi4Z/meetup-lesswrong-montreal-biweekly-meetup", "linkUrl": "https://www.lesswrong.com/posts/CKjmb4r4kxSWTWi4Z/meetup-lesswrong-montreal-biweekly-meetup", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Montreal%20Biweekly%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Montreal%20Biweekly%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKjmb4r4kxSWTWi4Z%2Fmeetup-lesswrong-montreal-biweekly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Montreal%20Biweekly%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKjmb4r4kxSWTWi4Z%2Fmeetup-lesswrong-montreal-biweekly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKjmb4r4kxSWTWi4Z%2Fmeetup-lesswrong-montreal-biweekly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ld'>LessWrong Montreal Biweekly Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 April 2013 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Ave. Du President Kennedy Montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetups are now Tuesday, and biweekly!</p>\n\n<p>See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ld'>LessWrong Montreal Biweekly Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CKjmb4r4kxSWTWi4Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1610328214181953e-06, "legacy": true, "legacyId": "22227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal_Biweekly_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ld\">LessWrong Montreal Biweekly Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 April 2013 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Ave. Du President Kennedy Montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetups are now Tuesday, and biweekly!</p>\n\n<p>See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal_Biweekly_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ld\">LessWrong Montreal Biweekly Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Montreal Biweekly Meetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal_Biweekly_Meetup", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Montreal Biweekly Meetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal_Biweekly_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T05:14:44.154Z", "modifiedAt": null, "url": null, "title": "[Link] Values Spreading is Often More Important than Extinction Risk", "slug": "link-values-spreading-is-often-more-important-than", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nbd4aB24ktozQeHwL/link-values-spreading-is-often-more-important-than", "pageUrlRelative": "/posts/nbd4aB24ktozQeHwL/link-values-spreading-is-often-more-important-than", "linkUrl": "https://www.lesswrong.com/posts/nbd4aB24ktozQeHwL/link-values-spreading-is-often-more-important-than", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Values%20Spreading%20is%20Often%20More%20Important%20than%20Extinction%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Values%20Spreading%20is%20Often%20More%20Important%20than%20Extinction%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbd4aB24ktozQeHwL%2Flink-values-spreading-is-often-more-important-than%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Values%20Spreading%20is%20Often%20More%20Important%20than%20Extinction%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbd4aB24ktozQeHwL%2Flink-values-spreading-is-often-more-important-than", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbd4aB24ktozQeHwL%2Flink-values-spreading-is-often-more-important-than", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>In a <a href=\"http://www.utilitarian-essays.com/values-spreading.html\">recent essay</a>, Brian Tomasik argues that meme-spreading has higher expected utility than x-risk reduction. His analysis assumes a classical utilitarian ethic, but it may be generalizable to other value systems. &nbsp;Here's the summary:</p>\n<blockquote>\n<p>I personally do not support efforts to reduce extinction risk because I think space colonization would potentially give rise to astronomical amounts of suffering. However, even if I thought reducing extinction risk was a good idea, I would not work on it, because spreading your particular values has generally much higher leverage than being one more voice for safety measures against extinction in a world where reducing extinction risk is hard and almost everyone has some incentives to invest in the issue.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nbd4aB24ktozQeHwL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 14, "extendedScore": null, "score": 1.1612062764181174e-06, "legacy": true, "legacyId": "22232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T06:47:17.470Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Helpless Individuals", "slug": "seq-rerun-helpless-individuals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:33.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M8zSfWW3dTKy26663/seq-rerun-helpless-individuals", "pageUrlRelative": "/posts/M8zSfWW3dTKy26663/seq-rerun-helpless-individuals", "linkUrl": "https://www.lesswrong.com/posts/M8zSfWW3dTKy26663/seq-rerun-helpless-individuals", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Helpless%20Individuals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Helpless%20Individuals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zSfWW3dTKy26663%2Fseq-rerun-helpless-individuals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Helpless%20Individuals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zSfWW3dTKy26663%2Fseq-rerun-helpless-individuals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zSfWW3dTKy26663%2Fseq-rerun-helpless-individuals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 273, "htmlBody": "<p>Today's post, <a href=\"/lw/64/helpless_individuals/\">Helpless Individuals</a> was originally published on 30 March 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you consider that our grouping instincts are optimized for 50-person hunter-gatherer bands where everyone knows everyone else, it begins to seem miraculous that modern-day large institutions survive at all. And in fact, the vast majority of large modern-day institutions simply fail to exist in the first place. This is why funding of Science is largely through money thrown at Science rather than donations from individuals - research isn't a good emotional fit for the rare problems that individuals can manage to coordinate on. In fact very few things are, which is why e.g. 200 million adult Americans have such tremendous trouble supervising the 535 members of Congress. Modern humanity manages to put forth very little in the way of coordinated individual effort to serve our collective individual interests.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h59/seq_rerun_rationality_common_interest_of_many/\">Rationality: Common Interest of Many Causes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M8zSfWW3dTKy26663", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.1612696150749734e-06, "legacy": true, "legacyId": "22235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f42BHX7rMw2dyFJfT", "BRBWv3x5FEfE9Zroa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T13:40:41.264Z", "modifiedAt": null, "url": null, "title": "What Rate of Return Should You Expect?", "slug": "what-rate-of-return-should-you-expect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yt5QiEkaASsA2iYrN/what-rate-of-return-should-you-expect", "pageUrlRelative": "/posts/yt5QiEkaASsA2iYrN/what-rate-of-return-should-you-expect", "linkUrl": "https://www.lesswrong.com/posts/yt5QiEkaASsA2iYrN/what-rate-of-return-should-you-expect", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Rate%20of%20Return%20Should%20You%20Expect%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Rate%20of%20Return%20Should%20You%20Expect%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyt5QiEkaASsA2iYrN%2Fwhat-rate-of-return-should-you-expect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Rate%20of%20Return%20Should%20You%20Expect%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyt5QiEkaASsA2iYrN%2Fwhat-rate-of-return-should-you-expect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyt5QiEkaASsA2iYrN%2Fwhat-rate-of-return-should-you-expect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p>If you invest money, perhaps because you want to <a href=\"/r/discussion/lw/h3q/a_rational_altruist_punch_in_the_stomach/\">give money in the future</a>, you want a good estimate of the real rate of return so you can compare it to just giving now. Small differences in this estimate have <a href=\"http://www.jefftk.com/news/2012-07-18\">very large effects</a> over decades, and just today I've seen people giving numbers as far apart as <a href=\"http://www.overcomingbias.com/2010/03/parable-of-the-multiplier-hole.html\">2%</a> and <a href=\"http://slatestarcodex.com/2013/04/05/investment-and-inefficient-charity/#comment-2448\">8%</a>, which over 40 years mean 2.2x growth and 22x growth respectively.</p>\n<p>What is reasonable here? If you look around online at personal finance sites you'll see numbers like <a href=\"http://www.popeconomics.com/2010/01/18/what-investment-return-should-you-plan-for/\">6%</a>, <a href=\"http://money.cnn.com/2011/01/28/pf/expert/retirement_investment_return.moneymag/index.htm\">7%</a>, <a href=\"http://usatoday30.usatoday.com/money/perfi/columnist/krantz/2011-06-13-big-investment-returns_n.htm\">10%</a>, <a href=\"http://www.fool.com/money/401k/401k02.htm\">11%</a>, and even <a href=\"http://www.daveramsey.com/article/the-12-reality/lifeandmoney_investing/\">12%</a>. First off, these all ignore inflation, which has averaged about 3%. Naively subtracting, this gives us real rates of 3%, 4%, 7%, and 9%. There's also a lot of cherry-picking: if you're willing to choose specific years and specific indexes you can get quite good numbers.</p>\n<p>There's an even bigger cherry-picking problem, however, in that they're all based on US returns. Other countries have had worse economic performance, or have even had their stock markets wiped out. Dimson, Marsh, and Staunton (2013) (<a href=\"http://www.investmenteurope.net/digital_assets/6305/2013_yearbook_final_web.pdf\">pdf</a>) (<a href=\"http://www.economist.com/news/finance-and-economics/21571443-investors-may-have-developed-too-rosy-view-equity-returns-beware-bias\">summary</a>) conclude that investors should expect much lower returns:</p>\n<blockquote>We have estimated that over the next 20-30 years, global investors, paying low levels of witholding tax and management fees, can expect to earn an annualized real return of no more than 3.5% on an all-equity fund and 2% on a fund split equally between equities and government bonds.</blockquote>\n<p>The difference between 7% interest and 3.5% interest is the difference between 60x and 8x over 60 years. This is really important if you're trying to decide whether you do better to save money now and donate later after several decades of compound interest. [1]</p>\n<p><br /> [1] If you're saving for your own retirement <a href=\"http://www.jefftk.com/news/2012-07-18#gp-1342671046850\">tax incentives are probably more important</a> than interest rates.</p>\n<p><em><small>Cross-posted from <a href=\"http://www.jefftk.com/news/2013-04-06\">my blog</a></small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yt5QiEkaASsA2iYrN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 1.1615525909705744e-06, "legacy": true, "legacyId": "22237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TYAA4iNCFaDvPD6gB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T15:34:26.215Z", "modifiedAt": null, "url": null, "title": "On moving posts from Main to Discussion", "slug": "on-moving-posts-from-main-to-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:55.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SiEkSaifqyycEn2Be/on-moving-posts-from-main-to-discussion", "pageUrlRelative": "/posts/SiEkSaifqyycEn2Be/on-moving-posts-from-main-to-discussion", "linkUrl": "https://www.lesswrong.com/posts/SiEkSaifqyycEn2Be/on-moving-posts-from-main-to-discussion", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20moving%20posts%20from%20Main%20to%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20moving%20posts%20from%20Main%20to%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiEkSaifqyycEn2Be%2Fon-moving-posts-from-main-to-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20moving%20posts%20from%20Main%20to%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiEkSaifqyycEn2Be%2Fon-moving-posts-from-main-to-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiEkSaifqyycEn2Be%2Fon-moving-posts-from-main-to-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>Yesterday, someone moved one of my posts from Main to Discussion without telling me.&nbsp; Again.</p>\n<p>I encourage the site administrators to show some basic courtesy to the posters who provide the content for the site.&nbsp; I believe this would be a better way of doing things:</p>\n<p>1. Have a policy on what has to happen to move a post from Main to Discussion.&nbsp; Who can do it?&nbsp; How many admins are there who can do this?&nbsp; State this policy in a FAQ.</p>\n<p>2. When you move a post from Main to Discussion, make a comment on the post saying you have done so and why you have done so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SiEkSaifqyycEn2Be", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 15, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "22238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-07T23:48:11.497Z", "modifiedAt": null, "url": null, "title": "Utility Quilting", "slug": "utility-quilting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:27.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xtqWDLcS548MQn35D/utility-quilting", "pageUrlRelative": "/posts/xtqWDLcS548MQn35D/utility-quilting", "linkUrl": "https://www.lesswrong.com/posts/xtqWDLcS548MQn35D/utility-quilting", "postedAtFormatted": "Sunday, April 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utility%20Quilting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtility%20Quilting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtqWDLcS548MQn35D%2Futility-quilting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utility%20Quilting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtqWDLcS548MQn35D%2Futility-quilting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtqWDLcS548MQn35D%2Futility-quilting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1792, "htmlBody": "<p><strong>Related</strong>: <a href=\"/lw/ggm/pinpointing_utility/\">Pinpointing Utility</a></p>\n<p>Let's go for lunch at the Hypothetical Diner; I have something I want to discuss with you.</p>\n<p>We will pick our lunch from the set of possible orders, and we will recieve a meal drawn from the set of possible meals, <code>O</code>.</p>\n<p>Speaking in general, each possible order has an associated probability distribution over <code>O</code>. The Hypothetical Diner takes care to simplify your analysis; the probability distribution is trivial; you always get exactly what you ordered.</p>\n<p>Again to simplify your lunch, the Hypothetical Diner offers only two choices on the menu: the Soup, and the Bagel.</p>\n<p>To then complicate things so that we have something to talk about, suppose there is some set <code>M</code> of ways other things could be that may affect your preferences. Perhaps you have sore teeth on some days.</p>\n<p>Suppose for the purposes of this hypothetical lunch date that you are VNM rational. Shocking, I know, but the hypothetical results are clear: you have a utility function, <code>U</code>. The domain of the utility function is the product of all the variables that affect your preferences (which meal, and whether your teeth are sore): <code>U: M x O -&gt; utility</code>.</p>\n<p>In our case, if your teeth are sore, you prefer the soup, as it is less painful. If your teeth are not sore, you prefer the bagel, because it is tastier:</p>\n<pre><code>U(sore &amp; soup) &gt; U(sore &amp; bagel)\nU(~sore &amp; soup) &lt; U(~sore &amp; bagel)\n</code></pre>\n<p>Your global utility function can be partially applied to some m in M to get an \"object-level\" utility function <code>U_m: O -&gt; utility</code>. Note that the restrictions of U made in this way need not have any resemblance to each other; they are completely separate.</p>\n<p>It is convenient to think about and define these restricted \"utility function patches\" separately. Let's pick some units and datums so we can get concrete numbers for our utilities:</p>\n<pre><code>U_sore(soup) = 1 ; U_sore(bagel) = 0\nU_unsore(soup) = 0 ; U_unsore(bagel) = 1\n</code></pre>\n<p>Those are separate utility functions, now, so we could pick units and datum seperately. Because of this, the sore numbers are totally incommensurable to the unsore numbers. *Don't try to comapre them between the UF's or you will get <a href=\"/lw/ggm/pinpointing_utility/\">type-poisoning</a>. The actual numbers are just a straightforward encoding of the preferences mentioned above.</p>\n<p>What if we are unsure about where we fall in M? That is, we won't know whether our teeth are sore until we take the first bite. That is, we have a probability distribution over M. Maybe we are 70% sure that your teeth won't hurt you today. What should you order?</p>\n<p>Well, it's usually a good idea to maximize expected utility:</p>\n<pre><code>EU(soup) = 30%*U(sore&amp;soup) + 70%*U(~sore&amp;soup) = ???\nEU(bagel) = 30%*U(sore&amp;bagel) + 70%*U(~sore&amp;bagel) = ???\n</code></pre>\n<p>Suddenly we need those utility function patches to be commensuarable, so that we can actually compute these, but we went and defined them separately, darn. All is not lost though, recall that they are just restrictions of a global utility function to particular soreness-circumstance, with some (positive) linear transforms, <code>f_m</code>, thrown in to make the numbers nice:</p>\n<pre><code>f_sore(U(sore&amp;soup)) = 1 ; f_sore(U(sore&amp;bagel)) = 0\nf_unsore(U(~sore&amp;soup)) = 0 ; f_unsore(U(~sore&amp;bagel)) = 1\n</code></pre>\n<p>At this point, it's just a bit of clever function-inverting and all is dandy. We can pick some linear transform <code>g</code> to be canonical, and transform all the utility function patches into that basis. So for all m, we can get g(U(m &amp; o)) by inverting the <code>f_m</code> and then applying <code>g</code>:</p>\n<pre><code>g.U(sore &amp; x) = (g.inv(f_sore).f_sore)(U(sore &amp; x))\n= k_sore*U_sore(x) + c_sore\ng.U(~sore &amp; x) = (g.inv(f_unsore).f_unsore)(U(~sore &amp; x))\n= k_unsore*U_unsore(x) + c_unsore\n</code></pre>\n<p>(I'm using <code>.</code> to represent composition of those transforms. I hope that's not too confusing.)</p>\n<p>Linear transforms are really nice; all the inverting and composing collapses down to a scale <code>k</code> and an offset <code>c</code> for each utility function patch. Now we've turned our bag of utility function patches into a utility function quilt! One more bit of math before we get back to deciding what to eat:</p>\n<pre><code>EU(x) = P(sore) *(k_sore *U_sore(x) + c_sore) +\n(1-P(sore))*(k_unsore*U_unsore(x) + c_unsore)\n</code></pre>\n<p>Notice that the terms involving <code>c_m</code> do not involve <code>x</code>, meaning that the <code>c_m</code> terms don't affect our decision, so we can cancel them out and forget they ever existed! This is only true because I've implicitly assumed that P(m) does not depend on our actions. If it did, like if we could go to the dentist or take some painkillers, then it would be <code>P(m | x)</code> and <code>c_m</code> would be relevent in the whole joint decision.</p>\n<p>We can define the canonical utility basis <code>g</code> to be whatever we like (among positive linear transforms); for example, we can make it equal to <code>f_sore</code> so that we can at least keep the simple numbers from <code>U_sore</code>. Then we throw all the <code>c_m</code>s away, because they don't matter. Then it's just a matter of getting the remaining <code>k_m</code>s.</p>\n<p>Ok, sorry, those last few paragraphs were rather abstract. Back to lunch. We just need to define these mysterious scaling constants and then we can order lunch. There is only one left; <code>k_unsore</code>. In general there will be <code>n-1</code>, where <code>n</code> is the size of <code>M</code>. I think the easiest way to approach this is to let <code>k_unsore = 1/5</code> and see what that implies:</p>\n<pre><code>g.U(sore &amp; soup) = 1 ; g.U(sore &amp; bagel) = 0\ng.U(~sore &amp; soup) = 0 ; g.U(~sore &amp; bagel) = 1/5\nEU(soup) = (1-P(~sore))*1 = 0.3\nEU(bagel) = P(~sore)*k_unsore = 0.14\nEU(soup) &gt; EU(bagel)\n</code></pre>\n<p>After all the arithmetic, it looks like if <code>k_unsore = 1/5</code>, even if we expect you to have nonsore teeth with <code>P(sore) = 0.3</code>, we are unsure enough and the relative importance is big enough that we should play safe safe and go with the soup anyways. In general we would choose soup if <code>P(~sore) &lt; 1/(k_unsore+1)</code>, or equivalently, if <code>k_unsore &lt; (1-P(~sore)/P(~sore)</code>.</p>\n<p>So <code>k</code> is somehow the relative importance of possible preference stuctures under uncertainty. A smaller <code>k</code> in this lunch example means that the tastiness of a bagel over a soup is small relative to the pain saved by eating the soup instead. With this intuition, we can see that <code>1/5</code> is a somewhat reasonable value for this scenario, and for example, <code>1</code> would not be, and neither would <code>1/20</code></p>\n<p>What if we are uncertain about <code>k</code>? Are we simply pushing the problem up some meta-chain? It turns out that no, we are not. Because <code>k</code> is linearly related to utility, you can simply use its expected value if it is uncertain.</p>\n<p>It's kind of ugly to have these <code>k_m</code>'s and these <code>U_m</code>'s, so we can just reason over the product <code>K x M</code> instead of <code>M</code> and <code>K</code> seperately. This is nothing weird, it just means we have more utility function patches (Many of which encode the exact same object-level preferences).</p>\n<p>In the most general case, the utility function patches in <code>KxM</code> are the space of all functions <code>O -&gt; RR</code>, with offset equivalence, <em>but not scale equivalence</em> (Sovereign utility functions have full linear-transform equivalence, but these patches are only equivalent under offset). Remember, though, that these are just restricted patches of a single global utility function.</p>\n<p>So what is the point of all this? Are we just playing in the VNM sandbox, or is this result actually interesting for anything besides sore teeth?</p>\n<p>Perhaps Moral/Preference Uncertainty? I didn't mention it until now because it's easier to think about lunch than a <a href=\"/lw/gm9/philosophical_landmines/\">philosophical minefield</a>, but it is the point of this post. Sorry about that. Let's conclude with everything restated in terms of moral uncertainty.</p>\n<h2>TL;DR:</h2>\n<p>If we have:</p>\n<ol>\n<li>\n<p>A set of object-level outcomes <code>O</code>,</p>\n</li>\n<li>\n<p>A set of \"epiphenomenal\" (outside of <code>O</code>) 'moral' outcomes <code>M</code>,</p>\n</li>\n<li>\n<p>A probability distribution over <code>M</code>, possibly correlated with uncertainty about <code>O</code>, but not in a way that allows our actions to influence uncertainty over <code>M</code> (that is, assuming moral facts cannot be changed by your actions.),</p>\n</li>\n<li>\n<p>A utility function over <code>O</code> for each possible value of <code>M</code>, (these can be arbitrary VNM-rational moral theories, as long as they share the same object-level),</p>\n</li>\n<li>\n<p>And we wish to be VNM rational over whatever uncertainty we have</p>\n</li>\n</ol>\n<p>then we can quilt together a global utility function <code>U: (M,K,O) -&gt; RR</code> where and <code>U(m,k,o) = k*U_m(o)</code> so that <code>EU(o)</code> is the sum of all <code>P(m)*E(k | m)*U_m(o)</code></p>\n<p>Somehow this all seems like legal VNM.</p>\n<h2>Implications</h2>\n<p>So. Just the possible object-level preferences and a probability distribution over those is <em>not enough</em> to define our behaviour. We need to know the scale for each so we know how to act when uncertain. This is analogous to the switch from ordinal preferences to interval preferences when dealing with object-level uncertainty.</p>\n<p>Now we have a well-defined framework for reasoning about preference uncertainty, if all our possible moral theories are VNM rational, moral facts are immutable, and we have a joint probability distribution over <code>OxMxK</code>.</p>\n<p>In particular, updating your moral beliefs upon hearing new arguments is no longer a mysterious dynamic, it is just a bayesian update over possible moral theories.</p>\n<p>This requires a \"moral prior\" that corellates moral outcomes and their relative scales to the observable evidence. In the lunch example, we implicitly used such a moral prior to update on observable thought experiments and conclude that <code>1/5</code> was a plausible value for <code>k_unsore</code>.</p>\n<p>Moral evidence is probably things like preference thought-experiments, neuroscience and physics results, etc. The actual model for this, and discussion about the issues with defining and reasoning on such a prior are outside the scope of this post.</p>\n<p>This whole argument couldn't prove its way out of a wet paper bag, and is merely suggestive. Bits and peices may be found incorrect, and formalization might change things a bit.</p>\n<p>This framework requires that we have already worked out the outcome-space <code>O</code> (which we haven't), have limited our moral confusion to a set of VNM-rational moral theories over <code>O</code> (which we haven't), and have defined a \"Moral Prior\" so we can have a probability distribution over moral theories and their wieghts (which we haven't).</p>\n<p>Nonetheless, we can sometimes get those things in special limited cases, and even in the general case, having a model for moral uncertainty and updating is a huge step up from the terrifying confusion I (and everyone I've talked to) had before working this out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xtqWDLcS548MQn35D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 10, "extendedScore": null, "score": 1.161968657587508e-06, "legacy": true, "legacyId": "22223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related</strong>: <a href=\"/lw/ggm/pinpointing_utility/\">Pinpointing Utility</a></p>\n<p>Let's go for lunch at the Hypothetical Diner; I have something I want to discuss with you.</p>\n<p>We will pick our lunch from the set of possible orders, and we will recieve a meal drawn from the set of possible meals, <code>O</code>.</p>\n<p>Speaking in general, each possible order has an associated probability distribution over <code>O</code>. The Hypothetical Diner takes care to simplify your analysis; the probability distribution is trivial; you always get exactly what you ordered.</p>\n<p>Again to simplify your lunch, the Hypothetical Diner offers only two choices on the menu: the Soup, and the Bagel.</p>\n<p>To then complicate things so that we have something to talk about, suppose there is some set <code>M</code> of ways other things could be that may affect your preferences. Perhaps you have sore teeth on some days.</p>\n<p>Suppose for the purposes of this hypothetical lunch date that you are VNM rational. Shocking, I know, but the hypothetical results are clear: you have a utility function, <code>U</code>. The domain of the utility function is the product of all the variables that affect your preferences (which meal, and whether your teeth are sore): <code>U: M x O -&gt; utility</code>.</p>\n<p>In our case, if your teeth are sore, you prefer the soup, as it is less painful. If your teeth are not sore, you prefer the bagel, because it is tastier:</p>\n<pre><code>U(sore &amp; soup) &gt; U(sore &amp; bagel)\nU(~sore &amp; soup) &lt; U(~sore &amp; bagel)\n</code></pre>\n<p>Your global utility function can be partially applied to some m in M to get an \"object-level\" utility function <code>U_m: O -&gt; utility</code>. Note that the restrictions of U made in this way need not have any resemblance to each other; they are completely separate.</p>\n<p>It is convenient to think about and define these restricted \"utility function patches\" separately. Let's pick some units and datums so we can get concrete numbers for our utilities:</p>\n<pre><code>U_sore(soup) = 1 ; U_sore(bagel) = 0\nU_unsore(soup) = 0 ; U_unsore(bagel) = 1\n</code></pre>\n<p>Those are separate utility functions, now, so we could pick units and datum seperately. Because of this, the sore numbers are totally incommensurable to the unsore numbers. *Don't try to comapre them between the UF's or you will get <a href=\"/lw/ggm/pinpointing_utility/\">type-poisoning</a>. The actual numbers are just a straightforward encoding of the preferences mentioned above.</p>\n<p>What if we are unsure about where we fall in M? That is, we won't know whether our teeth are sore until we take the first bite. That is, we have a probability distribution over M. Maybe we are 70% sure that your teeth won't hurt you today. What should you order?</p>\n<p>Well, it's usually a good idea to maximize expected utility:</p>\n<pre><code>EU(soup) = 30%*U(sore&amp;soup) + 70%*U(~sore&amp;soup) = ???\nEU(bagel) = 30%*U(sore&amp;bagel) + 70%*U(~sore&amp;bagel) = ???\n</code></pre>\n<p>Suddenly we need those utility function patches to be commensuarable, so that we can actually compute these, but we went and defined them separately, darn. All is not lost though, recall that they are just restrictions of a global utility function to particular soreness-circumstance, with some (positive) linear transforms, <code>f_m</code>, thrown in to make the numbers nice:</p>\n<pre><code>f_sore(U(sore&amp;soup)) = 1 ; f_sore(U(sore&amp;bagel)) = 0\nf_unsore(U(~sore&amp;soup)) = 0 ; f_unsore(U(~sore&amp;bagel)) = 1\n</code></pre>\n<p>At this point, it's just a bit of clever function-inverting and all is dandy. We can pick some linear transform <code>g</code> to be canonical, and transform all the utility function patches into that basis. So for all m, we can get g(U(m &amp; o)) by inverting the <code>f_m</code> and then applying <code>g</code>:</p>\n<pre><code>g.U(sore &amp; x) = (g.inv(f_sore).f_sore)(U(sore &amp; x))\n= k_sore*U_sore(x) + c_sore\ng.U(~sore &amp; x) = (g.inv(f_unsore).f_unsore)(U(~sore &amp; x))\n= k_unsore*U_unsore(x) + c_unsore\n</code></pre>\n<p>(I'm using <code>.</code> to represent composition of those transforms. I hope that's not too confusing.)</p>\n<p>Linear transforms are really nice; all the inverting and composing collapses down to a scale <code>k</code> and an offset <code>c</code> for each utility function patch. Now we've turned our bag of utility function patches into a utility function quilt! One more bit of math before we get back to deciding what to eat:</p>\n<pre><code>EU(x) = P(sore) *(k_sore *U_sore(x) + c_sore) +\n(1-P(sore))*(k_unsore*U_unsore(x) + c_unsore)\n</code></pre>\n<p>Notice that the terms involving <code>c_m</code> do not involve <code>x</code>, meaning that the <code>c_m</code> terms don't affect our decision, so we can cancel them out and forget they ever existed! This is only true because I've implicitly assumed that P(m) does not depend on our actions. If it did, like if we could go to the dentist or take some painkillers, then it would be <code>P(m | x)</code> and <code>c_m</code> would be relevent in the whole joint decision.</p>\n<p>We can define the canonical utility basis <code>g</code> to be whatever we like (among positive linear transforms); for example, we can make it equal to <code>f_sore</code> so that we can at least keep the simple numbers from <code>U_sore</code>. Then we throw all the <code>c_m</code>s away, because they don't matter. Then it's just a matter of getting the remaining <code>k_m</code>s.</p>\n<p>Ok, sorry, those last few paragraphs were rather abstract. Back to lunch. We just need to define these mysterious scaling constants and then we can order lunch. There is only one left; <code>k_unsore</code>. In general there will be <code>n-1</code>, where <code>n</code> is the size of <code>M</code>. I think the easiest way to approach this is to let <code>k_unsore = 1/5</code> and see what that implies:</p>\n<pre><code>g.U(sore &amp; soup) = 1 ; g.U(sore &amp; bagel) = 0\ng.U(~sore &amp; soup) = 0 ; g.U(~sore &amp; bagel) = 1/5\nEU(soup) = (1-P(~sore))*1 = 0.3\nEU(bagel) = P(~sore)*k_unsore = 0.14\nEU(soup) &gt; EU(bagel)\n</code></pre>\n<p>After all the arithmetic, it looks like if <code>k_unsore = 1/5</code>, even if we expect you to have nonsore teeth with <code>P(sore) = 0.3</code>, we are unsure enough and the relative importance is big enough that we should play safe safe and go with the soup anyways. In general we would choose soup if <code>P(~sore) &lt; 1/(k_unsore+1)</code>, or equivalently, if <code>k_unsore &lt; (1-P(~sore)/P(~sore)</code>.</p>\n<p>So <code>k</code> is somehow the relative importance of possible preference stuctures under uncertainty. A smaller <code>k</code> in this lunch example means that the tastiness of a bagel over a soup is small relative to the pain saved by eating the soup instead. With this intuition, we can see that <code>1/5</code> is a somewhat reasonable value for this scenario, and for example, <code>1</code> would not be, and neither would <code>1/20</code></p>\n<p>What if we are uncertain about <code>k</code>? Are we simply pushing the problem up some meta-chain? It turns out that no, we are not. Because <code>k</code> is linearly related to utility, you can simply use its expected value if it is uncertain.</p>\n<p>It's kind of ugly to have these <code>k_m</code>'s and these <code>U_m</code>'s, so we can just reason over the product <code>K x M</code> instead of <code>M</code> and <code>K</code> seperately. This is nothing weird, it just means we have more utility function patches (Many of which encode the exact same object-level preferences).</p>\n<p>In the most general case, the utility function patches in <code>KxM</code> are the space of all functions <code>O -&gt; RR</code>, with offset equivalence, <em>but not scale equivalence</em> (Sovereign utility functions have full linear-transform equivalence, but these patches are only equivalent under offset). Remember, though, that these are just restricted patches of a single global utility function.</p>\n<p>So what is the point of all this? Are we just playing in the VNM sandbox, or is this result actually interesting for anything besides sore teeth?</p>\n<p>Perhaps Moral/Preference Uncertainty? I didn't mention it until now because it's easier to think about lunch than a <a href=\"/lw/gm9/philosophical_landmines/\">philosophical minefield</a>, but it is the point of this post. Sorry about that. Let's conclude with everything restated in terms of moral uncertainty.</p>\n<h2 id=\"TL_DR_\">TL;DR:</h2>\n<p>If we have:</p>\n<ol>\n<li>\n<p>A set of object-level outcomes <code>O</code>,</p>\n</li>\n<li>\n<p>A set of \"epiphenomenal\" (outside of <code>O</code>) 'moral' outcomes <code>M</code>,</p>\n</li>\n<li>\n<p>A probability distribution over <code>M</code>, possibly correlated with uncertainty about <code>O</code>, but not in a way that allows our actions to influence uncertainty over <code>M</code> (that is, assuming moral facts cannot be changed by your actions.),</p>\n</li>\n<li>\n<p>A utility function over <code>O</code> for each possible value of <code>M</code>, (these can be arbitrary VNM-rational moral theories, as long as they share the same object-level),</p>\n</li>\n<li>\n<p>And we wish to be VNM rational over whatever uncertainty we have</p>\n</li>\n</ol>\n<p>then we can quilt together a global utility function <code>U: (M,K,O) -&gt; RR</code> where and <code>U(m,k,o) = k*U_m(o)</code> so that <code>EU(o)</code> is the sum of all <code>P(m)*E(k | m)*U_m(o)</code></p>\n<p>Somehow this all seems like legal VNM.</p>\n<h2 id=\"Implications\">Implications</h2>\n<p>So. Just the possible object-level preferences and a probability distribution over those is <em>not enough</em> to define our behaviour. We need to know the scale for each so we know how to act when uncertain. This is analogous to the switch from ordinal preferences to interval preferences when dealing with object-level uncertainty.</p>\n<p>Now we have a well-defined framework for reasoning about preference uncertainty, if all our possible moral theories are VNM rational, moral facts are immutable, and we have a joint probability distribution over <code>OxMxK</code>.</p>\n<p>In particular, updating your moral beliefs upon hearing new arguments is no longer a mysterious dynamic, it is just a bayesian update over possible moral theories.</p>\n<p>This requires a \"moral prior\" that corellates moral outcomes and their relative scales to the observable evidence. In the lunch example, we implicitly used such a moral prior to update on observable thought experiments and conclude that <code>1/5</code> was a plausible value for <code>k_unsore</code>.</p>\n<p>Moral evidence is probably things like preference thought-experiments, neuroscience and physics results, etc. The actual model for this, and discussion about the issues with defining and reasoning on such a prior are outside the scope of this post.</p>\n<p>This whole argument couldn't prove its way out of a wet paper bag, and is merely suggestive. Bits and peices may be found incorrect, and formalization might change things a bit.</p>\n<p>This framework requires that we have already worked out the outcome-space <code>O</code> (which we haven't), have limited our moral confusion to a set of VNM-rational moral theories over <code>O</code> (which we haven't), and have defined a \"Moral Prior\" so we can have a probability distribution over moral theories and their wieghts (which we haven't).</p>\n<p>Nonetheless, we can sometimes get those things in special limited cases, and even in the general case, having a model for moral uncertainty and updating is a huge step up from the terrifying confusion I (and everyone I've talked to) had before working this out.</p>", "sections": [{"title": "TL;DR:", "anchor": "TL_DR_", "level": 1}, {"title": "Implications", "anchor": "Implications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CQkGJ2t5Rw8GcZKJm", "L4HQ3gnSrBETRdcGu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T02:00:38.710Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes April 2013", "slug": "rationality-quotes-april-2013", "viewCount": null, "lastCommentedAt": "2018-10-27T23:03:05.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FJHq2HgHBojYvkCuc/rationality-quotes-april-2013", "pageUrlRelative": "/posts/FJHq2HgHBojYvkCuc/rationality-quotes-april-2013", "linkUrl": "https://www.lesswrong.com/posts/FJHq2HgHBojYvkCuc/rationality-quotes-april-2013", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20April%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20April%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJHq2HgHBojYvkCuc%2Frationality-quotes-april-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20April%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJHq2HgHBojYvkCuc%2Frationality-quotes-april-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJHq2HgHBojYvkCuc%2Frationality-quotes-april-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>Another monthly installment of the rationality quotes thread. The usual rules apply:</p>\n<div>\n<div class=\"md\">\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, Overcoming Bias, or HPMoR.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FJHq2HgHBojYvkCuc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 1.1620594072463272e-06, "legacy": true, "legacyId": "22173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 285, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T05:01:03.797Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Purchase Fuzzies and Utilons Separately", "slug": "seq-rerun-purchase-fuzzies-and-utilons-separately", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.893Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DzuRwTGYW6ABgrDnN/seq-rerun-purchase-fuzzies-and-utilons-separately", "pageUrlRelative": "/posts/DzuRwTGYW6ABgrDnN/seq-rerun-purchase-fuzzies-and-utilons-separately", "linkUrl": "https://www.lesswrong.com/posts/DzuRwTGYW6ABgrDnN/seq-rerun-purchase-fuzzies-and-utilons-separately", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Purchase%20Fuzzies%20and%20Utilons%20Separately&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Purchase%20Fuzzies%20and%20Utilons%20Separately%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDzuRwTGYW6ABgrDnN%2Fseq-rerun-purchase-fuzzies-and-utilons-separately%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Purchase%20Fuzzies%20and%20Utilons%20Separately%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDzuRwTGYW6ABgrDnN%2Fseq-rerun-purchase-fuzzies-and-utilons-separately", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDzuRwTGYW6ABgrDnN%2Fseq-rerun-purchase-fuzzies-and-utilons-separately", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>Today's post, <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a> was originally published on 01 April 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Wealthy philanthropists typically make the mistake of trying to purchase warm fuzzy feelings, status among friends, and actual utilitarian gains, simultaneously; this results in vague pushes along all three dimensions and a mediocre final result. It should be far more effective to spend some money/effort on buying altruistic fuzzies at maximum optimized efficiency (e.g. by helping people in person and seeing the results in person), buying status at maximum efficiency (e.g. by donating to something sexy that you can brag about, regardless of effectiveness), and spending most of your money on expected utilons (chosen through sheer cold-blooded shut-up-and-multiply calculation, without worrying about status or fuzzies).</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h5n/seq_rerun_helpless_individuals/\">Helpless Individuals</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DzuRwTGYW6ABgrDnN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.1621830396841327e-06, "legacy": true, "legacyId": "22242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "M8zSfWW3dTKy26663", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T19:20:15.317Z", "modifiedAt": null, "url": null, "title": "Best causal/dependency diagram software for fluid capture?", "slug": "best-causal-dependency-diagram-software-for-fluid-capture", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:37.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jf5fChnfvStHx3Lbw/best-causal-dependency-diagram-software-for-fluid-capture", "pageUrlRelative": "/posts/jf5fChnfvStHx3Lbw/best-causal-dependency-diagram-software-for-fluid-capture", "linkUrl": "https://www.lesswrong.com/posts/jf5fChnfvStHx3Lbw/best-causal-dependency-diagram-software-for-fluid-capture", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20causal%2Fdependency%20diagram%20software%20for%20fluid%20capture%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20causal%2Fdependency%20diagram%20software%20for%20fluid%20capture%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf5fChnfvStHx3Lbw%2Fbest-causal-dependency-diagram-software-for-fluid-capture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20causal%2Fdependency%20diagram%20software%20for%20fluid%20capture%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf5fChnfvStHx3Lbw%2Fbest-causal-dependency-diagram-software-for-fluid-capture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf5fChnfvStHx3Lbw%2Fbest-causal-dependency-diagram-software-for-fluid-capture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>I've found most graphing software too clunky, or having too much mental friction, for my purpose of creating graphically represented plans, to convert written diagrams into digital form, or to do preference inference based on the structure of my goals (amongst other things).</p>\n<p>So far the only tool that I've seen that reduces this friction is <a href=\"http://www.graphviz.org/Home.php\">GraphViz</a> [1], since I think I can literally just list down connection after connection in markup, with no care for structure or reasonableness, and then prune connections after I see how the entire thing looks. Point and click is for suckers.</p>\n<p>However, I also like the approach of Freemind that quickly outputs a visual map that is easily traversable; but it doesn't do much for me when the causality is more involved.</p>\n<p>Are there any alternatives that anyone is aware of?</p>\n<p>[1] If you are not familiar with GraphViz, see <a href=\"http://robrhinehart.com/?p=119\">this amusing introduction</a> that maps the social network in R. Kelly's hit hip hopera, \"Trapped<a href=\"http://www.youtube.com/watch?v=OnNKRv8jviU\"> in the Closet</a>\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jf5fChnfvStHx3Lbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1627721248140702e-06, "legacy": true, "legacyId": "22244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T21:15:40.587Z", "modifiedAt": null, "url": null, "title": "LW Study Group Facebook Page", "slug": "lw-study-group-facebook-page", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benito", "createdAt": "2012-03-14T21:58:54.405Z", "isAdmin": true, "displayName": "Ben Pace"}, "userId": "EQNTWXLKMeWMp2FQS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oNQnTJSDsMXWZALTD/lw-study-group-facebook-page", "pageUrlRelative": "/posts/oNQnTJSDsMXWZALTD/lw-study-group-facebook-page", "linkUrl": "https://www.lesswrong.com/posts/oNQnTJSDsMXWZALTD/lw-study-group-facebook-page", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Study%20Group%20Facebook%20Page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Study%20Group%20Facebook%20Page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNQnTJSDsMXWZALTD%2Flw-study-group-facebook-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Study%20Group%20Facebook%20Page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNQnTJSDsMXWZALTD%2Flw-study-group-facebook-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNQnTJSDsMXWZALTD%2Flw-study-group-facebook-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 471, "htmlBody": "<p><strong>Update: </strong>There is now an online sign up to groups with workflowy, based on subject and current ability. You do not have to be signed up to Facebook to join a group, but do add an email address so that the group can contact you: &nbsp;<span style=\"font-family: '.HelveticaNeueUI'; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">https://workflowy.com/shared/cf1fd9ca-885f-c1b9-c2e8-e3a315f70138/</span></p>\n<p>&nbsp;</p>\n<p><a title=\"The recent Main article\" href=\"/lw/h4z/anybody_want_to_join_a_math_club/\" target=\"_blank\">The recent Main article</a>, searching for interest in LWers studying maths together, had many comments showing enthusiasm, but nothing really happened.</p>\n<p>On an aside, I think that on LessWrong, we tend not to work together all that well. The wiki isn't kept bright and shiny, and most of the ideas we search for are in loose blog posts that often take a while to find. However, I think having a single place in which to work together on a specific topic, might encourage effect groups. Especially if it's in a place that you get fairly regular reminders from.</p>\n<p>So, here's a Less Wrong Study Group Facebook Page:&nbsp;https://www.facebook.com/groups/131607983690959/</p>\n<p>Rixie <a title=\"suggested\" href=\"/lw/h4z/anybody_want_to_join_a_math_club/8pta\" target=\"_blank\">suggested</a>&nbsp;that we could split into smaller groups, based on age. I was thinking perhaps ability. Maybe even a group leader. However, before sitting and pondering this for eternity (just until we have a perfect structure), perhaps we should 'just try it'.</p>\n<p>So, who exactly do I think should join the group?<br /><br />Well, if you're interested in learning maths, and think that being surrounded by LWers might enhance your learning, this group is intended for you. If you're interested in learning maths, but you think that reading a textbook on your own is daunting, or you've tried and had difficulty previously, then this group is intended for you.</p>\n<p>Also, if you're interested in learning other LessWrongy subjects (perhaps some cognitive science, or more economics-y stuffs) then this group could do that. If ten people join who want a basic idea economics, then they can work together. This isn't specificly maths, it's whatever we make it.</p>\n<p>Personally, when I read a textbook, there's often a paragraph describing a key idea, and the author's words fly right over my head. I've often thought the best thing for me, would to have someone else who I could talk through that bit with. Maybe he or she would see it easily. Maybe I'd see something they wouldn't get.</p>\n<p>I also wouldn't worry about level of prior knowledge. Mainly, because mine is zilch :)</p>\n<p>So, what are you waiting for?</p>\n<p>(No seriously. <a title=\"Just try it\" href=\"/lw/5a5/no_seriously_just_try_it/\" target=\"_blank\">Just try it</a>.)</p>\n<p>&nbsp;</p>\n<p>Edit: It is true that anonymity is difficult to preserve on Facebook. I am entirely unfamiliar with google, and I certainly would have to make that <a title=\"regular effort\" href=\"/lw/f1/beware_trivial_inconveniences/\" target=\"_blank\">regular effort</a>&nbsp;to check it there too. If you do wish to join but have issues with public knowledge, please PM me, and I'll keep in contact with you through email (or other if you prefer). I will discuss with you there how to best take part in a study group.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oNQnTJSDsMXWZALTD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 19, "extendedScore": null, "score": 1.162851300906203e-06, "legacy": true, "legacyId": "22246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHy3HWKrqXcKkhmct", "Zmfo388RA9oky3KYe", "reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T21:29:33.099Z", "modifiedAt": "2021-02-15T01:22:59.659Z", "url": null, "title": "Problems in Education", "slug": "problems-in-education", "viewCount": null, "lastCommentedAt": "2016-10-06T04:49:40.433Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "ThinkOfTheChildren", "user": {"username": "ThinkOfTheChildren", "createdAt": "2013-04-08T19:31:47.779Z", "isAdmin": false, "displayName": "ThinkOfTheChildren"}, "userId": "nwz7jbhybAPKzt2gJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EuMkZ67vDincGSkYp/problems-in-education", "pageUrlRelative": "/posts/EuMkZ67vDincGSkYp/problems-in-education", "linkUrl": "https://www.lesswrong.com/posts/EuMkZ67vDincGSkYp/problems-in-education", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Problems%20in%20Education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProblems%20in%20Education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuMkZ67vDincGSkYp%2Fproblems-in-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Problems%20in%20Education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuMkZ67vDincGSkYp%2Fproblems-in-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuMkZ67vDincGSkYp%2Fproblems-in-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>Post will be returning in Main, after a rewrite by the company's writing staff. Citations Galore.</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EuMkZ67vDincGSkYp", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 107, "baseScore": 92, "extendedScore": null, "score": 0.00023256968489066235, "legacy": true, "legacyId": "22247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": true, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 322, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-04-08T21:29:33.099Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-08T21:30:24.813Z", "modifiedAt": null, "url": null, "title": "Willing gamblers, spherical cows, and AIs", "slug": "willing-gamblers-spherical-cows-and-ais", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:24.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a7trPRarmwPkZ6d8D/willing-gamblers-spherical-cows-and-ais", "pageUrlRelative": "/posts/a7trPRarmwPkZ6d8D/willing-gamblers-spherical-cows-and-ais", "linkUrl": "https://www.lesswrong.com/posts/a7trPRarmwPkZ6d8D/willing-gamblers-spherical-cows-and-ais", "postedAtFormatted": "Monday, April 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Willing%20gamblers%2C%20spherical%20cows%2C%20and%20AIs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWilling%20gamblers%2C%20spherical%20cows%2C%20and%20AIs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7trPRarmwPkZ6d8D%2Fwilling-gamblers-spherical-cows-and-ais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Willing%20gamblers%2C%20spherical%20cows%2C%20and%20AIs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7trPRarmwPkZ6d8D%2Fwilling-gamblers-spherical-cows-and-ais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7trPRarmwPkZ6d8D%2Fwilling-gamblers-spherical-cows-and-ais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1513, "htmlBody": "<p><em>Note: posting this in Main rather than Discussion in light of recent <a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/8h10\">discussion</a> that people don't post in Main enough and their reasons for not doing so aren't necessarily good ones. But I suspect I may be reinventing the wheel here, and someone else has in fact gotten farther on this problem than I have. If so, I'd be very happy if someone could point me to existing discussion of the issue in the comments.</em></p>\n<p><em>tldr; Gambling-based arguments in the philosophy of probability can be seen as depending on a convenient simplification of assuming people are far more willing to gamble than they are in real life. Some justifications for this simplification can be given, but it's unclear to me how far they can go and where the justification starts to become problematic.</em></p>\n<p>In \"Intelligence Explosion: Evidence and Import,\" Luke and Anna mention the fact that, \"Except for weather forecasters (Murphy and Winkler 1984), and successful professional gamblers, nearly all of us give inaccurate probability estimates...\" When I read this, it struck me as an odd thing to say in a paper on artificial intelligence. I mean, those of us who are not professional accountants tend to make bookkeeping errors, and those of us who are not math, physics, engineering, or&nbsp;economics&nbsp;majors make mistakes on GRE quant questions that we were supposed to have learned how to do in our first two years of high school. Why focus on this <em>particular </em>human failing?</p>\n<p>A related point can be made about <a href=\"http://plato.stanford.edu/entries/dutch-book/\">Dutch Book Arguments</a>&nbsp;in the philosophy of probability. Dutch Book Arguments claim, in a nutshell, that you should reason in accordance with the axioms of probability because if you don't, a clever bookie will be able to take all your money. But another way to prevent a clever bookie from taking all your money is to <em>not gamble. </em>Which many people don't, or at least do rarely.</p>\n<p>Dutch Book Arguments seem to implicitly make what we might call the \"willing gambler assumption\": everyone always has a precise probability assignment for every proposition, and they're willing to take any bet which has a non-negative expected value given their probability assignments. (Or perhaps: everyone is always willing to take at least one side of any proposed bet.) Needless to say, even people who gamble a lot generally aren't <em>that </em>eager to gamble.</p>\n<p>So how does anyone get away with using Dutch Book arguments for anything? A plausible answer comes from a joke Luke recently told in his article on <a href=\"/lw/h5e/fermi_estimates/\">Fermi estimates</a>:</p>\n<blockquote>\n<p>Milk production at a dairy farm was low, so the farmer asked a local university for help. A multidisciplinary team of professors was assembled, headed by a theoretical physicist. After two weeks of observation and analysis, the physicist told the farmer, \"I have the solution, but it only works in the case of spherical cows in a vacuum.\"</p>\n</blockquote>\n<p>If you've studied physics, you know that physicists <em>don't </em>just use those kinds of approximations when doing Fermi estimates; often they can be counted on to yield results that are in fact very close to reality. So maybe the willing gambler assumption works as a sort of spherical cow, that allows philosophers working on issues related to probability to generate important results in spite of the unrealistic nature of the assumption.</p>\n<p>Some parts of how this would work are fairly clear. In real life, bets have transaction costs; they take time and effort to set up and collect. But it doesn't seem too bad to ignore that fact in thought experiments. Similarly, in real life money has declining marginal utility; the utility of doubling your money is less than the disutility of losing your last dollar. In principle, if you know someone's utility function over money, you can take a bet with zero expected value in dollar terms and replace it with a bet that has zero expected value in utility terms. But ignoring that and just using dollars for your thought experiments seems like an acceptable simplification for convenience's sake.</p>\n<p>Even making those assumptions so that it isn't definitely harmful to accept bets with zero expected (dollar) value, we might still wonder why our spherical cow gambler should accept them. Answer: because if necessary you could just add one penny to the side of the bet you want the gambler to take, but always having to mention the extra penny is annoying, so you may as well assume the gambler takes any bet with non-negative expected value rather than require positive expected value.</p>\n<p>Another thing that keeps people from gambling more in real life is the principle that if you can't spot the sucker in the room, it's probably you. If you're unsure whether an offered bet is favorable to you, the mere fact that someone is offering it to you is pretty strong evidence that it's in their favor. One way to avoid this problem is to stipulate that in Dutch Book Arguments, we just assume the bookie doesn't know anything more about whatever the bets are about than the person being offered the bet, and the person being offered the bet knows this. The bookie has to construct her book primarily based on knowing the propensities of the other person to bet. Nick Bostrom explicitly makes such an assumption in <a href=\"http://www.anthropic-principle.com/preprints/beauty/synthesis.pdf\">a paper on the sleeping beauty problem</a>. Maybe other people explicitly make this assumption, I don't know.</p>\n<p>In this last case, though, it's not totally clear whether limiting the bookie's knowledge is all you need to bridge the gap between the willing gambler assumption and how people behave in real life. In real life, people don't often make very exact probability assignments, and may be aware of their confusion about how to make exact probability assignments. Given that, it seems reasonable to hesitate in making bets (even if you ignore transaction costs and declining marginal utility and know that the bookie doesn't know any more about the subject of the bet than you do), because you'd still know&nbsp;the bookie might be trying to exploit your confusion over how to make exact probability assignments.</p>\n<p>At an even simpler level, you might adopt a rule, \"before making multiple bets on related questions, check to make sure you aren't guaranteeing you'll lose money.\" After all, <em>real </em>bookies offer odds such that if anyone was stupid enough to bet on each side of a question with the same bookie, they'd be guaranteed to lose money. In a sense, bookies could be interpreted as \"money pumping\" the public as a whole. But somehow, it turns out that any single individual will rarely be stupid enough to take both sides of the same bet from the same bookie, in spite of the fact that they're apparently irrational enough to be gambling in the first place.</p>\n<p>In the end, I'm confused about how useful the willing gambler assumption really is when doing philosophy of probability. It certainly <em>seems</em> like worthwhile work gets done based on it, but just how applicable are those results to real life? How do we tell when we should reject a result because the willing gambler assumption causes problems in that particular case? I don't know.</p>\n<p>One possible justification for the willing gambler assumption is that even those of us who don't literally gamble, ever, still must make decisions where the outcome is not certain, and we therefore we need to do a decent job of making probability assignments for those situations. But there are lots of people who are successful at their chosen field (including in fields that require decisions with uncertain outcomes) who aren't weather forecasters or professional gamblers, and therefore can be expected to make inaccurate probability estimates. Conversely, it doesn't seem that the skills acquired by successful professional gamblers give them much of an edge in other fields. Therefore, it seems that the relationship between being able to make accurate probability estimates and success in fields that don't specifically require them is weak.</p>\n<p>Another justification for pursing lines of inquiry based on the willing gambler assumption, a justification that will be particularly salient for people on LessWrong, is that if we want to build an AI based on an idealization of how rational agents think (Bayesianism or whatever), we need tools like the willing gambler assumption to figure out how to get the idealization right. That sounds like a plausible thought at first. But if we flawed humans have any hope of building a good AI, it seems like an AI that's as flawed as (but no more flawed than) humans should also have a hope of self-improving into something better. An AI might be programmed in a way that makes it a bad gambler, but aware of this limitation, and left to decide for itself whether, when it self-improves, it wants to focus on improving its gambling ability or improving other aspects of itself.</p>\n<p>As someone who cares a lot about AI, this issue of just how useful various idealizations are for thinking about AI and possibly programming an AI one day are especially important to me. Unfortunately, I'm not sure what to say about them, so at this point I'll turn the question over to the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AeqCtS3BaY3cwzKAs": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a7trPRarmwPkZ6d8D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 25, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "22248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PsEppdvgRisz5xAHG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-09T02:58:58.442Z", "modifiedAt": null, "url": null, "title": "New applied rationality workshops (April, May, and July)", "slug": "new-applied-rationality-workshops-april-may-and-july", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:28.543Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Julia_Galef", "createdAt": "2009-12-20T01:44:38.850Z", "isAdmin": false, "displayName": "Julia_Galef"}, "userId": "qkDSxJnyKhPCJyKdD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jGuT3LTrGP7GtX3ZQ/new-applied-rationality-workshops-april-may-and-july", "pageUrlRelative": "/posts/jGuT3LTrGP7GtX3ZQ/new-applied-rationality-workshops-april-may-and-july", "linkUrl": "https://www.lesswrong.com/posts/jGuT3LTrGP7GtX3ZQ/new-applied-rationality-workshops-april-may-and-july", "postedAtFormatted": "Tuesday, April 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20applied%20rationality%20workshops%20(April%2C%20May%2C%20and%20July)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20applied%20rationality%20workshops%20(April%2C%20May%2C%20and%20July)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGuT3LTrGP7GtX3ZQ%2Fnew-applied-rationality-workshops-april-may-and-july%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20applied%20rationality%20workshops%20(April%2C%20May%2C%20and%20July)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGuT3LTrGP7GtX3ZQ%2Fnew-applied-rationality-workshops-april-may-and-july", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGuT3LTrGP7GtX3ZQ%2Fnew-applied-rationality-workshops-april-may-and-july", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1209, "htmlBody": "<p>In the early days of the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a>, Anna Salamon and I had a disagreement about whether we were ready to run our first applied rationality workshops in six weeks. My inside view said \"No way\"; Her inside view said \"Should be fine\"; My outside view noted that Anna had more relevant experience than I did, and therefore cowed my inside view into grudgingly shutting up.</p>\n<p>It turned out well. Granted, the first couple of workshops were a bit chaotic (hey, sleeping in a dogpile on the living room builds character, amiright May minicampers?). But it's clear in retrospect that we got a lot more value out of diving in than we would have from the extra time spent planning.</p>\n<p>The \"try stuff fast\" habit is responsible for a lot of the techniques in our curriculum; we test out classes on each other and on volunteers, observe \"Oh hey, this helps other people too\" or \"Oh hey, no one else thinks this is useful, turns out I'm just weird,\" and tweak our curriculum accordingly.</p>\n<p>And because we cannot help going recursively meta, we've built a lot of material into our curriculum to make people better at trying things that could make them better at pursuing their goals. Quick, off-the-cuff value of information (VOI) calculations help you decide when it's worth it to spend the time, or money or risk, to try something new. Againstness helps you notice and alleviate the stress responses that can keep you from trying something, once you've noticed that you should. Comfort zone expansion is basically a \"try a bunch of new things\" drill.</p>\n<p>For more details on our curriculum, check out a <a href=\"http://appliedrationality.org/schedule\">sample schedule</a>. I also made a simplified map of some of our classes, so you can see how I think of them fitting into the bigger picture of rationality (<em>click to enlarge</em>):</p>\n<p><a href=\"http://measureofdoubt.files.wordpress.com/2013/04/simplified-map.jpg\"><img style=\"font-size: 16px; font-family: Times; text-align: justify;\" src=\"http://measureofdoubt.files.wordpress.com/2013/04/simplified-map.jpg\" alt=\"\" width=\"700\" height=\"240\" /></a></p>\n<p>To the extent that I've improved my own rationality skills over the last year, I give a lot of credit to \"try stuff fast.\" Like many Less Wrongers I have historically been more of a \"thinking about things\" person than a \"trying stuff fast\" person; given the choice of an afternoon spent debating ignorance priors or one spent figuring out how to improve my public speaking skills, I'd pick the former every time, even though the latter would be more useful to me.</p>\n<p>I'm partially reformed now, thanks in part to the influence of Anna, whom you'll frequently overhear saying things like \"I think I'll try teaching the class as if I were Val\" or \"We should try a different meeting format today, it's high VOI.\" So now I'm much more likely to notice, \"Hey, in this situation I always do X (e.g., ask for feedback later, by email), so this time let me try X-prime (e.g., ask for feedback in person on the spot) -- the cost is low and it's plausible I'll learn that I like it better than my default.\"</p>\n<p>In that spirit, I recommend coming to one of our upcoming <a href=\"http://appliedrationality.org/workshops\">workshops</a> in April, May or July, where you will not only be introduced to all the stuff that we've tried and found promising so far, but will also be plugged into a growing network of several hundred other thoughtful and creative people who have developed their own habits you can borrow and try (we certainly do &ndash; past participants have been the origin of some of our best material). And being surrounded by other people with similar aspirations, during the workshop and in the alumni network afterwards, is the best way I know of to keep your motivation and your discipline strong.</p>\n<p>At $3900, it's an investment, but a low-risk one, since we have a money-back guarantee. If you don't feel like what you got out of it was worth it, we'll refund your money without hesitation or complaint.</p>\n<h2>Here are the basics:</h2>\n<p>You can <a href=\"http://appliedrationality.org/workshops\">apply here</a> for any of our next three applied rationality workshops:</p>\n<ul>\n<li>Friday, April 26 - Monday, April 29</li>\n<li>Friday, May 17 - Monday, May 20</li>\n<li>Saturday, July 20 - Tuesday, July 23</li>\n</ul>\n<p>Each workshop will consist of an immersive four days at a retreat near San Francisco, training you in the art of actually using rationality. That means figuring out what your goals are, and what you can be doing to pursue them more effectively; noticing when you're acting out of habit or impulse; cultivating curiosity about the world and how it works; and learning to use both your intuitive (System 1) and analytical (System 2) thinking systems to their fullest.</p>\n<p>We're soliciting applications not just from Less Wrongers, but from other entrepreneurs, students, teachers, scientists, engineers, activists -- anyone who is analytical, friendly, and motivated to make their own careers, personal lives, and/or societies better. &nbsp;</p>\n<p>For more information on our content, check out our <a href=\"http://appliedrationality.org/workshops\">workshop webpage</a>, our <a href=\"http://appliedrationality.org/checklist\">checklist of rationality habits</a>, or a detailed <a href=\"http://appliedrationality.org/schedule\">sample schedule</a>.</p>\n<p>We're constantly tinkering with our curriculum (as mentioned earlier), and collecting follow-up data on what works well. So while you should be aware that our material hasn't yet been subjected to rigorous long-term studies, our alumni do tend to report that they've gotten a lot of value out of their experience. Here are a few write-ups from Less Wrongers about their CFAR workshop experience and any changes they've made as a result: <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sll\">toner</a>, <a href=\"/lw/fc7/nov_1618_rationality_for_entrepreneurs/7sqa\">palladias</a>, <a href=\"/r/discussion/lw/gid/thoughts_on_the_january_cfar_workshop/\">Qiaochu_Yuan</a>, <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66nz\">thejash</a>, <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/66lv\">BrandonReinhart</a>, <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Fh4p%2Fapplied_rationality_workshops_april_may_and_july%2F&amp;v=1&amp;libid=1365385436485&amp;out=http%3A%2F%2Fmindsarentmagic.wordpress.com%2Fauthor%2Fciphergoth%2F&amp;ref=http%3A%2F%2Flesswrong.com%2Frecentposts%2F&amp;title=Applied%20Rationality%20Workshops%3A%20April%2C%20May%20and%20July%202013%20-%20Less%20Wrong&amp;txt=ciphergoth&amp;jsonp=vglnk_jsonp_13653872828212\">ciphergoth</a>, and <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Fh4p%2Fapplied_rationality_workshops_april_may_and_july%2F&amp;v=1&amp;libid=1365385436485&amp;out=http%3A%2F%2Fwiki.lesswrong.com%2Fwiki%2FRationality_Diary&amp;ref=http%3A%2F%2Flesswrong.com%2Frecentposts%2F&amp;title=Applied%20Rationality%20Workshops%3A%20April%2C%20May%20and%20July%202013%20-%20Less%20Wrong&amp;txt=bunch%20of%20other%20people&amp;jsonp=vglnk_jsonp_13653872933713\">a bunch of other people</a>.</p>\n<p>The total cost is $3900, and that includes:</p>\n<ul>\n<li><strong>Three days of classes</strong> -- Six hours of class a day, with small class sizes (4-6 people) so you get a lot of personal attention from the instructors. We&nbsp;rearrange those small groups several times throughout the workshop to give you a chance to get to know everyone.</li>\n<li><strong>One day of practice</strong> &ndash; Optional but recommended, so instructors can help you make and troubleshoot a plan to use the material going forward. (If you choose to skip this day, the total cost is $3400.)</li>\n<li><strong>Six weeks of personal follow-ups</strong> &ndash; Talk to our staff in one-on-one follow-ups to help you get the most value out of what you've learned.</li>\n<li><strong>Staying on site</strong> &ndash; We rent out lovely retreat centers (lodging and food included in the cost of the workshop) so you can get to know the instructors and other participants in the evenings, during meals, and on breaks. Evenings include everything from unconferences, to parties, to impromptu Rubix-cube lessons.</li>\n<li><strong>An alumni network</strong> -- You'll be included in all future CFAR alumni events, parties, online forums, and so on. We'll make every effort to connect you to alumni from other workshops with whom we think you'll hit it off or have opportunities for collaboration.&nbsp;</li>\n</ul>\n<p><strong>Scholarships</strong><strong>&nbsp;and financial aid</strong> are available -- including for many who thought they wouldn't qualify. &nbsp;So if you're interested in attending, definitely apply, and mention you'd like to be considered for this. We'll set up a call to discuss.</p>\n<p>And please don't hesitate to email me (Julia at appliedrationality dot org). CFAR staff will also be in this comment thread to field questions, and some of the alumni who frequent Less Wrong may be there as well.&nbsp;</p>\n<p><a href=\"http://appliedrationality.org/workshops\">Apply here</a> (the form takes less than 10 minutes, so you should do it now rather than planning on getting to it later!).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 1, "DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jGuT3LTrGP7GtX3ZQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 44, "extendedScore": null, "score": 1.1630868510097919e-06, "legacy": true, "legacyId": "22241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9FfxfaLQN2rRvSjp7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-09T04:10:30.365Z", "modifiedAt": null, "url": null, "title": "How to Evaluate Data?", "slug": "how-to-evaluate-data", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:39.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jetm", "createdAt": "2013-04-09T01:52:29.570Z", "isAdmin": false, "displayName": "jetm"}, "userId": "8J57dSfhRtt9b76GT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mGNsfdG85qaYobtej/how-to-evaluate-data", "pageUrlRelative": "/posts/mGNsfdG85qaYobtej/how-to-evaluate-data", "linkUrl": "https://www.lesswrong.com/posts/mGNsfdG85qaYobtej/how-to-evaluate-data", "postedAtFormatted": "Tuesday, April 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Evaluate%20Data%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Evaluate%20Data%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGNsfdG85qaYobtej%2Fhow-to-evaluate-data%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Evaluate%20Data%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGNsfdG85qaYobtej%2Fhow-to-evaluate-data", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGNsfdG85qaYobtej%2Fhow-to-evaluate-data", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>What I'm trying to figure out is, how to I determine whether a source I'm looking at is telling the truth? For an example, let's take this page from Metamed:&nbsp;<a href=\"http://www.metamed.com/vital-facts-and-statistics\">http://www.metamed.com/vital-facts-and-statistics</a></p>\n<p>At first glance, I see some obvious things I ought to consider. It often gives numbers for how many die in hospitals/year, but for my purposes I ought to interpret it in light of how many hospitals are in the US, as well as how many patients are in each hospital. I also notice that as they are trying to promote their site, they probably selected the data that would best serve that purpose.</p>\n<p>So where do I go from here? Evaluating each source they reference seems like a waste of time. I do not think it would be wrong to trust that they are not actively lying to me. But how do I move from here to an accurate picture of general doctor competence?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mGNsfdG85qaYobtej", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 1.1631359426110846e-06, "legacy": true, "legacyId": "22252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-09T05:43:49.871Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Selecting Rationalist Groups", "slug": "seq-rerun-selecting-rationalist-groups", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tJvNKsygrHoWSj9N5/seq-rerun-selecting-rationalist-groups", "pageUrlRelative": "/posts/tJvNKsygrHoWSj9N5/seq-rerun-selecting-rationalist-groups", "linkUrl": "https://www.lesswrong.com/posts/tJvNKsygrHoWSj9N5/seq-rerun-selecting-rationalist-groups", "postedAtFormatted": "Tuesday, April 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Selecting%20Rationalist%20Groups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Selecting%20Rationalist%20Groups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJvNKsygrHoWSj9N5%2Fseq-rerun-selecting-rationalist-groups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Selecting%20Rationalist%20Groups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJvNKsygrHoWSj9N5%2Fseq-rerun-selecting-rationalist-groups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJvNKsygrHoWSj9N5%2Fseq-rerun-selecting-rationalist-groups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>Today's post, <a href=\"/lw/77/selecting_rationalist_groups/\">Selecting Rationalist Groups</a> was originally published on 02 April 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying to breed e.g. egg-laying chickens by individual selection can produce odd side effects on the farm level, since a more dominant hen can produce more egg mass <em>at the expense of other hens</em>. Group selection is nearly impossible in Nature, but easy to impose in the laboratory, and group-selecting hens produced substantial increases in efficiency. Though most of my essays are about individual rationality - and indeed, Traditional Rationality also praises the lone heretic more than evil Authority - the real effectiveness of \"rationalists\" may end up determined by their performance in groups.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/h5u/seq_rerun_purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tJvNKsygrHoWSj9N5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1631999960438741e-06, "legacy": true, "legacyId": "22253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZEj9ATpv3P22LSmnC", "DzuRwTGYW6ABgrDnN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-09T15:07:10.565Z", "modifiedAt": null, "url": null, "title": "Litany of Instrumentarski", "slug": "litany-of-instrumentarski", "viewCount": null, "lastCommentedAt": "2019-06-14T04:35:45.587Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FEgoiwab5QjYHDzFk/litany-of-instrumentarski", "pageUrlRelative": "/posts/FEgoiwab5QjYHDzFk/litany-of-instrumentarski", "linkUrl": "https://www.lesswrong.com/posts/FEgoiwab5QjYHDzFk/litany-of-instrumentarski", "postedAtFormatted": "Tuesday, April 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Litany%20of%20Instrumentarski&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALitany%20of%20Instrumentarski%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEgoiwab5QjYHDzFk%2Flitany-of-instrumentarski%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Litany%20of%20Instrumentarski%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEgoiwab5QjYHDzFk%2Flitany-of-instrumentarski", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEgoiwab5QjYHDzFk%2Flitany-of-instrumentarski", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">The Litany of Tarski</a> (<a href=\"/lw/jz/the_meditation_on_curiosity/\">formulated by Eliezer, not Tarski</a>)&nbsp;reads</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12.499999046325684px; line-height: 19.04296875px;\">If the box contains a diamond,</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-size: 12.499999046325684px; font-family: Arial, Helvetica, sans-serif; line-height: 19.04296875px;\">I desire to believe that the box contains a diamond;</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-size: 12.499999046325684px; font-family: Arial, Helvetica, sans-serif; line-height: 19.04296875px;\">If the box does not contain a diamond,</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-size: 12.499999046325684px; font-family: Arial, Helvetica, sans-serif; line-height: 19.04296875px;\">I desire to believe that the box does not contain a diamond;</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12.800000190734863px; line-height: 19.037500381469727px;\">Let me not become attached to beliefs I may not want.</span></p>\n<p>This works for a physical realist, but I have been feeling uncomfortable with it for some time now. So I have decided to reformulate it in a more instrumental way, replacing existential statements with testable predictions. I had to find a new name for it, so I call it the Litany of Instrumentarski:</p>\n<p><span style=\"text-decoration: line-through;\"> </span></p>\n<p style=\"padding-left: 30px;\">If believing that there is a diamond in the box lets me find the diamond in the box,</p>\n<p style=\"padding-left: 30px;\">I desire to believe that there is a diamond in the box;</p>\n<p style=\"padding-left: 30px;\">If believing that there is a diamond in the box leaves me with an empty box,</p>\n<p style=\"padding-left: 30px;\">I desire to believe that there is no diamond in the box;</p>\n<p style=\"padding-left: 30px;\">Let me not become attached to inaccurate beliefs.</p>\n<p>Posting it here in a hope that someone else also finds it more palatable and unassuming than straight-up realism.&nbsp;</p>\n<p>EDIT: It seems to me that this modification also guides you to straight-up one-box on Newcomb, where the original one is mired in the EDT vs CDT issues.</p>\n<p>EDIT2: Looks like the above version resulting in people confusing desiring accurate beliefs with desiring diamonds. It's about accurate accounting, not about utility of a certain form of crystallized carbon.</p>\n<p>Maybe the first line should be modified to something like \"If I <strong>later</strong> find a diamond in the box...\", or something. How about the following?</p>\n<p style=\"padding-left: 30px;\">If I will find a diamond in the box,</p>\n<p style=\"padding-left: 30px;\">I desire to believe that I will find a diamond in the box;</p>\n<p style=\"padding-left: 30px;\">If I will find no diamond in the box,</p>\n<p style=\"padding-left: 30px;\">I desire to believe that&nbsp;I will find no diamond in the box;</p>\n<p style=\"padding-left: 30px;\">Let me not become attached to inaccurate beliefs.</p>\n<p>For some reason the editor does not let me use the &lt;strike&gt; tag to cross out the previous version, not sure how to work around it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FEgoiwab5QjYHDzFk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "22257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3nZMgRTfFEfHp34Gb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-09T23:33:29.127Z", "modifiedAt": null, "url": null, "title": "Explicit and tacit rationality", "slug": "explicit-and-tacit-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:26.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NLJ6NyHFZPJ2oNSZ8/explicit-and-tacit-rationality", "pageUrlRelative": "/posts/NLJ6NyHFZPJ2oNSZ8/explicit-and-tacit-rationality", "linkUrl": "https://www.lesswrong.com/posts/NLJ6NyHFZPJ2oNSZ8/explicit-and-tacit-rationality", "postedAtFormatted": "Tuesday, April 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Explicit%20and%20tacit%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExplicit%20and%20tacit%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLJ6NyHFZPJ2oNSZ8%2Fexplicit-and-tacit-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Explicit%20and%20tacit%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLJ6NyHFZPJ2oNSZ8%2Fexplicit-and-tacit-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLJ6NyHFZPJ2oNSZ8%2Fexplicit-and-tacit-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1337, "htmlBody": "<p><a href=\"http://web.archive.org/web/20010205221413/http://sysopmind.com/eliezer.html\">Like Eliezer</a>, I \"do my best thinking into a keyboard.\" It starts with a <a href=\"http://wiki.lesswrong.com/wiki/Curiosity\">burning itch</a> to figure something out. I collect ideas and arguments and evidence and sources. I arrange them, tweak them, criticize them. I explain it all in my own words so I can understand it better. By then it is nearly something that others would want to read, so I clean it up and publish, say, <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>. I write <a href=\"http://www.paulgraham.com/essay.html\">essays</a> in the <a href=\"http://is.gd/FaHjkf\">original sense</a> of the word: \"attempts.\"</p>\n<p>This time, I'm trying to figure out something we might call \"tacit rationality\" (c.f. <a href=\"http://en.wikipedia.org/wiki/Tacit_knowledge\">tacit knowledge</a>).</p>\n<p>I tried and failed to write a <em>good</em> post about tacit rationality, so I wrote a <em>bad</em> post instead &mdash; one that is basically a patchwork of somewhat-related musings on explicit and tacit rationality. Therefore I'm posting this article to LW Discussion. I hope the ensuing discussion ends up leading somewhere with more clarity and usefulness.</p>\n<p>&nbsp;</p>\n<h3>Three methods for training rationality</h3>\n<p>Which of these three options do you think will train rationality (i.e. <a href=\"/lw/7i/rationality_is_systematized_winning/\">systematized winning</a>, or \"winning-rationality\") most effectively?</p>\n<ol>\n<li>Spend one year reading and re-reading <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a></em>, studying the math and cognitive science of rationality, and discussing rationality online and at Less Wrong meetups.</li>\n<li>Attend a <a href=\"http://appliedrationality.org/workshops/\">CFAR workshop</a>, then spend the next year practicing those skills and <a href=\"/lw/fc3/checklist_of_rationality_habits/\">other rationality habits</a> every week.</li>\n<li>Run a startup or small business for one year.</li>\n</ol>\n<p>Option 1 seems to be pretty effective at training people to talk intelligently <em>about</em> rationality (let's call that \"talking-rationality\"), and it seems to inoculate people against some common philosophical mistakes.</p>\n<p>We don't yet have any examples of someone doing Option 2 (the first CFAR workshop was May 2012), but I'd expect Option 2 &mdash; if actually executed &mdash; to result in more winning-rationality than Option 1, and also a modicum of talking-rationality.</p>\n<p>What about Option 3? Unlike Option 2 or especially Option 1, I'd expect it to train almost no ability to talk intelligently about rationality. But I <em>would</em> expect it to result in relatively good winning-rationality, due to its tight feedback loops.</p>\n<p>&nbsp;</p>\n<h3>Talking-rationality and winning-rationality can come apart</h3>\n<blockquote>\n<p>I've come to believe... that the best way to succeed is to discover what you love and then find a way to offer it to others in the form of service, working hard, and also allowing the energy of the universe to lead you.</p>\n</blockquote>\n<p align=\"right\"><a href=\"http://www.quotationspage.com/quote/33464.html\">Oprah Winfrey</a></p>\n<p>Oprah isn't known for being a rational thinker. She is a known <a href=\"http://www.thedailybeast.com/newsweek/2009/05/29/live-your-best-life-ever.print.html\">peddler of pseudoscience</a>, and she attributes her success (in part) to allowing \"the energy of the universe\" to lead her.</p>\n<p>Yet she must be doing <em>something</em> right. Oprah is a true rags-to-riches story. Born in Mississippi to an unwed teenage housemaid, she was so poor she wore dresses made of potato sacks. She was molested by a cousin, an uncle, and a family friend. She became pregnant at age 14.</p>\n<p>But in high school she became an honors student, won oratory contests and a beauty pageant, and was hired by a local radio station to report the news. She became the youngest-ever news anchor at Nashville's WLAC-TV, then hosted several shows in Baltimore, then moved to Chicago and within months her own talk show shot from last place to first place in the ratings there. Shortly afterward her show went national. She also produced and starred in several TV shows, was nominated for an Oscar for her role in a Steven Spielberg movie, launched her own TV cable network and her own magazine (the \"most successful startup ever in the [magazine] industry\" according to <em><a href=\"http://money.cnn.com/magazines/fortune/fortune_archive/2002/04/01/320634/\">Fortune</a></em>), and became the world's first female black billionaire.</p>\n<p>I'd like to suggest that Oprah's climb probably didn't come <em>merely</em> through inborn talent, hard work, and luck. To get from potato sack dresses to the Forbes billionaire list, Oprah had to make thousands of pretty good decisions. She had to make pretty accurate guesses about the likely consequences of various actions she could take. When she was wrong, she had to correct course fairly quickly. In short, she had to be fairly <em>rational</em>, at least in some domains of her life.</p>\n<p>Similarly, I know plenty of business managers and entrepreneurs who have a steady track record of good decisions and wise judgments, and yet they are religious, or they commit basic errors in logic and probability when they talk about non-business subjects.</p>\n<p>What's going on here? My guess is that successful entrepreneurs and business managers and other people must have pretty good <em>tacit rationality</em>, even if they aren't very proficient with the \"rationality\" concepts that Less Wrongers tend to discuss on a daily basis. Stated another way, successful businesspeople make fairly rational decisions and judgments, even though they may confabulate rather silly <em>explanations</em> for their success, and even though they don't understand the math or science of rationality well.</p>\n<p>LWers can probably outperform Mark Zuckerberg on the CRT and the Berlin Numeracy Test, but Zuckerberg is laughing at them from atop a huge pile of utility.</p>\n<p>&nbsp;</p>\n<h3>Explicit and tacit rationality</h3>\n<p>Patri Friedman, in <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality</a>, reminded us that skill acquisition comes from <a href=\"https://www.google.com/search?q=%22deliberate+practice%22&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">deliberate practice</a>, and reading LW is a \"shiny distraction,\" not deliberate practice. He said a <em>real</em> rationality practice would look more like... well, what Patri describes <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/8q54\">is basically CFAR</a>, though CFAR didn't exist at the time.</p>\n<p>In response, and again long before CFAR existed, Anna Salamon wrote <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">Goals for which Less Wrong does (and doesn't) help</a>. Summary: Some domains provide rich, cheap feedback, so you don't need much LW-style rationality to become successful in those domains. But many of us have goals in domains that don't offer rapid feedback: e.g. whether to buy cryonics, which 40-year investments are safe, which metaethics to endorse. For this kind of thing you need LW-style rationality. (We could also state this as \"Domains with rapid feedback train tacit rationality with respect to those domains, but for domains without rapid feedback you've got to do the best you can with LW-style \"explicit rationality\".)</p>\n<p>The good news is that you should be able to combine explicit and tacit rationality. Explicit rationality can help you realize that you should force tight feedback loops into whichever domains you want to succeed in, so that you can have develop good intuitions about how to succeed in those domains. (See also: <a href=\"http://www.amazon.com/The-Lean-Startup-Entrepreneurs-Continuous/dp/0307887898/\">Lean Startup</a> or <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">Lean Nonprofit</a> methods.)</p>\n<p>Explicit rationality could also help you realize that the cognitive biases most-discussed in the literature aren't necessarily the ones you should focus on ameliorating, as Aaron Swartz <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/75an\">wrote</a>:</p>\n<blockquote>\n<p>Cognitive biases cause people to make choices that are <em>most obviously</em> irrational, but not <em>most importantly</em> irrational... Since cognitive biases are the primary focus of research into rationality, rationality tests mostly measure how good you are at avoiding them... LW readers tend to be fairly good at avoiding cognitive biases... But there a whole series of much more important irrationalities that LWers suffer from. (Let's call them \"practical biases\" as opposed to \"cognitive biases,\" even though both are ultimately practical and cognitive.)</p>\n<p>...Rationality, properly understood, is in fact a predictor of success. Perhaps if LWers used success as their metric (as opposed to getting better at avoiding obvious mistakes), they might focus on their most important irrationalities (instead of their most obvious ones), which would lead them to be more rational and more successful.</p>\n</blockquote>\n<h3><br /></h3>\n<h3>Final scattered thoughts</h3>\n<ul>\n<li>If someone is consistently winning, and not just because they have tons of wealth or fame, then maybe you should conclude they have pretty good tacit rationality even if their explicit rationality is terrible. </li>\n<li>The positive effects of tight feedback loops might trump the effects of explicit rationality training. </li>\n<li>Still, I suspect explicit rationality <em>plus</em> tight feedback loops could lead to the best results of all. </li>\n<li>I really hope we can develop a real <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/8q54\">rationality dojo</a>. </li>\n<li>If you're reading this post, you're probably spending too <em>much</em> time reading Less Wrong, and too <em>little</em> time <a href=\"http://www.amazon.com/The-Motivation-Hacker-ebook/dp/B00C8N4FNK/\">hacking your motivation system</a>, <a href=\"/lw/5p6/how_and_why_to_granularize/\">learning social skills</a>, and <a href=\"http://www.amazon.com/The-Lean-Startup-Entrepreneurs-Continuous/dp/0307887898/\">learning</a> how to inject tight feedback loops into everything you can.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fR7QfYx4JA3BnptT9": 2, "Ng8Gice9KNkncxqcj": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NLJ6NyHFZPJ2oNSZ8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 60, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "22259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://web.archive.org/web/20010205221413/http://sysopmind.com/eliezer.html\">Like Eliezer</a>, I \"do my best thinking into a keyboard.\" It starts with a <a href=\"http://wiki.lesswrong.com/wiki/Curiosity\">burning itch</a> to figure something out. I collect ideas and arguments and evidence and sources. I arrange them, tweak them, criticize them. I explain it all in my own words so I can understand it better. By then it is nearly something that others would want to read, so I clean it up and publish, say, <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>. I write <a href=\"http://www.paulgraham.com/essay.html\">essays</a> in the <a href=\"http://is.gd/FaHjkf\">original sense</a> of the word: \"attempts.\"</p>\n<p>This time, I'm trying to figure out something we might call \"tacit rationality\" (c.f. <a href=\"http://en.wikipedia.org/wiki/Tacit_knowledge\">tacit knowledge</a>).</p>\n<p>I tried and failed to write a <em>good</em> post about tacit rationality, so I wrote a <em>bad</em> post instead \u2014 one that is basically a patchwork of somewhat-related musings on explicit and tacit rationality. Therefore I'm posting this article to LW Discussion. I hope the ensuing discussion ends up leading somewhere with more clarity and usefulness.</p>\n<p>&nbsp;</p>\n<h3 id=\"Three_methods_for_training_rationality\">Three methods for training rationality</h3>\n<p>Which of these three options do you think will train rationality (i.e. <a href=\"/lw/7i/rationality_is_systematized_winning/\">systematized winning</a>, or \"winning-rationality\") most effectively?</p>\n<ol>\n<li>Spend one year reading and re-reading <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a></em>, studying the math and cognitive science of rationality, and discussing rationality online and at Less Wrong meetups.</li>\n<li>Attend a <a href=\"http://appliedrationality.org/workshops/\">CFAR workshop</a>, then spend the next year practicing those skills and <a href=\"/lw/fc3/checklist_of_rationality_habits/\">other rationality habits</a> every week.</li>\n<li>Run a startup or small business for one year.</li>\n</ol>\n<p>Option 1 seems to be pretty effective at training people to talk intelligently <em>about</em> rationality (let's call that \"talking-rationality\"), and it seems to inoculate people against some common philosophical mistakes.</p>\n<p>We don't yet have any examples of someone doing Option 2 (the first CFAR workshop was May 2012), but I'd expect Option 2 \u2014 if actually executed \u2014 to result in more winning-rationality than Option 1, and also a modicum of talking-rationality.</p>\n<p>What about Option 3? Unlike Option 2 or especially Option 1, I'd expect it to train almost no ability to talk intelligently about rationality. But I <em>would</em> expect it to result in relatively good winning-rationality, due to its tight feedback loops.</p>\n<p>&nbsp;</p>\n<h3 id=\"Talking_rationality_and_winning_rationality_can_come_apart\">Talking-rationality and winning-rationality can come apart</h3>\n<blockquote>\n<p>I've come to believe... that the best way to succeed is to discover what you love and then find a way to offer it to others in the form of service, working hard, and also allowing the energy of the universe to lead you.</p>\n</blockquote>\n<p align=\"right\"><a href=\"http://www.quotationspage.com/quote/33464.html\">Oprah Winfrey</a></p>\n<p>Oprah isn't known for being a rational thinker. She is a known <a href=\"http://www.thedailybeast.com/newsweek/2009/05/29/live-your-best-life-ever.print.html\">peddler of pseudoscience</a>, and she attributes her success (in part) to allowing \"the energy of the universe\" to lead her.</p>\n<p>Yet she must be doing <em>something</em> right. Oprah is a true rags-to-riches story. Born in Mississippi to an unwed teenage housemaid, she was so poor she wore dresses made of potato sacks. She was molested by a cousin, an uncle, and a family friend. She became pregnant at age 14.</p>\n<p>But in high school she became an honors student, won oratory contests and a beauty pageant, and was hired by a local radio station to report the news. She became the youngest-ever news anchor at Nashville's WLAC-TV, then hosted several shows in Baltimore, then moved to Chicago and within months her own talk show shot from last place to first place in the ratings there. Shortly afterward her show went national. She also produced and starred in several TV shows, was nominated for an Oscar for her role in a Steven Spielberg movie, launched her own TV cable network and her own magazine (the \"most successful startup ever in the [magazine] industry\" according to <em><a href=\"http://money.cnn.com/magazines/fortune/fortune_archive/2002/04/01/320634/\">Fortune</a></em>), and became the world's first female black billionaire.</p>\n<p>I'd like to suggest that Oprah's climb probably didn't come <em>merely</em> through inborn talent, hard work, and luck. To get from potato sack dresses to the Forbes billionaire list, Oprah had to make thousands of pretty good decisions. She had to make pretty accurate guesses about the likely consequences of various actions she could take. When she was wrong, she had to correct course fairly quickly. In short, she had to be fairly <em>rational</em>, at least in some domains of her life.</p>\n<p>Similarly, I know plenty of business managers and entrepreneurs who have a steady track record of good decisions and wise judgments, and yet they are religious, or they commit basic errors in logic and probability when they talk about non-business subjects.</p>\n<p>What's going on here? My guess is that successful entrepreneurs and business managers and other people must have pretty good <em>tacit rationality</em>, even if they aren't very proficient with the \"rationality\" concepts that Less Wrongers tend to discuss on a daily basis. Stated another way, successful businesspeople make fairly rational decisions and judgments, even though they may confabulate rather silly <em>explanations</em> for their success, and even though they don't understand the math or science of rationality well.</p>\n<p>LWers can probably outperform Mark Zuckerberg on the CRT and the Berlin Numeracy Test, but Zuckerberg is laughing at them from atop a huge pile of utility.</p>\n<p>&nbsp;</p>\n<h3 id=\"Explicit_and_tacit_rationality\">Explicit and tacit rationality</h3>\n<p>Patri Friedman, in <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality</a>, reminded us that skill acquisition comes from <a href=\"https://www.google.com/search?q=%22deliberate+practice%22&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">deliberate practice</a>, and reading LW is a \"shiny distraction,\" not deliberate practice. He said a <em>real</em> rationality practice would look more like... well, what Patri describes <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/8q54\">is basically CFAR</a>, though CFAR didn't exist at the time.</p>\n<p>In response, and again long before CFAR existed, Anna Salamon wrote <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">Goals for which Less Wrong does (and doesn't) help</a>. Summary: Some domains provide rich, cheap feedback, so you don't need much LW-style rationality to become successful in those domains. But many of us have goals in domains that don't offer rapid feedback: e.g. whether to buy cryonics, which 40-year investments are safe, which metaethics to endorse. For this kind of thing you need LW-style rationality. (We could also state this as \"Domains with rapid feedback train tacit rationality with respect to those domains, but for domains without rapid feedback you've got to do the best you can with LW-style \"explicit rationality\".)</p>\n<p>The good news is that you should be able to combine explicit and tacit rationality. Explicit rationality can help you realize that you should force tight feedback loops into whichever domains you want to succeed in, so that you can have develop good intuitions about how to succeed in those domains. (See also: <a href=\"http://www.amazon.com/The-Lean-Startup-Entrepreneurs-Continuous/dp/0307887898/\">Lean Startup</a> or <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">Lean Nonprofit</a> methods.)</p>\n<p>Explicit rationality could also help you realize that the cognitive biases most-discussed in the literature aren't necessarily the ones you should focus on ameliorating, as Aaron Swartz <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/75an\">wrote</a>:</p>\n<blockquote>\n<p>Cognitive biases cause people to make choices that are <em>most obviously</em> irrational, but not <em>most importantly</em> irrational... Since cognitive biases are the primary focus of research into rationality, rationality tests mostly measure how good you are at avoiding them... LW readers tend to be fairly good at avoiding cognitive biases... But there a whole series of much more important irrationalities that LWers suffer from. (Let's call them \"practical biases\" as opposed to \"cognitive biases,\" even though both are ultimately practical and cognitive.)</p>\n<p>...Rationality, properly understood, is in fact a predictor of success. Perhaps if LWers used success as their metric (as opposed to getting better at avoiding obvious mistakes), they might focus on their most important irrationalities (instead of their most obvious ones), which would lead them to be more rational and more successful.</p>\n</blockquote>\n<h3><br></h3>\n<h3 id=\"Final_scattered_thoughts\">Final scattered thoughts</h3>\n<ul>\n<li>If someone is consistently winning, and not just because they have tons of wealth or fame, then maybe you should conclude they have pretty good tacit rationality even if their explicit rationality is terrible. </li>\n<li>The positive effects of tight feedback loops might trump the effects of explicit rationality training. </li>\n<li>Still, I suspect explicit rationality <em>plus</em> tight feedback loops could lead to the best results of all. </li>\n<li>I really hope we can develop a real <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/8q54\">rationality dojo</a>. </li>\n<li>If you're reading this post, you're probably spending too <em>much</em> time reading Less Wrong, and too <em>little</em> time <a href=\"http://www.amazon.com/The-Motivation-Hacker-ebook/dp/B00C8N4FNK/\">hacking your motivation system</a>, <a href=\"/lw/5p6/how_and_why_to_granularize/\">learning social skills</a>, and <a href=\"http://www.amazon.com/The-Lean-Startup-Entrepreneurs-Continuous/dp/0307887898/\">learning</a> how to inject tight feedback loops into everything you can.</li>\n</ul>", "sections": [{"title": "Three methods for training rationality", "anchor": "Three_methods_for_training_rationality", "level": 1}, {"title": "Talking-rationality and winning-rationality can come apart", "anchor": "Talking_rationality_and_winning_rationality_can_come_apart", "level": 1}, {"title": "Explicit and tacit rationality", "anchor": "Explicit_and_tacit_rationality", "level": 1}, {"title": "Final scattered thoughts", "anchor": "Final_scattered_thoughts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "77 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RWo4LwFzpHNQCTcYt", "4ARtkT3EYox3THYjF", "ttGbpJQ8shBi8hDhh", "uFYQaGCRwt3wKtyZP", "7dRGYDqA2z6Zt7Q4h", "jTk9m75y2bpujwRfb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-10T00:59:48.204Z", "modifiedAt": null, "url": null, "title": "Taking Charity Seriously: Toby Ord talk on charity effectivess", "slug": "taking-charity-seriously-toby-ord-talk-on-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:55.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iCa9JXAXAuCzuv3DT/taking-charity-seriously-toby-ord-talk-on-charity", "pageUrlRelative": "/posts/iCa9JXAXAuCzuv3DT/taking-charity-seriously-toby-ord-talk-on-charity", "linkUrl": "https://www.lesswrong.com/posts/iCa9JXAXAuCzuv3DT/taking-charity-seriously-toby-ord-talk-on-charity", "postedAtFormatted": "Wednesday, April 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taking%20Charity%20Seriously%3A%20Toby%20Ord%20talk%20on%20charity%20effectivess&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaking%20Charity%20Seriously%3A%20Toby%20Ord%20talk%20on%20charity%20effectivess%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCa9JXAXAuCzuv3DT%2Ftaking-charity-seriously-toby-ord-talk-on-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taking%20Charity%20Seriously%3A%20Toby%20Ord%20talk%20on%20charity%20effectivess%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCa9JXAXAuCzuv3DT%2Ftaking-charity-seriously-toby-ord-talk-on-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCa9JXAXAuCzuv3DT%2Ftaking-charity-seriously-toby-ord-talk-on-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10978, "htmlBody": "<p>Video: (<a href=\"http://youtu.be/iGCVRA7T7FE\">youtube</a>)</p>\n<p>Transcript:</p>\n<blockquote>I'm Toby Ord.  I'm here today from Oxford University where           I'm a Research Fellow in Philosophy, and also I'm the           president of an organization called <a href=\"http://givingwhatwecan.org/\">Giving What We Can</a>.\n<p>I should say as well that I haven't always been a           philosopher.  I used to be a computer scientist beforehand,           but I decided I was interested in logic and things like           these, which at my university, the Philosophy Department was           mainly in charge of.  I spent more and more time over there           and I was also very interested in a whole lot of ethical           questions and ended up being sucked away from Computer           Science to the world of Philosophy.  But, hopefully, you'll           see that I've got an approach to some of these philosophical           or ethical questions which is perhaps more quantitative than           you are used to seeing from people who looked at these           things.  So, let's start.</p>\n<p>I have always been interested in charity and helping other           people and in particular, when I was an undergraduate, I           used to get into conversations through the night with my           friends.  And they would say to me, you know, in some heated           political debates, say something, like, \"if you think that,           why don't you send all your money to help people starving in           Africa?\", or some other cliche like this. This was meant to           be a reduction of my position, that you can't possibly think           that because you'd have to spend a lot of your time and           effort helping these other people.  But I started to treat           it more like, \"maybe I should be doing this\".  Maybe I           should be thinking about what I'm doing with my income and           maybe actually that is a logical conclusion.  If you think           all people are equal and so that you can help people in poor           counties for less, maybe I should be doing something like           that.  Maybe I should investigate it.</p>\n<p>I noticed a couple of philosophers have written on this.           Peter Singer has very good papers on this: <a href=\"http://www.utilitarian.net/singer/by/1972----.htm&quot;\">Famine,           Affluence, and Morality</a>, for anyone who wants to look           that up.  There are some very strong arguments so I           took this seriously and started to think about what I could           do in my life if I really wanted to help people who are less           fortunate than myself.</p>\n<p>When I was thinking about this, I divided the different ways           I could think of helping up into these four categories.  One           is voluntary work, one was my career, so what good could           come through my career.  Another one is the personal           relationships that I have.  And another one was donations.           I thought that [donations are] something that a lot of           people neglect as a way that they can have a lot of impact           in the world.  It was also the thing that was most amendable           to analysis.  So, I decided I wanted to work out what, in           terms of donations, could I do?  If I really wanted to, what           could I achieve?  And that's the topic of this talk really.</p>\n<p>I worked out that over my career, I'd be able to earn           about one and a half million pounds, and I encourage all of           you to try to work out what you'll earn of your career.  I           think this is an informative thing to do.  Particularly as           you can work out then what you could achieve if you really           wanted to.  And it's nice to know that through donations.           It might be that you can achieve even more through your           career.</p>\n<p>I worked out that I only really needed about 500,000 pounds           over my life if I can kind of kept on my current lifestyle           which was (and is) very good.  At that point, I was a           graduate student and I had pretty much all the things I           really needed in my life. I have a lovely wife and great           friends, wonderful conversations, get to see beautiful           places and travel to Europe once or twice a year on           that&mdash;living in the UK is a nice advantage though           [laughter].</p>\n<p>But, you know, I can take some travel and listen to some,           you know, the best music ever recorded and the things that I           can do would rival a lot of things that a king could do a           century ago; we've got it pretty good in the world today.           And so I thought \"actually I have a lot of these things           which are a lot of value and if I spent this extra million           pounds on myself, it wouldn't actually make my life much           better\" because the things which are of highest value in           your life you get first and then you get less and less           effective things added on later.  And so I thought actually,           you know, I could probably do that if I really wanted and           carry on living at alightly above that kind of grad           student lifestyle and donate about a million           pounds.</p>\n<p>So I thought, well, what can I do with that?  Once you start           to get a big number like this, it's worth putting in more           effort.  What can I do with the million pounds as opposed           to, is this 10 pounds I'm donating for this fun run, is that           effective, or is this other thing effective.  If it's on           that kind of level.  (I should use dollars; I'll try.  A           pound is one and a half dollars.) So, try to think about           these things and if it is just these very small sums, it's           not really worth doing the analysis as to what's going on.           But if you realize that over your life, you can do a lot           with a million pounds, I mean, if you are spending a million           pounds on house or something, you'd put in some time doing           some research on that because you might be able to get, say           a house that's, you know, 10% better, you know, it's a good           deal.  You know, you spend 900,000 pounds for house that           would be worth a million pounds or something like that.</p>\n<p>So, I want to think about that kind of thing when it comes           to charity and trying to get a good deal.  There's many ways           that I could help people with my money.  For example,           education, empowerment of disempowered groups such as women           in all countries or promoting peace.  I mean, the list is           very long.  I'm going to focus on health in this talk           because there's some really good evidence about health and           so you can get quantitative and get into the nitty-gritty.           And I also think that it's plausible that health in poor           countries is one of the best ways that we can help people           without money.  If you think that actually one           of these other things is even better than health, that's           great because it means this is a lower bound for how much           you can achieve in your life.</p>\n<p>The first question here is, what metric should you use to try           to work out what you can achieve with a certain amount of           money?  One common thing that's talked about is the           percentage spent on overhead and administration.            You often hear something where they say that only 10% of           this origination's budget is spent on administration.  I           don't think that that's a very useful metric because what a           charity does is much more important than how much it spends           on these.  There's a lot of variety between charities in           terms of what they fund, and I think the key question           instead is for a given donation how much benefit do people           receive?</p>\n<p>It could be, for example, that if an organization spends its           money on program evaluation which is checking to see after           they try something how well it worked.  Suppose they build           some wells in Kenya.  If they spend some money to go back           and find out whether those wells are being used or whether           they're still working 10 years later, try to find out the           lifespan of the well to work out how good a project it was           to compare it with other projects, that counts as overhead.           It also the exact type of thing that organizations should be           doing, so the issue is that overhead money is not 'wasted'.           Even if it were wasted, the difference of 10% or 20% or           something like that between charities, as you'll see later           in this talk, is actually very small.  The differences get           much bigger than that and so this would be relatively bad           way of assessing things.  There's a lot of research that can           help us answer this question of \"for a given donation how           much benefit the people receive?\"  Most people aren't aware           of it and the greatest difference here comes from the talk           of intervention performed.</p>\n<p>For example, you could distribute malaria nets to help           prevent children getting malaria, distribute some medication           to limit the effect of HIV on your system, distribute           condoms to try to avoid new people getting this, build           wells, or provide sanitation systems such as toilets and           sewers: there's a whole lot of these different things you           could be doing with money and some of them are much more           effective than others, as we'll see.</p>\n<p>Just as an example to motivate this, it's quite easy to           understand.  If you're providing someone with a guide dog to           try to combat the bad effects of blindness, this costs about           $40,000.  In contrast, it costs about $20 to completely cure           someone with blindness caused by trachoma.  If for example,           if you wanted to donate $40,000 in order to fight blindness,           you could either provide a single guide dog or you could           completely cure 3,000 people.  There's a very big difference           there.  Presumably also, having a guide dog is less good           than&mdash;well, being blind is less good than having your           blindness cured as well. It seems like in that case,           there's something of a factor of 2,000 or so between           these different types of interventions.  And this type of           thing does come up quite a lot; this is not just an           aberrant case.</p>\n<p>But you might wonder, how can we take this idea further?           This is very much an apples to apples comparison, but in           many cases, you won't be able to do that.  It will be, say,           curing blindness as opposed to saving someone from dying of           AIDS.  They're quite different; it's hard to compare           them.  We want some way to broaden the universe of           interventions which can be compared with each other in order           to work out what's most effective within that universal           possibilities.  How can we compare apples and oranges?</p>\n<p>Here are a couple of ways you might try to do that.  The           first one you might think of as: \"how many lives can we           save?\"  And that was the subtitle, I think, of this talk.           That's the type of question which is interesting because I           think most people don't ask that.  They might hear something           like, \"it only costs a thousand dollars to save a life with           charity,\" and they might respond to that by saying \"well,           okay, here's a thousand dollars.\"  But they don't tend to           say, \"well, hang on a second, if you'd actually save           someone's life literally for a thousand dollars, maybe, I           should give you another thousand and another thousand!\" I           mean, if there was someone on the streets who you met who           was dying you would take this more seriously in a certain           way.  What I'm trying to think of is you try and take these           ideas about what we can do more seriously, to look at the           hard figures for it and then to try to take them seriously.</p>\n<p>Now, a big problem with using \"how many lives can we save\"           as a metric, is that there's this truism public health that           \"no one's ever saved a life\" because the person tends to die           later, right?  We don't&mdash;unless, you would have become           immortal, that would be really be saving someone's life in           that sense.  And that'd be pretty good.  Actually, I           would recommend funding that if that was an           option.&mdash;But what you're more talking about is helping           someone who'll still die, it's just they die at a           different time.  And it's not just at a different time,           right?  Because it means that you have more life, right?           The distance between your birth and your death is longer and           you get more years of life in which to do things that are           good for you.  So that's the first issue, is that we can           extend lives and that's what we're really talking about.           So you might replace that with a question and say, \"how           many life years can we save?\", and I think that's a much           better question.</p>\n<p>But there still is this bit about \"what's the quality of           that life?\"  And you might imagine a case where, say, you're           in hospital, and you've been diagnosed with a form of           cancer, and you are given an option of a treatment for that           cancer which would mean that you will live longer.  But that           the quality of life will be much lower; maybe it's a very           invasive chemotherapy.  It's not obvious that you should           take that.  I mean, if you had ten years of life at low           quality versus nine years at high quality.  Quite possibly,           nine years would be better.  So there's cases where you           want to trade off between quality and quantity, and it would           be foolish to just use the quantity here.</p>\n<p>There are also cases where you want to compare non           life-extending treatments.  For example, if you cure someone           of blindness, actually that does have a bit of an impact in           how long they live, they live a bit longer if they're cured           the blindness.  But if we abstract that away, you can see           there's also a lot of the benefit comes from turning normal           years with where you're blind into years where you're           sighted, and that's a lot of a benefit.  So, you want to be           able to capture both of those things.  The way that people           in public health, health economics, and practical ethics           tend to think about this is in terms of <a href=\"http://en.wikipedia.org/wiki/Quality-adjusted_life_year\">quality-adjusted           life years</a>.  This takes into account both the           quality and the quantity, and it's also known as QALYs.  So,           I'm going to use that term a bit, quality-adjusted life           years or QALYs.  Now, to explain exactly what that is, I've           got a slide here:</p>\n<p><img src=\"http://www.jefftk.com/ord/the-qaly-approach.png\" alt=\"\" /></p>\n<p>This is a schematic of the health-related components of           someone's life.  And you can see here that it starts at a           hundred percent quality of life.  It could start lower than           that; it's just sketching something.  You can see that their           life goes down in quality that they perhaps become sick           during the early part, and then they recover, and then           there's sickness in middle age which is also recovered from,           and then various stages of gradual decline towards death at           about 75 years old.  And you could imagine a curve like this           for someone, maybe actually it would saturate a lot more.           You know, you get up in the morning, you stub your toe while           you get into the shower, and you drop, you know, to zero           quality of life or something, or maybe negative.  And then           you bounce back up again and various things.  You can think           about your overall quality of life.</p>\n<p>This is just meant to be health-related quality of life, but           you get the basic idea.  It's a measure of how well you're           doing and maybe, a hundred percent, maybe, you could get           about that.  But in terms of health-related quality of life,           we tend to think a hundred percent of someone who's           perfectly healthy at that time.  And zero is someone who's           health-related quality is equivalent to being dead.  So           maybe they'd be indifferent between being at that quality or           being dead.  Maybe being in a coma is a good example of zero           or maybe being in a level of pain which is so intense.           Maybe there's levels of pain so intense that below zero, and           there's some levels such that a bit above that and they're           at zero.  That's the basic idea.</p>\n<p>And then you can see that to apply it in           practice, you could imagine kind of stylized version of this           question.  Well, we have two lives here, which are, I think,           both lived until 60 years at 70 percent quality:</p>\n<p><img src=\"http://www.jefftk.com/ord/two-equal-lives.png\" alt=\"\" /></p>\n<p>Here's two different ways you could improve that           life, maybe two different health interventions:</p>\n<p><img src=\"http://www.jefftk.com/ord/two-interventions.png\" alt=\"\" /></p>\n<p>The first one improves the quality of that life to 90           percent, the second one improves the length of the life to           70 years.  Then what you do to compare which one is the           greater benefit is you just look at the area of them.  So,           it's just taking the integral.  This, you'll be perhaps           happy to know, is the standard approach used in public           health that's reasonably quantitative.  They allow us to           compare apples and oranges within health, which is the           key benefit of using this approach.</p>\n<p>It's important to say they're only a rough measure, so, you           might've been thinking, \"how you'd measure this quantity of           life\"? You can measure when someone is born and you can           measure which day they die, but I don't really understand           how you could measure the quality of the life.  It seems           very hard.</p>\n<p>There are surveys which are designed to elicit responses to           this.  For example, you ask questions such as \"if you could           either live for ten years in this health stage (perhaps           being blind) or live seven years in full health, which would           you prefer?\"  And then you start moving those numbers around           and you keep asking the question; you can use that to elicit           a quality.  That's called a time-trade-off.</p>\n<p>There's other ways as well where you imagine there is a           chance you'll die.  You can either be perfectly healthy with           a chance to die or you can have this health state, how much           chance to die would you prepare to put up with to avoid that           health state?  So, there are different ways like that and           they have slightly different methodologies and so on, and           they elicit answers to these questions.</p>\n<p>But even then, they're probably still often going to be out           by up to a factor of two.  You might think that's a pretty           bad metric; if I had like a yard ruler which is so           inaccurate that it often got off by a factor of two, you           might think that was a very bad measure, but it turns out           that in the universe of interventions that we're considering           a factor of two is enough to make quite a lot of progress.           And I think it would often be less than a factor of two, but           I just wanted to flag that.  It is an inherently rough           measure.  And what, in some ways, is just surprising is that           with such a rough measure, you can make so much progress.</p>\n<p>So the first question here is, what is the QALY worth?  A           quality-adjusted life year, you know, what's one of them           worth?  Well, here is something.  In Britain we have the           National Health Service and they're prepared to spend 20,000           pounds for a quality-adjusted life year.  If a new drug           comes on the market, which costs less than 20,000 pounds           ($30,000) for each QALY they will fund it.  If it costs more           than 30,000 pounds ($45,000), they won't fund it.  If it's           in between, they'll look at extra considerations, but they           have an approach like this.  There are 8,760 hours in a           year, so this comes to about two pounds thirty ($3.50) for           an hour of healthy life.</p>\n<p>I don't know what you think an hour of healthy life is worth           for yourself, but for me, I would definitely think that           this is a pretty good deal.  If someone came into this room           and was selling hours of healthy life and was reputable,           trustworthy distributor of quality-adjusted health hours, I           would be using all the money that I have available, buy           them, and also get some more money from the bank and say,           well--I think that's a pretty good deal.  We often go to the           cinema, for example, and much more than three dollars fifty           to upgrade a normal, perfectly healthy hour of our life into           an hour where you're also at the cinema.  It seems this           a lot less good than creating in time new hours of your life           in which you could do other activities and so on, so I           would think actually we should be out willing to spend quite           a lot more than that after per hour of our life.  So maybe           the NHS should try to get more funding so that they can           increase that limit.  If actually all the individuals in the           society would be prepared to pay more out of their own           pocket for it, then maybe the government should as well,           that's separate question.  This is trying to get an idea of           what you think it's worth.</p>\n<p>Another question is, would you spend 20,000 pounds to get a           whole year?  That's a bit harder because it's very large           lump of life.  You know it's not this fine grained.  I think           if you're just prepared to spend two pounds thirty for an           hour then you should be probably prepared to spend 20,000           pounds per year.  It's just like buying a lot of hours.  But           maybe you don't have that amount of money in the bank so           it's hard to thinking those terms.  But here's another way           to think about it, is would you prefer to have a salary of           20,000 pounds per annum or a salary of 30,000 pounds per           annum but be blind, one of those for the rest of your life?           Or you could bump those numbers up, you could say a hundred           and twenty thousand pounds versus a hundred and thirty           pounds but be blind, or something like that.  And for me, I           would definitely think that this money is just not worth           that much to me compared to my health and I would           definitely choose the one in the left-hand side unless it           got really, really low.  So I think that I would also be           prepared to spend 20,000 pounds per year.  So let's move on           then.  So that's what it's worth.  Right?  It's worth at           least that amount of money to us.</p>\n<p>And what does it cost?  Which is a different question.           Often things cost less than they're worth, which is why we           buy them.  If they cost more than they're worth, you should           not buy them.  So spending 20,000 pounds in a QALY is a           pretty good deal.  Can we get one to less than that?  Well,           let's look at a question of preventing or curing HIV/AIDS.           HIV being the virus, AIDS being the syndrome which you'd get           if your viral load gets high enough in this kind of late           stage where you're immunocompromised.  We all now it's a           major cause of death and disability in developing countries.           And there are many different approaches to dealing with it.           And so let's look at a few at the meantime.  So here's a           comparison between two different health interventions:</p>\n<p><img src=\"http://www.jefftk.com/ord/two-health-interventions.png\" alt=\"\" /></p>\n<p>The first of treatment Kaposi's sarcoma which is a           particular AIDS defining illness, it's a skin condition,           which you only get if you're really immunocompromised.           So it's one of the ways that people decide whether to say           that HIV has progressed to be AIDS.  And the second one is           Antiretroviral Therapy which is paying for drugs which fight           the viral load and help prevent HIV turning into AIDS.  As           you can see, the second one is much more effective than the           first one.  This is a longer bars are better type           graph, and they're better in proportion to the length.</p>\n<p>What we have measured in here is how many quality-adjusted           life years per a thousand pounds compared to one and a half           thousand dollars.  And what I've drawn on there is a line           which is the level it would need to be, it needs to be at           least as long as that to be as effective as we thought, you           know, the NHS use, which was just a yardstick that we had           beforehand.  And the NHS actually fund treatment of Kaposi's           sarcoma and it's funded in the US as well.  It's considered           to be kind of on the line but to be effective enough in rich           world's context.  But you can say there's a very large           discrepancy there in how much benefit you can produce for a           given amount of donation but it's even more interesting if           we zoom out and then consider another intervention,           prevention of transmission during pregnancies:</p>\n<p><img src=\"http://www.jefftk.com/ord/three-health-interventions.png\" alt=\"\" /></p>\n<p>So this is giving drugs to the mother--well to the would-be           mother during the stages of pregnancy so that the new child           doesn't get HIV.  And that's a very targeted intervention           because you can target that pregnant women who have HIV and           so that reduces the cost and avoids an entire lifetime of           HIV which is really good.  And so the effectiveness of this           is \"quite a lot more effective\".</p>\n<p>I should say that these figures come from a report called <a href=\"http://www.dcp2.org/main/Home.html\">Disease Control           Priorities, second edition</a> which is a big aggregation of           the whole lot of the cost-effectiveness figures.  There is a           literature on this stuff where people try to estimate how           cost-effective these different things are.  They do           different types of trials including randomized control           trials.  And some of it they have less information.  They           have to do more modeling and it becomes a bit more tentative           but this is the estimate for this.  But we can zoom out           again and see more interventions:</p>\n<p><img src=\"http://www.jefftk.com/ord/four-health-interventions.png\" alt=\"\" /></p>\n<p>We can compare it to distribution of condoms is a lot more           effective again.  As it's very cheap, you distribute them           and you can avoid a lot of cases of HIV.  And you can only           just see this NHS cost-effectiveness threshold.  Actually,           we've moved it a little bit so you can actually see it.  It           would just be invisible at this point.  And these things are           just extremely cost-effective.  And if we zoom out one more           time, we can look at something&mdash;this case is not an           AIDS-related intervention but distribution of bed nets to           prevent malaria&mdash;that is even more cost-effective again:</p>\n<p><img src=\"http://www.jefftk.com/ord/five-health-interventions.png\" alt=\"\" /></p>\n<p>At this point if we hadn't slightly enlarged it you wouldn't           be able to see the bar for treatment of Kaposi's sarcoma.           This new option is about a thousand times more effective.           So you can do a thousand times more health gain for a given           size of donation.  To look at the actual number, you can see           it's nearing 60 years of healthy life for every thousand           pounds, which is very effective.</p>\n<p>Let's just pause for a moment and say, you know, what can we           learn from this?  One thing is that health programs in           developing countries can be amazingly cost-effective, about           a thousand times more so than we're willing to spend in our           own countries on health.  And about a thousand times more           effective than it's needed to be a good deal according to           what we judged earlier when we said how much health do you           need to get for a thousand pounds to be good deal and so on.           And another thing is that our money is worth much more than           we might have thought.  Five pounds ($7) to thirty pounds           ($45): that's the kind of estimate range for that most           effective one; it's enough to buy a year of healthy life.           So, it's quite interesting because you might not have           thought that you're carrying around enough money in your           pocket today is like a whole year of healthy life for           someone else.</p>\n<p>But you can only get these types of benefits with your money           if it's used on the best programs and also, only if it's           used on other people.  So your money is worth a lot more           than what you might thought if it's used for others.  What's           the catch?  Otherwise, it would be great, because I'd be           telling you how to get a thousand times more value for your           money, for yourselves which would be a, you know, a roaring           success of talk.  But the point of the talk is to how get           a thousand times much value for your money for other people           which is a bit less of a roaring success.  But I hope           it's still at least interesting.</p>\n<p>These things I talked about are mainly focused on by policy           makers and [the DCP2] was aimed by policy makers, people who           are administers of health in poor countries and also people           who run aid programs and so forth.  But they're also of           interest to all of us when it comes to charity and trying to           work out how we can help people most with the money that we           decide to give to charity.  It maybe would also make us           think more about how much we should be giving once we           realize it can be so effective.  So, when we try to donate           money, we're trying to help people and to help them as much           as we can.  If we had some option, and said look at two           programs and the second program helps people less, you know,           we wouldn't do that, right?  It seems like we are trying to           help people a lot.</p>\n<p>Suppose we wanted to donate money to help fight AIDS, it           really matters actually, whether that money is used to fund           treatment of Kaposi's sarcoma or to fund condom distribution           for example.  It's a huge difference in effectiveness           between these different options.  And it's not actually           enough to just know that the organization is trying to fight           AIDS.  You'd actually want to know more because some of the           ways of doing that are, you know, not as effective as other           ways.</p>\n<p>You might think that actually the organizations would           be the experts on this and would know and would be the most           effective one.  But often they're not, and one of the           reasons for that is because they don't do as much program           evaluation as we would like because they're trying to keep           the overheads ratio really good, so that people will donate           to them who've been kind of sucked in by the idea of the           overhead ratio, which is a sad fact.  But also, they're just           generally aren't aware of these figures.  There's been not           enough movement to really make people aware of these figures           and to be aware of how good these things can be.</p>\n<p>Often if you talk to someone, who's say, funding a less           effective treatment, I'll say that they're a efficient           organization because there's no way that they could fund           that treatment more effectively than what they are doing.           And--but that's a different question too.  Hang on.  If           we're really actually just trying to fight AIDS, maybe they           are other ways to fight AIDS which would be more effective           for those people.  We don't have to always use the same           intervention.  And so it is relatively rare that people           think of stepping to a different intervention.  It's even           rarer that they think of stepping to a different course,           maybe they would think what we're really trying to do is           make people healthier and avoid--let's say premature death           or disability.  In which case, maybe we should be focusing           on malaria instead of AIDS and close down our AIDS, you           know, start up malaria net distribution charity.  I've never           heard of any charity doing that.</p>\n<p>There's a kind of mindset you can get into and if you're not           familiar with these numbers, you think, well, this is where           our expertise is and so on.  We must be really good of doing           of this.  But it turns out, some of the things if you are           expert at them and 20% better at doing them than anyone           else, there's something else you could be doing but if           you're average at, it would be ten times more effective.           You're a thousand percent better.  Which I think is really           interesting.  It's kind of the main point of this talk.</p>\n<p>So here's a few more summaries about, you know, what does           this all mean.  I think one headline figure is that, where           to give can be even more important than whether to give.           So, suppose you had three options.  One is that you don't           donate anything.  The second option is that you donate           enough to save a hundred quality-adjusted life years by           funding HIV transmission prevention which we saw was one of           the middle level effectiveness.  Or alternatively, you save           seven hundred quality-adjusted life years by funding malaria           net distribution.  Well, it's interesting that the           difference in absolute terms between two and three is much           bigger than the difference between one and two.  That's what           I mean by saying \"where to give\".  That second question can be           even more important than the first question about whether to           give at all.</p>\n<p>Also, we can see that we can do pretty amazing things that           we might not have thought of.  So, well, you might have           thought that in order to really help people, you have to go           another country, away from your friends and family, do a           different career not the thing that you're passionate about           and that you wouldn't able to live the kind of affluent           lifestyle that you are used to.  But it's interesting to see           that if you are to stay in your home country, do whatever           job you wanted to do with your friends and family and donate           10% of your future income which would still leave you very           affluent by world standards.  That you could save at least           3,000 quality-adjusted life years during your career which           is three millennia of life at perfect health or thirty           millennia of life, you know, improving the health by 10% or           something, you know, equivalent.  I think that's pretty           interesting that these options are there for all of us if we           want to take them.  It's not just a radical choice with huge           amount of sacrifice.  There are options with very little           sacrifice which would actually help a lot.</p>\n<p>So, you can consider this question, how much can we achieve           if we really wanted to?  I mean, I consider these options,           spend the one and a half million pounds on myself, have my           great kind of grad-student-plus life with also some extra,           you know, a better car or something and bigger house and           whatever.  Or I could have my grad-student-plus life, you           know, and also donate a million pounds to others.  And my           choice would be the second thing because getting extra           \"plus\" there, just wouldn't actually improve my life much.           So, I chose the second path and I still have a great quality           of life and I can save at least 30,000 QALYs which is           equivalent to saving 300 centuries of perfectly healthy           life, which is is pretty cool, right?  You know.           It's a pretty exciting thing to be able to do I think.</p>\n<p>So I wanted to stop for a moment because you might be           thinking, how can this be right?  These numbers must be           wrong.  How can people in, you know, our shoes do that much           to help people.  And I think that this helps to make it           clear and this graph is the world income distribution.  And           I think of it is the most important chart in economics           because it's the best kind of summary of the state of the           world today:</p>\n<p><img src=\"http://www.jefftk.com/world-income-distribution.png\" alt=\"\" /></p>\n<p>What we have here is slightly complicated.  On the Y axis is           income per household member.  So that's a measure which if           you look at how big the household is, you look at the           household income, you divide by the number of people in the           household.  So that children count on this as well.  And a           child in a poor family counts different to a child in a rich           family.  Of course, if you're using a more naive measure,           that doesn't work properly because it turns out they've got           no income because they don't work a job and so it doesn't           really come out right.</p>\n<p>So it is household income per member           and it's measured in US dollars adjusted for <a href=\"http://en.wikipedia.org/wiki/Purchasing_power_parity\">purchasing           power parity</a>.  PPP means that you take into account           the fact that money goes further in poorer in countries.           For example, it goes about four times further in India than           it does in Main Street, USA.  So, what they do is, they           effectively, you know, multiply someone's actual income in           India by four in this graph.  If you didn't do that, this           effect would be even more extreme.  So, but they are taking           that into account already.  And then what we've done is           we've lined everyone in the world of up, all seven billion           people from the poorest to the richest and we've looked at           how much income they have.</p>\n<p>You can see that there's this extreme spike at the           right-hand side and if you're looking at this on a large           projection screen, then on that screen, which is only itself           about a yard and a half tall, on that scale, you can see           that there is a ellipses there that I haven't been able to           fit it the entirety of this curve.  This is something where           if you had fit in this entire curve, it would be taller than           Mt. Everest at this scale.  It's a pretty extreme skewed           distribution. I think it's a fascinating chart for people           who are into that kind of stuff.</p>\n<p>What's particularly interesting about it, is if you actually           look at it, you might do what I did, which is to look at           this and think, oh, yeah, I'm somewhere in that bit where it           curves around.  You know, I'm rich but I'm not like really           wealthy.  Okay.  And I won't be.  You know, there's these           annoying people who earn this millions of dollars per annum           and isn't that unfair?  And I'm quite wealthy compared to           these other people.  But then if you actually look at these           numbers, you realize that at the moment, particularly if           you're a young person without children or later on in your           life when children leave home, you're going to be probably           pretty wealthy on this and probably a lot of people watching           this are in the richest one percent of the world's           population or will soon be in the richest one percent of the           world's population.  So, they're in this area here.  This is           the richest two percent.</p>\n<p>There's been a lot of interest recently in America, Britain,           and other rich countries about the 1% and the 99% on a           national scale.  But it's interesting that in the global           scale, we are the 1% and we often don't pay enough attention           to that.  But that's what's going on here.  That's why           the stuff that's in your pocket can buy years of healthy           life, is because the stuff that's in your pocket is worth a           crazy amount of money because we're all really rich.  It's           just that we tend to compare ourselves to our peers and so           we don't notice that as much.</p>\n<p>I'll just talk a little bit, before questions, about the           organization that I've set up, which tries to take these           ideas and put them into practice, and it's called <a href=\"http://www.givingwhatwecan.org/\">Giving What We           Can</a>.  It's a community of people who feel strongly about           improving the lives of those people in developing countries.           And it's a group where each member of the group has made a           pledge to donate at least 10% of their income to wherever it           is that they think can do the most to help to people in poor           countries.</p>\n<p>We also run a website that collects and shares information           on the most cost-effective charities to help out members and           to also help the general public.  So even if one thinks 10%,           maybe that's it a bit much for me, but I'd really love to           find out more about this, how do I do more effective giving,           which charities are they that actually implement some of           these most effective interventions, and--that have done           extra diligence on and so forth.  Well, you know, you can go           to our website and check that out.</p>\n<p>There's two aims that we have in this organization and they           fit in with what I've been talking about in this talk.  So           the first aim is getting people to give more.  So this           square represents something like what people were giving           previously, on average in the US:</p>\n<p><img src=\"http://www.jefftk.com/ord/current-giving.png\" alt=\"\" /></p>\nFirstly, I want to try to convince people that they could           give a lot more than they do.  It's actually not that much           of a sacrifice because most of the things that are really           important in our lives are actually very cheap.  People say           this all the time, turns out they're right.  It's true.  And           you can get a lot of the value of your life on, you know,           not much of the money of it:\n<p><img src=\"http://www.jefftk.com/ord/more-given.png\" alt=\"\" /></p>\nSo getting people to give 10% in this case.  And the second           aim is getting people to give to more effective           organizations, which works out something like this:\n<p><img src=\"http://www.jefftk.com/ord/more-given-more-effective.png\" alt=\"\" /></p>\nSo together, we massively increase the total impact.  And           it's very plausible that anyone who's hearing this talk           could actually improve the impact they're going to have           through charitable donations over their life by a factor of           a hundred.  Quite possibly by more than that, actually.  But           because you've got these two different dimensions and it's           not additive, but it's multiplicative, it leads to a really           big benefit if you're doing both of these things right.\n<p>Now, we have 292 members, I think, or maybe slightly more           now, who've pledged to donate at least at 10% of their           income.  This comes to more than a hundred millions dollars           of income that's pledged over their lives, which was enough           to fund between two and eleven million quality adjusted life           year.  To put that into some kind of context, 5 million           years is the period of time since humans and chimpanzees           diverged evolutionarily.  Or 200,000 years is the amount of           time Homo sapiens has been around.  So this is quite like a           large amount of life.  And we've got local chapters to try           to discuss these ideas and get people interested in various           places including Oxford, Cambridge, London, Princeton,           Harvard, Rutgers, San Diego, Canberra.  Some of them are           universities, some of them are towns.  And we try to get           people involved and talking about these ideas.</p>\n<p>To conclude, we're exceptionally wealthy living in rich           countries in comparison to people in the poor countries, and           our money can do hundreds of times more for them than it can           for us.  In fact, I gave a pretty good argument, I think, it           could do a thousand times as much good.  Because we think           that if we spent the money on some of these health           interventions where I had that barrier, that that would be           better than how we spend our money.  And then we realize           that that would produce an amount of health which is 1,000           as much as we could produce for other people.  So it's a           good pretty argument that therefore we can do at least a           thousand times as much to help other people.  But maybe you           think some of these numbers are a bit uncertain and let's,           you know, be safe and say just a hundred times as much,           okay?</p>\n<p>However, it can only achieve this if it's given wisely.  So           we really want to spend a lot of time thinking about where           to give or at least just spend a very small amount of time           checking website of people who do spend a large amount of           time spending probably more than hundreds of hours, maybe           thousands of hours now thinking about where that is and           looking at the research on this and trying to work out the           best approaches.  This idea, that where to give is even more           important than whether to give.</p>\n<p>Finally, the world is an unfair place with a great deal of           suffering, as I'm sure you all knew before you came into the           room.  But we can each do a tremendous amount to help make           it better and let's do so.  Thank you.</p>\n<p><em>Question:</em> your example of blindness in particular,           got me thinking about all these cognitive biases that people           have and how they rigidly overestimate what they think is           only an impact of disabilities in their life and now we're           much more on a <a href=\"http://en.wikipedia.org/wiki/Hedonic_treadmill\">hedonic           treadmill</a> and we adapt really quickly after even losing           a limb or becoming blind or something like that.  How do you           think we should adjust these, you know, quality figures for           that, sort of, cognitive bias?</p>\n<p><em>Ord:</em> That's a great question, actually.  People in           public health, including here at Harvard, are looking at           these types of questions and it turns out if you elicit           these quality ratings by a group of healthy           people&mdash;let's say in the case of blindness&mdash;they           say that a year of blindness is at about half quality.  It's           like they'd be indifferent between two years of blind life           versus one year perfectly healthy life.  However, if you ask           people who've been blind&mdash;particularly not just in the first           six months or so, the first six months or so probably is           really bad, adjusting to that&mdash;they tend to say           it's less bad than that.  Although interestingly, it turns           out that if they regain sight, they then say, it was           actually worse than they were saying when they were blind.</p>\n<p>So there's an extra effect to this cognitive biases thing.           That there is this extra aspect where it partly depends on           the state they're in.  And I think we often like to think           the state we're in is particularly good compared to other           states, so we can feel happy about our lives.  But it's           difficult to know exactly how to take that as your account           because it seems to keep fluctuating.  But I would think           that maybe just taking the average of a group of sighted           people and a group of blind people would at least be a good           first step on that.  If you did that then maybe, you know,           it would say that instead of being half as good and having           quality weight of .5, the quality weight should actually be           .6 or .7 or something.  That won't change the numbers that           we're using very much though.  But it is the type of thing           that if we could more fine grained than this and tried to           work out some more nitty gritty questions, that it'd be an           improvement of the accuracy of an estimate by 20% is           certainly worth having.</p>\n<p><em>Question:</em> I have two questions to this, one is could           you address the next hurdle that people have to giving.  So,           the first hurdle is, will it make a difference, and this           talk addresses that.  The next question is, is this the           difference that I want to make?  Let's pretend that the most           effective way was to give food to people who are starving in           some location.  And you could feed them, and let's say this           was the cheapest thing that could be done and, you know, it           would give the most quality adjusted life years.  But then           the thing is that some people might feel that doing this           just means that there's going to be extra millions of people           who are going to starve the next year.  So many people won't           give because this maybe the most effective way is to, you           know, prevent people from dying by giving them mosquito nets           and so on but it's just a bigger problem for another day, so           that's my first question.</p>\n<p>The second question is, if we really take this argument all           the way to its extreme, let's just say the US government           within the US borders, then we don't have to deal with the           issue of helping and other countries.  But everything that's           done could be a, you know, in some sense, put on this           utilitarian scale, if I build a new bridge, we can talk           about how that helps people.  Anyways, I mean, it's actually           is not a good example but there are other kind of things           like this where you can say that, well, there's other things           that we do that improve people lives.  And if we just           determine that there are two things, let's say, that are the           most effective at improving people lives, why doesn't           government do only those two things and stop doing almost           anything else that's not necessary, right?  So, like, cut           all the welfare programs, cut everything else, cut anything,           you know, for special needs children whatever because this           is the one, you know, these two programs that we've           determine are the only ones that are, you know, going to           give us the most effective results.  So could you address           those two concerns?</p>\n<p><em>Ord:</em> They're both big questions.  I'll take the           second one first.  So one thing to say is the government in           the US actually does do this but they use a different           methodology.  The one I'm talking about is called           cost-effectiveness analysis, the one the US government uses           is called cost benefit analysis.  And that's an approach           where you measure the benefits in terms of dollars and you           measure the cost in terms of dollars.  They try kind of pull           together benefits based on how much people will be willing           to pay in order to get those benefits.  So they do think           about this in the US and measure things like this.  Although           that methodology, I think has quite a lot more flaws in it.           It's got a lot of more room for ethical problems to arise           than from the one I'm talking about.  But anyway, the thing           is that if you really did want to help people as much as           possible then going with a sophisticated version of this           approach is the way to do that.  It's just defined to be the           way that helps people as much as possible.  Therefore, I           think that's a pretty good idea to go with.</p>\n<p>As in your particular case, it will turn out that if there           are just a couple of interventions that they will end up           with diminishing marginal returns.  So what we've tried to           get here is estimates as to how effective it is the margin.           So if you're going to save one more life from malaria and           with the current amount of malaria's mosquito net           distribution, how much would that cost?  But if you roll           that out to say 80% of the population then you're trying to           get the last 20% covered, then that can be much more costly.</p>\n<p>So sometimes there are issues where that will change and so           what you want to think of is, for each of these           interventions, you want to think of its cost curve, would be           the most sophisticated way to do it.  But also even then,           you'll saturate these things.  So for example mosquito net           distribution, there are--the number of people dying from           malaria per year is uncertain.  It's somewhere between about           700,000 per annum and 2 million per annum.  Let's say its           one and a half million per annum and the amount it costs to           save a life due to malaria on average by distributing nets           is about $3,300.  So, and now I've set up a multiplication           problem for me.  But there's something around about $3           billion worth of money that would be needed to totally cover           that which is actually very small compared, to say, the US           budget.</p>\n<p>And so with some cases like this that there are very           effective it turns out that you can actually completely           saturate them and it won't be the case that there's just two           things that end up getting done.  The really effective           things will get done and then they'll get to move on to the           next most effective and the next most effective and they'll           get quite far down the list.  So, it's pretty valuable.  In           fact, in terms of these interventions I've got a <a href=\"http://www.jefftk.com/daly-per-1000-usd.png\">nice chart which I didn't           present</a> but if you look at all of the interventions that           the DCP2 looked at, and you do a chart of all of them, you           can find that their log normally distributed that the most           effective 20%, if you were to fund them all equally, 80% of           the benefits would come for the most effective 20%.  So, if           you do work your way down from the top you actually cover a           huge amount of benefit.</p>\n<p>The first question was about additional issues.  I           definitely think that QALY's and the cost-effective           analysis at this kind of level is not the be all and end all           about this.  I'm trying to just get these ideas onto the           table in terms of how you can think about this with a kind           of simple examples and obviously you could have, you know,           more and more hours making it more sophisticated and           trying to get better data and so on.  But in the particular           case of overpopulation, that's another thing that you'd want           to think about as to the effects of that.  Ideally, you want           to take that into account.</p>\n<p>Now, I can say that it turns out that there's a couple of           big points you can make there.  One of them is that it turns           out that saving infant lives probably actually helps with           overpopulation.  It actually reduces the expected number of           people on the planet because it turns out that there's a           process called the demographic transition where once people           get wealthier, they and their children start surviving           better and have fewer children.  And it's more than           proportional, so it compensates and this is what has           happened in rich countries and it seems to be happening in           some of the poor countries.  So, it looks like if you get           actually child survival to be improved, it actually would           fight overpopulation rather than exacerbate it.</p>\n<p>The other thing I can say is even if you didn't believe that           you could try to find some organizations and we recommend a           couple on our website which don't save lives but just           improve quality of life.  So, that would be a way to do it           that where that other argument wouldn't come up.</p>\n<p>Then a third thing to say is that if you really thought that           overpopulation was so bad that you're willing to just the           let a whole lot of people die in order to decrease the           overpopulation, the badness of helping was going to be           bigger than the goodness of helping those people, then you           must think that overpopulation is really, really bad.  It's           so bad that that even if we could spend, you know, 15 Pounds           to save a year of life, that the overpopulation effect is           even bigger, in which case you should just donate money to           fight overpopulation.  That would be the logical conclusion.           The conclusion wouldn't be, there's this absolutely terrible           thing that's so horrific that it overwhelms these amazingly           big benefits therefore I won't do anything.  But therefore           I'll donate money to a group that, let's say, distributes           condoms or, you know, educates people about family planning           and so on, there's plenty of those groups as well.  So, I           think that style of response ends up, you know, not quite           blocking the pulse of this argument.  I hope I've answered           your questions.</p>\n<p><em>Follow-up question:</em> Thank you.  I think that I'm in           the same way that a year of life extended can be adjusted to           be a quality adjusted of a year of life extended.  I'd like           to see this analysis, I'd be really, really happy to see if           this analysis could be further extended one more thing about           whether it is also generating or getting rid of a problem.           In another words one more adjustment that would be more           convincing to people I think then that yes, if I donate to           this I'm not only giving extra quality adjusted life years,           but I'm also diminishing and not increasing the problem for           tomorrow.</p>\n<p><em>Ord:</em> I agree.  There's definitely potential           additional considerations before taking it into account.           I'm totally on board with that.</p>\n<p><em>Question:</em> You showed how giving effectively is much,           much better than giving ineffectively.  Have you considered           the other ways of giving, volunteering, personal           relationships, et cetera, work was the other one.  Maybe           they can be even more effective for some of us than giving           money.</p>\n<p><em>Ord:</em> That's a great question.  And it's plausible.  I           think personal relationships is one that occurs to a lot of           people when they're thinking about this is.  Like, you know,           how can I be a good person in terms of my family and           friends?  But it seems that once you get to these scales,           it's hard to see that really the best things that you can           achieve overall in some kind of neutral, impartial sense are           going to be through that.</p>\n<p>But the other areas of volunteering is potentially very           valuable, as is work, depending on what you do.  So for           example it could be the case that&mdash;I mean I have, in my           spare time, set up this organization which has done a lot of           good.  And so probably I've done more good through my           volunteering than I will achieve actually through all of my           donations over my life, which is pretty cool.  And similarly           if you can find some really high impact volunteering           approaches then you could do a lot of good.</p>\n<p>It is definitely harder through volunteering or like of           things in rich countries to have as much impact as we've           seen.  Often in fact one could have even more impact if one           worked a few extra hours and then donated the money.  That           might seems surprising but if you remember that, using           economics, that's we have a comparative advantage in some           kind of field and that's the field where we get the most           money.  And so if go into that field and earn money and then           it's very effective.  Because then we can pay someone who           does the thing that were interested in.  So we get, for           example, an expert in computing to, instead of donating           their computing services to a poor person, to use that to           get money to pay the person, for personal needs and to pay           someone who is actually an expert in those things.  As           opposed to someone who's more of a dilettante.  It does           depend, but volunteering could be potentially even more           important if you get really high leverage on it.</p>\n<p>Work could be really important as well.  It's hard to know           in terms of improving technology.  Obviously there's been a           lot of technologies which have been very useful for poor           countries.  For example allowing them to leapfrog landlines           and to go to cell phones in order to get connected.  I think           a lot of people would have thought it would take a lot           longer before that would be useful.  So there are           potentially a lot of approaches there.  And also maybe doing           your job has various other effects.  So obviously it depends           on what it is, but I don't know exactly how to asses these           things.  They're quite a lot harder to asses.  So I'm not           claiming that they're less valuable really.  So much           as, there's at least quite a high bar to beat.  But maybe           they can be even more valuable than that.  And I think that           people should think about those things quite seriously.</p>\n<p><em>Question:</em> It seems that core to the high quality cost           effectiveness is the fact that the donated dollars are used           in developing countries.  What are the implications for           those who want to give locally for example in developed           countries?</p>\n<p><em>Ord:</em> That's a good question.  So there are various           implications.  So one implication is that you might think           within your local area whether that's your country or your           community, that you also want to take these ideas in light           and to try to give effectively within those areas.  I don't           have particularly good data on that, but you'd probably want           to start thinking about it a bit because once you see           that there are several orders of magnitude separating the           best and worst interventions in poor country settings, you           might think that there actually could be quite a lot of           discrepancy between the interventions in rich country           settings.  And in fact there are in term of health at least.           Similar kind of curve for health interventions in rich           countries.</p>\n<p>So that's something where maybe also in other forms of           volunteering or donation you could see that happening.  And           you'd start to see this lesson, that choosing where to give,           it could be worth maybe spending half the time           you'd spent earning to donate the money, just doing research           on this problem, and then donating half the money to           something that's more than twice as effective.  That's quite           plausible.  So at least to spend, say 10% of the time or           whatever on the question about where.</p>\n<p>Another question though, I think would be to really think           about why it is that you want to help locally instead of           globally.  I've had a nice advantage on this which is that I           was born in Australia.  And so my local thing would have           been to help Australians, and wouldn't care about helping           British people over, say Kenyan people because I didn't know           any British people or Kenyan people.  And so, it seemed           fairly obvious that if I could help more people in Kenya           than I could in Britain that that would be the better thing           to do.  Otherwise I'd be kind of unduly privileging British           people for some reason.</p>\n<p>Now, that I moved to Britain I still kind of am in the           situation and I also kind of feel, why would I help           Australians rather than helping more people somewhere else           or helping them in better ways.  So it's helped to kind of           break some of that partiality for me in terms of this type           of donation.  But I think that I can kind of help the world           more in these ways.  And so one would also want to think           that it's very difficult to help in such a extreme ways and           to spend some time thinking about the ratios that you would           end up with.  Is it the case that for the most effective way           you can help people locally, you could help a hundred times           as many people globally?  If you, then you might think, you           know, can I really justify a hundred to one ratio in terms           of that.  Maybe if I had a chocolate cake I would give to my           friend rather than to a stranger, other things being equal           or something.  But if there was a case I had to stop a           hundred strangers having their chocolate cakes in order to           give one to my friend, maybe I wouldn't do that.  So it           could be interesting thinking quantitatively about some of           those things as well.  But there's a lot of people who think           that they're not swayed by these reasons and so on, if so           that's fine.  I'm just trying to give arguments.</p>\n<p><em>Question:</em> What is your opinion of <a href=\"http://www.givewell.org/\">GiveWell</a>'s           recommendations?  Why don't you just outsource all of your           donations to them, because they seem to be looking for very           similar things?</p>\n<p><em>Ord:</em> They're great.  So for those people who aren't           aware, GiveWell is a charity evaluator, formerly operating           from New York, I think now from San Francisco.  And they are           doing this kind of thing, trying to look at different           charities and find where you can help people the most for           your donations.  I think that they're great and have done           good research on this.  When they started up, they managed           to launch a couple of years before Giving What We Can as I           had to finish my PhD, but we kind of were thinking about           these things in a similar time and were in contact with each           other.</p>\n<p>We're both really frustrated about the fact that there's a           lot of really good research being done that's not being           shared with the public.  And so foundations, say the Gates           Foundation, I'm sure they do a lot of realty fantastic           research on where to give, and they use up those           opportunities as well and they don't kind of share this           information more broadly.  And I think that it was great to           share it.  And that was GiveWell's approach to share that           really publicly.  I think that's they're real big advantage.</p>\n<p>And so we at Give What We Can, take their advice very           seriously.  It's one of the several inputs that we take.  We           also talk to people at the World Health Organization, the           World Bank, Oxfam, lots of places.  And try to find out what           going on and what we think about different interventions.           And some of those we come up with different conclusions to           GiveWell, but of all those kind of inputs to our decision           making process, they're probably the largest and we take           them very seriously.  So definitely do search for GiveWell           as well and have a look of what they say.</p>\n<p>Thank you.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhfefamXXee6c2CH8": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iCa9JXAXAuCzuv3DT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "22260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-04-10T03:52:56.494Z", "modifiedAt": null, "url": null, "title": "Meetup Interest Probe: Urbana-Champaign, Illinois", "slug": "meetup-interest-probe-urbana-champaign-illinois", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.714Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7mWzXsBkCdrfRMF9e/meetup-interest-probe-urbana-champaign-illinois", "pageUrlRelative": "/posts/7mWzXsBkCdrfRMF9e/meetup-interest-probe-urbana-champaign-illinois", "linkUrl": "https://www.lesswrong.com/posts/7mWzXsBkCdrfRMF9e/meetup-interest-probe-urbana-champaign-illinois", "postedAtFormatted": "Wednesday, April 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Interest%20Probe%3A%20Urbana-Champaign%2C%20Illinois&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Interest%20Probe%3A%20Urbana-Champaign%2C%20Illinois%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mWzXsBkCdrfRMF9e%2Fmeetup-interest-probe-urbana-champaign-illinois%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Interest%20Probe%3A%20Urbana-Champaign%2C%20Illinois%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mWzXsBkCdrfRMF9e%2Fmeetup-interest-probe-urbana-champaign-illinois", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mWzXsBkCdrfRMF9e%2Fmeetup-interest-probe-urbana-champaign-illinois", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>I'm going to be starting graduate school in Urbana-Champaign this fall, and want to know if there are LessWrongers there (or like me, going to be there) who want to hold some meetups. Google&nbsp;shows me that someone else tried <a href=\"/lw/d2l/interest_in_meetups_starting_this_fall_at_the/\">asking</a>&nbsp;the same thing last year and met with some success, but <a href=\"/lw/ekx/meetup_urbanachampaign_il/\">after</a>&nbsp;<a href=\"/meetups/eg\">three</a>&nbsp;<a href=\"/lw/fni/meetup_weekly_meetup_champaign_il_cafe_paradiso/\">meetups</a>, they went extinct.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7mWzXsBkCdrfRMF9e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1641129052830468e-06, "legacy": true, "legacyId": "22262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wg8G5NKbwmiZdM646", "MK7z3qcTL96sDgdXE", "vtHyY662iyARzyxYz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}